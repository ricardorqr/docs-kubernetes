<div><div class="td-content"><p>The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your <a class="glossary-tooltip" title="A set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node." href="/docs/reference/glossary/?all=true#term-cluster" target="_blank">cluster</a>, and helps you obtain a deeper understanding of how Kubernetes works.</p><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/concepts/overview/">Overview</a></h5><p>Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</p></div><div class="entry"><h5><a href="/docs/concepts/architecture/">Cluster Architecture</a></h5><p>The architectural concepts behind Kubernetes.</p></div><div class="entry"><h5><a href="/docs/concepts/containers/">Containers</a></h5><p>Technology for packaging an application along with its runtime dependencies.</p></div><div class="entry"><h5><a href="/docs/concepts/workloads/">Workloads</a></h5><p>Understand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/">Services, Load Balancing, and Networking</a></h5><p>Concepts and resources behind networking in Kubernetes.</p></div><div class="entry"><h5><a href="/docs/concepts/storage/">Storage</a></h5><p>Ways to provide both long-term and temporary storage to Pods in your cluster.</p></div><div class="entry"><h5><a href="/docs/concepts/configuration/">Configuration</a></h5><p>Resources that Kubernetes provides for configuring Pods.</p></div><div class="entry"><h5><a href="/docs/concepts/security/">Security</a></h5><p>Concepts for keeping your cloud-native workload secure.</p></div><div class="entry"><h5><a href="/docs/concepts/policy/">Policies</a></h5><p>Manage security and best-practices with policies.</p></div><div class="entry"><h5><a href="/docs/concepts/scheduling-eviction/">Scheduling, Preemption and Eviction</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/cluster-administration/">Cluster Administration</a></h5><p>Lower-level detail relevant to creating or administering a Kubernetes cluster.</p></div><div class="entry"><h5><a href="/docs/concepts/windows/">Windows in Kubernetes</a></h5><p>Kubernetes supports nodes that run Microsoft Windows.</p></div><div class="entry"><h5><a href="/docs/concepts/extend-kubernetes/">Extending Kubernetes</a></h5><p>Different ways to change the behavior of your Kubernetes cluster.</p></div></div></div></div><div><div class="td-content"><h1>Overview</h1><div class="lead">Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</div><p>This page is an overview of Kubernetes.</p><p>The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation
results from counting the eight letters between the "K" and the "s". Google open-sourced the
Kubernetes project in 2014. Kubernetes combines
<a href="/blog/2015/04/borg-predecessor-to-kubernetes/">over 15 years of Google's experience</a> running
production workloads at scale with best-of-breed ideas and practices from the community.</p><h2 id="why-you-need-kubernetes-and-what-can-it-do">Why you need Kubernetes and what it can do</h2><p>Containers are a good way to bundle and run your applications. In a production
environment, you need to manage the containers that run the applications and
ensure that there is no downtime. For example, if a container goes down, another
container needs to start. Wouldn't it be easier if this behavior was handled by a system?</p><p>That's how Kubernetes comes to the rescue! Kubernetes provides you with a framework
to run distributed systems resiliently. It takes care of scaling and failover for
your application, provides deployment patterns, and more. For example: Kubernetes
can easily manage a canary deployment for your system.</p><p>Kubernetes provides you with:</p><ul><li><strong>Service discovery and load balancing</strong>
Kubernetes can expose a container using the DNS name or using their own IP address.
If traffic to a container is high, Kubernetes is able to load balance and distribute
the network traffic so that the deployment is stable.</li><li><strong>Storage orchestration</strong>
Kubernetes allows you to automatically mount a storage system of your choice, such as
local storages, public cloud providers, and more.</li><li><strong>Automated rollouts and rollbacks</strong>
You can describe the desired state for your deployed containers using Kubernetes,
and it can change the actual state to the desired state at a controlled rate.
For example, you can automate Kubernetes to create new containers for your
deployment, remove existing containers and adopt all their resources to the new container.</li><li><strong>Automatic bin packing</strong>
You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks.
You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit
containers onto your nodes to make the best use of your resources.</li><li><strong>Self-healing</strong>
Kubernetes restarts containers that fail, replaces containers, kills containers that don't
respond to your user-defined health check, and doesn't advertise them to clients until they
are ready to serve.</li><li><strong>Secret and configuration management</strong>
Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens,
and SSH keys. You can deploy and update secrets and application configuration without
rebuilding your container images, and without exposing secrets in your stack configuration.</li><li><strong>Batch execution</strong>
In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.</li><li><strong>Horizontal scaling</strong>
Scale your application up and down with a simple command, with a UI, or automatically based on CPU usage.</li><li><strong>IPv4/IPv6 dual-stack</strong>
Allocation of IPv4 and IPv6 addresses to Pods and Services</li><li><strong>Designed for extensibility</strong>
Add features to your Kubernetes cluster without changing upstream source code.</li></ul><h2 id="what-kubernetes-is-not">What Kubernetes is not</h2><p>Kubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system.
Since Kubernetes operates at the container level rather than at the hardware level,
it provides some generally applicable features common to PaaS offerings, such as
deployment, scaling, load balancing, and lets users integrate their logging, monitoring,
and alerting solutions. However, Kubernetes is not monolithic, and these default solutions
are optional and pluggable. Kubernetes provides the building blocks for building developer
platforms, but preserves user choice and flexibility where it is important.</p><p>Kubernetes:</p><ul><li>Does not limit the types of applications supported. Kubernetes aims to support an
extremely diverse variety of workloads, including stateless, stateful, and data-processing
workloads. If an application can run in a container, it should run great on Kubernetes.</li><li>Does not deploy source code and does not build your application. Continuous Integration,
Delivery, and Deployment (CI/CD) workflows are determined by organization cultures and
preferences as well as technical requirements.</li><li>Does not provide application-level services, such as middleware (for example, message buses),
data-processing frameworks (for example, Spark), databases (for example, MySQL), caches, nor
cluster storage systems (for example, Ceph) as built-in services. Such components can run on
Kubernetes, and/or can be accessed by applications running on Kubernetes through portable
mechanisms, such as the <a href="https://openservicebrokerapi.org/">Open Service Broker</a>.</li><li>Does not dictate logging, monitoring, or alerting solutions. It provides some integrations
as proof of concept, and mechanisms to collect and export metrics.</li><li>Does not provide nor mandate a configuration language/system (for example, Jsonnet). It provides
a declarative API that may be targeted by arbitrary forms of declarative specifications.</li><li>Does not provide nor adopt any comprehensive machine configuration, maintenance, management,
or self-healing systems.</li><li>Additionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need
for orchestration. The technical definition of orchestration is execution of a defined workflow:
first do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable
control processes that continuously drive the current state towards the provided desired state.
It shouldn't matter how you get from A to C. Centralized control is also not required. This
results in a system that is easier to use and more powerful, robust, resilient, and extensible.</li></ul><h2 id="going-back-in-time">Historical context for Kubernetes</h2><p>Let's take a look at why Kubernetes is so useful by going back in time.</p><p><img alt="Deployment evolution" src="/images/docs/Container_Evolution.svg"></p><p><strong>Traditional deployment era:</strong></p><p>Early on, organizations ran applications on physical servers. There was no way to define
resource boundaries for applications in a physical server, and this caused resource
allocation issues. For example, if multiple applications run on a physical server, there
can be instances where one application would take up most of the resources, and as a result,
the other applications would underperform. A solution for this would be to run each application
on a different physical server. But this did not scale as resources were underutilized, and it
was expensive for organizations to maintain many physical servers.</p><p><strong>Virtualized deployment era:</strong></p><p>As a solution, virtualization was introduced. It allows you
to run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization
allows applications to be isolated between VMs and provides a level of security as the
information of one application cannot be freely accessed by another application.</p><p>Virtualization allows better utilization of resources in a physical server and allows
better scalability because an application can be added or updated easily, reduces
hardware costs, and much more. With virtualization you can present a set of physical
resources as a cluster of disposable virtual machines.</p><p>Each VM is a full machine running all the components, including its own operating
system, on top of the virtualized hardware.</p><p><strong>Container deployment era:</strong></p><p>Containers are similar to VMs, but they have relaxed
isolation properties to share the Operating System (OS) among the applications.
Therefore, containers are considered lightweight. Similar to a VM, a container
has its own filesystem, share of CPU, memory, process space, and more. As they
are decoupled from the underlying infrastructure, they are portable across clouds
and OS distributions.</p><p>Containers have become popular because they provide extra benefits, such as:</p><ul><li>Agile application creation and deployment: increased ease and efficiency of
container image creation compared to VM image use.</li><li>Continuous development, integration, and deployment: provides for reliable
and frequent container image build and deployment with quick and efficient
rollbacks (due to image immutability).</li><li>Dev and Ops separation of concerns: create application container images at
build/release time rather than deployment time, thereby decoupling
applications from infrastructure.</li><li>Observability: not only surfaces OS-level information and metrics, but also
application health and other signals.</li><li>Environmental consistency across development, testing, and production: runs
the same on a laptop as it does in the cloud.</li><li>Cloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises,
on major public clouds, and anywhere else.</li><li>Application-centric management: raises the level of abstraction from running an
OS on virtual hardware to running an application on an OS using logical resources.</li><li>Loosely coupled, distributed, elastic, liberated micro-services: applications are
broken into smaller, independent pieces and can be deployed and managed dynamically &#8211;
not a monolithic stack running on one big single-purpose machine.</li><li>Resource isolation: predictable application performance.</li><li>Resource utilization: high efficiency and density.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Take a look at the <a href="/docs/concepts/overview/components/">Kubernetes Components</a></li><li>Take a look at the <a href="/docs/concepts/overview/kubernetes-api/">The Kubernetes API</a></li><li>Take a look at the <a href="/docs/concepts/architecture/">Cluster Architecture</a></li><li>Ready to <a href="/docs/setup/">Get Started</a>?</li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Kubernetes Components</h1><div class="lead">An overview of the key components that make up a Kubernetes cluster.</div><p>This page provides a high-level overview of the essential components that make up a Kubernetes cluster.</p><figure class="diagram-large clickable-zoom"><img src="/images/docs/components-of-kubernetes.svg" alt="Components of Kubernetes"><figcaption><p>The components of a Kubernetes cluster</p></figcaption></figure><h2 id="core-components">Core Components</h2><p>A Kubernetes cluster consists of a control plane and one or more worker nodes.
Here's a brief overview of the main components:</p><h3 id="control-plane-components">Control Plane Components</h3><p>Manage the overall state of the cluster:</p><dl><dt><a href="/docs/concepts/architecture/#kube-apiserver">kube-apiserver</a></dt><dd>The core component server that exposes the Kubernetes HTTP API.</dd><dt><a href="/docs/concepts/architecture/#etcd">etcd</a></dt><dd>Consistent and highly-available key value store for all API server data.</dd><dt><a href="/docs/concepts/architecture/#kube-scheduler">kube-scheduler</a></dt><dd>Looks for Pods not yet bound to a node, and assigns each Pod to a suitable node.</dd><dt><a href="/docs/concepts/architecture/#kube-controller-manager">kube-controller-manager</a></dt><dd>Runs <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a> to implement Kubernetes API behavior.</dd><dt><a href="/docs/concepts/architecture/#cloud-controller-manager">cloud-controller-manager</a> (optional)</dt><dd>Integrates with underlying cloud provider(s).</dd></dl><h3 id="node-components">Node Components</h3><p>Run on every node, maintaining running pods and providing the Kubernetes runtime environment:</p><dl><dt><a href="/docs/concepts/architecture/#kubelet">kubelet</a></dt><dd>Ensures that Pods are running, including their containers.</dd><dt><a href="/docs/concepts/architecture/#kube-proxy">kube-proxy</a> (optional)</dt><dd>Maintains network rules on nodes to implement <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Services</a>.</dd><dt><a href="/docs/concepts/architecture/#container-runtime">Container runtime</a></dt><dd>Software responsible for running containers. Read
<a href="/docs/setup/production-environment/container-runtimes/">Container Runtimes</a> to learn more.</dd></dl><div class="alert alert-secondary callout third-party-content">&#128711; This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><p>Your cluster may require additional software on each node; for example, you might also
run <a href="https://systemd.io/">systemd</a> on a Linux node to supervise local components.</p><h2 id="addons">Addons</h2><p>Addons extend the functionality of Kubernetes. A few important examples include:</p><dl><dt><a href="/docs/concepts/architecture/#dns">DNS</a></dt><dd>For cluster-wide DNS resolution.</dd><dt><a href="/docs/concepts/architecture/#web-ui-dashboard">Web UI</a> (Dashboard)</dt><dd>For cluster management via a web interface.</dd><dt><a href="/docs/concepts/architecture/#container-resource-monitoring">Container Resource Monitoring</a></dt><dd>For collecting and storing container metrics.</dd><dt><a href="/docs/concepts/architecture/#cluster-level-logging">Cluster-level Logging</a></dt><dd>For saving container logs to a central log store.</dd></dl><h2 id="flexibility-in-architecture">Flexibility in Architecture</h2><p>Kubernetes allows for flexibility in how these components are deployed and managed.
The architecture can be adapted to various needs, from small development environments
to large-scale production deployments.</p><p>For more detailed information about each component and various ways to configure your
cluster architecture, see the <a href="/docs/concepts/architecture/">Cluster Architecture</a> page.</p></div></div><div><div class="td-content"><h1>Objects In Kubernetes</h1><div class="lead">Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Learn about the Kubernetes object model and how to work with these objects.</div><p>This page explains how Kubernetes objects are represented in the Kubernetes API, and how you can
express them in <code>.yaml</code> format.</p><h2 id="kubernetes-objects">Understanding Kubernetes objects</h2><p><em>Kubernetes objects</em> are persistent entities in the Kubernetes system. Kubernetes uses these
entities to represent the state of your cluster. Specifically, they can describe:</p><ul><li>What containerized applications are running (and on which nodes)</li><li>The resources available to those applications</li><li>The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance</li></ul><p>A Kubernetes object is a "record of intent"--once you create the object, the Kubernetes system
will constantly work to ensure that the object exists. By creating an object, you're effectively
telling the Kubernetes system what you want your cluster's workload to look like; this is your
cluster's <em>desired state</em>.</p><p>To work with Kubernetes objects&#8212;whether to create, modify, or delete them&#8212;you'll need to use the
<a href="/docs/concepts/overview/kubernetes-api/">Kubernetes API</a>. When you use the <code>kubectl</code> command-line
interface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use
the Kubernetes API directly in your own programs using one of the
<a href="/docs/reference/using-api/client-libraries/">Client Libraries</a>.</p><h3 id="object-spec-and-status">Object spec and status</h3><p>Almost every Kubernetes object includes two nested object fields that govern
the object's configuration: the object <em><code>spec</code></em> and the object <em><code>status</code></em>.
For objects that have a <code>spec</code>, you have to set this when you create the object,
providing a description of the characteristics you want the resource to have:
its <em>desired state</em>.</p><p>The <code>status</code> describes the <em>current state</em> of the object, supplied and updated
by the Kubernetes system and its components. The Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> continually
and actively manages every object's actual state to match the desired state you
supplied.</p><p>For example: in Kubernetes, a Deployment is an object that can represent an
application running on your cluster. When you create the Deployment, you
might set the Deployment <code>spec</code> to specify that you want three replicas of
the application to be running. The Kubernetes system reads the Deployment
spec and starts three instances of your desired application--updating
the status to match your spec. If any of those instances should fail
(a status change), the Kubernetes system responds to the difference
between spec and status by making a correction--in this case, starting
a replacement instance.</p><p>For more information on the object spec, status, and metadata, see the
<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md">Kubernetes API Conventions</a>.</p><h3 id="describing-a-kubernetes-object">Describing a Kubernetes object</h3><p>When you create an object in Kubernetes, you must provide the object spec that describes its
desired state, as well as some basic information about the object (such as a name). When you use
the Kubernetes API to create the object (either directly or via <code>kubectl</code>), that API request must
include that information as JSON in the request body.
Most often, you provide the information to <code>kubectl</code> in a file known as a <em>manifest</em>.
By convention, manifests are YAML (you could also use JSON format).
Tools such as <code>kubectl</code> convert the information from a manifest into JSON or another supported
serialization format when making the API request over HTTP.</p><p>Here's an example manifest that shows the required fields and object spec for a Kubernetes
Deployment:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/deployment.yaml"><code>application/deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/deployment.yaml to clipboard"></div><div class="includecode" id="application-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span> </span><span># tells deployment to run 2 pods matching the template</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>One way to create a Deployment using a manifest file like the one above is to use the
<a href="/docs/reference/generated/kubectl/kubectl-commands#apply"><code>kubectl apply</code></a> command
in the <code>kubectl</code> command-line interface, passing the <code>.yaml</code> file as an argument. Here's an example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/deployment.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment created
</code></pre><h3 id="required-fields">Required fields</h3><p>In the manifest (YAML or JSON file) for the Kubernetes object you want to create, you'll need to set values for
the following fields:</p><ul><li><code>apiVersion</code> - Which version of the Kubernetes API you're using to create this object</li><li><code>kind</code> - What kind of object you want to create</li><li><code>metadata</code> - Data that helps uniquely identify the object, including a <code>name</code> string, <code>UID</code>, and optional <code>namespace</code></li><li><code>spec</code> - What state you desire for the object</li></ul><p>The precise format of the object <code>spec</code> is different for every Kubernetes object, and contains
nested fields specific to that object. The <a href="/docs/reference/kubernetes-api/">Kubernetes API Reference</a>
can help you find the spec format for all of the objects you can create using Kubernetes.</p><p>For example, see the <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec"><code>spec</code> field</a>
for the Pod API reference.
For each Pod, the <code>.spec</code> field specifies the pod and its desired state (such as the container image name for
each container within that pod).
Another example of an object specification is the
<a href="/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#StatefulSetSpec"><code>spec</code> field</a>
for the StatefulSet API. For StatefulSet, the <code>.spec</code> field specifies the StatefulSet and
its desired state.
Within the <code>.spec</code> of a StatefulSet is a <a href="/docs/concepts/workloads/pods/#pod-templates">template</a>
for Pod objects. That template describes Pods that the StatefulSet controller will create in order to
satisfy the StatefulSet specification.
Different kinds of objects can also have different <code>.status</code>; again, the API reference pages
detail the structure of that <code>.status</code> field, and its content for each different type of object.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>See <a href="/docs/concepts/configuration/overview/">Configuration Best Practices</a> for additional
information on writing YAML configuration files.</div><h2 id="server-side-field-validation">Server side field validation</h2><p>Starting with Kubernetes v1.25, the API server offers server side
<a href="/docs/reference/using-api/api-concepts/#field-validation">field validation</a>
that detects unrecognized or duplicate fields in an object. It provides all the functionality
of <code>kubectl --validate</code> on the server side.</p><p>The <code>kubectl</code> tool uses the <code>--validate</code> flag to set the level of field validation. It accepts the
values <code>ignore</code>, <code>warn</code>, and <code>strict</code> while also accepting the values <code>true</code> (equivalent to <code>strict</code>)
and <code>false</code> (equivalent to <code>ignore</code>). The default validation setting for <code>kubectl</code> is <code>--validate=true</code>.</p><dl><dt><code>Strict</code></dt><dd>Strict field validation, errors on validation failure</dd><dt><code>Warn</code></dt><dd>Field validation is performed, but errors are exposed as warnings rather than failing the request</dd><dt><code>Ignore</code></dt><dd>No server side field validation is performed</dd></dl><p>When <code>kubectl</code> cannot connect to an API server that supports field validation it will fall back
to using client-side validation. Kubernetes 1.27 and later versions always offer field validation;
older Kubernetes releases might not. If your cluster is older than v1.27, check the documentation
for your version of Kubernetes.</p><h2 id="what-s-next">What's next</h2><p>If you're new to Kubernetes, read more about the following:</p><ul><li><a href="/docs/concepts/workloads/pods/">Pods</a> which are the most important basic Kubernetes objects.</li><li><a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> objects.</li><li><a href="/docs/concepts/architecture/controller/">Controllers</a> in Kubernetes.</li><li><a href="/docs/reference/kubectl/">kubectl</a> and <a href="/docs/reference/generated/kubectl/kubectl-commands">kubectl commands</a>.</li></ul><p><a href="/docs/concepts/overview/working-with-objects/object-management/">Kubernetes Object Management</a>
explains how to use <code>kubectl</code> to manage objects.
You might need to <a href="/docs/tasks/tools/#kubectl">install kubectl</a> if you don't already have it available.</p><p>To learn about the Kubernetes API in general, visit:</p><ul><li><a href="/docs/reference/using-api/">Kubernetes API overview</a></li></ul><p>To learn about objects in Kubernetes in more depth, read other pages in this section:</p><div class="section-index"><ul><li><a href="/docs/concepts/overview/working-with-objects/object-management/">Kubernetes Object Management</a></li><li><a href="/docs/concepts/overview/working-with-objects/names/">Object Names and IDs</a></li><li><a href="/docs/concepts/overview/working-with-objects/labels/">Labels and Selectors</a></li><li><a href="/docs/concepts/overview/working-with-objects/namespaces/">Namespaces</a></li><li><a href="/docs/concepts/overview/working-with-objects/annotations/">Annotations</a></li><li><a href="/docs/concepts/overview/working-with-objects/field-selectors/">Field Selectors</a></li><li><a href="/docs/concepts/overview/working-with-objects/finalizers/">Finalizers</a></li><li><a href="/docs/concepts/overview/working-with-objects/owners-dependents/">Owners and Dependents</a></li><li><a href="/docs/concepts/overview/working-with-objects/common-labels/">Recommended Labels</a></li></ul></div></div></div><div><div class="td-content"><h1>Kubernetes Object Management</h1><p>The <code>kubectl</code> command-line tool supports several different ways to create and manage
Kubernetes <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">objects</a>. This document provides an overview of the different
approaches. Read the <a href="https://kubectl.docs.kubernetes.io">Kubectl book</a> for
details of managing objects by Kubectl.</p><h2 id="management-techniques">Management techniques</h2><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>A Kubernetes object should be managed using only one technique. Mixing
and matching techniques for the same object results in undefined behavior.</div><table><thead><tr><th>Management technique</th><th>Operates on</th><th>Recommended environment</th><th>Supported writers</th><th>Learning curve</th></tr></thead><tbody><tr><td>Imperative commands</td><td>Live objects</td><td>Development projects</td><td>1+</td><td>Lowest</td></tr><tr><td>Imperative object configuration</td><td>Individual files</td><td>Production projects</td><td>1</td><td>Moderate</td></tr><tr><td>Declarative object configuration</td><td>Directories of files</td><td>Production projects</td><td>1+</td><td>Highest</td></tr></tbody></table><h2 id="imperative-commands">Imperative commands</h2><p>When using imperative commands, a user operates directly on live objects
in a cluster. The user provides operations to
the <code>kubectl</code> command as arguments or flags.</p><p>This is the recommended way to get started or to run a one-off task in
a cluster. Because this technique operates directly on live
objects, it provides no history of previous configurations.</p><h3 id="examples">Examples</h3><p>Run an instance of the nginx container by creating a Deployment object:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl create deployment nginx --image nginx
</span></span></code></pre></div><h3 id="trade-offs">Trade-offs</h3><p>Advantages compared to object configuration:</p><ul><li>Commands are expressed as a single action word.</li><li>Commands require only a single step to make changes to the cluster.</li></ul><p>Disadvantages compared to object configuration:</p><ul><li>Commands do not integrate with change review processes.</li><li>Commands do not provide an audit trail associated with changes.</li><li>Commands do not provide a source of records except for what is live.</li><li>Commands do not provide a template for creating new objects.</li></ul><h2 id="imperative-object-configuration">Imperative object configuration</h2><p>In imperative object configuration, the kubectl command specifies the
operation (create, replace, etc.), optional flags and at least one file
name. The file specified must contain a full definition of the object
in YAML or JSON format.</p><p>See the <a href="/docs/reference/generated/kubernetes-api/v1.34/">API reference</a>
for more details on object definitions.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>The imperative <code>replace</code> command replaces the existing
spec with the newly provided one, dropping all changes to the object missing from
the configuration file. This approach should not be used with resource
types whose specs are updated independently of the configuration file.
Services of type <code>LoadBalancer</code>, for example, have their <code>externalIPs</code> field updated
independently from the configuration by the cluster.</div><h3 id="examples-1">Examples</h3><p>Create the objects defined in a configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl create -f nginx.yaml
</span></span></code></pre></div><p>Delete the objects defined in two configuration files:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl delete -f nginx.yaml -f redis.yaml
</span></span></code></pre></div><p>Update the objects defined in a configuration file by overwriting
the live configuration:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl replace -f nginx.yaml
</span></span></code></pre></div><h3 id="trade-offs-1">Trade-offs</h3><p>Advantages compared to imperative commands:</p><ul><li>Object configuration can be stored in a source control system such as Git.</li><li>Object configuration can integrate with processes such as reviewing changes before push and audit trails.</li><li>Object configuration provides a template for creating new objects.</li></ul><p>Disadvantages compared to imperative commands:</p><ul><li>Object configuration requires basic understanding of the object schema.</li><li>Object configuration requires the additional step of writing a YAML file.</li></ul><p>Advantages compared to declarative object configuration:</p><ul><li>Imperative object configuration behavior is simpler and easier to understand.</li><li>As of Kubernetes version 1.5, imperative object configuration is more mature.</li></ul><p>Disadvantages compared to declarative object configuration:</p><ul><li>Imperative object configuration works best on files, not directories.</li><li>Updates to live objects must be reflected in configuration files, or they will be lost during the next replacement.</li></ul><h2 id="declarative-object-configuration">Declarative object configuration</h2><p>When using declarative object configuration, a user operates on object
configuration files stored locally, however the user does not define the
operations to be taken on the files. Create, update, and delete operations
are automatically detected per-object by <code>kubectl</code>. This enables working on
directories, where different operations might be needed for different objects.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Declarative object configuration retains changes made by other
writers, even if the changes are not merged back to the object configuration file.
This is possible by using the <code>patch</code> API operation to write only
observed differences, instead of using the <code>replace</code>
API operation to replace the entire object configuration.</div><h3 id="examples-2">Examples</h3><p>Process all object configuration files in the <code>configs</code> directory, and create or
patch the live objects. You can first <code>diff</code> to see what changes are going to be
made, and then apply:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl diff -f configs/
</span></span><span><span>kubectl apply -f configs/
</span></span></code></pre></div><p>Recursively process directories:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl diff -R -f configs/
</span></span><span><span>kubectl apply -R -f configs/
</span></span></code></pre></div><h3 id="trade-offs-2">Trade-offs</h3><p>Advantages compared to imperative object configuration:</p><ul><li>Changes made directly to live objects are retained, even if they are not merged back into the configuration files.</li><li>Declarative object configuration has better support for operating on directories and automatically detecting operation types (create, patch, delete) per-object.</li></ul><p>Disadvantages compared to imperative object configuration:</p><ul><li>Declarative object configuration is harder to debug and understand results when they are unexpected.</li><li>Partial updates using diffs create complex merge and patch operations.</li></ul><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/manage-kubernetes-objects/imperative-command/">Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href="/docs/tasks/manage-kubernetes-objects/imperative-config/">Imperative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href="/docs/tasks/manage-kubernetes-objects/declarative-config/">Declarative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href="/docs/tasks/manage-kubernetes-objects/kustomization/">Declarative Management of Kubernetes Objects Using Kustomize</a></li><li><a href="/docs/reference/generated/kubectl/kubectl-commands/">Kubectl Command Reference</a></li><li><a href="https://kubectl.docs.kubernetes.io">Kubectl Book</a></li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/">Kubernetes API Reference</a></li></ul></div></div><div><div class="td-content"><h1>Object Names and IDs</h1><p>Each <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">object</a> in your cluster has a <a href="#names"><em>Name</em></a> that is unique for that type of resource.
Every Kubernetes object also has a <a href="#uids"><em>UID</em></a> that is unique across your whole cluster.</p><p>For example, you can only have one Pod named <code>myapp-1234</code> within the same <a href="/docs/concepts/overview/working-with-objects/namespaces/">namespace</a>, but you can have one Pod and one Deployment that are each named <code>myapp-1234</code>.</p><p>For non-unique user-provided attributes, Kubernetes provides <a href="/docs/concepts/overview/working-with-objects/labels/">labels</a> and <a href="/docs/concepts/overview/working-with-objects/annotations/">annotations</a>.</p><h2 id="names">Names</h2><p>A client-provided string that refers to an object in a <a class="glossary-tooltip" title="A Kubernetes entity, representing an endpoint on the Kubernetes API server." href="/docs/reference/using-api/api-concepts/#standard-api-terminology" target="_blank">resource</a>
URL, such as <code>/api/v1/pods/some-name</code>.</p><p>Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name.</p><p><strong>Names must be unique across all <a href="/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning">API versions</a>
of the same resource. API resources are distinguished by their API group, resource type, namespace
(for namespaced resources), and name. In other words, API version is irrelevant in this context.</strong></p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In cases when objects represent a physical entity, like a Node representing a physical host, when the host is re-created under the same name without deleting and re-creating the Node, Kubernetes treats the new host as the old one, which may lead to inconsistencies.</div><p>The server may generate a name when <code>generateName</code> is provided instead of <code>name</code> in a resource create request.
When <code>generateName</code> is used, the provided value is used as a name prefix, which server appends a generated suffix
to. Even though the name is generated, it may conflict with existing names resulting in an HTTP 409 response. This
became far less likely to happen in Kubernetes v1.31 and later, since the server will make up to 8 attempts to generate a
unique name before returning an HTTP 409 response.</p><p>Below are four types of commonly used name constraints for resources.</p><h3 id="dns-subdomain-names">DNS Subdomain Names</h3><p>Most resource types require a name that can be used as a DNS subdomain name
as defined in <a href="https://tools.ietf.org/html/rfc1123">RFC 1123</a>.
This means the name must:</p><ul><li>contain no more than 253 characters</li><li>contain only lowercase alphanumeric characters, '-' or '.'</li><li>start with an alphanumeric character</li><li>end with an alphanumeric character</li></ul><h3 id="dns-label-names">RFC 1123 Label Names</h3><p>Some resource types require their names to follow the DNS
label standard as defined in <a href="https://tools.ietf.org/html/rfc1123">RFC 1123</a>.
This means the name must:</p><ul><li>contain at most 63 characters</li><li>contain only lowercase alphanumeric characters or '-'</li><li>start with an alphabetic character</li><li>end with an alphanumeric character</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When the <code>RelaxedServiceNameValidation</code> feature gate is enabled,
Service object names are allowed to start with a digit.</div><h3 id="rfc-1035-label-names">RFC 1035 Label Names</h3><p>Some resource types require their names to follow the DNS
label standard as defined in <a href="https://tools.ietf.org/html/rfc1035">RFC 1035</a>.
This means the name must:</p><ul><li>contain at most 63 characters</li><li>contain only lowercase alphanumeric characters or '-'</li><li>start with an alphabetic character</li><li>end with an alphanumeric character</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>While RFC 1123 technically allows labels to start with digits, the current
Kubernetes implementation requires both RFC 1035 and RFC 1123 labels to start
with an alphabetic character. The exception is when the <code>RelaxedServiceNameValidation</code>
feature gate is enabled for Service objects, which allows Service names to start with digits.</div><h3 id="path-segment-names">Path Segment Names</h3><p>Some resource types require their names to be able to be safely encoded as a
path segment. In other words, the name may not be "." or ".." and the name may
not contain "/" or "%".</p><p>Here's an example manifest for a Pod named <code>nginx-demo</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-demo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Some resource types have additional restrictions on their names.</div><h2 id="uids">UIDs</h2><p>A Kubernetes systems-generated string to uniquely identify objects.</p><p>Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.</p><p>Kubernetes UIDs are universally unique identifiers (also known as UUIDs).
UUIDs are standardized as ISO/IEC 9834-8 and as ITU-T X.667.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/overview/working-with-objects/labels/">labels</a> and <a href="/docs/concepts/overview/working-with-objects/annotations/">annotations</a> in Kubernetes.</li><li>See the <a href="https://git.k8s.io/design-proposals-archive/architecture/identifiers.md">Identifiers and Names in Kubernetes</a> design document.</li></ul></div></div><div><div class="td-content"><h1>Labels and Selectors</h1><p><em>Labels</em> are key/value pairs that are attached to
<a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">objects</a> such as Pods.
Labels are intended to be used to specify identifying attributes of objects
that are meaningful and relevant to users, but do not directly imply semantics
to the core system. Labels can be used to organize and to select subsets of
objects. Labels can be attached to objects at creation time and subsequently
added and modified at any time. Each object can have a set of key/value labels
defined. Each Key must be unique for a given object.</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span><span>"metadata"</span><span>:</span> {
</span></span><span><span>  <span>"labels"</span>: {
</span></span><span><span>    <span>"key1"</span> : <span>"value1"</span>,
</span></span><span><span>    <span>"key2"</span> : <span>"value2"</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><p>Labels allow for efficient queries and watches and are ideal for use in UIs
and CLIs. Non-identifying information should be recorded using
<a href="/docs/concepts/overview/working-with-objects/annotations/">annotations</a>.</p><h2 id="motivation">Motivation</h2><p>Labels enable users to map their own organizational structures onto system objects
in a loosely coupled fashion, without requiring clients to store these mappings.</p><p>Service deployments and batch processing pipelines are often multi-dimensional entities
(e.g., multiple partitions or deployments, multiple release tracks, multiple tiers,
multiple micro-services per tier). Management often requires cross-cutting operations,
which breaks encapsulation of strictly hierarchical representations, especially rigid
hierarchies determined by the infrastructure rather than by users.</p><p>Example labels:</p><ul><li><code>"release" : "stable"</code>, <code>"release" : "canary"</code></li><li><code>"environment" : "dev"</code>, <code>"environment" : "qa"</code>, <code>"environment" : "production"</code></li><li><code>"tier" : "frontend"</code>, <code>"tier" : "backend"</code>, <code>"tier" : "cache"</code></li><li><code>"partition" : "customerA"</code>, <code>"partition" : "customerB"</code></li><li><code>"track" : "daily"</code>, <code>"track" : "weekly"</code></li></ul><p>These are examples of
<a href="/docs/concepts/overview/working-with-objects/common-labels/">commonly used labels</a>;
you are free to develop your own conventions.
Keep in mind that label Key must be unique for a given object.</p><h2 id="syntax-and-character-set">Syntax and character set</h2><p><em>Labels</em> are key/value pairs. Valid label keys have two segments: an optional
prefix and name, separated by a slash (<code>/</code>). The name segment is required and
must be 63 characters or less, beginning and ending with an alphanumeric
character (<code>[a-z0-9A-Z]</code>) with dashes (<code>-</code>), underscores (<code>_</code>), dots (<code>.</code>),
and alphanumerics between. The prefix is optional. If specified, the prefix
must be a DNS subdomain: a series of DNS labels separated by dots (<code>.</code>),
not longer than 253 characters in total, followed by a slash (<code>/</code>).</p><p>If the prefix is omitted, the label Key is presumed to be private to the user.
Automated system components (e.g. <code>kube-scheduler</code>, <code>kube-controller-manager</code>,
<code>kube-apiserver</code>, <code>kubectl</code>, or other third-party automation) which add labels
to end-user objects must specify a prefix.</p><p>The <code>kubernetes.io/</code> and <code>k8s.io/</code> prefixes are
<a href="/docs/reference/labels-annotations-taints/">reserved</a> for Kubernetes core components.</p><p>Valid label value:</p><ul><li>must be 63 characters or less (can be empty),</li><li>unless empty, must begin and end with an alphanumeric character (<code>[a-z0-9A-Z]</code>),</li><li>could contain dashes (<code>-</code>), underscores (<code>_</code>), dots (<code>.</code>), and alphanumerics between.</li></ul><p>For example, here's a manifest for a Pod that has two labels
<code>environment: production</code> and <code>app: nginx</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>label-demo<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>environment</span>:<span> </span>production<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div><h2 id="label-selectors">Label selectors</h2><p>Unlike <a href="/docs/concepts/overview/working-with-objects/names/">names and UIDs</a>, labels
do not provide uniqueness. In general, we expect many objects to carry the same label(s).</p><p>Via a <em>label selector</em>, the client/user can identify a set of objects.
The label selector is the core grouping primitive in Kubernetes.</p><p>The API currently supports two types of selectors: <em>equality-based</em> and <em>set-based</em>.
A label selector can be made of multiple <em>requirements</em> which are comma-separated.
In the case of multiple requirements, all must be satisfied so the comma separator
acts as a logical <em>AND</em> (<code>&amp;&amp;</code>) operator.</p><p>The semantics of empty or non-specified selectors are dependent on the context,
and API types that use selectors should document the validity and meaning of
them.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>For some API types, such as ReplicaSets, the label selectors of two instances must
not overlap within a namespace, or the controller can see that as conflicting
instructions and fail to determine how many replicas should be present.</div><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>For both equality-based and set-based conditions there is no logical <em>OR</em> (<code>||</code>) operator.
Ensure your filter statements are structured accordingly.</div><h3 id="equality-based-requirement"><em>Equality-based</em> requirement</h3><p><em>Equality-</em> or <em>inequality-based</em> requirements allow filtering by label keys and values.
Matching objects must satisfy all of the specified label constraints, though they may
have additional labels as well. Three kinds of operators are admitted <code>=</code>,<code>==</code>,<code>!=</code>.
The first two represent <em>equality</em> (and are synonyms), while the latter represents <em>inequality</em>.
For example:</p><pre tabindex="0"><code>environment = production
tier != frontend
</code></pre><p>The former selects all resources with key equal to <code>environment</code> and value equal to <code>production</code>.
The latter selects all resources with key equal to <code>tier</code> and value distinct from <code>frontend</code>,
and all resources with no labels with the <code>tier</code> key. One could filter for resources in <code>production</code>
excluding <code>frontend</code> using the comma operator: <code>environment=production,tier!=frontend</code></p><p>One usage scenario for equality-based label requirement is for Pods to specify
node selection criteria. For example, the sample Pod below selects nodes where
the <code>accelerator</code> label exists and is set to <code>nvidia-tesla-p100</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cuda-test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>cuda-test<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span><span>"registry.k8s.io/cuda-vector-add:v0.1"</span><span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span>
</span></span></span><span><span><span>        </span><span>limits</span>:<span>
</span></span></span><span><span><span>          </span><span>nvidia.com/gpu</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>accelerator</span>:<span> </span>nvidia-tesla-p100<span>
</span></span></span></code></pre></div><h3 id="set-based-requirement"><em>Set-based</em> requirement</h3><p><em>Set-based</em> label requirements allow filtering keys according to a set of values.
Three kinds of operators are supported: <code>in</code>,<code>notin</code> and <code>exists</code> (only the key identifier).
For example:</p><pre tabindex="0"><code>environment in (production, qa)
tier notin (frontend, backend)
partition
!partition
</code></pre><ul><li>The first example selects all resources with key equal to <code>environment</code> and value
equal to <code>production</code> or <code>qa</code>.</li><li>The second example selects all resources with key equal to <code>tier</code> and values other
than <code>frontend</code> and <code>backend</code>, and all resources with no labels with the <code>tier</code> key.</li><li>The third example selects all resources including a label with key <code>partition</code>;
no values are checked.</li><li>The fourth example selects all resources without a label with key <code>partition</code>;
no values are checked.</li></ul><p>Similarly the comma separator acts as an <em>AND</em> operator. So filtering resources
with a <code>partition</code> key (no matter the value) and with <code>environment</code> different
than <code>qa</code> can be achieved using <code>partition,environment notin (qa)</code>.
The <em>set-based</em> label selector is a general form of equality since
<code>environment=production</code> is equivalent to <code>environment in (production)</code>;
similarly for <code>!=</code> and <code>notin</code>.</p><p><em>Set-based</em> requirements can be mixed with <em>equality-based</em> requirements.
For example: <code>partition in (customerA, customerB),environment!=qa</code>.</p><h2 id="api">API</h2><h3 id="list-and-watch-filtering">LIST and WATCH filtering</h3><p>For <strong>list</strong> and <strong>watch</strong> operations, you can specify label selectors to filter the sets of objects
returned; you specify the filter using a query parameter.
(To learn in detail about watches in Kubernetes, read
<a href="/docs/reference/using-api/api-concepts/#efficient-detection-of-changes">efficient detection of changes</a>).
Both requirements are permitted
(presented here as they would appear in a URL query string):</p><ul><li><em>equality-based</em> requirements: <code>?labelSelector=environment%3Dproduction,tier%3Dfrontend</code></li><li><em>set-based</em> requirements: <code>?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29</code></li></ul><p>Both label selector styles can be used to list or watch resources via a REST client.
For example, targeting <code>apiserver</code> with <code>kubectl</code> and using <em>equality-based</em> one may write:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>environment</span><span>=</span>production,tier<span>=</span>frontend
</span></span></code></pre></div><p>or using <em>set-based</em> requirements:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>'environment in (production),tier in (frontend)'</span>
</span></span></code></pre></div><p>As already mentioned <em>set-based</em> requirements are more expressive.
For instance, they can implement the <em>OR</em> operator on values:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>'environment in (production, qa)'</span>
</span></span></code></pre></div><p>or restricting negative matching via <em>notin</em> operator:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>'environment,environment notin (frontend)'</span>
</span></span></code></pre></div><h3 id="set-references-in-api-objects">Set references in API objects</h3><p>Some Kubernetes objects, such as <a href="/docs/concepts/services-networking/service/"><code>services</code></a>
and <a href="/docs/concepts/workloads/controllers/replicationcontroller/"><code>replicationcontrollers</code></a>,
also use label selectors to specify sets of other resources, such as
<a href="/docs/concepts/workloads/pods/">pods</a>.</p><h4 id="service-and-replicationcontroller">Service and ReplicationController</h4><p>The set of pods that a <code>service</code> targets is defined with a label selector.
Similarly, the population of pods that a <code>replicationcontroller</code> should
manage is also defined with a label selector.</p><p>Label selectors for both objects are defined in <code>json</code> or <code>yaml</code> files using maps,
and only <em>equality-based</em> requirement selectors are supported:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span><span>"selector"</span><span>:</span> {
</span></span><span><span>    <span>"component"</span> : <span>"redis"</span>,
</span></span><span><span>}
</span></span></code></pre></div><p>or</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>selector</span>:<span>
</span></span></span><span><span><span>  </span><span>component</span>:<span> </span>redis<span>
</span></span></span></code></pre></div><p>This selector (respectively in <code>json</code> or <code>yaml</code> format) is equivalent to
<code>component=redis</code> or <code>component in (redis)</code>.</p><h4 id="resources-that-support-set-based-requirements">Resources that support set-based requirements</h4><p>Newer resources, such as <a href="/docs/concepts/workloads/controllers/job/"><code>Job</code></a>,
<a href="/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a>,
<a href="/docs/concepts/workloads/controllers/replicaset/"><code>ReplicaSet</code></a>, and
<a href="/docs/concepts/workloads/controllers/daemonset/"><code>DaemonSet</code></a>,
support <em>set-based</em> requirements as well.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>selector</span>:<span>
</span></span></span><span><span><span>  </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>    </span><span>component</span>:<span> </span>redis<span>
</span></span></span><span><span><span>  </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>    </span>- {<span> </span><span>key: tier, operator: In, values</span>:<span> </span>[cache] }<span>
</span></span></span><span><span><span>    </span>- {<span> </span><span>key: environment, operator: NotIn, values</span>:<span> </span>[dev] }<span>
</span></span></span></code></pre></div><p><code>matchLabels</code> is a map of <code>{key,value}</code> pairs. A single <code>{key,value}</code> in the
<code>matchLabels</code> map is equivalent to an element of <code>matchExpressions</code>, whose <code>key</code>
field is "key", the <code>operator</code> is "In", and the <code>values</code> array contains only "value".
<code>matchExpressions</code> is a list of pod selector requirements. Valid operators include
In, NotIn, Exists, and DoesNotExist. The values set must be non-empty in the case of
In and NotIn. All of the requirements, from both <code>matchLabels</code> and <code>matchExpressions</code>
are ANDed together -- they must all be satisfied in order to match.</p><h4 id="selecting-sets-of-nodes">Selecting sets of nodes</h4><p>One use case for selecting over labels is to constrain the set of nodes onto which
a pod can schedule. See the documentation on
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/">node selection</a> for more information.</p><h2 id="using-labels-effectively">Using labels effectively</h2><p>You can apply a single label to any resources, but this is not always the
best practice. There are many scenarios where multiple labels should be used to
distinguish resource sets from one another.</p><p>For instance, different applications would use different values for the <code>app</code> label, but a
multi-tier application, such as the <a href="https://github.com/kubernetes/examples/tree/master/web/guestbook/">guestbook example</a>,
would additionally need to distinguish each tier. The frontend could carry the following labels:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>labels</span>:<span>
</span></span></span><span><span><span>  </span><span>app</span>:<span> </span>guestbook<span>
</span></span></span><span><span><span>  </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span></code></pre></div><p>while the Redis master and replica would have different <code>tier</code> labels, and perhaps even an
additional <code>role</code> label:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>labels</span>:<span>
</span></span></span><span><span><span>  </span><span>app</span>:<span> </span>guestbook<span>
</span></span></span><span><span><span>  </span><span>tier</span>:<span> </span>backend<span>
</span></span></span><span><span><span>  </span><span>role</span>:<span> </span>master<span>
</span></span></span></code></pre></div><p>and</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>labels</span>:<span>
</span></span></span><span><span><span>  </span><span>app</span>:<span> </span>guestbook<span>
</span></span></span><span><span><span>  </span><span>tier</span>:<span> </span>backend<span>
</span></span></span><span><span><span>  </span><span>role</span>:<span> </span>replica<span>
</span></span></span></code></pre></div><p>The labels allow for slicing and dicing the resources along any dimension specified by a label:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml
</span></span><span><span>kubectl get pods -Lapp -Ltier -Lrole
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                           READY  STATUS    RESTARTS   AGE   APP         TIER       ROLE
guestbook-fe-4nlpb             1/1    Running   0          1m    guestbook   frontend   &lt;none&gt;
guestbook-fe-ght6d             1/1    Running   0          1m    guestbook   frontend   &lt;none&gt;
guestbook-fe-jpy62             1/1    Running   0          1m    guestbook   frontend   &lt;none&gt;
guestbook-redis-master-5pg3b   1/1    Running   0          1m    guestbook   backend    master
guestbook-redis-replica-2q2yf  1/1    Running   0          1m    guestbook   backend    replica
guestbook-redis-replica-qgazl  1/1    Running   0          1m    guestbook   backend    replica
my-nginx-divi2                 1/1    Running   0          29m   nginx       &lt;none&gt;     &lt;none&gt;
my-nginx-o0ef1                 1/1    Running   0          29m   nginx       &lt;none&gt;     &lt;none&gt;
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -lapp<span>=</span>guestbook,role<span>=</span>replica
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                           READY  STATUS   RESTARTS  AGE
guestbook-redis-replica-2q2yf  1/1    Running  0         3m
guestbook-redis-replica-qgazl  1/1    Running  0         3m
</code></pre><h2 id="updating-labels">Updating labels</h2><p>Sometimes you may want to relabel existing pods and other resources before creating
new resources. This can be done with <code>kubectl label</code>.
For example, if you want to label all your NGINX Pods as frontend tier, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl label pods -l <span>app</span><span>=</span>nginx <span>tier</span><span>=</span>fe
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">pod/my-nginx-2035384211-j5fhi labeled
pod/my-nginx-2035384211-u2c7e labeled
pod/my-nginx-2035384211-u3t6x labeled
</code></pre><p>This first filters all pods with the label "app=nginx", and then labels them with the "tier=fe".
To see the pods you labeled, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>nginx -L tier
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                        READY     STATUS    RESTARTS   AGE       TIER
my-nginx-2035384211-j5fhi   1/1       Running   0          23m       fe
my-nginx-2035384211-u2c7e   1/1       Running   0          23m       fe
my-nginx-2035384211-u3t6x   1/1       Running   0          23m       fe
</code></pre><p>This outputs all "app=nginx" pods, with an additional label column of pods' tier
(specified with <code>-L</code> or <code>--label-columns</code>).</p><p>For more information, please see <a href="/docs/reference/generated/kubectl/kubectl-commands/#label">kubectl label</a>.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn how to <a href="/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node">add a label to a node</a></li><li>Find <a href="/docs/reference/labels-annotations-taints/">Well-known labels, Annotations and Taints</a></li><li>See <a href="/docs/concepts/overview/working-with-objects/common-labels/">Recommended labels</a></li><li><a href="/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/">Enforce Pod Security Standards with Namespace Labels</a></li><li>Read a blog on <a href="/blog/2021/06/21/writing-a-controller-for-pod-labels/">Writing a Controller for Pod Labels</a></li></ul></div></div><div><div class="td-content"><h1>Namespaces</h1><p>In Kubernetes, <em>namespaces</em> provide a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">objects</a> <em>(e.g. Deployments, Services, etc.)</em> and not for cluster-wide objects <em>(e.g. StorageClass, Nodes, PersistentVolumes, etc.)</em>.</p><h2 id="when-to-use-multiple-namespaces">When to Use Multiple Namespaces</h2><p>Namespaces are intended for use in environments with many users spread across multiple
teams, or projects. For clusters with a few to tens of users, you should not
need to create or think about namespaces at all. Start using namespaces when you
need the features they provide.</p><p>Namespaces provide a scope for names. Names of resources need to be unique within a namespace,
but not across namespaces. Namespaces cannot be nested inside one another and each Kubernetes
resource can only be in one namespace.</p><p>Namespaces are a way to divide cluster resources between multiple users (via <a href="/docs/concepts/policy/resource-quotas/">resource quota</a>).</p><p>It is not necessary to use multiple namespaces to separate slightly different
resources, such as different versions of the same software: use
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">labels</a> to distinguish
resources within the same namespace.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>For a production cluster, consider <em>not</em> using the <code>default</code> namespace. Instead, make other namespaces and use those.</div><h2 id="initial-namespaces">Initial namespaces</h2><p>Kubernetes starts with four initial namespaces:</p><dl><dt><code>default</code></dt><dd>Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.</dd><dt><code>kube-node-lease</code></dt><dd>This namespace holds <a href="/docs/concepts/architecture/leases/">Lease</a> objects associated with each node. Node leases allow the kubelet to send <a href="/docs/concepts/architecture/nodes/#node-heartbeats">heartbeats</a> so that the control plane can detect node failure.</dd><dt><code>kube-public</code></dt><dd>This namespace is readable by <em>all</em> clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.</dd><dt><code>kube-system</code></dt><dd>The namespace for objects created by the Kubernetes system.</dd></dl><h2 id="working-with-namespaces">Working with Namespaces</h2><p>Creation and deletion of namespaces are described in the
<a href="/docs/tasks/administer-cluster/namespaces/">Admin Guide documentation for namespaces</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Avoid creating namespaces with the prefix <code>kube-</code>, since it is reserved for Kubernetes system namespaces.</div><h3 id="viewing-namespaces">Viewing namespaces</h3><p>You can list the current namespaces in a cluster using:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get namespace
</span></span></code></pre></div><pre tabindex="0"><code>NAME              STATUS   AGE
default           Active   1d
kube-node-lease   Active   1d
kube-public       Active   1d
kube-system       Active   1d
</code></pre><h3 id="setting-the-namespace-for-a-request">Setting the namespace for a request</h3><p>To set the namespace for a current request, use the <code>--namespace</code> flag.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl run nginx --image<span>=</span>nginx --namespace<span>=</span>&lt;insert-namespace-name-here&gt;
</span></span><span><span>kubectl get pods --namespace<span>=</span>&lt;insert-namespace-name-here&gt;
</span></span></code></pre></div><h3 id="setting-the-namespace-preference">Setting the namespace preference</h3><p>You can permanently save the namespace for all subsequent kubectl commands in that
context.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config set-context --current --namespace<span>=</span>&lt;insert-namespace-name-here&gt;
</span></span><span><span><span># Validate it</span>
</span></span><span><span>kubectl config view --minify | grep namespace:
</span></span></code></pre></div><h2 id="namespaces-and-dns">Namespaces and DNS</h2><p>When you create a <a href="/docs/concepts/services-networking/service/">Service</a>,
it creates a corresponding <a href="/docs/concepts/services-networking/dns-pod-service/">DNS entry</a>.
This entry is of the form <code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code>, which means
that if a container only uses <code>&lt;service-name&gt;</code>, it will resolve to the service which
is local to a namespace. This is useful for using the same configuration across
multiple namespaces such as Development, Staging and Production. If you want to reach
across namespaces, you need to use the fully qualified domain name (FQDN).</p><p>As a result, all namespace names must be valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">RFC 1123 DNS labels</a>.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4><p>By creating namespaces with the same name as <a href="https://data.iana.org/TLD/tlds-alpha-by-domain.txt">public top-level
domains</a>, Services in these
namespaces can have short DNS names that overlap with public DNS records.
Workloads from any namespace performing a DNS lookup without a <a href="https://datatracker.ietf.org/doc/html/rfc1034#page-8">trailing dot</a> will
be redirected to those services, taking precedence over public DNS.</p><p>To mitigate this, limit privileges for creating namespaces to trusted users. If
required, you could additionally configure third-party security controls, such
as <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">admission
webhooks</a>,
to block creating any namespace with the name of <a href="https://data.iana.org/TLD/tlds-alpha-by-domain.txt">public
TLDs</a>.</p></div><h2 id="not-all-objects-are-in-a-namespace">Not all objects are in a namespace</h2><p>Most Kubernetes resources (e.g. pods, services, replication controllers, and others) are
in some namespaces. However namespace resources are not themselves in a namespace.
And low-level resources, such as
<a href="/docs/concepts/architecture/nodes/">nodes</a> and
<a href="/docs/concepts/storage/persistent-volumes/">persistentVolumes</a>, are not in any namespace.</p><p>To see which Kubernetes resources are and aren't in a namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># In a namespace</span>
</span></span><span><span>kubectl api-resources --namespaced<span>=</span><span>true</span>
</span></span><span><span>
</span></span><span><span><span># Not in a namespace</span>
</span></span><span><span>kubectl api-resources --namespaced<span>=</span><span>false</span>
</span></span></code></pre></div><h2 id="automatic-labelling">Automatic labelling</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes 1.22 [stable]</code></div><p>The Kubernetes control plane sets an immutable <a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">label</a>
<code>kubernetes.io/metadata.name</code> on all namespaces.
The value of the label is the namespace name.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace">creating a new namespace</a>.</li><li>Learn more about <a href="/docs/tasks/administer-cluster/namespaces/#deleting-a-namespace">deleting a namespace</a>.</li></ul></div></div><div><div class="td-content"><h1>Annotations</h1><p>You can use Kubernetes annotations to attach arbitrary non-identifying metadata
to <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">objects</a>.
Clients such as tools and libraries can retrieve this metadata.</p><h2 id="attaching-metadata-to-objects">Attaching metadata to objects</h2><p>You can use either labels or annotations to attach metadata to Kubernetes
objects. Labels can be used to select objects and to find
collections of objects that satisfy certain conditions. In contrast, annotations
are not used to identify and select objects. The metadata
in an annotation can be small or large, structured or unstructured, and can
include characters not permitted by labels. It is possible to use labels as
well as annotations in the metadata of the same object.</p><p>Annotations, like labels, are key/value maps:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span><span>"metadata"</span><span>:</span> {
</span></span><span><span>  <span>"annotations"</span>: {
</span></span><span><span>    <span>"key1"</span> : <span>"value1"</span>,
</span></span><span><span>    <span>"key2"</span> : <span>"value2"</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The keys and the values in the map must be strings. In other words, you cannot use
numeric, boolean, list or other types for either the keys or the values.</div><p>Here are some examples of information that could be recorded in annotations:</p><ul><li><p>Fields managed by a declarative configuration layer. Attaching these fields
as annotations distinguishes them from default values set by clients or
servers, and from auto-generated fields and fields set by
auto-sizing or auto-scaling systems.</p></li><li><p>Build, release, or image information like timestamps, release IDs, git branch,
PR numbers, image hashes, and registry address.</p></li><li><p>Pointers to logging, monitoring, analytics, or audit repositories.</p></li><li><p>Client library or tool information that can be used for debugging purposes:
for example, name, version, and build information.</p></li><li><p>User or tool/system provenance information, such as URLs of related objects
from other ecosystem components.</p></li><li><p>Lightweight rollout tool metadata: for example, config or checkpoints.</p></li><li><p>Phone or pager numbers of persons responsible, or directory entries that
specify where that information can be found, such as a team web site.</p></li><li><p>Directives from the end-user to the implementations to modify behavior or
engage non-standard features.</p></li></ul><p>Instead of using annotations, you could store this type of information in an
external database or directory, but that would make it much harder to produce
shared client libraries and tools for deployment, management, introspection,
and the like.</p><h2 id="syntax-and-character-set">Syntax and character set</h2><p><em>Annotations</em> are key/value pairs. Valid annotation keys have two segments: an optional prefix and name, separated by a slash (<code>/</code>). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character (<code>[a-z0-9A-Z]</code>) with dashes (<code>-</code>), underscores (<code>_</code>), dots (<code>.</code>), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (<code>.</code>), not longer than 253 characters in total, followed by a slash (<code>/</code>).</p><p>If the prefix is omitted, the annotation Key is presumed to be private to the user. Automated system components (e.g. <code>kube-scheduler</code>, <code>kube-controller-manager</code>, <code>kube-apiserver</code>, <code>kubectl</code>, or other third-party automation) which add annotations to end-user objects must specify a prefix.</p><p>The <code>kubernetes.io/</code> and <code>k8s.io/</code> prefixes are reserved for Kubernetes core components.</p><p>For example, here's a manifest for a Pod that has the annotation <code>imageregistry: https://hub.docker.com/</code> :</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>annotations-demo<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>imageregistry</span>:<span> </span><span>"https://hub.docker.com/"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/overview/working-with-objects/labels/">Labels and Selectors</a>.</li><li>Find <a href="/docs/reference/labels-annotations-taints/">Well-known labels, Annotations and Taints</a></li></ul></div></div><div><div class="td-content"><h1>Field Selectors</h1><p><em>Field selectors</em> let you select Kubernetes <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">objects</a> based on the
value of one or more resource fields. Here are some examples of field selector queries:</p><ul><li><code>metadata.name=my-service</code></li><li><code>metadata.namespace!=default</code></li><li><code>status.phase=Pending</code></li></ul><p>This <code>kubectl</code> command selects all Pods for which the value of the <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase"><code>status.phase</code></a> field is <code>Running</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --field-selector status.phase<span>=</span>Running
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Field selectors are essentially resource <em>filters</em>. By default, no selectors/filters are applied, meaning that all resources of the specified type are selected. This makes the <code>kubectl</code> queries <code>kubectl get pods</code> and <code>kubectl get pods --field-selector ""</code> equivalent.</div><h2 id="supported-fields">Supported fields</h2><p>Supported field selectors vary by Kubernetes resource type. All resource types support the <code>metadata.name</code> and <code>metadata.namespace</code> fields. Using unsupported field selectors produces an error. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get ingress --field-selector foo.bar<span>=</span>baz
</span></span></code></pre></div><pre tabindex="0"><code>Error from server (BadRequest): Unable to find "ingresses" that match label selector "", field selector "foo.bar=baz": "foo.bar" is not a known field selector: only "metadata.name", "metadata.namespace"
</code></pre><h3 id="list-of-supported-fields">List of supported fields</h3><table><thead><tr><th>Kind</th><th>Fields</th></tr></thead><tbody><tr><td>Pod</td><td><code>spec.nodeName</code><br><code>spec.restartPolicy</code><br><code>spec.schedulerName</code><br><code>spec.serviceAccountName</code><br><code>spec.hostNetwork</code><br><code>status.phase</code><br><code>status.podIP</code><br><code>status.nominatedNodeName</code></td></tr><tr><td>Event</td><td><code>involvedObject.kind</code><br><code>involvedObject.namespace</code><br><code>involvedObject.name</code><br><code>involvedObject.uid</code><br><code>involvedObject.apiVersion</code><br><code>involvedObject.resourceVersion</code><br><code>involvedObject.fieldPath</code><br><code>reason</code><br><code>reportingComponent</code><br><code>source</code><br><code>type</code></td></tr><tr><td>Secret</td><td><code>type</code></td></tr><tr><td>Namespace</td><td><code>status.phase</code></td></tr><tr><td>ReplicaSet</td><td><code>status.replicas</code></td></tr><tr><td>ReplicationController</td><td><code>status.replicas</code></td></tr><tr><td>Job</td><td><code>status.successful</code></td></tr><tr><td>Node</td><td><code>spec.unschedulable</code></td></tr><tr><td>CertificateSigningRequest</td><td><code>spec.signerName</code></td></tr></tbody></table><h3 id="custom-resources-fields">Custom resources fields</h3><p>All custom resource types support the <code>metadata.name</code> and <code>metadata.namespace</code> fields.</p><p>Additionally, the <code>spec.versions[*].selectableFields</code> field of a <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinition</a>
declares which other fields in a custom resource may be used in field selectors. See <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#crd-selectable-fields">selectable fields for custom resources</a>
for more information about how to use field selectors with CustomResourceDefinitions.</p><h2 id="supported-operators">Supported operators</h2><p>You can use the <code>=</code>, <code>==</code>, and <code>!=</code> operators with field selectors (<code>=</code> and <code>==</code> mean the same thing). This <code>kubectl</code> command, for example, selects all Kubernetes Services that aren't in the <code>default</code> namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get services  --all-namespaces --field-selector metadata.namespace!<span>=</span>default
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><a href="/docs/concepts/overview/working-with-objects/labels/#set-based-requirement">Set-based operators</a>
(<code>in</code>, <code>notin</code>, <code>exists</code>) are not supported for field selectors.</div><h2 id="chained-selectors">Chained selectors</h2><p>As with <a href="/docs/concepts/overview/working-with-objects/labels/">label</a> and other selectors, field selectors can be chained together as a comma-separated list. This <code>kubectl</code> command selects all Pods for which the <code>status.phase</code> does not equal <code>Running</code> and the <code>spec.restartPolicy</code> field equals <code>Always</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods --field-selector<span>=</span>status.phase!<span>=</span>Running,spec.restartPolicy<span>=</span>Always
</span></span></code></pre></div><h2 id="multiple-resource-types">Multiple resource types</h2><p>You can use field selectors across multiple resource types. This <code>kubectl</code> command selects all Statefulsets and Services that are not in the <code>default</code> namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!<span>=</span>default
</span></span></code></pre></div></div></div><div><div class="td-content"><h1>Finalizers</h1><p>Finalizers are namespaced keys that tell Kubernetes to wait until specific
conditions are met before it fully deletes <a class="glossary-tooltip" title="A Kubernetes entity, representing an endpoint on the Kubernetes API server." href="/docs/reference/using-api/api-concepts/#standard-api-terminology" target="_blank">resources</a>
that are marked for deletion.
Finalizers alert <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a>
to clean up resources the deleted object owned.</p><p>When you tell Kubernetes to delete an object that has finalizers specified for
it, the Kubernetes API marks the object for deletion by populating <code>.metadata.deletionTimestamp</code>,
and returns a <code>202</code> status code (HTTP "Accepted"). The target object remains in a terminating state while the
control plane, or other components, take the actions defined by the finalizers.
After these actions are complete, the controller removes the relevant finalizers
from the target object. When the <code>metadata.finalizers</code> field is empty,
Kubernetes considers the deletion complete and deletes the object.</p><p>You can use finalizers to control <a class="glossary-tooltip" title="A collective term for the various mechanisms Kubernetes uses to clean up cluster resources." href="/docs/concepts/architecture/garbage-collection/" target="_blank">garbage collection</a>
of resources. For example, you can define a finalizer to clean up related
<a class="glossary-tooltip" title="A Kubernetes entity, representing an endpoint on the Kubernetes API server." href="/docs/reference/using-api/api-concepts/#standard-api-terminology" target="_blank">API resources</a> or infrastructure before the controller
deletes the object being finalized.</p><p>You can use finalizers to control <a class="glossary-tooltip" title="A collective term for the various mechanisms Kubernetes uses to clean up cluster resources." href="/docs/concepts/architecture/garbage-collection/" target="_blank">garbage collection</a>
of <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">objects</a> by alerting <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a>
to perform specific cleanup tasks before deleting the target resource.</p><p>Finalizers don't usually specify the code to execute. Instead, they are
typically lists of keys on a specific resource similar to annotations.
Kubernetes specifies some finalizers automatically, but you can also specify
your own.</p><h2 id="how-finalizers-work">How finalizers work</h2><p>When you create a resource using a manifest file, you can specify finalizers in
the <code>metadata.finalizers</code> field. When you attempt to delete the resource, the
API server handling the delete request notices the values in the <code>finalizers</code> field
and does the following:</p><ul><li>Modifies the object to add a <code>metadata.deletionTimestamp</code> field with the
time you started the deletion.</li><li>Prevents the object from being removed until all items are removed from its <code>metadata.finalizers</code> field</li><li>Returns a <code>202</code> status code (HTTP "Accepted")</li></ul><p>The controller managing that finalizer notices the update to the object setting the
<code>metadata.deletionTimestamp</code>, indicating deletion of the object has been requested.
The controller then attempts to satisfy the requirements of the finalizers
specified for that resource. Each time a finalizer condition is satisfied, the
controller removes that key from the resource's <code>finalizers</code> field. When the
<code>finalizers</code> field is emptied, an object with a <code>deletionTimestamp</code> field set
is automatically deleted. You can also use finalizers to prevent deletion of unmanaged resources.</p><p>A common example of a finalizer is <code>kubernetes.io/pv-protection</code>, which prevents
accidental deletion of <code>PersistentVolume</code> objects. When a <code>PersistentVolume</code>
object is in use by a Pod, Kubernetes adds the <code>pv-protection</code> finalizer. If you
try to delete the <code>PersistentVolume</code>, it enters a <code>Terminating</code> status, but the
controller can't delete it because the finalizer exists. When the Pod stops
using the <code>PersistentVolume</code>, Kubernetes clears the <code>pv-protection</code> finalizer,
and the controller deletes the volume.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li><p>When you <code>DELETE</code> an object, Kubernetes adds the deletion timestamp for that object and then
immediately starts to restrict changes to the <code>.metadata.finalizers</code> field for the object that is
now pending deletion. You can remove existing finalizers (deleting an entry from the <code>finalizers</code>
list) but you cannot add a new finalizer. You also cannot modify the <code>deletionTimestamp</code> for an
object once it is set.</p></li><li><p>After the deletion is requested, you can not resurrect this object. The only way is to delete it and make a new similar object.</p></li></ul></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Custom finalizer names <strong>must</strong> be publicly qualified finalizer names, such as <code>example.com/finalizer-name</code>.
Kubernetes enforces this format; the API server rejects writes to objects where the change does not use qualified finalizer names for any custom finalizer.</div><h2 id="owners-labels-finalizers">Owner references, labels, and finalizers</h2><p>Like <a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">labels</a>,
<a href="/docs/concepts/overview/working-with-objects/owners-dependents/">owner references</a>
describe the relationships between objects in Kubernetes, but are used for a
different purpose. When a
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> manages objects
like Pods, it uses labels to track changes to groups of related objects. For
example, when a <a class="glossary-tooltip" title="A finite or batch task that runs to completion." href="/docs/concepts/workloads/controllers/job/" target="_blank">Job</a> creates one or
more Pods, the Job controller applies labels to those pods and tracks changes to
any Pods in the cluster with the same label.</p><p>The Job controller also adds <em>owner references</em> to those Pods, pointing at the
Job that created the Pods. If you delete the Job while these Pods are running,
Kubernetes uses the owner references (not labels) to determine which Pods in the
cluster need cleanup.</p><p>Kubernetes also processes finalizers when it identifies owner references on a
resource targeted for deletion.</p><p>In some situations, finalizers can block the deletion of dependent objects,
which can cause the targeted owner object to remain for
longer than expected without being fully deleted. In these situations, you
should check finalizers and owner references on the target owner and dependent
objects to troubleshoot the cause.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In cases where objects are stuck in a deleting state, avoid manually
removing finalizers to allow deletion to continue. Finalizers are usually added
to resources for a reason, so forcefully removing them can lead to issues in
your cluster. This should only be done when the purpose of the finalizer is
understood and is accomplished in another way (for example, manually cleaning
up some dependent object).</div><h2 id="what-s-next">What's next</h2><ul><li>Read <a href="/blog/2021/05/14/using-finalizers-to-control-deletion/">Using Finalizers to Control Deletion</a>
on the Kubernetes blog.</li></ul></div></div><div><div class="td-content"><h1>Owners and Dependents</h1><p>In Kubernetes, some <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">objects</a> are
<em>owners</em> of other objects. For example, a
<a class="glossary-tooltip" title="ReplicaSet ensures that a specified number of Pod replicas are running at one time" href="/docs/concepts/workloads/controllers/replicaset/" target="_blank">ReplicaSet</a> is the owner
of a set of Pods. These owned objects are <em>dependents</em> of their owner.</p><p>Ownership is different from the <a href="/docs/concepts/overview/working-with-objects/labels/">labels and selectors</a>
mechanism that some resources also use. For example, consider a Service that
creates <code>EndpointSlice</code> objects. The Service uses <a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">labels</a> to allow the control plane to
determine which <code>EndpointSlice</code> objects are used for that Service. In addition
to the labels, each <code>EndpointSlice</code> that is managed on behalf of a Service has
an owner reference. Owner references help different parts of Kubernetes avoid
interfering with objects they don&#8217;t control.</p><h2 id="owner-references-in-object-specifications">Owner references in object specifications</h2><p>Dependent objects have a <code>metadata.ownerReferences</code> field that references their
owner object. A valid owner reference consists of the object name and a <a class="glossary-tooltip" title="A Kubernetes systems-generated string to uniquely identify objects." href="/docs/concepts/overview/working-with-objects/names" target="_blank">UID</a>
within the same <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a> as the dependent object. Kubernetes sets the value of
this field automatically for objects that are dependents of other objects like
ReplicaSets, DaemonSets, Deployments, Jobs and CronJobs, and ReplicationControllers.
You can also configure these relationships manually by changing the value of
this field. However, you usually don't need to and can allow Kubernetes to
automatically manage the relationships.</p><p>Dependent objects also have an <code>ownerReferences.blockOwnerDeletion</code> field that
takes a boolean value and controls whether specific dependents can block garbage
collection from deleting their owner object. Kubernetes automatically sets this
field to <code>true</code> if a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>
(for example, the Deployment controller) sets the value of the
<code>metadata.ownerReferences</code> field. You can also set the value of the
<code>blockOwnerDeletion</code> field manually to control which dependents block garbage
collection.</p><p>A Kubernetes admission controller controls user access to change this field for
dependent resources, based on the delete permissions of the owner. This control
prevents unauthorized users from delaying owner object deletion.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Cross-namespace owner references are disallowed by design.
Namespaced dependents can specify cluster-scoped or namespaced owners.
A namespaced owner <strong>must</strong> exist in the same namespace as the dependent.
If it does not, the owner reference is treated as absent, and the dependent
is subject to deletion once all owners are verified absent.</p><p>Cluster-scoped dependents can only specify cluster-scoped owners.
In v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner,
it is treated as having an unresolvable owner reference, and is not able to be garbage collected.</p><p>In v1.20+, if the garbage collector detects an invalid cross-namespace <code>ownerReference</code>,
or a cluster-scoped dependent with an <code>ownerReference</code> referencing a namespaced kind, a warning Event
with a reason of <code>OwnerRefInvalidNamespace</code> and an <code>involvedObject</code> of the invalid dependent is reported.
You can check for that kind of Event by running
<code>kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace</code>.</p></div><h2 id="ownership-and-finalizers">Ownership and finalizers</h2><p>When you tell Kubernetes to delete a resource, the API server allows the
managing controller to process any <a href="/docs/concepts/overview/working-with-objects/finalizers/">finalizer rules</a>
for the resource. <a class="glossary-tooltip" title="A namespaced key that tells Kubernetes to wait until specific conditions are met before it fully deletes an object marked for deletion." href="/docs/concepts/overview/working-with-objects/finalizers/" target="_blank">Finalizers</a>
prevent accidental deletion of resources your cluster may still need to function
correctly. For example, if you try to delete a <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolume</a> that is still
in use by a Pod, the deletion does not happen immediately because the
<code>PersistentVolume</code> has the <code>kubernetes.io/pv-protection</code> finalizer on it.
Instead, the <a href="/docs/concepts/storage/volumes/">volume</a> remains in the <code>Terminating</code> status until Kubernetes clears
the finalizer, which only happens after the <code>PersistentVolume</code> is no longer
bound to a Pod.</p><p>Kubernetes also adds finalizers to an owner resource when you use either
<a href="/docs/concepts/architecture/garbage-collection/#cascading-deletion">foreground or orphan cascading deletion</a>.
In foreground deletion, it adds the <code>foreground</code> finalizer so that the
controller must delete dependent resources that also have
<code>ownerReferences.blockOwnerDeletion=true</code> before it deletes the owner. If you
specify an orphan deletion policy, Kubernetes adds the <code>orphan</code> finalizer so
that the controller ignores dependent resources after it deletes the owner
object.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/overview/working-with-objects/finalizers/">Kubernetes finalizers</a>.</li><li>Learn about <a href="/docs/concepts/architecture/garbage-collection/">garbage collection</a>.</li><li>Read the API reference for <a href="/docs/reference/kubernetes-api/common-definitions/object-meta/#System">object metadata</a>.</li></ul></div></div><div><div class="td-content"><h1>Recommended Labels</h1><p>You can visualize and manage Kubernetes objects with more tools than kubectl and
the dashboard. A common set of labels allows tools to work interoperably, describing
objects in a common manner that all tools can understand.</p><p>In addition to supporting tooling, the recommended labels describe applications
in a way that can be queried.</p><p>The metadata is organized around the concept of an <em>application</em>. Kubernetes is not
a platform as a service (PaaS) and doesn't have or enforce a formal notion of an application.
Instead, applications are informal and described with metadata. The definition of
what an application contains is loose.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>These are recommended labels. They make it easier to manage applications
but aren't required for any core tooling.</div><p>Shared labels and annotations share a common prefix: <code>app.kubernetes.io</code>. Labels
without a prefix are private to users. The shared prefix ensures that shared labels
do not interfere with custom user labels.</p><h2 id="labels">Labels</h2><p>In order to take full advantage of using these labels, they should be applied
on every resource object.</p><table><thead><tr><th>Key</th><th>Description</th><th>Example</th><th>Type</th></tr></thead><tbody><tr><td><code>app.kubernetes.io/name</code></td><td>The name of the application</td><td><code>mysql</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/instance</code></td><td>A unique name identifying the instance of an application</td><td><code>mysql-abcxyz</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/version</code></td><td>The current version of the application (e.g., a <a href="https://semver.org/spec/v1.0.0.html">SemVer 1.0</a>, revision hash, etc.)</td><td><code>5.7.21</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/component</code></td><td>The component within the architecture</td><td><code>database</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/part-of</code></td><td>The name of a higher level application this one is part of</td><td><code>wordpress</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/managed-by</code></td><td>The tool being used to manage the operation of an application</td><td><code>Helm</code></td><td>string</td></tr></tbody></table><p>To illustrate these labels in action, consider the following <a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSet</a> object:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># This is an excerpt</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StatefulSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/instance</span>:<span> </span>mysql-abcxyz<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/version</span>:<span> </span><span>"5.7.21"</span><span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/component</span>:<span> </span>database<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/part-of</span>:<span> </span>wordpress<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/managed-by</span>:<span> </span>Helm<span>
</span></span></span></code></pre></div><h2 id="applications-and-instances-of-applications">Applications And Instances Of Applications</h2><p>An application can be installed one or more times into a Kubernetes cluster and,
in some cases, the same namespace. For example, WordPress can be installed more
than once where different websites are different installations of WordPress.</p><p>The name of an application and the instance name are recorded separately. For
example, WordPress has a <code>app.kubernetes.io/name</code> of <code>wordpress</code> while it has
an instance name, represented as <code>app.kubernetes.io/instance</code> with a value of
<code>wordpress-abcxyz</code>. This enables the application and instance of the application
to be identifiable. Every instance of an application must have a unique name.</p><h2 id="examples">Examples</h2><p>To illustrate different ways to use these labels the following examples have varying complexity.</p><h3 id="a-simple-stateless-service">A Simple Stateless Service</h3><p>Consider the case for a simple stateless service deployed using <code>Deployment</code> and <code>Service</code> objects. The following two snippets represent how the labels could be used in their simplest form.</p><p>The <code>Deployment</code> is used to oversee the pods running the application itself.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>myservice<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/instance</span>:<span> </span>myservice-abcxyz<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>The <code>Service</code> is used to expose the application.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>myservice<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/instance</span>:<span> </span>myservice-abcxyz<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><h3 id="web-application-with-a-database">Web Application With A Database</h3><p>Consider a slightly more complicated application: a web application (WordPress)
using a database (MySQL), installed using Helm. The following snippets illustrate
the start of objects used to deploy this application.</p><p>The start to the following <code>Deployment</code> is used for WordPress:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>wordpress<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/instance</span>:<span> </span>wordpress-abcxyz<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/version</span>:<span> </span><span>"4.9.4"</span><span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/managed-by</span>:<span> </span>Helm<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/component</span>:<span> </span>server<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/part-of</span>:<span> </span>wordpress<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>The <code>Service</code> is used to expose WordPress:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>wordpress<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/instance</span>:<span> </span>wordpress-abcxyz<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/version</span>:<span> </span><span>"4.9.4"</span><span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/managed-by</span>:<span> </span>Helm<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/component</span>:<span> </span>server<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/part-of</span>:<span> </span>wordpress<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>MySQL is exposed as a <code>StatefulSet</code> with metadata for both it and the larger application it belongs to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StatefulSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/instance</span>:<span> </span>mysql-abcxyz<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/version</span>:<span> </span><span>"5.7.21"</span><span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/managed-by</span>:<span> </span>Helm<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/component</span>:<span> </span>database<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/part-of</span>:<span> </span>wordpress<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>The <code>Service</code> is used to expose MySQL as part of WordPress:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/instance</span>:<span> </span>mysql-abcxyz<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/version</span>:<span> </span><span>"5.7.21"</span><span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/managed-by</span>:<span> </span>Helm<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/component</span>:<span> </span>database<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/part-of</span>:<span> </span>wordpress<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>With the MySQL <code>StatefulSet</code> and <code>Service</code> you'll notice information about both MySQL and WordPress, the broader application, are included.</p></div></div><div><div class="td-content"><h1>The Kubernetes API</h1><div class="lead">The Kubernetes API lets you query and manipulate the state of objects in Kubernetes. The core of Kubernetes' control plane is the API server and the HTTP API that it exposes. Users, the different parts of your cluster, and external components all communicate with one another through the API server.</div><p>The core of Kubernetes' <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>
is the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a>. The API server
exposes an HTTP API that lets end users, different parts of your cluster, and
external components communicate with one another.</p><p>The Kubernetes API lets you query and manipulate the state of API objects in Kubernetes
(for example: Pods, Namespaces, ConfigMaps, and Events).</p><p>Most operations can be performed through the <a href="/docs/reference/kubectl/">kubectl</a>
command-line interface or other command-line tools, such as
<a href="/docs/reference/setup-tools/kubeadm/">kubeadm</a>, which in turn use the API.
However, you can also access the API directly using REST calls. Kubernetes
provides a set of <a href="/docs/reference/using-api/client-libraries/">client libraries</a>
for those looking to
write applications using the Kubernetes API.</p><p>Each Kubernetes cluster publishes the specification of the APIs that the cluster serves.
There are two mechanisms that Kubernetes uses to publish these API specifications; both are useful
to enable automatic interoperability. For example, the <code>kubectl</code> tool fetches and caches the API
specification for enabling command-line completion and other features.
The two supported mechanisms are as follows:</p><ul><li><p><a href="#discovery-api">The Discovery API</a> provides information about the Kubernetes APIs:
API names, resources, versions, and supported operations. This is a Kubernetes
specific term as it is a separate API from the Kubernetes OpenAPI.
It is intended to be a brief summary of the available resources and it does not
detail specific schema for the resources. For reference about resource schemas,
please refer to the OpenAPI document.</p></li><li><p>The <a href="#openapi-interface-definition">Kubernetes OpenAPI Document</a> provides (full)
<a href="https://www.openapis.org/">OpenAPI v2.0 and 3.0 schemas</a> for all Kubernetes API
endpoints.
The OpenAPI v3 is the preferred method for accessing OpenAPI as it
provides
a more comprehensive and accurate view of the API. It includes all the available
API paths, as well as all resources consumed and produced for every operations
on every endpoints. It also includes any extensibility components that a cluster supports.
The data is a complete specification and is significantly larger than that from the
Discovery API.</p></li></ul><h2 id="discovery-api">Discovery API</h2><p>Kubernetes publishes a list of all group versions and resources supported via
the Discovery API. This includes the following for each resource:</p><ul><li>Name</li><li>Cluster or namespaced scope</li><li>Endpoint URL and supported verbs</li><li>Alternative names</li><li>Group, version, kind</li></ul><p>The API is available in both aggregated and unaggregated form. The aggregated
discovery serves two endpoints, while the unaggregated discovery serves a
separate endpoint for each group version.</p><h3 id="aggregated-discovery">Aggregated discovery</h3><div class="feature-state-notice feature-stable" title="Feature Gate: AggregatedDiscoveryEndpoint"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [stable]</code> (enabled by default: true)</div><p>Kubernetes offers stable support for <em>aggregated discovery</em>, publishing
all resources supported by a cluster through two endpoints (<code>/api</code> and
<code>/apis</code>). Requesting this
endpoint drastically reduces the number of requests sent to fetch the
discovery data from the cluster. You can access the data by
requesting the respective endpoints with an <code>Accept</code> header indicating
the aggregated discovery resource:
<code>Accept: application/json;v=v2;g=apidiscovery.k8s.io;as=APIGroupDiscoveryList</code>.</p><p>Without indicating the resource type using the <code>Accept</code> header, the default
response for the <code>/api</code> and <code>/apis</code> endpoint is an unaggregated discovery
document.</p><p>The <a href="https://github.com/kubernetes/kubernetes/blob/release-1.34/api/discovery/aggregated_v2.json">discovery document</a>
for the built-in resources can be found in the Kubernetes GitHub repository.
This Github document can be used as a reference of the base set of the available resources
if a Kubernetes cluster is not available to query.</p><p>The endpoint also supports ETag and protobuf encoding.</p><h3 id="unaggregated-discovery">Unaggregated discovery</h3><p>Without discovery aggregation, discovery is published in levels, with the root
endpoints publishing discovery information for downstream documents.</p><p>A list of all group versions supported by a cluster is published at
the <code>/api</code> and <code>/apis</code> endpoints. Example:</p><pre tabindex="0"><code>{
  "kind": "APIGroupList",
  "apiVersion": "v1",
  "groups": [
    {
      "name": "apiregistration.k8s.io",
      "versions": [
        {
          "groupVersion": "apiregistration.k8s.io/v1",
          "version": "v1"
        }
      ],
      "preferredVersion": {
        "groupVersion": "apiregistration.k8s.io/v1",
        "version": "v1"
      }
    },
    {
      "name": "apps",
      "versions": [
        {
          "groupVersion": "apps/v1",
          "version": "v1"
        }
      ],
      "preferredVersion": {
        "groupVersion": "apps/v1",
        "version": "v1"
      }
    },
    ...
}
</code></pre><p>Additional requests are needed to obtain the discovery document for each group version at
<code>/apis/&lt;group&gt;/&lt;version&gt;</code> (for example:
<code>/apis/rbac.authorization.k8s.io/v1alpha1</code>), which advertises the list of
resources served under a particular group version. These endpoints are used by
kubectl to fetch the list of resources supported by a cluster.</p><a id="#api-specification"><h2 id="openapi-interface-definition">OpenAPI interface definition</h2><p>For details about the OpenAPI specifications, see the <a href="https://www.openapis.org/">OpenAPI documentation</a>.</p><p>Kubernetes serves both OpenAPI v2.0 and OpenAPI v3.0. OpenAPI v3 is the
preferred method of accessing the OpenAPI because it offers a more comprehensive
(lossless) representation of Kubernetes resources. Due to limitations of OpenAPI
version 2, certain fields are dropped from the published OpenAPI including but not
limited to <code>default</code>, <code>nullable</code>, <code>oneOf</code>.</p><h3 id="openapi-v2">OpenAPI V2</h3><p>The Kubernetes API server serves an aggregated OpenAPI v2 spec via the
<code>/openapi/v2</code> endpoint. You can request the response format using
request headers as follows:</p><table><caption>Valid request header values for OpenAPI v2 queries</caption><thead><tr><th>Header</th><th>Possible values</th><th>Notes</th></tr></thead><tbody><tr><td><code>Accept-Encoding</code></td><td><code>gzip</code></td><td><em>not supplying this header is also acceptable</em></td></tr><tr><td rowspan="3"><code>Accept</code></td><td><code>application/com.github.proto-openapi.spec.v2@v1.0+protobuf</code></td><td><em>mainly for intra-cluster use</em></td></tr><tr><td><code>application/json</code></td><td><em>default</em></td></tr><tr><td><code>*</code></td><td><em>serves </em><code>application/json</code></td></tr></tbody></table><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>The validation rules published as part of OpenAPI schemas may not be complete, and usually aren't.
Additional validation occurs within the API server. If you want precise and complete verification,
a <code>kubectl apply --dry-run=server</code> runs all the applicable validation (and also activates admission-time
checks).</div><h3 id="openapi-v3">OpenAPI V3</h3><div class="feature-state-notice feature-stable" title="Feature Gate: OpenAPIV3"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [stable]</code> (enabled by default: true)</div><p>Kubernetes supports publishing a description of its APIs as OpenAPI v3.</p><p>A discovery endpoint <code>/openapi/v3</code> is provided to see a list of all
group/versions available. This endpoint only returns JSON. These
group/versions are provided in the following format:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>{<span>
</span></span></span><span><span><span>    </span><span>"paths": </span>{<span>
</span></span></span><span><span><span>        </span>...,<span>
</span></span></span><span><span><span>        </span><span>"api/v1": </span>{<span>
</span></span></span><span><span><span>            </span><span>"serverRelativeURL": </span><span>"/openapi/v3/api/v1?hash=CC0E9BFD992D8C59AEC98A1E2336F899E8318D3CF4C68944C3DEC640AF5AB52D864AC50DAA8D145B3494F75FA3CFF939FCBDDA431DAD3CA79738B297795818CF"</span><span>
</span></span></span><span><span><span>        </span>},<span>
</span></span></span><span><span><span>        </span><span>"apis/admissionregistration.k8s.io/v1": </span>{<span>
</span></span></span><span><span><span>            </span><span>"serverRelativeURL": </span><span>"/openapi/v3/apis/admissionregistration.k8s.io/v1?hash=E19CC93A116982CE5422FC42B590A8AFAD92CDE9AE4D59B5CAAD568F083AD07946E6CB5817531680BCE6E215C16973CD39003B0425F3477CFD854E89A9DB6597"</span><span>
</span></span></span><span><span><span>        </span>},<span>
</span></span></span><span><span><span>        </span>....<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>The relative URLs are pointing to immutable OpenAPI descriptions, in
order to improve client-side caching. The proper HTTP caching headers
are also set by the API server for that purpose (<code>Expires</code> to 1 year in
the future, and <code>Cache-Control</code> to <code>immutable</code>). When an obsolete URL is
used, the API server returns a redirect to the newest URL.</p><p>The Kubernetes API server publishes an OpenAPI v3 spec per Kubernetes
group version at the <code>/openapi/v3/apis/&lt;group&gt;/&lt;version&gt;?hash=&lt;hash&gt;</code>
endpoint.</p><p>Refer to the table below for accepted request headers.</p><table><caption>Valid request header values for OpenAPI v3 queries</caption><thead><tr><th>Header</th><th>Possible values</th><th>Notes</th></tr></thead><tbody><tr><td><code>Accept-Encoding</code></td><td><code>gzip</code></td><td><em>not supplying this header is also acceptable</em></td></tr><tr><td rowspan="3"><code>Accept</code></td><td><code>application/com.github.proto-openapi.spec.v3@v1.0+protobuf</code></td><td><em>mainly for intra-cluster use</em></td></tr><tr><td><code>application/json</code></td><td><em>default</em></td></tr><tr><td><code>*</code></td><td><em>serves </em><code>application/json</code></td></tr></tbody></table><p>A Golang implementation to fetch the OpenAPI V3 is provided in the package
<a href="https://pkg.go.dev/k8s.io/client-go/openapi3"><code>k8s.io/client-go/openapi3</code></a>.</p><p>Kubernetes 1.34 publishes
OpenAPI v2.0 and v3.0; there are no plans to support 3.1 in the near future.</p><h3 id="protobuf-serialization">Protobuf serialization</h3><p>Kubernetes implements an alternative Protobuf based serialization format that
is primarily intended for intra-cluster communication. For more information
about this format, see the <a href="https://git.k8s.io/design-proposals-archive/api-machinery/protobuf.md">Kubernetes Protobuf serialization</a>
design proposal and the
Interface Definition Language (IDL) files for each schema located in the Go
packages that define the API objects.</p><h2 id="persistence">Persistence</h2><p>Kubernetes stores the serialized state of objects by writing them into
<a class="glossary-tooltip" title="Consistent and highly-available key value store used as backing store of Kubernetes for all cluster data." href="/docs/tasks/administer-cluster/configure-upgrade-etcd/" target="_blank">etcd</a>.</p><h2 id="api-groups-and-versioning">API groups and versioning</h2><p>To make it easier to eliminate fields or restructure resource representations,
Kubernetes supports multiple API versions, each at a different API path, such
as <code>/api/v1</code> or <code>/apis/rbac.authorization.k8s.io/v1alpha1</code>.</p><p>Versioning is done at the API level rather than at the resource or field level
to ensure that the API presents a clear, consistent view of system resources
and behavior, and to enable controlling access to end-of-life and/or
experimental APIs.</p><p>To make it easier to evolve and to extend its API, Kubernetes implements
<a href="/docs/reference/using-api/#api-groups">API groups</a> that can be
<a href="/docs/reference/using-api/#enabling-or-disabling">enabled or disabled</a>.</p><p>API resources are distinguished by their API group, resource type, namespace
(for namespaced resources), and name. The API server handles the conversion between
API versions transparently: all the different versions are actually representations
of the same persisted data. The API server may serve the same underlying data
through multiple API versions.</p><p>For example, suppose there are two API versions, <code>v1</code> and <code>v1beta1</code>, for the same
resource. If you originally created an object using the <code>v1beta1</code> version of its
API, you can later read, update, or delete that object using either the <code>v1beta1</code>
or the <code>v1</code> API version, until the <code>v1beta1</code> version is deprecated and removed.
At that point you can continue accessing and modifying the object using the <code>v1</code> API.</p><h3 id="api-changes">API changes</h3><p>Any system that is successful needs to grow and change as new use cases emerge or existing ones change.
Therefore, Kubernetes has designed the Kubernetes API to continuously change and grow.
The Kubernetes project aims to <em>not</em> break compatibility with existing clients, and to maintain that
compatibility for a length of time so that other projects have an opportunity to adapt.</p><p>In general, new API resources and new resource fields can be added often and frequently.
Elimination of resources or fields requires following the
<a href="/docs/reference/using-api/deprecation-policy/">API deprecation policy</a>.</p><p>Kubernetes makes a strong commitment to maintain compatibility for official Kubernetes APIs
once they reach general availability (GA), typically at API version <code>v1</code>. Additionally,
Kubernetes maintains compatibility with data persisted via <em>beta</em> API versions of official Kubernetes APIs,
and ensures that data can be converted and accessed via GA API versions when the feature goes stable.</p><p>If you adopt a beta API version, you will need to transition to a subsequent beta or stable API version
once the API graduates. The best time to do this is while the beta API is in its deprecation period,
since objects are simultaneously accessible via both API versions. Once the beta API completes its
deprecation period and is no longer served, the replacement API version must be used.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Although Kubernetes also aims to maintain compatibility for <em>alpha</em> APIs versions, in some
circumstances this is not possible. If you use any alpha API versions, check the release notes
for Kubernetes when upgrading your cluster, in case the API did change in incompatible
ways that require deleting all existing alpha objects prior to upgrade.</div><p>Refer to <a href="/docs/reference/using-api/#api-versioning">API versions reference</a>
for more details on the API version level definitions.</p><h2 id="api-extension">API Extension</h2><p>The Kubernetes API can be extended in one of two ways:</p><ol><li><a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom resources</a>
let you declaratively define how the API server should provide your chosen resource API.</li><li>You can also extend the Kubernetes API by implementing an
<a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">aggregation layer</a>.</li></ol><h2 id="what-s-next">What's next</h2><ul><li>Learn how to extend the Kubernetes API by adding your own
<a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">CustomResourceDefinition</a>.</li><li><a href="/docs/concepts/security/controlling-access/">Controlling Access To The Kubernetes API</a> describes
how the cluster manages authentication and authorization for API access.</li><li>Learn about API endpoints, resource types and samples by reading
<a href="/docs/reference/kubernetes-api/">API Reference</a>.</li><li>Learn about what constitutes a compatible change, and how to change the API, from
<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#readme">API changes</a>.</li></ul></a></div></div><div><div class="td-content"><h1>Cluster Architecture</h1><div class="lead">The architectural concepts behind Kubernetes.</div><p>A Kubernetes cluster consists of a control plane plus a set of worker machines, called nodes,
that run containerized applications. Every cluster needs at least one worker node in order to run Pods.</p><p>The worker node(s) host the Pods that are the components of the application workload.
The control plane manages the worker nodes and the Pods in the cluster. In production
environments, the control plane usually runs across multiple computers and a cluster
usually runs multiple nodes, providing fault-tolerance and high availability.</p><p>This document outlines the various components you need to have for a complete and working Kubernetes cluster.</p><figure class="diagram-large"><img src="/images/docs/kubernetes-cluster-architecture.svg" alt="The control plane (kube-apiserver, etcd, kube-controller-manager, kube-scheduler) and several nodes. Each node is running a kubelet and kube-proxy."><figcaption><p>Figure 1. Kubernetes cluster components.</p></figcaption></figure><details><summary>About this architecture</summary><div class="details-inner"><p>The diagram in Figure 1 presents an example reference architecture for a Kubernetes cluster.
The actual distribution of components can vary based on specific cluster setups and requirements.</p><p>In the diagram, each node runs the <a href="#kube-proxy"><code>kube-proxy</code></a> component. You need a
network proxy component on each node to ensure that the
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a> API and associated behaviors
are available on your cluster network. However, some network plugins provide their own,
third party implementation of proxying. When you use that kind of network plugin,
the node does not need to run <code>kube-proxy</code>.</p></div></details><h2 id="control-plane-components">Control plane components</h2><p>The control plane's components make global decisions about the cluster (for example, scheduling),
as well as detecting and responding to cluster events (for example, starting up a new
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">pod</a> when a Deployment's
<code><a class="glossary-tooltip" title="Replicas are copies of pods, ensuring availability, scalability, and fault tolerance by maintaining identical instances." href="/docs/reference/glossary/?all=true#term-replica" target="_blank">replicas</a></code> field is unsatisfied).</p><p>Control plane components can be run on any machine in the cluster. However, for simplicity, setup scripts
typically start all control plane components on the same machine, and do not run user containers on this machine.
See <a href="/docs/setup/production-environment/tools/kubeadm/high-availability/">Creating Highly Available clusters with kubeadm</a>
for an example control plane setup that runs across multiple machines.</p><h3 id="kube-apiserver">kube-apiserver</h3><p>The API server is a component of the Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> that exposes the Kubernetes API.
The API server is the front end for the Kubernetes control plane.</p><p>The main implementation of a Kubernetes API server is <a href="/docs/reference/generated/kube-apiserver/">kube-apiserver</a>.
kube-apiserver is designed to scale horizontally&#8212;that is, it scales by deploying more instances.
You can run several instances of kube-apiserver and balance traffic between those instances.</p><h3 id="etcd">etcd</h3><p>Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</p><p>If your Kubernetes cluster uses etcd as its backing store, make sure you have a
<a href="/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster">back up</a> plan
for the data.</p><p>You can find in-depth information about etcd in the official <a href="https://etcd.io/docs/">documentation</a>.</p><h3 id="kube-scheduler">kube-scheduler</h3><p>Control plane component that watches for newly created
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> with no assigned
<a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a>, and selects a node for them
to run on.</p><p>Factors taken into account for scheduling decisions include:
individual and collective <a class="glossary-tooltip" title="A defined amount of infrastructure available for consumption (CPU, memory, etc)." href="/docs/reference/glossary/?all=true#term-infrastructure-resource" target="_blank">resource</a>
requirements, hardware/software/policy constraints, affinity and anti-affinity specifications,
data locality, inter-workload interference, and deadlines.</p><h3 id="kube-controller-manager">kube-controller-manager</h3><p>Control plane component that runs <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> processes.</p><p>Logically, each <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.</p><p>There are many different types of controllers. Some examples of them are:</p><ul><li>Node controller: Responsible for noticing and responding when nodes go down.</li><li>Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.</li><li>EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods).</li><li>ServiceAccount controller: Create default ServiceAccounts for new namespaces.</li></ul><p>The above is not an exhaustive list.</p><h3 id="cloud-controller-manager">cloud-controller-manager</h3>A Kubernetes <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.<p>The cloud-controller-manager only runs controllers that are specific to your cloud provider.
If you are running Kubernetes on your own premises, or in a learning environment inside your
own PC, the cluster does not have a cloud controller manager.</p><p>As with the kube-controller-manager, the cloud-controller-manager combines several logically
independent control loops into a single binary that you run as a single process. You can scale
horizontally (run more than one copy) to improve performance or to help tolerate failures.</p><p>The following controllers can have cloud provider dependencies:</p><ul><li>Node controller: For checking the cloud provider to determine if a node has been
deleted in the cloud after it stops responding</li><li>Route controller: For setting up routes in the underlying cloud infrastructure</li><li>Service controller: For creating, updating and deleting cloud provider load balancers</li></ul><hr><h2 id="node-components">Node components</h2><p>Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.</p><h3 id="kubelet">kubelet</h3><p>An agent that runs on each <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a> in the cluster. It makes sure that <a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." href="/docs/concepts/containers/" target="_blank">containers</a> are running in a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>.</p><p>The <a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> takes a set of PodSpecs that
are provided through various mechanisms and ensures that the containers described in those
PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by
Kubernetes.</p><h3 id="kube-proxy">kube-proxy (optional)</h3><p><p>kube-proxy is a network proxy that runs on each
<a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a> in your cluster,
implementing part of the Kubernetes
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a> concept.</p><p><a href="/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a>
maintains network rules on nodes. These network rules allow network
communication to your Pods from network sessions inside or outside of
your cluster.</p><p>kube-proxy uses the operating system packet filtering layer if there is one
and it's available. Otherwise, kube-proxy forwards the traffic itself.</p>If you use a <a href="#network-plugins">network plugin</a> that implements packet forwarding for Services
by itself, and providing equivalent behavior to kube-proxy, then you do not need to run
kube-proxy on the nodes in your cluster.</p><h3 id="container-runtime">Container runtime</h3><p>A fundamental component that empowers Kubernetes to run containers effectively.
It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.</p><p>Kubernetes supports container runtimes such as
<a class="glossary-tooltip" title="A container runtime with an emphasis on simplicity, robustness and portability" href="https://containerd.io/docs/" target="_blank">containerd</a>, <a class="glossary-tooltip" title="A lightweight container runtime specifically for Kubernetes" href="https://cri-o.io/#what-is-cri-o" target="_blank">CRI-O</a>,
and any other implementation of the <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">Kubernetes CRI (Container Runtime
Interface)</a>.</p><h2 id="addons">Addons</h2><p>Addons use Kubernetes resources (<a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a>,
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a>, etc) to implement cluster features.
Because these are providing cluster-level features, namespaced resources for
addons belong within the <code>kube-system</code> namespace.</p><p>Selected addons are described below; for an extended list of available addons,
please see <a href="/docs/concepts/cluster-administration/addons/">Addons</a>.</p><h3 id="dns">DNS</h3><p>While the other addons are not strictly required, all Kubernetes clusters should have
<a href="/docs/concepts/services-networking/dns-pod-service/">cluster DNS</a>, as many examples rely on it.</p><p>Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment,
which serves DNS records for Kubernetes services.</p><p>Containers started by Kubernetes automatically include this DNS server in their DNS searches.</p><h3 id="web-ui-dashboard">Web UI (Dashboard)</h3><p><a href="/docs/tasks/access-application-cluster/web-ui-dashboard/">Dashboard</a> is a general purpose,
web-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications
running in the cluster, as well as the cluster itself.</p><h3 id="container-resource-monitoring">Container resource monitoring</h3><p><a href="/docs/tasks/debug/debug-cluster/resource-usage-monitoring/">Container Resource Monitoring</a>
records generic time-series metrics about containers in a central database, and provides a UI for browsing that data.</p><h3 id="cluster-level-logging">Cluster-level Logging</h3><p>A <a href="/docs/concepts/cluster-administration/logging/">cluster-level logging</a> mechanism is responsible
for saving container logs to a central log store with a search/browsing interface.</p><h3 id="network-plugins">Network plugins</h3><p><a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">Network plugins</a>
are software components that implement the container network interface (CNI) specification.
They are responsible for allocating IP addresses to pods and enabling them to communicate
with each other within the cluster.</p><h2 id="architecture-variations">Architecture variations</h2><p>While the core components of Kubernetes remain consistent, the way they are deployed and
managed can vary. Understanding these variations is crucial for designing and maintaining
Kubernetes clusters that meet specific operational needs.</p><h3 id="control-plane-deployment-options">Control plane deployment options</h3><p>The control plane components can be deployed in several ways:</p><dl><dt>Traditional deployment</dt><dd>Control plane components run directly on dedicated machines or VMs, often managed as systemd services.</dd><dt>Static Pods</dt><dd>Control plane components are deployed as static Pods, managed by the kubelet on specific nodes.
This is a common approach used by tools like kubeadm.</dd><dt>Self-hosted</dt><dd>The control plane runs as Pods within the Kubernetes cluster itself, managed by Deployments
and StatefulSets or other Kubernetes primitives.</dd><dt>Managed Kubernetes services</dt><dd>Cloud providers often abstract away the control plane, managing its components as part of their service offering.</dd></dl><h3 id="workload-placement-considerations">Workload placement considerations</h3><p>The placement of workloads, including the control plane components, can vary based on cluster size,
performance requirements, and operational policies:</p><ul><li>In smaller or development clusters, control plane components and user workloads might run on the same nodes.</li><li>Larger production clusters often dedicate specific nodes to control plane components,
separating them from user workloads.</li><li>Some organizations run critical add-ons or monitoring tools on control plane nodes.</li></ul><h3 id="cluster-management-tools">Cluster management tools</h3><p>Tools like kubeadm, kops, and Kubespray offer different approaches to deploying and managing clusters,
each with its own method of component layout and management.</p><p>The flexibility of Kubernetes architecture allows organizations to tailor their clusters to specific needs,
balancing factors such as operational complexity, performance, and management overhead.</p><h3 id="customization-and-extensibility">Customization and extensibility</h3><p>Kubernetes architecture allows for significant customization:</p><ul><li>Custom schedulers can be deployed to work alongside the default Kubernetes scheduler or to replace it entirely.</li><li>API servers can be extended with CustomResourceDefinitions and API Aggregation.</li><li>Cloud providers can integrate deeply with Kubernetes using the cloud-controller-manager.</li></ul><p>The flexibility of Kubernetes architecture allows organizations to tailor their clusters to specific needs,
balancing factors such as operational complexity, performance, and management overhead.</p><h2 id="what-s-next">What's next</h2><p>Learn more about the following:</p><ul><li><a href="/docs/concepts/architecture/nodes/">Nodes</a> and
<a href="/docs/concepts/architecture/control-plane-node-communication/">their communication</a>
with the control plane.</li><li>Kubernetes <a href="/docs/concepts/architecture/controller/">controllers</a>.</li><li><a href="/docs/concepts/scheduling-eviction/kube-scheduler/">kube-scheduler</a> which is the default scheduler for Kubernetes.</li><li>Etcd's official <a href="https://etcd.io/docs/">documentation</a>.</li><li>Several <a href="/docs/setup/production-environment/container-runtimes/">container runtimes</a> in Kubernetes.</li><li>Integrating with cloud providers using <a href="/docs/concepts/architecture/cloud-controller/">cloud-controller-manager</a>.</li><li><a href="/docs/reference/generated/kubectl/kubectl-commands">kubectl</a> commands.</li></ul><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/concepts/architecture/nodes/">Nodes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/architecture/control-plane-node-communication/">Communication between Nodes and the Control Plane</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/architecture/controller/">Controllers</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/architecture/leases/">Leases</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/architecture/cloud-controller/">Cloud Controller Manager</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/architecture/cgroups/">About cgroup v2</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/architecture/self-healing/">Kubernetes Self-Healing</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/architecture/garbage-collection/">Garbage Collection</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/architecture/mixed-version-proxy/">Mixed Version Proxy</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Nodes</h1><p>Kubernetes runs your <a class="glossary-tooltip" title="A workload is an application running on Kubernetes." href="/docs/concepts/workloads/" target="_blank">workload</a>
by placing containers into Pods to run on <em>Nodes</em>.
A node may be a virtual or physical machine, depending on the cluster. Each node
is managed by the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>
and contains the services necessary to run
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>.</p><p>Typically you have several nodes in a cluster; in a learning or resource-limited
environment, you might have only one node.</p><p>The <a href="/docs/concepts/architecture/#node-components">components</a> on a node include the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a>, a
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>, and the
<a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank">kube-proxy</a>.</p><h2 id="management">Management</h2><p>There are two main ways to have Nodes added to the
<a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a>:</p><ol><li>The kubelet on a node self-registers to the control plane</li><li>You (or another human user) manually add a Node object</li></ol><p>After you create a Node <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">object</a>,
or the kubelet on a node self-registers, the control plane checks whether the new Node object
is valid. For example, if you try to create a Node from the following JSON manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"kind"</span>: <span>"Node"</span>,
</span></span><span><span>  <span>"apiVersion"</span>: <span>"v1"</span>,
</span></span><span><span>  <span>"metadata"</span>: {
</span></span><span><span>    <span>"name"</span>: <span>"10.240.79.157"</span>,
</span></span><span><span>    <span>"labels"</span>: {
</span></span><span><span>      <span>"name"</span>: <span>"my-first-k8s-node"</span>
</span></span><span><span>    }
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><p>Kubernetes creates a Node object internally (the representation). Kubernetes checks
that a kubelet has registered to the API server that matches the <code>metadata.name</code>
field of the Node. If the node is healthy (i.e. all necessary services are running),
then it is eligible to run a Pod. Otherwise, that node is ignored for any cluster activity
until it becomes healthy.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Kubernetes keeps the object for the invalid Node and continues checking to see whether
it becomes healthy.</p><p>You, or a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>, must explicitly
delete the Node object to stop that health checking.</p></div><p>The name of a Node object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><h3 id="node-name-uniqueness">Node name uniqueness</h3><p>The <a href="/docs/concepts/overview/working-with-objects/names/#names">name</a> identifies a Node. Two Nodes
cannot have the same name at the same time. Kubernetes also assumes that a resource with the same
name is the same object. In case of a Node, it is implicitly assumed that an instance using the
same name will have the same state (e.g. network settings, root disk contents) and attributes like
node labels. This may lead to inconsistencies if an instance was modified without changing its name.
If the Node needs to be replaced or updated significantly, the existing Node object needs to be
removed from API server first and re-added after the update.</p><h3 id="self-registration-of-nodes">Self-registration of Nodes</h3><p>When the kubelet flag <code>--register-node</code> is true (the default), the kubelet will attempt to
register itself with the API server. This is the preferred pattern, used by most distros.</p><p>For self-registration, the kubelet is started with the following options:</p><ul><li><p><code>--kubeconfig</code> - Path to credentials to authenticate itself to the API server.</p></li><li><p><code>--cloud-provider</code> - How to talk to a <a class="glossary-tooltip" title="An organization that offers a cloud computing platform." href="/docs/reference/glossary/?all=true#term-cloud-provider" target="_blank">cloud provider</a>
to read metadata about itself.</p></li><li><p><code>--register-node</code> - Automatically register with the API server.</p></li><li><p><code>--register-with-taints</code> - Register the node with the given list of
<a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">taints</a> (comma separated <code>&lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</code>).</p><p>No-op if <code>register-node</code> is false.</p></li><li><p><code>--node-ip</code> - Optional comma-separated list of the IP addresses for the node.
You can only specify a single address for each address family.
For example, in a single-stack IPv4 cluster, you set this value to be the IPv4 address that the
kubelet should use for the node.
See <a href="/docs/concepts/services-networking/dual-stack/#configure-ipv4-ipv6-dual-stack">configure IPv4/IPv6 dual stack</a>
for details of running a dual-stack cluster.</p><p>If you don't provide this argument, the kubelet uses the node's default IPv4 address, if any;
if the node has no IPv4 addresses then the kubelet uses the node's default IPv6 address.</p></li><li><p><code>--node-labels</code> - <a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">Labels</a> to add when registering the node
in the cluster (see label restrictions enforced by the
<a href="/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction admission plugin</a>).</p></li><li><p><code>--node-status-update-frequency</code> - Specifies how often kubelet posts its node status to the API server.</p></li></ul><p>When the <a href="/docs/reference/access-authn-authz/node/">Node authorization mode</a> and
<a href="/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction admission plugin</a>
are enabled, kubelets are only authorized to create/modify their own Node resource.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>As mentioned in the <a href="#node-name-uniqueness">Node name uniqueness</a> section,
when Node configuration needs to be updated, it is a good practice to re-register
the node with the API server. For example, if the kubelet is being restarted with
a new set of <code>--node-labels</code>, but the same Node name is used, the change will
not take effect, as labels are only set (or modified) upon Node registration with the API server.</p><p>Pods already scheduled on the Node may misbehave or cause issues if the Node
configuration will be changed on kubelet restart. For example, already running
Pod may be tainted against the new labels assigned to the Node, while other
Pods, that are incompatible with that Pod will be scheduled based on this new
label. Node re-registration ensures all Pods will be drained and properly
re-scheduled.</p></div><h3 id="manual-node-administration">Manual Node administration</h3><p>You can create and modify Node objects using
<a class="glossary-tooltip" title="A command line tool for communicating with a Kubernetes cluster." href="/docs/reference/kubectl/" target="_blank">kubectl</a>.</p><p>When you want to create Node objects manually, set the kubelet flag <code>--register-node=false</code>.</p><p>You can modify Node objects regardless of the setting of <code>--register-node</code>.
For example, you can set labels on an existing Node or mark it unschedulable.</p><p>You can set optional node role(s) for nodes by adding one or more <code>node-role.kubernetes.io/&lt;role&gt;: &lt;role&gt;</code> labels to the node where characters of <code>&lt;role&gt;</code>
are limited by the <a href="/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">syntax</a> rules for labels.</p><p>Kubernetes ignores the label value for node roles; by convention, you can set it to the same string you used for the node role in the label key.</p><p>You can use labels on Nodes in conjunction with node selectors on Pods to control
scheduling. For example, you can constrain a Pod to only be eligible to run on
a subset of the available nodes.</p><p>Marking a node as unschedulable prevents the scheduler from placing new pods onto
that Node but does not affect existing Pods on the Node. This is useful as a
preparatory step before a node reboot or other maintenance.</p><p>To mark a Node unschedulable, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl cordon <span>$NODENAME</span>
</span></span></code></pre></div><p>See <a href="/docs/tasks/administer-cluster/safely-drain-node/">Safely Drain a Node</a>
for more details.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Pods that are part of a <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a> tolerate
being run on an unschedulable Node. DaemonSets typically provide node-local services
that should run on the Node even if it is being drained of workload applications.</div><h2 id="node-status">Node status</h2><p>A Node's status contains the following information:</p><ul><li><a href="/docs/reference/node/node-status/#addresses">Addresses</a></li><li><a href="/docs/reference/node/node-status/#condition">Conditions</a></li><li><a href="/docs/reference/node/node-status/#capacity">Capacity and Allocatable</a></li><li><a href="/docs/reference/node/node-status/#info">Info</a></li></ul><p>You can use <code>kubectl</code> to view a Node's status and other details:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe node &lt;insert-node-name-here&gt;
</span></span></code></pre></div><p>See <a href="/docs/reference/node/node-status/">Node Status</a> for more details.</p><h2 id="node-heartbeats">Node heartbeats</h2><p>Heartbeats, sent by Kubernetes nodes, help your cluster determine the
availability of each node, and to take action when failures are detected.</p><p>For nodes there are two forms of heartbeats:</p><ul><li>Updates to the <a href="/docs/reference/node/node-status/"><code>.status</code></a> of a Node.</li><li><a href="/docs/concepts/architecture/leases/">Lease</a> objects
within the <code>kube-node-lease</code>
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>.
Each Node has an associated Lease object.</li></ul><h2 id="node-controller">Node controller</h2><p>The node <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> is a
Kubernetes control plane component that manages various aspects of nodes.</p><p>The node controller has multiple roles in a node's life. The first is assigning a
CIDR block to the node when it is registered (if CIDR assignment is turned on).</p><p>The second is keeping the node controller's internal list of nodes up to date with
the cloud provider's list of available machines. When running in a cloud
environment and whenever a node is unhealthy, the node controller asks the cloud
provider if the VM for that node is still available. If not, the node
controller deletes the node from its list of nodes.</p><p>The third is monitoring the nodes' health. The node controller is
responsible for:</p><ul><li>In the case that a node becomes unreachable, updating the <code>Ready</code> condition
in the Node's <code>.status</code> field. In this case the node controller sets the
<code>Ready</code> condition to <code>Unknown</code>.</li><li>If a node remains unreachable: triggering
<a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated eviction</a>
for all of the Pods on the unreachable node. By default, the node controller
waits 5 minutes between marking the node as <code>Unknown</code> and submitting
the first eviction request.</li></ul><p>By default, the node controller checks the state of each node every 5 seconds.
This period can be configured using the <code>--node-monitor-period</code> flag on the
<code>kube-controller-manager</code> component.</p><h3 id="rate-limits-on-eviction">Rate limits on eviction</h3><p>In most cases, the node controller limits the eviction rate to
<code>--node-eviction-rate</code> (default 0.1) per second, meaning it won't evict pods
from more than 1 node per 10 seconds.</p><p>The node eviction behavior changes when a node in a given availability zone
becomes unhealthy. The node controller checks what percentage of nodes in the zone
are unhealthy (the <code>Ready</code> condition is <code>Unknown</code> or <code>False</code>) at the same time:</p><ul><li>If the fraction of unhealthy nodes is at least <code>--unhealthy-zone-threshold</code>
(default 0.55), then the eviction rate is reduced.</li><li>If the cluster is small (i.e. has less than or equal to
<code>--large-cluster-size-threshold</code> nodes - default 50), then evictions are stopped.</li><li>Otherwise, the eviction rate is reduced to <code>--secondary-node-eviction-rate</code>
(default 0.01) per second.</li></ul><p>The reason these policies are implemented per availability zone is because one
availability zone might become partitioned from the control plane while the others remain
connected. If your cluster does not span multiple cloud provider availability zones,
then the eviction mechanism does not take per-zone unavailability into account.</p><p>A key reason for spreading your nodes across availability zones is so that the
workload can be shifted to healthy zones when one entire zone goes down.
Therefore, if all nodes in a zone are unhealthy, then the node controller evicts at
the normal rate of <code>--node-eviction-rate</code>. The corner case is when all zones are
completely unhealthy (none of the nodes in the cluster are healthy). In such a
case, the node controller assumes that there is some problem with connectivity
between the control plane and the nodes, and doesn't perform any evictions.
(If there has been an outage and some nodes reappear, the node controller does
evict pods from the remaining nodes that are unhealthy or unreachable).</p><p>The node controller is also responsible for evicting pods running on nodes with
<code>NoExecute</code> taints, unless those pods tolerate that taint.
The node controller also adds <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">taints</a>
corresponding to node problems like node unreachable or not ready. This means
that the scheduler won't place Pods onto unhealthy nodes.</p><h2 id="node-capacity">Resource capacity tracking</h2><p>Node objects track information about the Node's resource capacity: for example, the amount
of memory available and the number of CPUs.
Nodes that <a href="#self-registration-of-nodes">self register</a> report their capacity during
registration. If you <a href="#manual-node-administration">manually</a> add a Node, then
you need to set the node's capacity information when you add it.</p><p>The Kubernetes <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">scheduler</a> ensures that
there are enough resources for all the Pods on a Node. The scheduler checks that the sum
of the requests of containers on the node is no greater than the node's capacity.
That sum of requests includes all containers managed by the kubelet, but excludes any
containers started directly by the container runtime, and also excludes any
processes running outside of the kubelet's control.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you want to explicitly reserve resources for non-Pod processes, see
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved">reserve resources for system daemons</a>.</div><h2 id="node-topology">Node topology</h2><div class="feature-state-notice feature-stable" title="Feature Gate: TopologyManager"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [stable]</code> (enabled by default: true)</div><p>If you have enabled the <code>TopologyManager</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>, then
the kubelet can use topology hints when making resource assignment decisions.
See <a href="/docs/tasks/administer-cluster/topology-manager/">Control Topology Management Policies on a Node</a>
for more information.</p><h2 id="what-s-next">What's next</h2><p>Learn more about the following:</p><ul><li><a href="/docs/concepts/architecture/#node-components">Components</a> that make up a node.</li><li><a href="/docs/reference/generated/kubernetes-api/v1.34/#node-v1-core">API definition for Node</a>.</li><li><a href="https://git.k8s.io/design-proposals-archive/architecture/architecture.md#the-kubernetes-node">Node</a>
section of the architecture design document.</li><li><a href="/docs/concepts/cluster-administration/node-shutdown/">Graceful/non-graceful node shutdown</a>.</li><li><a href="/docs/concepts/cluster-administration/node-autoscaling/">Node autoscaling</a> to
manage the number and size of nodes in your cluster.</li><li><a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">Taints and Tolerations</a>.</li><li><a href="/docs/concepts/policy/node-resource-managers/">Node Resource Managers</a>.</li><li><a href="/docs/concepts/configuration/windows-resource-management/">Resource Management for Windows nodes</a>.</li></ul></div></div><div><div class="td-content"><h1>Communication between Nodes and the Control Plane</h1><p>This document catalogs the communication paths between the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a>
and the Kubernetes <a class="glossary-tooltip" title="A set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node." href="/docs/reference/glossary/?all=true#term-cluster" target="_blank">cluster</a>.
The intent is to allow users to customize their installation to harden the network configuration
such that the cluster can be run on an untrusted network (or on fully public IPs on a cloud
provider).</p><h2 id="node-to-control-plane">Node to Control Plane</h2><p>Kubernetes has a "hub-and-spoke" API pattern. All API usage from nodes (or the pods they run)
terminates at the API server. None of the other control plane components are designed to expose
remote services. The API server is configured to listen for remote connections on a secure HTTPS
port (typically 443) with one or more forms of client
<a href="/docs/reference/access-authn-authz/authentication/">authentication</a> enabled.
One or more forms of <a href="/docs/reference/access-authn-authz/authorization/">authorization</a> should be
enabled, especially if <a href="/docs/reference/access-authn-authz/authentication/#anonymous-requests">anonymous requests</a>
or <a href="/docs/reference/access-authn-authz/authentication/#service-account-tokens">service account tokens</a>
are allowed.</p><p>Nodes should be provisioned with the public root <a class="glossary-tooltip" title="A cryptographically secure file used to validate access to the Kubernetes cluster." href="/docs/tasks/tls/managing-tls-in-a-cluster/" target="_blank">certificate</a> for the cluster such that they can
connect securely to the API server along with valid client credentials. A good approach is that the
client credentials provided to the kubelet are in the form of a client certificate. See
<a href="/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/">kubelet TLS bootstrapping</a>
for automated provisioning of kubelet client certificates.</p><p><a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> that wish to connect to the API server can do so securely by leveraging a service account so
that Kubernetes will automatically inject the public root certificate and a valid bearer token
into the pod when it is instantiated.
The <code>kubernetes</code> service (in <code>default</code> namespace) is configured with a virtual IP address that is
redirected (via <code><a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank">kube-proxy</a></code>) to the HTTPS endpoint on the API server.</p><p>The control plane components also communicate with the API server over the secure port.</p><p>As a result, the default operating mode for connections from the nodes and pod running on the
nodes to the control plane is secured by default and can run over untrusted and/or public
networks.</p><h2 id="control-plane-to-node">Control plane to node</h2><p>There are two primary communication paths from the control plane (the API server) to the nodes.
The first is from the API server to the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> process which runs on each node in the cluster.
The second is from the API server to any node, pod, or service through the API server's <em>proxy</em>
functionality.</p><h3 id="api-server-to-kubelet">API server to kubelet</h3><p>The connections from the API server to the kubelet are used for:</p><ul><li>Fetching logs for pods.</li><li>Attaching (usually through <code>kubectl</code>) to running pods.</li><li>Providing the kubelet's port-forwarding functionality.</li></ul><p>These connections terminate at the kubelet's HTTPS endpoint. By default, the API server does not
verify the kubelet's serving certificate, which makes the connection subject to man-in-the-middle
attacks and <strong>unsafe</strong> to run over untrusted and/or public networks.</p><p>To verify this connection, use the <code>--kubelet-certificate-authority</code> flag to provide the API
server with a root certificate bundle to use to verify the kubelet's serving certificate.</p><p>If that is not possible, use <a href="#ssh-tunnels">SSH tunneling</a> between the API server and kubelet if
required to avoid connecting over an
untrusted or public network.</p><p>Finally, <a href="/docs/reference/access-authn-authz/kubelet-authn-authz/">Kubelet authentication and/or authorization</a>
should be enabled to secure the kubelet API.</p><h3 id="api-server-to-nodes-pods-and-services">API server to nodes, pods, and services</h3><p>The connections from the API server to a node, pod, or service default to plain HTTP connections
and are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS
connection by prefixing <code>https:</code> to the node, pod, or service name in the API URL, but they will
not validate the certificate provided by the HTTPS endpoint nor provide client credentials. So
while the connection will be encrypted, it will not provide any guarantees of integrity. These
connections <strong>are not currently safe</strong> to run over untrusted or public networks.</p><h3 id="ssh-tunnels">SSH tunnels</h3><p>Kubernetes supports <a href="https://www.ssh.com/academy/ssh/tunneling">SSH tunnels</a> to protect the control plane to nodes communication paths. In this
configuration, the API server initiates an SSH tunnel to each node in the cluster (connecting to
the SSH server listening on port 22) and passes all traffic destined for a kubelet, node, pod, or
service through the tunnel.
This tunnel ensures that the traffic is not exposed outside of the network in which the nodes are
running.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>SSH tunnels are currently deprecated, so you shouldn't opt to use them unless you know what you
are doing. The <a href="#konnectivity-service">Konnectivity service</a> is a replacement for this
communication channel.</div><h3 id="konnectivity-service">Konnectivity service</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [beta]</code></div><p>As a replacement to the SSH tunnels, the Konnectivity service provides TCP level proxy for the
control plane to cluster communication. The Konnectivity service consists of two parts: the
Konnectivity server in the control plane network and the Konnectivity agents in the nodes network.
The Konnectivity agents initiate connections to the Konnectivity server and maintain the network
connections.
After enabling the Konnectivity service, all control plane to nodes traffic goes through these
connections.</p><p>Follow the <a href="/docs/tasks/extend-kubernetes/setup-konnectivity/">Konnectivity service task</a> to set
up the Konnectivity service in your cluster.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about the <a href="/docs/concepts/architecture/#control-plane-components">Kubernetes control plane components</a></li><li>Learn more about <a href="https://book.kubebuilder.io/multiversion-tutorial/conversion-concepts.html#hubs-spokes-and-other-wheel-metaphors">Hubs and Spoke model</a></li><li>Learn how to <a href="/docs/tasks/administer-cluster/securing-a-cluster/">Secure a Cluster</a></li><li>Learn more about the <a href="/docs/concepts/overview/kubernetes-api/">Kubernetes API</a></li><li><a href="/docs/tasks/extend-kubernetes/setup-konnectivity/">Set up Konnectivity service</a></li><li><a href="/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">Use Port Forwarding to Access Applications in a Cluster</a></li><li>Learn how to <a href="/docs/tasks/debug/debug-application/debug-running-pod/#examine-pod-logs">Fetch logs for Pods</a>, <a href="/docs/tasks/access-application-cluster/port-forward-access-application-cluster/#forward-a-local-port-to-a-port-on-the-pod">use kubectl port-forward</a></li></ul></div></div><div><div class="td-content"><h1>Controllers</h1><p>In robotics and automation, a <em>control loop</em> is
a non-terminating loop that regulates the state of a system.</p><p>Here is one example of a control loop: a thermostat in a room.</p><p>When you set the temperature, that's telling the thermostat
about your <em>desired state</em>. The actual room temperature is the
<em>current state</em>. The thermostat acts to bring the current state
closer to the desired state, by turning equipment on or off.</p>In Kubernetes, controllers are control loops that watch the state of your
<a class="glossary-tooltip" title="A set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node." href="/docs/reference/glossary/?all=true#term-cluster" target="_blank">cluster</a>, then make or request
changes where needed.
Each controller tries to move the current cluster state closer to the desired
state.<h2 id="controller-pattern">Controller pattern</h2><p>A controller tracks at least one Kubernetes resource type.
These <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">objects</a>
have a spec field that represents the desired state. The
controller(s) for that resource are responsible for making the current
state come closer to that desired state.</p><p>The controller might carry the action out itself; more commonly, in Kubernetes,
a controller will send messages to the
<a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a> that have
useful side effects. You'll see examples of this below.</p><h3 id="control-via-api-server">Control via API server</h3><p>The <a class="glossary-tooltip" title="A finite or batch task that runs to completion." href="/docs/concepts/workloads/controllers/job/" target="_blank">Job</a> controller is an example of a
Kubernetes built-in controller. Built-in controllers manage state by
interacting with the cluster API server.</p><p>Job is a Kubernetes resource that runs a
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>, or perhaps several Pods, to carry out
a task and then stop.</p><p>(Once <a href="/docs/concepts/scheduling-eviction/">scheduled</a>, Pod objects become part of the
desired state for a kubelet).</p><p>When the Job controller sees a new task it makes sure that, somewhere
in your cluster, the kubelets on a set of Nodes are running the right
number of Pods to get the work done.
The Job controller does not run any Pods or containers
itself. Instead, the Job controller tells the API server to create or remove
Pods.
Other components in the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>
act on the new information (there are new Pods to schedule and run),
and eventually the work is done.</p><p>After you create a new Job, the desired state is for that Job to be completed.
The Job controller makes the current state for that Job be nearer to your
desired state: creating Pods that do the work you wanted for that Job, so that
the Job is closer to completion.</p><p>Controllers also update the objects that configure them.
For example: once the work is done for a Job, the Job controller
updates that Job object to mark it <code>Finished</code>.</p><p>(This is a bit like how some thermostats turn a light off to
indicate that your room is now at the temperature you set).</p><h3 id="direct-control">Direct control</h3><p>In contrast with Job, some controllers need to make changes to
things outside of your cluster.</p><p>For example, if you use a control loop to make sure there
are enough <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">Nodes</a>
in your cluster, then that controller needs something outside the
current cluster to set up new Nodes when needed.</p><p>Controllers that interact with external state find their desired state from
the API server, then communicate directly with an external system to bring
the current state closer in line.</p><p>(There actually is a <a href="https://github.com/kubernetes/autoscaler/">controller</a>
that horizontally scales the nodes in your cluster.)</p><p>The important point here is that the controller makes some changes to bring about
your desired state, and then reports the current state back to your cluster's API server.
Other control loops can observe that reported data and take their own actions.</p><p>In the thermostat example, if the room is very cold then a different controller
might also turn on a frost protection heater. With Kubernetes clusters, the control
plane indirectly works with IP address management tools, storage services,
cloud provider APIs, and other services by
<a href="/docs/concepts/extend-kubernetes/">extending Kubernetes</a> to implement that.</p><h2 id="desired-vs-current">Desired versus current state</h2><p>Kubernetes takes a cloud-native view of systems, and is able to handle
constant change.</p><p>Your cluster could be changing at any point as work happens and
control loops automatically fix failures. This means that,
potentially, your cluster never reaches a stable state.</p><p>As long as the controllers for your cluster are running and able to make
useful changes, it doesn't matter if the overall state is stable or not.</p><h2 id="design">Design</h2><p>As a tenet of its design, Kubernetes uses lots of controllers that each manage
a particular aspect of cluster state. Most commonly, a particular control loop
(controller) uses one kind of resource as its desired state, and has a different
kind of resource that it manages to make that desired state happen. For example,
a controller for Jobs tracks Job objects (to discover new work) and Pod objects
(to run the Jobs, and then to see when the work is finished). In this case
something else creates the Jobs, whereas the Job controller creates Pods.</p><p>It's useful to have simple controllers rather than one, monolithic set of control
loops that are interlinked. Controllers can fail, so Kubernetes is designed to
allow for that.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>There can be several controllers that create or update the same kind of object.
Behind the scenes, Kubernetes controllers make sure that they only pay attention
to the resources linked to their controlling resource.</p><p>For example, you can have Deployments and Jobs; these both create Pods.
The Job controller does not delete the Pods that your Deployment created,
because there is information (<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">labels</a>)
the controllers can use to tell those Pods apart.</p></div><h2 id="running-controllers">Ways of running controllers</h2><p>Kubernetes comes with a set of built-in controllers that run inside
the <a class="glossary-tooltip" title="Control Plane component that runs controller processes." href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank">kube-controller-manager</a>. These
built-in controllers provide important core behaviors.</p><p>The Deployment controller and Job controller are examples of controllers that
come as part of Kubernetes itself ("built-in" controllers).
Kubernetes lets you run a resilient control plane, so that if any of the built-in
controllers were to fail, another part of the control plane will take over the work.</p><p>You can find controllers that run outside the control plane, to extend Kubernetes.
Or, if you want, you can write a new controller yourself.
You can run your own controller as a set of Pods,
or externally to Kubernetes. What fits best will depend on what that particular
controller does.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about the <a href="/docs/concepts/architecture/#control-plane-components">Kubernetes control plane</a></li><li>Discover some of the basic <a href="/docs/concepts/overview/working-with-objects/">Kubernetes objects</a></li><li>Learn more about the <a href="/docs/concepts/overview/kubernetes-api/">Kubernetes API</a></li><li>If you want to write your own controller, see
<a href="/docs/concepts/extend-kubernetes/#extension-patterns">Kubernetes extension patterns</a>
and the <a href="https://github.com/kubernetes/sample-controller">sample-controller</a> repository.</li></ul></div></div><div><div class="td-content"><h1>Leases</h1><p>Distributed systems often have a need for <em>leases</em>, which provide a mechanism to lock shared resources
and coordinate activity between members of a set.
In Kubernetes, the lease concept is represented by <a href="/docs/reference/kubernetes-api/cluster-resources/lease-v1/">Lease</a>
objects in the <code>coordination.k8s.io</code> <a class="glossary-tooltip" title="A set of related paths in the Kubernetes API." href="/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning" target="_blank">API Group</a>,
which are used for system-critical capabilities such as node heartbeats and component-level leader election.</p><h2 id="node-heart-beats">Node heartbeats</h2><p>Kubernetes uses the Lease API to communicate kubelet node heartbeats to the Kubernetes API server.
For every <code>Node</code> , there is a <code>Lease</code> object with a matching name in the <code>kube-node-lease</code>
namespace. Under the hood, every kubelet heartbeat is an <strong>update</strong> request to this <code>Lease</code> object, updating
the <code>spec.renewTime</code> field for the Lease. The Kubernetes control plane uses the time stamp of this field
to determine the availability of this <code>Node</code>.</p><p>See <a href="/docs/concepts/architecture/nodes/#node-heartbeats">Node Lease objects</a> for more details.</p><h2 id="leader-election">Leader election</h2><p>Kubernetes also uses Leases to ensure only one instance of a component is running at any given time.
This is used by control plane components like <code>kube-controller-manager</code> and <code>kube-scheduler</code> in
HA configurations, where only one instance of the component should be actively running while the other
instances are on stand-by.</p><p>Read <a href="/docs/concepts/cluster-administration/coordinated-leader-election/">coordinated leader election</a>
to learn about how Kubernetes builds on the Lease API to select which component instance
acts as leader.</p><h2 id="api-server-identity">API server identity</h2><div class="feature-state-notice feature-beta" title="Feature Gate: APIServerIdentity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [beta]</code> (enabled by default: true)</div><p>Starting in Kubernetes v1.26, each <code>kube-apiserver</code> uses the Lease API to publish its identity to the
rest of the system. While not particularly useful on its own, this provides a mechanism for clients to
discover how many instances of <code>kube-apiserver</code> are operating the Kubernetes control plane.
Existence of kube-apiserver leases enables future capabilities that may require coordination between
each kube-apiserver.</p><p>You can inspect Leases owned by each kube-apiserver by checking for lease objects in the <code>kube-system</code> namespace
with the name <code>apiserver-&lt;sha256-hash&gt;</code>. Alternatively you can use the label selector <code>apiserver.kubernetes.io/identity=kube-apiserver</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl -n kube-system get lease -l apiserver.kubernetes.io/identity<span>=</span>kube-apiserver
</span></span></code></pre></div><pre tabindex="0"><code>NAME                                        HOLDER                                                                           AGE
apiserver-07a5ea9b9b072c4a5f3d1c3702        apiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05        5m33s
apiserver-7be9e061c59d368b3ddaf1376e        apiserver-7be9e061c59d368b3ddaf1376e_84f2a85d-37c1-4b14-b6b9-603e62e4896f        4m23s
apiserver-1dfef752bcb36637d2763d1868        apiserver-1dfef752bcb36637d2763d1868_c5ffa286-8a9a-45d4-91e7-61118ed58d2e        4m43s
</code></pre><p>The SHA256 hash used in the lease name is based on the OS hostname as seen by that API server. Each kube-apiserver should be
configured to use a hostname that is unique within the cluster. New instances of kube-apiserver that use the same hostname
will take over existing Leases using a new holder identity, as opposed to instantiating new Lease objects. You can check the
hostname used by kube-apiserver by checking the value of the <code>kubernetes.io/hostname</code> label:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl -n kube-system get lease apiserver-07a5ea9b9b072c4a5f3d1c3702 -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>coordination.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Lease<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span><span>"2023-07-02T13:16:48Z"</span><span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>apiserver.kubernetes.io/identity</span>:<span> </span>kube-apiserver<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/hostname</span>:<span> </span>master-1<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>apiserver-07a5ea9b9b072c4a5f3d1c3702<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span>  </span><span>resourceVersion</span>:<span> </span><span>"334899"</span><span>
</span></span></span><span><span><span>  </span><span>uid</span>:<span> </span>90870ab5-1ba9-4523-b215-e4d4e662acb1<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>holderIdentity</span>:<span> </span>apiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05<span>
</span></span></span><span><span><span>  </span><span>leaseDurationSeconds</span>:<span> </span><span>3600</span><span>
</span></span></span><span><span><span>  </span><span>renewTime</span>:<span> </span><span>"2023-07-04T21:58:48.065888Z"</span><span>
</span></span></span></code></pre></div><p>Expired leases from kube-apiservers that no longer exist are garbage collected by new kube-apiservers after 1 hour.</p><p>You can disable API server identity leases by disabling the <code>APIServerIdentity</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p><h2 id="custom-workload">Workloads</h2><p>Your own workload can define its own use of Leases. For example, you might run a custom
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> where a primary or leader member
performs operations that its peers do not. You define a Lease so that the controller replicas can select
or elect a leader, using the Kubernetes API for coordination.
If you do use a Lease, it's a good practice to define a name for the Lease that is obviously linked to
the product or component. For example, if you have a component named Example Foo, use a Lease named
<code>example-foo</code>.</p><p>If a cluster operator or another end user could deploy multiple instances of a component, select a name
prefix and pick a mechanism (such as hash of the name of the Deployment) to avoid name collisions
for the Leases.</p><p>You can use another approach so long as it achieves the same outcome: different software products do
not conflict with one another.</p></div></div><div><div class="td-content"><h1>Cloud Controller Manager</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.11 [beta]</code></div><p>Cloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds.
Kubernetes believes in automated, API-driven infrastructure without tight coupling between
components.</p><p><p>The cloud-controller-manager is a Kubernetes <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.</p></p><p>By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.</p><p>The cloud-controller-manager is structured using a plugin
mechanism that allows different cloud providers to integrate their platforms with Kubernetes.</p><h2 id="design">Design</h2><p><img alt="Kubernetes components" src="/images/docs/components-of-kubernetes.svg"></p><p>The cloud controller manager runs in the control plane as a replicated set of processes
(usually, these are containers in Pods). Each cloud-controller-manager implements
multiple <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a> in a single
process.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You can also run the cloud controller manager as a Kubernetes
<a class="glossary-tooltip" title="Resources that extend the functionality of Kubernetes." href="/docs/concepts/cluster-administration/addons/" target="_blank">addon</a> rather than as part
of the control plane.</div><h2 id="functions-of-the-ccm">Cloud controller manager functions</h2><p>The controllers inside the cloud controller manager include:</p><h3 id="node-controller">Node controller</h3><p>The node controller is responsible for updating <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">Node</a> objects
when new servers are created in your cloud infrastructure. The node controller obtains information about the
hosts running inside your tenancy with the cloud provider. The node controller performs the following functions:</p><ol><li>Update a Node object with the corresponding server's unique identifier obtained from the cloud provider API.</li><li>Annotating and labelling the Node object with cloud-specific information, such as the region the node
is deployed into and the resources (CPU, memory, etc) that it has available.</li><li>Obtain the node's hostname and network addresses.</li><li>Verifying the node's health. In case a node becomes unresponsive, this controller checks with
your cloud provider's API to see if the server has been deactivated / deleted / terminated.
If the node has been deleted from the cloud, the controller deletes the Node object from your Kubernetes
cluster.</li></ol><p>Some cloud provider implementations split this into a node controller and a separate node
lifecycle controller.</p><h3 id="route-controller">Route controller</h3><p>The route controller is responsible for configuring routes in the cloud
appropriately so that containers on different nodes in your Kubernetes
cluster can communicate with each other.</p><p>Depending on the cloud provider, the route controller might also allocate blocks
of IP addresses for the Pod network.</p><h3 id="service-controller">Service controller</h3><p><a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Services</a> integrate with cloud
infrastructure components such as managed load balancers, IP addresses, network
packet filtering, and target health checking. The service controller interacts with your
cloud provider's APIs to set up load balancers and other infrastructure components
when you declare a Service resource that requires them.</p><h2 id="authorization">Authorization</h2><p>This section breaks down the access that the cloud controller manager requires
on various API objects, in order to perform its operations.</p><h3 id="authorization-node-controller">Node controller</h3><p>The Node controller only works with Node objects. It requires full access
to read and modify Node objects.</p><p><code>v1/Node</code>:</p><ul><li>get</li><li>list</li><li>create</li><li>update</li><li>patch</li><li>watch</li><li>delete</li></ul><h3 id="authorization-route-controller">Route controller</h3><p>The route controller listens to Node object creation and configures
routes appropriately. It requires Get access to Node objects.</p><p><code>v1/Node</code>:</p><ul><li>get</li></ul><h3 id="authorization-service-controller">Service controller</h3><p>The service controller watches for Service object <strong>create</strong>, <strong>update</strong> and <strong>delete</strong> events and then
configures load balancers for those Services appropriately.</p><p>To access Services, it requires <strong>list</strong>, and <strong>watch</strong> access. To update Services, it requires
<strong>patch</strong> and <strong>update</strong> access to the <code>status</code> subresource.</p><p><code>v1/Service</code>:</p><ul><li>list</li><li>get</li><li>watch</li><li>patch</li><li>update</li></ul><h3 id="authorization-miscellaneous">Others</h3><p>The implementation of the core of the cloud controller manager requires access to create Event
objects, and to ensure secure operation, it requires access to create ServiceAccounts.</p><p><code>v1/Event</code>:</p><ul><li>create</li><li>patch</li><li>update</li></ul><p><code>v1/ServiceAccount</code>:</p><ul><li>create</li></ul><p>The <a class="glossary-tooltip" title="Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API." href="/docs/reference/access-authn-authz/rbac/" target="_blank">RBAC</a> ClusterRole for the cloud
controller manager looks like:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cloud-controller-manager<span>
</span></span></span><span><span><span></span><span>rules</span>:<span>
</span></span></span><span><span><span></span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>  </span>- <span>""</span><span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- events<span>
</span></span></span><span><span><span>  </span><span>verbs</span>:<span>
</span></span></span><span><span><span>  </span>- create<span>
</span></span></span><span><span><span>  </span>- patch<span>
</span></span></span><span><span><span>  </span>- update<span>
</span></span></span><span><span><span></span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>  </span>- <span>""</span><span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- nodes<span>
</span></span></span><span><span><span>  </span><span>verbs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>'*'</span><span>
</span></span></span><span><span><span></span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>  </span>- <span>""</span><span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- nodes/status<span>
</span></span></span><span><span><span>  </span><span>verbs</span>:<span>
</span></span></span><span><span><span>  </span>- patch<span>
</span></span></span><span><span><span></span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>  </span>- <span>""</span><span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- services<span>
</span></span></span><span><span><span>  </span><span>verbs</span>:<span>
</span></span></span><span><span><span>  </span>- list<span>
</span></span></span><span><span><span>  </span>- watch<span>
</span></span></span><span><span><span></span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>  </span>- <span>""</span><span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- services/status<span>
</span></span></span><span><span><span>  </span><span>verbs</span>:<span>
</span></span></span><span><span><span>  </span>- patch<span>
</span></span></span><span><span><span>  </span>- update<span>
</span></span></span><span><span><span></span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>  </span>- <span>""</span><span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- serviceaccounts<span>
</span></span></span><span><span><span>  </span><span>verbs</span>:<span>
</span></span></span><span><span><span>  </span>- create<span>
</span></span></span><span><span><span></span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>  </span>- <span>""</span><span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- persistentvolumes<span>
</span></span></span><span><span><span>  </span><span>verbs</span>:<span>
</span></span></span><span><span><span>  </span>- get<span>
</span></span></span><span><span><span>  </span>- list<span>
</span></span></span><span><span><span>  </span>- update<span>
</span></span></span><span><span><span>  </span>- watch<span>
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><p><a href="/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager">Cloud Controller Manager Administration</a>
has instructions on running and managing the cloud controller manager.</p></li><li><p>To upgrade a HA control plane to use the cloud controller manager, see
<a href="/docs/tasks/administer-cluster/controller-manager-leader-migration/">Migrate Replicated Control Plane To Use Cloud Controller Manager</a>.</p></li><li><p>Want to know how to implement your own cloud controller manager, or extend an existing project?</p><ul><li>The cloud controller manager uses Go interfaces, specifically, <code>CloudProvider</code> interface defined in
<a href="https://github.com/kubernetes/cloud-provider/blob/release-1.21/cloud.go#L42-L69"><code>cloud.go</code></a>
from <a href="https://github.com/kubernetes/cloud-provider">kubernetes/cloud-provider</a> to allow
implementations from any cloud to be plugged in.</li><li>The implementation of the shared controllers highlighted in this document (Node, Route, and Service),
and some scaffolding along with the shared cloudprovider interface, is part of the Kubernetes core.
Implementations specific to cloud providers are outside the core of Kubernetes and implement
the <code>CloudProvider</code> interface.</li><li>For more information about developing plugins,
see <a href="/docs/tasks/administer-cluster/developing-cloud-controller-manager/">Developing Cloud Controller Manager</a>.</li></ul></li></ul></div></div><div><div class="td-content"><h1>About cgroup v2</h1><p>On Linux, <a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." href="/docs/reference/glossary/?all=true#term-cgroup" target="_blank">control groups</a>
constrain resources that are allocated to processes.</p><p>The <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> and the
underlying container runtime need to interface with cgroups to enforce
<a href="/docs/concepts/configuration/manage-resources-containers/">resource management for pods and containers</a> which
includes cpu/memory requests and limits for containerized workloads.</p><p>There are two versions of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 is
the new generation of the <code>cgroup</code> API.</p><h2 id="cgroup-v2">What is cgroup v2?</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>cgroup v2 is the next version of the Linux <code>cgroup</code> API. cgroup v2 provides a
unified control system with enhanced resource management
capabilities.</p><p>cgroup v2 offers several improvements over cgroup v1, such as the following:</p><ul><li>Single unified hierarchy design in API</li><li>Safer sub-tree delegation to containers</li><li>Newer features like <a href="https://www.kernel.org/doc/html/latest/accounting/psi.html">Pressure Stall Information</a></li><li>Enhanced resource allocation management and isolation across multiple resources<ul><li>Unified accounting for different types of memory allocations (network memory, kernel memory, etc)</li><li>Accounting for non-immediate resource changes such as page cache write backs</li></ul></li></ul><p>Some Kubernetes features exclusively use cgroup v2 for enhanced resource
management and isolation. For example, the
<a href="/docs/concepts/workloads/pods/pod-qos/#memory-qos-with-cgroup-v2">MemoryQoS</a> feature improves memory QoS
and relies on cgroup v2 primitives.</p><h2 id="using-cgroupv2">Using cgroup v2</h2><p>The recommended way to use cgroup v2 is to use a Linux distribution that
enables and uses cgroup v2 by default.</p><p>To check if your distribution uses cgroup v2, refer to <a href="#check-cgroup-version">Identify cgroup version on Linux nodes</a>.</p><h3 id="requirements">Requirements</h3><p>cgroup v2 has the following requirements:</p><ul><li>OS distribution enables cgroup v2</li><li>Linux Kernel version is 5.8 or later</li><li>Container runtime supports cgroup v2. For example:<ul><li><a href="https://containerd.io/">containerd</a> v1.4 and later</li><li><a href="https://cri-o.io/">cri-o</a> v1.20 and later</li></ul></li><li>The kubelet and the container runtime are configured to use the <a href="/docs/setup/production-environment/container-runtimes/#systemd-cgroup-driver">systemd cgroup driver</a></li></ul><h3 id="linux-distribution-cgroup-v2-support">Linux Distribution cgroup v2 support</h3><p>For a list of Linux distributions that use cgroup v2, refer to the <a href="https://github.com/opencontainers/runc/blob/main/docs/cgroup-v2.md">cgroup v2 documentation</a></p><ul><li>Container Optimized OS (since M97)</li><li>Ubuntu (since 21.10, 22.04+ recommended)</li><li>Debian GNU/Linux (since Debian 11 bullseye)</li><li>Fedora (since 31)</li><li>Arch Linux (since April 2021)</li><li>RHEL and RHEL-like distributions (since 9)</li></ul><p>To check if your distribution is using cgroup v2, refer to your distribution's
documentation or follow the instructions in <a href="#check-cgroup-version">Identify the cgroup version on Linux nodes</a>.</p><p>You can also enable cgroup v2 manually on your Linux distribution by modifying
the kernel cmdline boot arguments. If your distribution uses GRUB,
<code>systemd.unified_cgroup_hierarchy=1</code> should be added in <code>GRUB_CMDLINE_LINUX</code>
under <code>/etc/default/grub</code>, followed by <code>sudo update-grub</code>. However, the
recommended approach is to use a distribution that already enables cgroup v2 by
default.</p><h3 id="migrating-cgroupv2">Migrating to cgroup v2</h3><p>To migrate to cgroup v2, ensure that you meet the <a href="#requirements">requirements</a>, then upgrade
to a kernel version that enables cgroup v2 by default.</p><p>The kubelet automatically detects that the OS is running on cgroup v2 and
performs accordingly with no additional configuration required.</p><p>There should not be any noticeable difference in the user experience when
switching to cgroup v2, unless users are accessing the cgroup file system
directly, either on the node or from within the containers.</p><p>cgroup v2 uses a different API than cgroup v1, so if there are any
applications that directly access the cgroup file system, they need to be
updated to newer versions that support cgroup v2. For example:</p><ul><li>Some third-party monitoring and security agents may depend on the cgroup filesystem.
Update these agents to versions that support cgroup v2.</li><li>If you run <a href="https://github.com/google/cadvisor">cAdvisor</a> as a stand-alone
DaemonSet for monitoring pods and containers, update it to v0.43.0 or later.</li><li>If you deploy Java applications, prefer to use versions which fully support cgroup v2:<ul><li><a href="https://bugs.openjdk.org/browse/JDK-8230305">OpenJDK / HotSpot</a>: jdk8u372, 11.0.16, 15 and later</li><li><a href="https://www.ibm.com/support/pages/apar/IJ46681">IBM Semeru Runtimes</a>: 8.0.382.0, 11.0.20.0, 17.0.8.0, and later</li><li><a href="https://www.ibm.com/support/pages/apar/IJ46681">IBM Java</a>: 8.0.8.6 and later</li></ul></li><li>If you are using the <a href="https://github.com/uber-go/automaxprocs">uber-go/automaxprocs</a> package, make sure
the version you use is v1.5.1 or higher.</li></ul><h2 id="check-cgroup-version">Identify the cgroup version on Linux Nodes</h2><p>The cgroup version depends on the Linux distribution being used and the
default cgroup version configured on the OS. To check which cgroup version your
distribution uses, run the <code>stat -fc %T /sys/fs/cgroup/</code> command on
the node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>stat -fc %T /sys/fs/cgroup/
</span></span></code></pre></div><p>For cgroup v2, the output is <code>cgroup2fs</code>.</p><p>For cgroup v1, the output is <code>tmpfs.</code></p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">cgroups</a></li><li>Learn more about <a href="/docs/concepts/architecture/cri">container runtime</a></li><li>Learn more about <a href="/docs/setup/production-environment/container-runtimes/#cgroup-drivers">cgroup drivers</a></li></ul></div></div><div><div class="td-content"><h1>Kubernetes Self-Healing</h1><p>Kubernetes is designed with self-healing capabilities that help maintain the health and availability of workloads.
It automatically replaces failed containers, reschedules workloads when nodes become unavailable, and ensures that the desired state of the system is maintained.</p><h2 id="self-healing-capabilities">Self-Healing capabilities</h2><ul><li><p><strong>Container-level restarts:</strong> If a container inside a Pod fails, Kubernetes restarts it based on the <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>restartPolicy</code></a>.</p></li><li><p><strong>Replica replacement:</strong> If a Pod in a <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> or <a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> fails, Kubernetes creates a replacement Pod to maintain the specified number of replicas.
If a Pod fails that is part of a <a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> fails, the control plane
creates a replacement Pod to run on the same node.</p></li><li><p><strong>Persistent storage recovery:</strong> If a node is running a Pod with a PersistentVolume (PV) attached, and the node fails, Kubernetes can reattach the volume to a new Pod on a different node.</p></li><li><p><strong>Load balancing for Services:</strong> If a Pod behind a <a href="/docs/concepts/services-networking/service/">Service</a> fails, Kubernetes automatically removes it from the Service's endpoints to route traffic only to healthy Pods.</p></li></ul><p>Here are some of the key components that provide Kubernetes self-healing:</p><ul><li><p><strong><a href="/docs/concepts/architecture/#kubelet">kubelet</a>:</strong> Ensures that containers are running, and restarts those that fail.</p></li><li><p><strong>ReplicaSet, StatefulSet and DaemonSet controller:</strong> Maintains the desired number of Pod replicas.</p></li><li><p><strong>PersistentVolume controller:</strong> Manages volume attachment and detachment for stateful workloads.</p></li></ul><h2 id="considerations">Considerations</h2><ul><li><p><strong>Storage Failures:</strong> If a persistent volume becomes unavailable, recovery steps may be required.</p></li><li><p><strong>Application Errors:</strong> Kubernetes can restart containers, but underlying application issues must be addressed separately.</p></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Read more about <a href="/docs/concepts/workloads/pods/">Pods</a></li><li>Learn about <a href="/docs/concepts/architecture/controller/">Kubernetes Controllers</a></li><li>Explore <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a></li><li>Read about <a href="/docs/concepts/cluster-administration/node-autoscaling/">node autoscaling</a>. Node autoscaling
also provides automatic healing if or when nodes fail in your cluster.</li></ul></div></div><div><div class="td-content"><h1>Garbage Collection</h1><p>Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up
cluster resources. This
allows the clean up of resources like the following:</p><ul><li><a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection">Terminated pods</a></li><li><a href="/docs/concepts/workloads/controllers/ttlafterfinished/">Completed Jobs</a></li><li><a href="#owners-dependents">Objects without owner references</a></li><li><a href="#containers-images">Unused containers and container images</a></li><li><a href="/docs/concepts/storage/persistent-volumes/#delete">Dynamically provisioned PersistentVolumes with a StorageClass reclaim policy of Delete</a></li><li><a href="/docs/reference/access-authn-authz/certificate-signing-requests/#request-signing-process">Stale or expired CertificateSigningRequests (CSRs)</a></li><li><a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">Nodes</a> deleted in the following scenarios:<ul><li>On a cloud when the cluster uses a <a href="/docs/concepts/architecture/cloud-controller/">cloud controller manager</a></li><li>On-premises when the cluster uses an addon similar to a cloud controller
manager</li></ul></li><li><a href="/docs/concepts/architecture/nodes/#heartbeats">Node Lease objects</a></li></ul><h2 id="owners-dependents">Owners and dependents</h2><p>Many objects in Kubernetes link to each other through <a href="/docs/concepts/overview/working-with-objects/owners-dependents/"><em>owner references</em></a>.
Owner references tell the control plane which objects are dependent on others.
Kubernetes uses owner references to give the control plane, and other API
clients, the opportunity to clean up related resources before deleting an
object. In most cases, Kubernetes manages owner references automatically.</p><p>Ownership is different from the <a href="/docs/concepts/overview/working-with-objects/labels/">labels and selectors</a>
mechanism that some resources also use. For example, consider a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a> that creates
<code>EndpointSlice</code> objects. The Service uses <em>labels</em> to allow the control plane to
determine which <code>EndpointSlice</code> objects are used for that Service. In addition
to the labels, each <code>EndpointSlice</code> that is managed on behalf of a Service has
an owner reference. Owner references help different parts of Kubernetes avoid
interfering with objects they don&#8217;t control.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Cross-namespace owner references are disallowed by design.
Namespaced dependents can specify cluster-scoped or namespaced owners.
A namespaced owner <strong>must</strong> exist in the same namespace as the dependent.
If it does not, the owner reference is treated as absent, and the dependent
is subject to deletion once all owners are verified absent.</p><p>Cluster-scoped dependents can only specify cluster-scoped owners.
In v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner,
it is treated as having an unresolvable owner reference, and is not able to be garbage collected.</p><p>In v1.20+, if the garbage collector detects an invalid cross-namespace <code>ownerReference</code>,
or a cluster-scoped dependent with an <code>ownerReference</code> referencing a namespaced kind, a warning Event
with a reason of <code>OwnerRefInvalidNamespace</code> and an <code>involvedObject</code> of the invalid dependent is reported.
You can check for that kind of Event by running
<code>kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace</code>.</p></div><h2 id="cascading-deletion">Cascading deletion</h2><p>Kubernetes checks for and deletes objects that no longer have owner
references, like the pods left behind when you delete a ReplicaSet. When you
delete an object, you can control whether Kubernetes deletes the object's
dependents automatically, in a process called <em>cascading deletion</em>. There are
two types of cascading deletion, as follows:</p><ul><li>Foreground cascading deletion</li><li>Background cascading deletion</li></ul><p>You can also control how and when garbage collection deletes resources that have
owner references using Kubernetes <a class="glossary-tooltip" title="A namespaced key that tells Kubernetes to wait until specific conditions are met before it fully deletes an object marked for deletion." href="/docs/concepts/overview/working-with-objects/finalizers/" target="_blank">finalizers</a>.</p><h3 id="foreground-deletion">Foreground cascading deletion</h3><p>In foreground cascading deletion, the owner object you're deleting first enters
a <em>deletion in progress</em> state. In this state, the following happens to the
owner object:</p><ul><li>The Kubernetes API server sets the object's <code>metadata.deletionTimestamp</code>
field to the time the object was marked for deletion.</li><li>The Kubernetes API server also sets the <code>metadata.finalizers</code> field to
<code>foregroundDeletion</code>.</li><li>The object remains visible through the Kubernetes API until the deletion
process is complete.</li></ul><p>After the owner object enters the <em>deletion in progress</em> state, the controller
deletes dependents it knows about. After deleting all the dependent objects it knows about,
the controller deletes the owner object. At this point, the object is no longer visible in the
Kubernetes API.</p><p>During foreground cascading deletion, the only dependents that block owner
deletion are those that have the <code>ownerReference.blockOwnerDeletion=true</code> field
and are in the garbage collection controller cache. The garbage collection controller
cache may not contain objects whose resource type cannot be listed / watched successfully,
or objects that are created concurrent with deletion of an owner object.
See <a href="/docs/tasks/administer-cluster/use-cascading-deletion/#use-foreground-cascading-deletion">Use foreground cascading deletion</a>
to learn more.</p><h3 id="background-deletion">Background cascading deletion</h3><p>In background cascading deletion, the Kubernetes API server deletes the owner
object immediately and the garbage collector controller (custom or default)
cleans up the dependent objects in the background.
If a finalizer exists, it ensures that objects are not deleted until all necessary clean-up tasks are completed.
By default, Kubernetes uses background cascading deletion unless
you manually use foreground deletion or choose to orphan the dependent objects.</p><p>See <a href="/docs/tasks/administer-cluster/use-cascading-deletion/#use-background-cascading-deletion">Use background cascading deletion</a>
to learn more.</p><h3 id="orphaned-dependents">Orphaned dependents</h3><p>When Kubernetes deletes an owner object, the dependents left behind are called
<em>orphan</em> objects. By default, Kubernetes deletes dependent objects. To learn how
to override this behaviour, see <a href="/docs/tasks/administer-cluster/use-cascading-deletion/#set-orphan-deletion-policy">Delete owner objects and orphan dependents</a>.</p><h2 id="containers-images">Garbage collection of unused containers and images</h2><p>The <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> performs garbage
collection on unused images every five minutes and on unused containers every
minute. You should avoid using external garbage collection tools, as these can
break the kubelet behavior and remove containers that should exist.</p><p>To configure options for unused container and image garbage collection, tune the
kubelet using a <a href="/docs/tasks/administer-cluster/kubelet-config-file/">configuration file</a>
and change the parameters related to garbage collection using the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>
resource type.</p><h3 id="container-image-lifecycle">Container image lifecycle</h3><p>Kubernetes manages the lifecycle of all images through its <em>image manager</em>,
which is part of the kubelet, with the cooperation of
<a class="glossary-tooltip" title="Tool that provides understanding of the resource usage and performance characteristics for containers" href="https://github.com/google/cadvisor/" target="_blank">cadvisor</a>. The kubelet
considers the following disk usage limits when making garbage collection
decisions:</p><ul><li><code>HighThresholdPercent</code></li><li><code>LowThresholdPercent</code></li></ul><p>Disk usage above the configured <code>HighThresholdPercent</code> value triggers garbage
collection, which deletes images in order based on the last time they were used,
starting with the oldest first. The kubelet deletes images
until disk usage reaches the <code>LowThresholdPercent</code> value.</p><h4 id="image-maximum-age-gc">Garbage collection for unused container images</h4><div class="feature-state-notice feature-beta" title="Feature Gate: ImageMaximumGCAge"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [beta]</code> (enabled by default: true)</div><p>As a beta feature, you can specify the maximum time a local image can be unused for,
regardless of disk usage. This is a kubelet setting that you configure for each node.</p><p>To configure the setting, you need to set a value for the <code>imageMaximumGCAge</code>
field in the kubelet configuration file.</p><p>The value is specified as a Kubernetes <a class="glossary-tooltip" title="A string value representing an amount of time." href="/docs/reference/glossary/?all=true#term-duration" target="_blank">duration</a>.
See <a href="/docs/reference/glossary/?all=true#term-duration">duration</a> in the glossary
for more details.</p><p>For example, you can set the configuration field to <code>12h45m</code>,
which means 12 hours and 45 minutes.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This feature does not track image usage across kubelet restarts. If the kubelet
is restarted, the tracked image age is reset, causing the kubelet to wait the full
<code>imageMaximumGCAge</code> duration before qualifying images for garbage collection
based on image age.</div><h3 id="container-image-garbage-collection">Container garbage collection</h3><p>The kubelet garbage collects unused containers based on the following variables,
which you can define:</p><ul><li><code>MinAge</code>: the minimum age at which the kubelet can garbage collect a
container. Disable by setting to <code>0</code>.</li><li><code>MaxPerPodContainer</code>: the maximum number of dead containers each Pod
can have. Disable by setting to less than <code>0</code>.</li><li><code>MaxContainers</code>: the maximum number of dead containers the cluster can have.
Disable by setting to less than <code>0</code>.</li></ul><p>In addition to these variables, the kubelet garbage collects unidentified and
deleted containers, typically starting with the oldest first.</p><p><code>MaxPerPodContainer</code> and <code>MaxContainers</code> may potentially conflict with each other
in situations where retaining the maximum number of containers per Pod
(<code>MaxPerPodContainer</code>) would go outside the allowable total of global dead
containers (<code>MaxContainers</code>). In this situation, the kubelet adjusts
<code>MaxPerPodContainer</code> to address the conflict. A worst-case scenario would be to
downgrade <code>MaxPerPodContainer</code> to <code>1</code> and evict the oldest containers.
Additionally, containers owned by pods that have been deleted are removed once
they are older than <code>MinAge</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The kubelet only garbage collects the containers it manages.</div><h2 id="configuring-gc">Configuring garbage collection</h2><p>You can tune garbage collection of resources by configuring options specific to
the controllers managing those resources. The following pages show you how to
configure garbage collection:</p><ul><li><a href="/docs/tasks/administer-cluster/use-cascading-deletion/">Configuring cascading deletion of Kubernetes objects</a></li><li><a href="/docs/concepts/workloads/controllers/ttlafterfinished/">Configuring cleanup of finished Jobs</a></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/overview/working-with-objects/owners-dependents/">ownership of Kubernetes objects</a>.</li><li>Learn more about Kubernetes <a href="/docs/concepts/overview/working-with-objects/finalizers/">finalizers</a>.</li><li>Learn about the <a href="/docs/concepts/workloads/controllers/ttlafterfinished/">TTL controller</a> that cleans up finished Jobs.</li></ul></div></div><div><div class="td-content"><h1>Mixed Version Proxy</h1><div class="feature-state-notice feature-alpha" title="Feature Gate: UnknownVersionInteroperabilityProxy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [alpha]</code> (enabled by default: false)</div><p>Kubernetes 1.34 includes an alpha feature that lets an
<a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API Server</a>
proxy a resource requests to other <em>peer</em> API servers. This is useful when there are multiple
API servers running different versions of Kubernetes in one cluster
(for example, during a long-lived rollout to a new release of Kubernetes).</p><p>This enables cluster administrators to configure highly available clusters that can be upgraded
more safely, by directing resource requests (made during the upgrade) to the correct kube-apiserver.
That proxying prevents users from seeing unexpected 404 Not Found errors that stem
from the upgrade process.</p><p>This mechanism is called the <em>Mixed Version Proxy</em>.</p><h2 id="enabling-the-mixed-version-proxy">Enabling the Mixed Version Proxy</h2><p>Ensure that <code>UnknownVersionInteroperabilityProxy</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled when you start the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API Server</a>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kube-apiserver <span>\
</span></span></span><span><span><span></span>--feature-gates<span>=</span><span>UnknownVersionInteroperabilityProxy</span><span>=</span><span>true</span> <span>\
</span></span></span><span><span><span></span><span># required command line arguments for this feature</span>
</span></span><span><span>--peer-ca-file<span>=</span>&lt;path to kube-apiserver CA cert&gt;
</span></span><span><span>--proxy-client-cert-file<span>=</span>&lt;path to aggregator proxy cert&gt;,
</span></span><span><span>--proxy-client-key-file<span>=</span>&lt;path to aggregator proxy key&gt;,
</span></span><span><span>--requestheader-client-ca-file<span>=</span>&lt;path to aggregator CA cert&gt;,
</span></span><span><span><span># requestheader-allowed-names can be set to blank to allow any Common Name</span>
</span></span><span><span>--requestheader-allowed-names<span>=</span>&lt;valid Common Names to verify proxy client cert against&gt;,
</span></span><span><span>
</span></span><span><span><span># optional flags for this feature</span>
</span></span><span><span>--peer-advertise-ip<span>=</span><span>`</span>IP of this kube-apiserver that should be used by peers to proxy requests<span>`</span>
</span></span><span><span>--peer-advertise-port<span>=</span><span>`</span>port of this kube-apiserver that should be used by peers to proxy requests<span>`</span>
</span></span><span><span>
</span></span><span><span><span># &#8230;and other flags as usual</span>
</span></span></code></pre></div><h3 id="transport-and-authn">Proxy transport and authentication between API servers</h3><ul><li><p>The source kube-apiserver reuses the
<a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/#kubernetes-apiserver-client-authentication">existing APIserver client authentication flags</a>
<code>--proxy-client-cert-file</code> and <code>--proxy-client-key-file</code> to present its identity that
will be verified by its peer (the destination kube-apiserver). The destination API server
verifies that peer connection based on the configuration you specify using the
<code>--requestheader-client-ca-file</code> command line argument.</p></li><li><p>To authenticate the destination server's serving certs, you must configure a certificate
authority bundle by specifying the <code>--peer-ca-file</code> command line argument to the <strong>source</strong> API server.</p></li></ul><h3 id="configuration-for-peer-api-server-connectivity">Configuration for peer API server connectivity</h3><p>To set the network location of a kube-apiserver that peers will use to proxy requests, use the
<code>--peer-advertise-ip</code> and <code>--peer-advertise-port</code> command line arguments to kube-apiserver or specify
these fields in the API server configuration file.
If these flags are unspecified, peers will use the value from either <code>--advertise-address</code> or
<code>--bind-address</code> command line argument to the kube-apiserver.
If those too, are unset, the host's default interface is used.</p><h2 id="mixed-version-proxying">Mixed version proxying</h2><p>When you enable mixed version proxying, the <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">aggregation layer</a>
loads a special filter that does the following:</p><ul><li>When a resource request reaches an API server that cannot serve that API
(either because it is at a version pre-dating the introduction of the API or the API is turned off on the API server)
the API server attempts to send the request to a peer API server that can serve the requested API.
It does so by identifying API groups / versions / resources that the local server doesn't recognise,
and tries to proxy those requests to a peer API server that is capable of handling the request.</li><li>If the peer API server fails to respond, the <em>source</em> API server responds with 503 ("Service Unavailable") error.</li></ul><h3 id="how-it-works-under-the-hood">How it works under the hood</h3><p>When an API Server receives a resource request, it first checks which API servers can
serve the requested resource. This check happens using the internal
<a href="/docs/reference/generated/kubernetes-api/v1.34/#storageversioncondition-v1alpha1-internal-apiserver-k8s-io"><code>StorageVersion</code> API</a>.</p><ul><li><p>If the resource is known to the API server that received the request
(for example, <code>GET /api/v1/pods/some-pod</code>), the request is handled locally.</p></li><li><p>If there is no internal <code>StorageVersion</code> object found for the requested resource
(for example, <code>GET /my-api/v1/my-resource</code>) and the configured APIService specifies proxying
to an extension API server, that proxying happens following the usual
<a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/">flow</a> for extension APIs.</p></li><li><p>If a valid internal <code>StorageVersion</code> object is found for the requested resource
(for example, <code>GET /batch/v1/jobs</code>) and the API server trying to handle the request
(the <em>handling API server</em>) has the <code>batch</code> API disabled, then the <em>handling API server</em>
fetches the peer API servers that do serve the relevant API group / version / resource
(<code>api/v1/batch</code> in this case) using the information in the fetched <code>StorageVersion</code> object.
The <em>handling API server</em> then proxies the request to one of the matching peer kube-apiservers
that are aware of the requested resource.</p><ul><li><p>If there is no peer known for that API group / version / resource, the handling API server
passes the request to its own handler chain which should eventually return a 404 ("Not Found") response.</p></li><li><p>If the handling API server has identified and selected a peer API server, but that peer fails
to respond (for reasons such as network connectivity issues, or a data race between the request
being received and a controller registering the peer's info into the control plane), then the handling
API server responds with a 503 ("Service Unavailable") error.</p></li></ul></li></ul></div></div><div><div class="td-content"><h1>Containers</h1><div class="lead">Technology for packaging an application along with its runtime dependencies.</div><p>This page will discuss containers and container images, as well as their use in operations and solution development.</p><p>The word <em>container</em> is an overloaded term. Whenever you use the word, check whether your audience uses the same definition.</p><p>Each container that you run is repeatable; the standardization from having
dependencies included means that you get the same behavior wherever you
run it.</p><p>Containers decouple applications from the underlying host infrastructure.
This makes deployment easier in different cloud or OS environments.</p><p>Each <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a> in a Kubernetes
cluster runs the containers that form the
<a href="/docs/concepts/workloads/pods/">Pods</a> assigned to that node.
Containers in a Pod are co-located and co-scheduled to run on the same node.</p><h2 id="container-images">Container images</h2><p>A <a href="/docs/concepts/containers/images/">container image</a> is a ready-to-run
software package containing everything needed to run an application:
the code and any runtime it requires, application and system libraries,
and default values for any essential settings.</p><p>Containers are intended to be stateless and
<a href="https://glossary.cncf.io/immutable-infrastructure/">immutable</a>:
you should not change
the code of a container that is already running. If you have a containerized
application and want to make changes, the correct process is to build a new
image that includes the change, then recreate the container to start from the
updated image.</p><h2 id="container-runtimes">Container runtimes</h2><p>A fundamental component that empowers Kubernetes to run containers effectively.
It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.</p><p>Kubernetes supports container runtimes such as
<a class="glossary-tooltip" title="A container runtime with an emphasis on simplicity, robustness and portability" href="https://containerd.io/docs/" target="_blank">containerd</a>, <a class="glossary-tooltip" title="A lightweight container runtime specifically for Kubernetes" href="https://cri-o.io/#what-is-cri-o" target="_blank">CRI-O</a>,
and any other implementation of the <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">Kubernetes CRI (Container Runtime
Interface)</a>.</p><p>Usually, you can allow your cluster to pick the default container runtime
for a Pod. If you need to use more than one container runtime in your cluster,
you can specify the <a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a>
for a Pod to make sure that Kubernetes runs those containers using a
particular container runtime.</p><p>You can also use RuntimeClass to run different Pods with the same container
runtime but with different settings.</p><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/concepts/containers/container-environment/">Container Environment</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/containers/container-lifecycle-hooks/">Container Lifecycle Hooks</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/containers/cri/">Container Runtime Interface (CRI)</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Images</h1><p>A container image represents binary data that encapsulates an application and all its
software dependencies. Container images are executable software bundles that can run
standalone and that make very well-defined assumptions about their runtime environment.</p><p>You typically create a container image of your application and push it to a registry
before referring to it in a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>.</p><p>This page provides an outline of the container image concept.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you are looking for the container images for a Kubernetes
release (such as v1.34, the latest minor release),
visit <a href="https://kubernetes.io/releases/download/">Download Kubernetes</a>.</div><h2 id="image-names">Image names</h2><p>Container images are usually given a name such as <code>pause</code>, <code>example/mycontainer</code>, or <code>kube-apiserver</code>.
Images can also include a registry hostname; for example: <code>fictional.registry.example/imagename</code>,
and possibly a port number as well; for example: <code>fictional.registry.example:10443/imagename</code>.</p><p>If you don't specify a registry hostname, Kubernetes assumes that you mean the <a href="https://hub.docker.com/">Docker public registry</a>.
You can change this behavior by setting a default image registry in the
<a href="/docs/setup/production-environment/container-runtimes/">container runtime</a> configuration.</p><p>After the image name part you can add a <em>tag</em> or <em>digest</em> (in the same way you would when using with commands
like <code>docker</code> or <code>podman</code>). Tags let you identify different versions of the same series of images.
Digests are a unique identifier for a specific version of an image. Digests are hashes of the image's content,
and are immutable. Tags can be moved to point to different images, but digests are fixed.</p><p>Image tags consist of lowercase and uppercase letters, digits, underscores (<code>_</code>),
periods (<code>.</code>), and dashes (<code>-</code>). A tag can be up to 128 characters long, and must
conform to the following regex pattern: <code>[a-zA-Z0-9_][a-zA-Z0-9._-]{0,127}</code>.
You can read more about it and find the validation regex in the
<a href="https://github.com/opencontainers/distribution-spec/blob/master/spec.md#workflow-categories">OCI Distribution Specification</a>.
If you don't specify a tag, Kubernetes assumes you mean the tag <code>latest</code>.</p><p>Image digests consists of a hash algorithm (such as <code>sha256</code>) and a hash value. For example:
<code>sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07</code>.
You can find more information about the digest format in the
<a href="https://github.com/opencontainers/image-spec/blob/master/descriptor.md#digests">OCI Image Specification</a>.</p><p>Some image name examples that Kubernetes can use are:</p><ul><li><code>busybox</code> &#8212; Image name only, no tag or digest. Kubernetes will use the Docker
public registry and latest tag. Equivalent to <code>docker.io/library/busybox:latest</code>.</li><li><code>busybox:1.32.0</code> &#8212; Image name with tag. Kubernetes will use the Docker
public registry. Equivalent to <code>docker.io/library/busybox:1.32.0</code>.</li><li><code>registry.k8s.io/pause:latest</code> &#8212; Image name with a custom registry and latest tag.</li><li><code>registry.k8s.io/pause:3.5</code> &#8212; Image name with a custom registry and non-latest tag.</li><li><code>registry.k8s.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07</code> &#8212;
Image name with digest.</li><li><code>registry.k8s.io/pause:3.5@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07</code> &#8212;
Image name with tag and digest. Only the digest will be used for pulling.</li></ul><h2 id="updating-images">Updating images</h2><p>When you first create a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a>,
<a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSet</a>, Pod, or other
object that includes a PodTemplate, and a pull policy was not explicitly specified,
then by default the pull policy of all containers in that Pod will be set to
<code>IfNotPresent</code>. This policy causes the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> to skip pulling an
image if it already exists.</p><h3 id="image-pull-policy">Image pull policy</h3><p>The <code>imagePullPolicy</code> for a container and the tag of the image both affect <em>when</em> the
<a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> attempts to pull
(download) the specified image.</p><p>Here's a list of the values you can set for <code>imagePullPolicy</code> and the effects
these values have:</p><dl><dt><code>IfNotPresent</code></dt><dd>the image is pulled only if it is not already present locally.</dd><dt><code>Always</code></dt><dd>every time the kubelet launches a container, the kubelet queries the container
image registry to resolve the name to an image
<a href="https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier">digest</a>.
If the kubelet has a container image with that exact digest cached locally, the kubelet uses its
cached image; otherwise, the kubelet pulls the image with the resolved digest, and uses that image
to launch the container.</dd><dt><code>Never</code></dt><dd>the kubelet does not try fetching the image. If the image is somehow already present
locally, the kubelet attempts to start the container; otherwise, startup fails.
See <a href="#pre-pulled-images">pre-pulled images</a> for more details.</dd></dl><p>The caching semantics of the underlying image provider make even
<code>imagePullPolicy: Always</code> efficient, as long as the registry is reliably accessible.
Your container runtime can notice that the image layers already exist on the node
so that they don't need to be downloaded again.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>You should avoid using the <code>:latest</code> tag when deploying containers in production as
it is harder to track which version of the image is running and more difficult to
roll back properly.</p><p>Instead, specify a meaningful tag such as <code>v1.42.0</code> and/or a digest.</p></div><p>To make sure the Pod always uses the same version of a container image, you can specify
the image's digest;
replace <code>&lt;image-name&gt;:&lt;tag&gt;</code> with <code>&lt;image-name&gt;@&lt;digest&gt;</code>
(for example, <code>image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2</code>).</p><p>When using image tags, if the image registry were to change the code that the tag on that image
represents, you might end up with a mix of Pods running the old and new code. An image digest
uniquely identifies a specific version of the image, so Kubernetes runs the same code every time
it starts a container with that image name and digest specified. Specifying an image by digest
pins the code that you run so that a change at the registry cannot lead to that mix of versions.</p><p>There are third-party <a href="/docs/reference/access-authn-authz/admission-controllers/">admission controllers</a>
that mutate Pods (and PodTemplates) when they are created, so that the
running workload is defined based on an image digest rather than a tag.
That might be useful if you want to make sure that your entire workload is
running the same code no matter what tag changes happen at the registry.</p><h4 id="imagepullpolicy-defaulting">Default image pull policy</h4><p>When you (or a controller) submit a new Pod to the API server, your cluster sets the
<code>imagePullPolicy</code> field when specific conditions are met:</p><ul><li>if you omit the <code>imagePullPolicy</code> field, and you specify the digest for the
container image, the <code>imagePullPolicy</code> is automatically set to <code>IfNotPresent</code>.</li><li>if you omit the <code>imagePullPolicy</code> field, and the tag for the container image is
<code>:latest</code>, <code>imagePullPolicy</code> is automatically set to <code>Always</code>.</li><li>if you omit the <code>imagePullPolicy</code> field, and you don't specify the tag for the
container image, <code>imagePullPolicy</code> is automatically set to <code>Always</code>.</li><li>if you omit the <code>imagePullPolicy</code> field, and you specify a tag for the container
image that isn't <code>:latest</code>, the <code>imagePullPolicy</code> is automatically set to
<code>IfNotPresent</code>.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The value of <code>imagePullPolicy</code> of the container is always set when the object is
first <em>created</em>, and is not updated if the image's tag or digest later changes.</p><p>For example, if you create a Deployment with an image whose tag is <em>not</em>
<code>:latest</code>, and later update that Deployment's image to a <code>:latest</code> tag, the
<code>imagePullPolicy</code> field will <em>not</em> change to <code>Always</code>. You must manually change
the pull policy of any object after its initial creation.</p></div><h4 id="required-image-pull">Required image pull</h4><p>If you would like to always force a pull, you can do one of the following:</p><ul><li>Set the <code>imagePullPolicy</code> of the container to <code>Always</code>.</li><li>Omit the <code>imagePullPolicy</code> and use <code>:latest</code> as the tag for the image to use;
Kubernetes will set the policy to <code>Always</code> when you submit the Pod.</li><li>Omit the <code>imagePullPolicy</code> and the tag for the image to use;
Kubernetes will set the policy to <code>Always</code> when you submit the Pod.</li><li>Enable the <a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages">AlwaysPullImages</a>
admission controller.</li></ul><h3 id="imagepullbackoff">ImagePullBackOff</h3><p>When a kubelet starts creating containers for a Pod using a container runtime,
it might be possible the container is in <a href="/docs/concepts/workloads/pods/pod-lifecycle/#container-state-waiting">Waiting</a>
state because of <code>ImagePullBackOff</code>.</p><p>The status <code>ImagePullBackOff</code> means that a container could not start because Kubernetes
could not pull a container image (for reasons such as invalid image name, or pulling
from a private registry without <code>imagePullSecret</code>). The <code>BackOff</code> part indicates
that Kubernetes will keep trying to pull the image, with an increasing back-off delay.</p><p>Kubernetes raises the delay between each attempt until it reaches a compiled-in limit,
which is 300 seconds (5 minutes).</p><h3 id="image-pull-per-runtime-class">Image pull per runtime class</h3><p><div class="feature-state-notice feature-alpha" title="Feature Gate: RuntimeClassInImageCriApi"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [alpha]</code> (enabled by default: false)</div>Kubernetes includes alpha support for performing image pulls based on the RuntimeClass of a Pod.</p><p>If you enable the <code>RuntimeClassInImageCriApi</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>,
the kubelet references container images by a tuple of image name and runtime handler
rather than just the image name or digest. Your
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>
may adapt its behavior based on the selected runtime handler.
Pulling images based on runtime class is useful for VM-based containers, such as
Windows Hyper-V containers.</p><h2 id="serial-and-parallel-image-pulls">Serial and parallel image pulls</h2><p>By default, the kubelet pulls images serially. In other words, the kubelet sends
only one image pull request to the image service at a time. Other image pull
requests have to wait until the one being processed is complete.</p><p>Nodes make image pull decisions in isolation. Even when you use serialized image
pulls, two different nodes can pull the same image in parallel.</p><p>If you would like to enable parallel image pulls, you can set the field
<code>serializeImagePulls</code> to false in the <a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration</a>.
With <code>serializeImagePulls</code> set to false, image pull requests will be sent to the image service immediately,
and multiple images will be pulled at the same time.</p><p>When enabling parallel image pulls, ensure that the image service of your container
runtime can handle parallel image pulls.</p><p>The kubelet never pulls multiple images in parallel on behalf of one Pod. For example,
if you have a Pod that has an init container and an application container, the image
pulls for the two containers will not be parallelized. However, if you have two
Pods that use different images, and the parallel image pull feature is enabled,
the kubelet will pull the images in parallel on behalf of the two different Pods.</p><h3 id="maximum-parallel-image-pulls">Maximum parallel image pulls</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [beta]</code></div><p>When <code>serializeImagePulls</code> is set to false, the kubelet defaults to no limit on
the maximum number of images being pulled at the same time. If you would like to
limit the number of parallel image pulls, you can set the field <code>maxParallelImagePulls</code>
in the kubelet configuration. With <code>maxParallelImagePulls</code> set to <em>n</em>, only <em>n</em>
images can be pulled at the same time, and any image pull beyond <em>n</em> will have to
wait until at least one ongoing image pull is complete.</p><p>Limiting the number of parallel image pulls prevents image pulling from consuming
too much network bandwidth or disk I/O, when parallel image pulling is enabled.</p><p>You can set <code>maxParallelImagePulls</code> to a positive number that is greater than or
equal to 1. If you set <code>maxParallelImagePulls</code> to be greater than or equal to 2,
you must set <code>serializeImagePulls</code> to false. The kubelet will fail to start
with an invalid <code>maxParallelImagePulls</code> setting.</p><h2 id="multi-architecture-images-with-image-indexes">Multi-architecture images with image indexes</h2><p>As well as providing binary images, a container registry can also serve a
<a href="https://github.com/opencontainers/image-spec/blob/master/image-index.md">container image index</a>.
An image index can point to multiple <a href="https://github.com/opencontainers/image-spec/blob/master/manifest.md">image manifests</a>
for architecture-specific versions of a container. The idea is that you can have
a name for an image (for example: <code>pause</code>, <code>example/mycontainer</code>, <code>kube-apiserver</code>)
and allow different systems to fetch the right binary image for the machine
architecture they are using.</p><p>The Kubernetes project typically creates container images for its releases with
names that include the suffix <code>-$(ARCH)</code>. For backward compatibility, generate
older images with suffixes. For instance, an image named as <code>pause</code> would be a
multi-architecture image containing manifests for all supported architectures,
while <code>pause-amd64</code> would be a backward-compatible version for older configurations,
or for YAML files with hardcoded image names containing suffixes.</p><h2 id="using-a-private-registry">Using a private registry</h2><p>Private registries may require authentication to be able to discover and/or pull
images from them.
Credentials can be provided in several ways:</p><ul><li><p><a href="#specifying-imagepullsecrets-on-a-pod">Specifying <code>imagePullSecrets</code> when you define a Pod</a></p><p>Only Pods which provide their own keys can access the private registry.</p></li><li><p><a href="#configuring-nodes-to-authenticate-to-a-private-registry">Configuring Nodes to Authenticate to a Private Registry</a></p><ul><li>All Pods can read any configured private registries.</li><li>Requires node configuration by cluster administrator.</li></ul></li><li><p>Using a <em>kubelet credential provider</em> plugin to <a href="#kubelet-credential-provider">dynamically fetch credentials for private registries</a></p><p>The kubelet can be configured to use credential provider exec plugin for the
respective private registry.</p></li><li><p><a href="#pre-pulled-images">Pre-pulled Images</a></p><ul><li>All Pods can use any images cached on a node.</li><li>Requires root access to all nodes to set up.</li></ul></li><li><p>Vendor-specific or local extensions</p><p>If you're using a custom node configuration, you (or your cloud provider) can
implement your mechanism for authenticating the node to the container registry.</p></li></ul><p>These options are explained in more detail below.</p><h3 id="specifying-imagepullsecrets-on-a-pod">Specifying <code>imagePullSecrets</code> on a Pod</h3><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This is the recommended approach to run containers based on images
in private registries.</div><p>Kubernetes supports specifying container image registry keys on a Pod.
All <code>imagePullSecrets</code> must be Secrets that exist in the same
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">Namespace</a> as the
Pod. These Secrets must be of type <code>kubernetes.io/dockercfg</code> or <code>kubernetes.io/dockerconfigjson</code>.</p><h3 id="configuring-nodes-to-authenticate-to-a-private-registry">Configuring nodes to authenticate to a private registry</h3><p>Specific instructions for setting credentials depends on the container runtime and registry you
chose to use. You should refer to your solution's documentation for the most accurate information.</p><p>For an example of configuring a private container image registry, see the
<a href="/docs/tasks/configure-pod-container/pull-image-private-registry/">Pull an Image from a Private Registry</a>
task. That example uses a private registry in Docker Hub.</p><h3 id="kubelet-credential-provider">Kubelet credential provider for authenticated image pulls</h3><p>You can configure the kubelet to invoke a plugin binary to dynamically fetch
registry credentials for a container image. This is the most robust and versatile
way to fetch credentials for private registries, but also requires kubelet-level
configuration to enable.</p><p>This technique can be especially useful for running <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static Pods</a>
that require container images hosted in a private registry.
Using a <a class="glossary-tooltip" title="Provides an identity for processes that run in a Pod." href="/docs/tasks/configure-pod-container/configure-service-account/" target="_blank">ServiceAccount</a> or a
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a> to provide private registry credentials
is not possible in the specification of a static Pod, because it <em>cannot</em>
have references to other API resources in its specification.</p><p>See <a href="/docs/tasks/administer-cluster/kubelet-credential-provider/">Configure a kubelet image credential provider</a> for more details.</p><h3 id="config-json">Interpretation of config.json</h3><p>The interpretation of <code>config.json</code> varies between the original Docker
implementation and the Kubernetes interpretation. In Docker, the <code>auths</code> keys
can only specify root URLs, whereas Kubernetes allows glob URLs as well as
prefix-matched paths. The only limitation is that glob patterns (<code>*</code>) have to
include the dot (<code>.</code>) for each subdomain. The amount of matched subdomains has
to be equal to the amount of glob patterns (<code>*.</code>), for example:</p><ul><li><code>*.kubernetes.io</code> will <em>not</em> match <code>kubernetes.io</code>, but will match
<code>abc.kubernetes.io</code>.</li><li><code>*.*.kubernetes.io</code> will <em>not</em> match <code>abc.kubernetes.io</code>, but will match
<code>abc.def.kubernetes.io</code>.</li><li><code>prefix.*.io</code> will match <code>prefix.kubernetes.io</code>.</li><li><code>*-good.kubernetes.io</code> will match <code>prefix-good.kubernetes.io</code>.</li></ul><p>This means that a <code>config.json</code> like this is valid:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>    <span>"auths"</span>: {
</span></span><span><span>        <span>"my-registry.example/images"</span>: { <span>"auth"</span>: <span>"&#8230;"</span> },
</span></span><span><span>        <span>"*.my-registry.example/images"</span>: { <span>"auth"</span>: <span>"&#8230;"</span> }
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>Image pull operations pass the credentials to the CRI container runtime for every
valid pattern. For example, the following container image names would match
successfully:</p><ul><li><code>my-registry.example/images</code></li><li><code>my-registry.example/images/my-image</code></li><li><code>my-registry.example/images/another-image</code></li><li><code>sub.my-registry.example/images/my-image</code></li></ul><p>However, these container image names would <em>not</em> match:</p><ul><li><code>a.sub.my-registry.example/images/my-image</code></li><li><code>a.b.sub.my-registry.example/images/my-image</code></li></ul><p>The kubelet performs image pulls sequentially for every found credential. This
means that multiple entries in <code>config.json</code> for different paths are possible, too:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>    <span>"auths"</span>: {
</span></span><span><span>        <span>"my-registry.example/images"</span>: {
</span></span><span><span>            <span>"auth"</span>: <span>"&#8230;"</span>
</span></span><span><span>        },
</span></span><span><span>        <span>"my-registry.example/images/subpath"</span>: {
</span></span><span><span>            <span>"auth"</span>: <span>"&#8230;"</span>
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>If now a container specifies an image <code>my-registry.example/images/subpath/my-image</code>
to be pulled, then the kubelet will try to download it using both authentication
sources if one of them fails.</p><h3 id="pre-pulled-images">Pre-pulled images</h3><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This approach is suitable if you can control node configuration. It
will not work reliably if your cloud provider manages nodes and replaces
them automatically.</div><p>By default, the kubelet tries to pull each image from the specified registry.
However, if the <code>imagePullPolicy</code> property of the container is set to <code>IfNotPresent</code> or <code>Never</code>,
then a local image is used (preferentially or exclusively, respectively).</p><p>If you want to rely on pre-pulled images as a substitute for registry authentication,
you must ensure all nodes in the cluster have the same pre-pulled images.</p><p>This can be used to preload certain images for speed or as an alternative to
authenticating to a private registry.</p><p>Similar to the usage of the <a href="#kubelet-credential-provider">kubelet credential provider</a>,
pre-pulled images are also suitable for launching
<a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static Pods</a> that depend
on images hosted in a private registry.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><div class="feature-state-notice feature-alpha" title="Feature Gate: KubeletEnsureSecretPulledImages"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>Access to pre-pulled images may be authorized according to <a href="#ensureimagepullcredentialverification">image pull credential verification</a>.</p></div><h4 id="ensureimagepullcredentialverification">Ensure image pull credential verification</h4><div class="feature-state-notice feature-alpha" title="Feature Gate: KubeletEnsureSecretPulledImages"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>If the <code>KubeletEnsureSecretPulledImages</code> feature gate is enabled for your cluster,
Kubernetes will validate image credentials for every image that requires credentials
to be pulled, even if that image is already present on the node. This validation
ensures that images in a Pod request which have not been successfully pulled
with the provided credentials must re-pull the images from the registry.
Additionally, image pulls that re-use the same credentials
which previously resulted in a successful image pull will not need to re-pull from
the registry and are instead validated locally without accessing the registry
(provided the image is available locally).
This is controlled by the<code>imagePullCredentialsVerificationPolicy</code> field in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-ImagePullCredentialsVerificationPolicy">Kubelet configuration</a>.</p><p>This configuration controls when image pull credentials must be verified if the
image is already present on the node:</p><ul><li><code>NeverVerify</code>: Mimics the behavior of having this feature gate disabled.
If the image is present locally, image pull credentials are not verified.</li><li><code>NeverVerifyPreloadedImages</code>: Images pulled outside the kubelet are not verified,
but all other images will have their credentials verified. This is the default behavior.</li><li><code>NeverVerifyAllowListedImages</code>: Images pulled outside the kubelet and mentioned within the
<code>preloadedImagesVerificationAllowlist</code> specified in the kubelet config are not verified.</li><li><code>AlwaysVerify</code>: All images will have their credentials verified
before they can be used.</li></ul><p>This verification applies to <a href="#pre-pulled-images">pre-pulled images</a>,
images pulled using node-wide secrets, and images pulled using Pod-level secrets.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In the case of credential rotation, the credentials previously used to pull the image
will continue to verify without the need to access the registry. New or rotated credentials
will require the image to be re-pulled from the registry.</div><h4 id="creating-a-secret-with-a-docker-config">Creating a Secret with a Docker config</h4><p>You need to know the username, registry password and client email address for authenticating
to the registry, as well as its hostname.
Run the following command, substituting placeholders with the appropriate values:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret docker-registry &lt;name&gt; <span>\
</span></span></span><span><span><span></span>  --docker-server<span>=</span>&lt;docker-registry-server&gt; <span>\
</span></span></span><span><span><span></span>  --docker-username<span>=</span>&lt;docker-user&gt; <span>\
</span></span></span><span><span><span></span>  --docker-password<span>=</span>&lt;docker-password&gt; <span>\
</span></span></span><span><span><span></span>  --docker-email<span>=</span>&lt;docker-email&gt;
</span></span></code></pre></div><p>If you already have a Docker credentials file then, rather than using the above
command, you can import the credentials file as a Kubernetes
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a>.
<a href="/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials">Create a Secret based on existing Docker credentials</a>
explains how to set this up.</p><p>This is particularly useful if you are using multiple private container
registries, as <code>kubectl create secret docker-registry</code> creates a Secret that
only works with a single private registry.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Pods can only reference image pull secrets in their own namespace,
so this process needs to be done one time per namespace.</div><h4 id="referring-to-imagepullsecrets-on-a-pod">Referring to <code>imagePullSecrets</code> on a Pod</h4><p>Now, you can create pods which reference that secret by adding the <code>imagePullSecrets</code>
section to a Pod definition. Each item in the <code>imagePullSecrets</code> array can only
reference one Secret in the same namespace.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF &gt; pod.yaml
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: Pod
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: foo
</span></span></span><span><span><span>  namespace: awesomeapps
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  containers:
</span></span></span><span><span><span>    - name: foo
</span></span></span><span><span><span>      image: janedoe/awesomeapp:v1
</span></span></span><span><span><span>  imagePullSecrets:
</span></span></span><span><span><span>    - name: myregistrykey
</span></span></span><span><span><span>EOF</span>
</span></span><span><span>
</span></span><span><span>cat <span>&lt;&lt;EOF &gt;&gt; ./kustomization.yaml
</span></span></span><span><span><span>resources:
</span></span></span><span><span><span>- pod.yaml
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>This needs to be done for each Pod that is using a private registry.</p><p>However, you can automate this process by specifying the <code>imagePullSecrets</code> section
in a <a href="/docs/tasks/configure-pod-container/configure-service-account/">ServiceAccount</a>
resource. See <a href="/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">Add ImagePullSecrets to a Service Account</a>
for detailed instructions.</p><p>You can use this in conjunction with a per-node <code>.docker/config.json</code>. The credentials
will be merged.</p><h2 id="use-cases">Use cases</h2><p>There are a number of solutions for configuring private registries. Here are some
common use cases and suggested solutions.</p><ol><li>Cluster running only non-proprietary (e.g. open-source) images. No need to hide images.<ul><li>Use public images from a public registry<ul><li>No configuration required.</li><li>Some cloud providers automatically cache or mirror public images, which improves
availability and reduces the time to pull images.</li></ul></li></ul></li><li>Cluster running some proprietary images which should be hidden to those outside the company, but
visible to all cluster users.<ul><li>Use a hosted private registry<ul><li>Manual configuration may be required on the nodes that need to access to private registry.</li></ul></li><li>Or, run an internal private registry behind your firewall with open read access.<ul><li>No Kubernetes configuration is required.</li></ul></li><li>Use a hosted container image registry service that controls image access<ul><li>It will work better with Node autoscaling than manual node configuration.</li></ul></li><li>Or, on a cluster where changing the node configuration is inconvenient, use <code>imagePullSecrets</code>.</li></ul></li><li>Cluster with proprietary images, a few of which require stricter access control.<ul><li>Ensure <a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages">AlwaysPullImages admission controller</a>
is active. Otherwise, all Pods potentially have access to all images.</li><li>Move sensitive data into a Secret resource, instead of packaging it in an image.</li></ul></li><li>A multi-tenant cluster where each tenant needs own private registry.<ul><li>Ensure <a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages">AlwaysPullImages admission controller</a>
is active. Otherwise, all Pods of all tenants potentially have access to all images.</li><li>Run a private registry with authorization required.</li><li>Generate registry credentials for each tenant, store into a Secret, and propagate
the Secret to every tenant namespace.</li><li>The tenant then adds that Secret to <code>imagePullSecrets</code> of each namespace.</li></ul></li></ol><p>If you need access to multiple registries, you can create one Secret per registry.</p><h2 id="legacy-built-in-kubelet-credential-provider">Legacy built-in kubelet credential provider</h2><p>In older versions of Kubernetes, the kubelet had a direct integration with cloud
provider credentials. This provided the ability to dynamically fetch credentials
for image registries.</p><p>There were three built-in implementations of the kubelet credential provider
integration: ACR (Azure Container Registry), ECR (Elastic Container Registry),
and GCR (Google Container Registry).</p><p>Starting with version 1.26 of Kubernetes, the legacy mechanism has been removed,
so you would need to either:</p><ul><li>configure a kubelet image credential provider on each node; or</li><li>specify image pull credentials using <code>imagePullSecrets</code> and at least one Secret.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Read the <a href="https://github.com/opencontainers/image-spec/blob/main/manifest.md">OCI Image Manifest Specification</a>.</li><li>Learn about <a href="/docs/concepts/architecture/garbage-collection/#container-image-garbage-collection">container image garbage collection</a>.</li><li>Learn more about <a href="/docs/tasks/configure-pod-container/pull-image-private-registry/">pulling an Image from a Private Registry</a>.</li></ul></div></div><div><div class="td-content"><h1>Container Environment</h1><p>This page describes the resources available to Containers in the Container environment.</p><h2 id="container-environment">Container environment</h2><p>The Kubernetes Container environment provides several important resources to Containers:</p><ul><li>A filesystem, which is a combination of an <a href="/docs/concepts/containers/images/">image</a> and one or more <a href="/docs/concepts/storage/volumes/">volumes</a>.</li><li>Information about the Container itself.</li><li>Information about other objects in the cluster.</li></ul><h3 id="container-information">Container information</h3><p>The <em>hostname</em> of a Container is the name of the Pod in which the Container is running.
It is available through the <code>hostname</code> command or the
<a href="https://man7.org/linux/man-pages/man2/gethostname.2.html"><code>gethostname</code></a>
function call in libc.</p><p>The Pod name and namespace are available as environment variables through the
<a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">downward API</a>.</p><p>User defined environment variables from the Pod definition are also available to the Container,
as are any environment variables specified statically in the container image.</p><h3 id="cluster-information">Cluster information</h3><p>A list of all services that were running when a Container was created is available to that Container as environment variables.
This list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.</p><p>For a service named <em>foo</em> that maps to a Container named <em>bar</em>,
the following variables are defined:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>FOO_SERVICE_HOST</span><span>=</span>&lt;the host the service is running on&gt;
</span></span><span><span><span>FOO_SERVICE_PORT</span><span>=</span>&lt;the port the service is running on&gt;
</span></span></code></pre></div><p>Services have dedicated IP addresses and are available to the Container via DNS,
if <a href="https://releases.k8s.io/v1.34.0/cluster/addons/dns/">DNS addon</a> is enabled.&#160;</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/containers/container-lifecycle-hooks/">Container lifecycle hooks</a>.</li><li>Get hands-on experience
<a href="/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">attaching handlers to Container lifecycle events</a>.</li></ul></div></div><div><div class="td-content"><h1>Runtime Class</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><p>This page describes the RuntimeClass resource and runtime selection mechanism.</p><p>RuntimeClass is a feature for selecting the container runtime configuration. The container runtime
configuration is used to run a Pod's containers.</p><h2 id="motivation">Motivation</h2><p>You can set a different RuntimeClass between different Pods to provide a balance of
performance versus security. For example, if part of your workload deserves a high
level of information security assurance, you might choose to schedule those Pods so
that they run in a container runtime that uses hardware virtualization. You'd then
benefit from the extra isolation of the alternative runtime, at the expense of some
additional overhead.</p><p>You can also use RuntimeClass to run different Pods with the same container runtime
but with different settings.</p><h2 id="setup">Setup</h2><ol><li>Configure the CRI implementation on nodes (runtime dependent)</li><li>Create the corresponding RuntimeClass resources</li></ol><h3 id="1-configure-the-cri-implementation-on-nodes">1. Configure the CRI implementation on nodes</h3><p>The configurations available through RuntimeClass are Container Runtime Interface (CRI)
implementation dependent. See the corresponding documentation (<a href="#cri-configuration">below</a>) for your
CRI implementation for how to configure.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>RuntimeClass assumes a homogeneous node configuration across the cluster by default (which means
that all nodes are configured the same way with respect to container runtimes). To support
heterogeneous node configurations, see <a href="#scheduling">Scheduling</a> below.</div><p>The configurations have a corresponding <code>handler</code> name, referenced by the RuntimeClass. The
handler must be a valid <a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label name</a>.</p><h3 id="2-create-the-corresponding-runtimeclass-resources">2. Create the corresponding RuntimeClass resources</h3><p>The configurations setup in step 1 should each have an associated <code>handler</code> name, which identifies
the configuration. For each handler, create a corresponding RuntimeClass object.</p><p>The RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name
(<code>metadata.name</code>) and the handler (<code>handler</code>). The object definition looks like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># RuntimeClass is defined in the node.k8s.io API group</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>node.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>RuntimeClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span># The name the RuntimeClass will be referenced by.</span><span>
</span></span></span><span><span><span>  </span><span># RuntimeClass is a non-namespaced resource.</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>myclass <span>
</span></span></span><span><span><span></span><span># The name of the corresponding CRI configuration</span><span>
</span></span></span><span><span><span></span><span>handler</span>:<span> </span>myconfiguration <span>
</span></span></span></code></pre></div><p>The name of a RuntimeClass object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>It is recommended that RuntimeClass write operations (create/update/patch/delete) be
restricted to the cluster administrator. This is typically the default. See
<a href="/docs/reference/access-authn-authz/authorization/">Authorization Overview</a> for more details.</div><h2 id="usage">Usage</h2><p>Once RuntimeClasses are configured for the cluster, you can specify a
<code>runtimeClassName</code> in the Pod spec to use it. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>runtimeClassName</span>:<span> </span>myclass<span>
</span></span></span><span><span><span>  </span><span># ...</span><span>
</span></span></span></code></pre></div><p>This will instruct the kubelet to use the named RuntimeClass to run this pod. If the named
RuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will enter the
<code>Failed</code> terminal <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase">phase</a>. Look for a
corresponding <a href="/docs/tasks/debug/debug-application/debug-running-pod/">event</a> for an
error message.</p><p>If no <code>runtimeClassName</code> is specified, the default RuntimeHandler will be used, which is equivalent
to the behavior when the RuntimeClass feature is disabled.</p><h3 id="cri-configuration">CRI Configuration</h3><p>For more details on setting up CRI runtimes, see <a href="/docs/setup/production-environment/container-runtimes/">CRI installation</a>.</p><h4 id="hahahugoshortcode1748s3hbhb"><a class="glossary-tooltip" title="A container runtime with an emphasis on simplicity, robustness and portability" href="https://containerd.io/docs/" target="_blank">containerd</a></h4><p>Runtime handlers are configured through containerd's configuration at
<code>/etc/containerd/config.toml</code>. Valid handlers are configured under the runtimes section:</p><pre tabindex="0"><code>[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.${HANDLER_NAME}]
</code></pre><p>See containerd's <a href="https://github.com/containerd/containerd/blob/main/docs/cri/config.md">config documentation</a>
for more details:</p><h4 id="hahahugoshortcode1748s4hbhb"><a class="glossary-tooltip" title="A lightweight container runtime specifically for Kubernetes" href="https://cri-o.io/#what-is-cri-o" target="_blank">CRI-O</a></h4><p>Runtime handlers are configured through CRI-O's configuration at <code>/etc/crio/crio.conf</code>. Valid
handlers are configured under the
<a href="https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table">crio.runtime table</a>:</p><pre tabindex="0"><code>[crio.runtime.runtimes.${HANDLER_NAME}]
  runtime_path = "${PATH_TO_BINARY}"
</code></pre><p>See CRI-O's <a href="https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md">config documentation</a> for more details.</p><h2 id="scheduling">Scheduling</h2><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.16 [beta]</code></div><p>By specifying the <code>scheduling</code> field for a RuntimeClass, you can set constraints to
ensure that Pods running with this RuntimeClass are scheduled to nodes that support it.
If <code>scheduling</code> is not set, this RuntimeClass is assumed to be supported by all nodes.</p><p>To ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have a
common label which is then selected by the <code>runtimeclass.scheduling.nodeSelector</code> field. The
RuntimeClass's nodeSelector is merged with the pod's nodeSelector in admission, effectively taking
the intersection of the set of nodes selected by each. If there is a conflict, the pod will be
rejected.</p><p>If the supported nodes are tainted to prevent other RuntimeClass pods from running on the node, you
can add <code>tolerations</code> to the RuntimeClass. As with the <code>nodeSelector</code>, the tolerations are merged
with the pod's tolerations in admission, effectively taking the union of the set of nodes tolerated
by each.</p><p>To learn more about configuring the node selector and tolerations, see
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/">Assigning Pods to Nodes</a>.</p><h3 id="pod-overhead">Pod Overhead</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>You can specify <em>overhead</em> resources that are associated with running a Pod. Declaring overhead allows
the cluster (including the scheduler) to account for it when making decisions about Pods and resources.</p><p>Pod overhead is defined in RuntimeClass through the <code>overhead</code> field. Through the use of this field,
you can specify the overhead of running pods utilizing this RuntimeClass and ensure these overheads
are accounted for in Kubernetes.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md">RuntimeClass Design</a></li><li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md#runtimeclass-scheduling">RuntimeClass Scheduling Design</a></li><li>Read about the <a href="/docs/concepts/scheduling-eviction/pod-overhead/">Pod Overhead</a> concept</li><li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead">PodOverhead Feature Design</a></li></ul></div></div><div><div class="td-content"><h1>Container Lifecycle Hooks</h1><p>This page describes how kubelet managed Containers can use the Container lifecycle hook framework
to run code triggered by events during their management lifecycle.</p><h2 id="overview">Overview</h2><p>Analogous to many programming language frameworks that have component lifecycle hooks, such as Angular,
Kubernetes provides Containers with lifecycle hooks.
The hooks enable Containers to be aware of events in their management lifecycle
and run code implemented in a handler when the corresponding lifecycle hook is executed.</p><h2 id="container-hooks">Container hooks</h2><p>There are two hooks that are exposed to Containers:</p><p><code>PostStart</code></p><p>This hook is executed immediately after a container is created.
However, there is no guarantee that the hook will execute before the container ENTRYPOINT.
No parameters are passed to the handler.</p><p><code>PreStop</code></p><p>This hook is called immediately before a container is terminated due to an API request or management
event such as a liveness/startup probe failure, preemption, resource contention and others. A call
to the <code>PreStop</code> hook fails if the container is already in a terminated or completed state and the
hook must complete before the TERM signal to stop the container can be sent. The Pod's termination
grace period countdown begins before the <code>PreStop</code> hook is executed, so regardless of the outcome of
the handler, the container will eventually terminate within the Pod's termination grace period. No
parameters are passed to the handler.</p><p>A more detailed description of the termination behavior can be found in
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">Termination of Pods</a>.</p><p><code>StopSignal</code></p><p>The StopSignal lifecycle can be used to define a stop signal which would be sent to the container when it is
stopped. If you set this, it overrides any <code>STOPSIGNAL</code> instruction defined within the container image.</p><p>A more detailed description of termination behaviour with custom stop signals can be found in
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-stop-signals">Stop Signals</a>.</p><h3 id="hook-handler-implementations">Hook handler implementations</h3><p>Containers can access a hook by implementing and registering a handler for that hook.
There are three types of hook handlers that can be implemented for Containers:</p><ul><li>Exec - Executes a specific command, such as <code>pre-stop.sh</code>, inside the cgroups and namespaces of the Container.
Resources consumed by the command are counted against the Container.</li><li>HTTP - Executes an HTTP request against a specific endpoint on the Container.</li><li>Sleep - Pauses the container for a specified duration.</li></ul><h3 id="hook-handler-execution">Hook handler execution</h3><p>When a Container lifecycle management hook is called,
the Kubernetes management system executes the handler according to the hook action,
<code>httpGet</code>, <code>tcpSocket</code> (<a href="/docs/reference/generated/kubernetes-api/v1.31/#lifecyclehandler-v1-core">deprecated</a>)
and <code>sleep</code> are executed by the kubelet process, and <code>exec</code> is executed in the container.</p><p>The <code>PostStart</code> hook handler call is initiated when a container is created,
meaning the container ENTRYPOINT and the <code>PostStart</code> hook are triggered simultaneously.
However, if the <code>PostStart</code> hook takes too long to execute or if it hangs,
it can prevent the container from transitioning to a <code>running</code> state.</p><p><code>PreStop</code> hooks are not executed asynchronously from the signal to stop the Container; the hook must
complete its execution before the TERM signal can be sent. If a <code>PreStop</code> hook hangs during
execution, the Pod's phase will be <code>Terminating</code> and remain there until the Pod is killed after its
<code>terminationGracePeriodSeconds</code> expires. This grace period applies to the total time it takes for
both the <code>PreStop</code> hook to execute and for the Container to stop normally. If, for example,
<code>terminationGracePeriodSeconds</code> is 60, and the hook takes 55 seconds to complete, and the Container
takes 10 seconds to stop normally after receiving the signal, then the Container will be killed
before it can stop normally, since <code>terminationGracePeriodSeconds</code> is less than the total time
(55+10) it takes for these two things to happen.</p><p>If either a <code>PostStart</code> or <code>PreStop</code> hook fails,
it kills the Container.</p><p>Users should make their hook handlers as lightweight as possible.
There are cases, however, when long running commands make sense,
such as when saving state prior to stopping a Container.</p><h3 id="hook-delivery-guarantees">Hook delivery guarantees</h3><p>Hook delivery is intended to be <em>at least once</em>,
which means that a hook may be called multiple times for any given event,
such as for <code>PostStart</code> or <code>PreStop</code>.
It is up to the hook implementation to handle this correctly.</p><p>Generally, only single deliveries are made.
If, for example, an HTTP hook receiver is down and is unable to take traffic,
there is no attempt to resend.
In some rare cases, however, double delivery may occur.
For instance, if a kubelet restarts in the middle of sending a hook,
the hook might be resent after the kubelet comes back up.</p><h3 id="debugging-hook-handlers">Debugging Hook handlers</h3><p>The logs for a Hook handler are not exposed in Pod events.
If a handler fails for some reason, it broadcasts an event.
For <code>PostStart</code>, this is the <code>FailedPostStartHook</code> event,
and for <code>PreStop</code>, this is the <code>FailedPreStopHook</code> event.
To generate a failed <code>FailedPostStartHook</code> event yourself, modify the
<a href="https://k8s.io/examples/pods/lifecycle-events.yaml">lifecycle-events.yaml</a>
file to change the postStart command to "badcommand" and apply it.
Here is some example output of the resulting events you see from running <code>kubectl describe pod lifecycle-demo</code>:</p><pre tabindex="0"><code>Events:
  Type     Reason               Age              From               Message
  ----     ------               ----             ----               -------
  Normal   Scheduled            7s               default-scheduler  Successfully assigned default/lifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2...
  Normal   Pulled               6s               kubelet            Successfully pulled image "nginx" in 229.604315ms
  Normal   Pulling              4s (x2 over 6s)  kubelet            Pulling image "nginx"
  Normal   Created              4s (x2 over 5s)  kubelet            Created container lifecycle-demo-container
  Normal   Started              4s (x2 over 5s)  kubelet            Started container lifecycle-demo-container
  Warning  FailedPostStartHook  4s (x2 over 5s)  kubelet            Exec lifecycle hook ([badcommand]) for Container "lifecycle-demo-container" in Pod "lifecycle-demo_default(30229739-9651-4e5a-9a32-a8f1688862db)" failed - error: command 'badcommand' exited with 126: , message: "OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \"badcommand\": executable file not found in $PATH: unknown\r\n"
  Normal   Killing              4s (x2 over 5s)  kubelet            FailedPostStartHook
  Normal   Pulled               4s               kubelet            Successfully pulled image "nginx" in 215.66395ms
  Warning  BackOff              2s (x2 over 3s)  kubelet            Back-off restarting failed container
</code></pre><h2 id="what-s-next">What's next</h2><ul><li>Learn more about the <a href="/docs/concepts/containers/container-environment/">Container environment</a>.</li><li>Get hands-on experience
<a href="/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">attaching handlers to Container lifecycle events</a>.</li></ul></div></div><div><div class="td-content"><h1>Container Runtime Interface (CRI)</h1><p>The CRI is a plugin interface which enables the kubelet to use a wide variety of
container runtimes, without having a need to recompile the cluster components.</p><p>You need a working
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a> on
each Node in your cluster, so that the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> can launch
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> and their containers.</p><p><p>The Container Runtime Interface (CRI) is the main protocol for the communication between the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> and Container Runtime.</p></p><p>The Kubernetes Container Runtime Interface (CRI) defines the main
<a href="https://grpc.io">gRPC</a> protocol for the communication between the
<a href="/docs/concepts/architecture/#node-components">node components</a>
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> and
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>.</p><h2 id="api">The API</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>The kubelet acts as a client when connecting to the container runtime via gRPC.
The runtime and image service endpoints have to be available in the container
runtime, which can be configured separately within the kubelet by using the
<code>--container-runtime-endpoint</code>
<a href="/docs/reference/command-line-tools-reference/kubelet/">command line flag</a>.</p><p>For Kubernetes v1.26 and later, the kubelet requires that the container runtime
supports the <code>v1</code> CRI API. If a container runtime does not support the <code>v1</code> API,
the kubelet will not register the node.</p><h2 id="upgrading">Upgrading</h2><p>When upgrading the Kubernetes version on a node, the kubelet restarts. If the
container runtime does not support the <code>v1</code> CRI API, the kubelet will fail to
register and report an error. If a gRPC re-dial is required because the container
runtime has been upgraded, the runtime must support the <code>v1</code> CRI API for the
connection to succeed. This might require a restart of the kubelet after the
container runtime is correctly configured.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about the CRI <a href="https://github.com/kubernetes/cri-api/blob/v0.33.1/pkg/apis/runtime/v1/api.proto">protocol definition</a></li></ul></div></div><div><div class="td-content"><h1>Workloads</h1><div class="lead">Understand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.</div><p>A workload is an application running on Kubernetes.
Whether your workload is a single component or several that work together, on Kubernetes you run
it inside a set of <a href="/docs/concepts/workloads/pods/"><em>pods</em></a>.
In Kubernetes, a Pod represents a set of running
<a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." href="/docs/concepts/containers/" target="_blank">containers</a> on your cluster.</p><p>Kubernetes pods have a <a href="/docs/concepts/workloads/pods/pod-lifecycle/">defined lifecycle</a>.
For example, once a pod is running in your cluster then a critical fault on the
<a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a> where that pod is running means that
all the pods on that node fail. Kubernetes treats that level of failure as final: you
would need to create a new Pod to recover, even if the node later becomes healthy.</p><p>However, to make life considerably easier, you don't need to manage each Pod directly.
Instead, you can use <em>workload resources</em> that manage a set of pods on your behalf.
These resources configure <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a>
that make sure the right number of the right kind of pod are running, to match the state
you specified.</p><p>Kubernetes provides several built-in workload resources:</p><ul><li><a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> and <a href="/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>
(replacing the legacy resource
<a class="glossary-tooltip" title="A (deprecated) API object that manages a replicated application." href="/docs/reference/glossary/?all=true#term-replication-controller" target="_blank">ReplicationController</a>).
Deployment is a good fit for managing a stateless application workload on your cluster,
where any Pod in the Deployment is interchangeable and can be replaced if needed.</li><li><a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> lets you
run one or more related Pods that do track state somehow. For example, if your workload
records data persistently, you can run a StatefulSet that matches each Pod with a
<a href="/docs/concepts/storage/persistent-volumes/">PersistentVolume</a>. Your code, running in the
Pods for that StatefulSet, can replicate data to other Pods in the same StatefulSet
to improve overall resilience.</li><li><a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> defines Pods that provide
facilities that are local to nodes.
Every time you add a node to your cluster that matches the specification in a DaemonSet,
the control plane schedules a Pod for that DaemonSet onto the new node.
Each pod in a DaemonSet performs a job similar to a system daemon on a classic Unix / POSIX
server. A DaemonSet might be fundamental to the operation of your cluster, such as
a plugin to run <a href="/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model">cluster networking</a>,
it might help you to manage the node,
or it could provide optional behavior that enhances the container platform you are running.</li><li><a href="/docs/concepts/workloads/controllers/job/">Job</a> and
<a href="/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a> provide different ways to
define tasks that run to completion and then stop.
You can use a <a href="/docs/concepts/workloads/controllers/job/">Job</a> to
define a task that runs to completion, just once. You can use a
<a href="/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a> to run
the same Job multiple times according a schedule.</li></ul><p>In the wider Kubernetes ecosystem, you can find third-party workload resources that provide
additional behaviors. Using a
<a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resource definition</a>,
you can add in a third-party workload resource if you want a specific behavior that's not part
of Kubernetes' core. For example, if you wanted to run a group of Pods for your application but
stop work unless <em>all</em> the Pods are available (perhaps for some high-throughput distributed task),
then you can implement or install an extension that does provide that feature.</p><h2 id="what-s-next">What's next</h2><p>As well as reading about each API kind for workload management, you can read how to
do specific tasks:</p><ul><li><a href="/docs/tasks/run-application/run-stateless-application-deployment/">Run a stateless application using a Deployment</a></li><li>Run a stateful application either as a <a href="/docs/tasks/run-application/run-single-instance-stateful-application/">single instance</a>
or as a <a href="/docs/tasks/run-application/run-replicated-stateful-application/">replicated set</a></li><li><a href="/docs/tasks/job/automated-tasks-with-cron-jobs/">Run automated tasks with a CronJob</a></li></ul><p>To learn about Kubernetes' mechanisms for separating code from configuration,
visit <a href="/docs/concepts/configuration/">Configuration</a>.</p><p>There are two supporting concepts that provide backgrounds about how Kubernetes manages pods
for applications:</p><ul><li><a href="/docs/concepts/architecture/garbage-collection/">Garbage collection</a> tidies up objects
from your cluster after their <em>owning resource</em> has been removed.</li><li>The <a href="/docs/concepts/workloads/controllers/ttlafterfinished/"><em>time-to-live after finished</em> controller</a>
removes Jobs once a defined time has passed since they completed.</li></ul><p>Once your application is running, you might want to make it available on the internet as
a <a href="/docs/concepts/services-networking/service/">Service</a> or, for web application only,
using an <a href="/docs/concepts/services-networking/ingress/">Ingress</a>.</p><div class="section-index"></div></div></div><div><div class="td-content"><h1>Pods</h1><p><em>Pods</em> are the smallest deployable units of computing that you can create and manage in Kubernetes.</p><p>A <em>Pod</em> (as in a pod of whales or pea pod) is a group of one or more
<a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." href="/docs/concepts/containers/" target="_blank">containers</a>, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and
co-scheduled, and run in a shared context. A Pod models an
application-specific "logical host": it contains one or more application
containers which are relatively tightly coupled.
In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.</p><p>As well as application containers, a Pod can contain
<a class="glossary-tooltip" title="One or more initialization containers that must run to completion before any app containers run." href="/docs/concepts/workloads/pods/init-containers/" target="_blank">init containers</a> that run
during Pod startup. You can also inject
<a class="glossary-tooltip" title="A type of container type that you can temporarily run inside a Pod" href="/docs/concepts/workloads/pods/ephemeral-containers/" target="_blank">ephemeral containers</a>
for debugging a running Pod.</p><h2 id="what-is-a-pod">What is a Pod?</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You need to install a <a href="/docs/setup/production-environment/container-runtimes/">container runtime</a>
into each node in the cluster so that Pods can run there.</div><p>The shared context of a Pod is a set of Linux namespaces, cgroups, and
potentially other facets of isolation - the same things that isolate a <a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." href="/docs/concepts/containers/" target="_blank">container</a>. Within a Pod's context, the individual applications may have
further sub-isolations applied.</p><p>A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.</p><p>Pods in a Kubernetes cluster are used in two main ways:</p><ul><li><p><strong>Pods that run a single container</strong>. The "one-container-per-Pod" model is the
most common Kubernetes use case; in this case, you can think of a Pod as a
wrapper around a single container; Kubernetes manages Pods rather than managing
the containers directly.</p></li><li><p><strong>Pods that run multiple containers that need to work together</strong>. A Pod can
encapsulate an application composed of
<a href="#how-pods-manage-multiple-containers">multiple co-located containers</a> that are
tightly coupled and need to share resources. These co-located containers
form a single cohesive unit.</p><p>Grouping multiple co-located and co-managed containers in a single Pod is a
relatively advanced use case. You should use this pattern only in specific
instances in which your containers are tightly coupled.</p><p>You don't need to run multiple containers to provide replication (for resilience
or capacity); if you need multiple replicas, see
<a href="/docs/concepts/workloads/controllers/">Workload management</a>.</p></li></ul><h2 id="using-pods">Using Pods</h2><p>The following is an example of a Pod which consists of a container running the image <code>nginx:1.14.2</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/simple-pod.yaml"><code>pods/simple-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/simple-pod.yaml to clipboard"></div><div class="includecode" id="pods-simple-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>To create the Pod shown above, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml
</span></span></code></pre></div><p>Pods are generally not created directly and are created using workload resources.
See <a href="#working-with-pods">Working with Pods</a> for more information on how Pods are used
with workload resources.</p><h3 id="workload-resources-for-managing-pods">Workload resources for managing pods</h3><p>Usually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a> or <a class="glossary-tooltip" title="A finite or batch task that runs to completion." href="/docs/concepts/workloads/controllers/job/" target="_blank">Job</a>.
If your Pods need to track state, consider the
<a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSet</a> resource.</p><p>Each Pod is meant to run a single instance of a given application. If you want to
scale your application horizontally (to provide more overall resources by running
more instances), you should use multiple Pods, one for each instance. In
Kubernetes, this is typically referred to as <em>replication</em>.
Replicated Pods are usually created and managed as a group by a workload resource
and its <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>.</p><p>See <a href="#pods-and-controllers">Pods and controllers</a> for more information on how
Kubernetes uses workload resources, and their controllers, to implement application
scaling and auto-healing.</p><p>Pods natively provide two kinds of shared resources for their constituent containers:
<a href="#pod-networking">networking</a> and <a href="#pod-storage">storage</a>.</p><h2 id="working-with-pods">Working with Pods</h2><p>You'll rarely create individual Pods directly in Kubernetes&#8212;even singleton Pods. This
is because Pods are designed as relatively ephemeral, disposable entities. When
a Pod gets created (directly by you, or indirectly by a
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>), the new Pod is
scheduled to run on a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">Node</a> in your cluster.
The Pod remains on that node until the Pod finishes execution, the Pod object is deleted,
the Pod is <em>evicted</em> for lack of resources, or the node fails.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Restarting a container in a Pod should not be confused with restarting a Pod. A Pod
is not a process, but an environment for running container(s). A Pod persists until
it is deleted.</div><p>The name of a Pod must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostname. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><h3 id="pod-os">Pod OS</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>You should set the <code>.spec.os.name</code> field to either <code>windows</code> or <code>linux</code> to indicate the OS on
which you want the pod to run. These two are the only operating systems supported for now by
Kubernetes. In the future, this list may be expanded.</p><p>In Kubernetes v1.34, the value of <code>.spec.os.name</code> does not affect
how the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">kube-scheduler</a>
picks a node for the Pod to run on. In any cluster where there is more than one operating system for
running nodes, you should set the
<a href="/docs/reference/labels-annotations-taints/#kubernetes-io-os">kubernetes.io/os</a>
label correctly on each node, and define pods with a <code>nodeSelector</code> based on the operating system
label. The kube-scheduler assigns your pod to a node based on other criteria and may or may not
succeed in picking a suitable node placement where the node OS is right for the containers in that Pod.
The <a href="/docs/concepts/security/pod-security-standards/">Pod security standards</a> also use this
field to avoid enforcing policies that aren't relevant to the operating system.</p><h3 id="pods-and-controllers">Pods and controllers</h3><p>You can use workload resources to create and manage multiple Pods for you. A controller
for the resource handles replication and rollout and automatic healing in case of
Pod failure. For example, if a Node fails, a controller notices that Pods on that
Node have stopped working and creates a replacement Pod. The scheduler places the
replacement Pod onto a healthy Node.</p><p>Here are some examples of workload resources that manage one or more Pods:</p><ul><li><a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a></li><li><a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSet</a></li><li><a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a></li></ul><h3 id="pod-templates">Pod templates</h3><p>Controllers for <a class="glossary-tooltip" title="A workload is an application running on Kubernetes." href="/docs/concepts/workloads/" target="_blank">workload</a> resources create Pods
from a <em>pod template</em> and manage those Pods on your behalf.</p><p>PodTemplates are specifications for creating Pods, and are included in workload resources such as
<a href="/docs/concepts/workloads/controllers/deployment/">Deployments</a>,
<a href="/docs/concepts/workloads/controllers/job/">Jobs</a>, and
<a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSets</a>.</p><p>Each controller for a workload resource uses the <code>PodTemplate</code> inside the workload
object to make actual Pods. The <code>PodTemplate</code> is part of the desired state of whatever
workload resource you used to run your app.</p><p>When you create a Pod, you can include
<a href="/docs/tasks/inject-data-application/define-environment-variable-container/">environment variables</a>
in the Pod template for the containers that run in the Pod.</p><p>The sample below is a manifest for a simple Job with a <code>template</code> that starts one
container. The container in that Pod prints a message then pauses.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hello<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span># This is the pod template</span><span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>hello<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'echo "Hello, Kubernetes!" &amp;&amp; sleep 3600'</span>]<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>OnFailure<span>
</span></span></span><span><span><span>    </span><span># The pod template ends here</span><span>
</span></span></span></code></pre></div><p>Modifying the pod template or switching to a new pod template has no direct effect
on the Pods that already exist. If you change the pod template for a workload
resource, that resource needs to create replacement Pods that use the updated template.</p><p>For example, the StatefulSet controller ensures that the running Pods match the current
pod template for each StatefulSet object. If you edit the StatefulSet to change its pod
template, the StatefulSet starts to create new Pods based on the updated template.
Eventually, all of the old Pods are replaced with new Pods, and the update is complete.</p><p>Each workload resource implements its own rules for handling changes to the Pod template.
If you want to read more about StatefulSet specifically, read
<a href="/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets">Update strategy</a> in the StatefulSet Basics tutorial.</p><p>On Nodes, the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> does not
directly observe or manage any of the details around pod templates and updates; those
details are abstracted away. That abstraction and separation of concerns simplifies
system semantics, and makes it feasible to extend the cluster's behavior without
changing existing code.</p><h2 id="pod-update-and-replacement">Pod update and replacement</h2><p>As mentioned in the previous section, when the Pod template for a workload
resource is changed, the controller creates new Pods based on the updated
template instead of updating or patching the existing Pods.</p><p>Kubernetes doesn't prevent you from managing Pods directly. It is possible to
update some fields of a running Pod, in place. However, Pod update operations
like
<a href="/docs/reference/generated/kubernetes-api/v1.34/#patch-pod-v1-core"><code>patch</code></a>, and
<a href="/docs/reference/generated/kubernetes-api/v1.34/#replace-pod-v1-core"><code>replace</code></a>
have some limitations:</p><ul><li><p>Most of the metadata about a Pod is immutable. For example, you cannot
change the <code>namespace</code>, <code>name</code>, <code>uid</code>, or <code>creationTimestamp</code> fields.</p></li><li><p>If the <code>metadata.deletionTimestamp</code> is set, no new entry can be added to the
<code>metadata.finalizers</code> list.</p></li><li><p>Pod updates may not change fields other than <code>spec.containers[*].image</code>,
<code>spec.initContainers[*].image</code>, <code>spec.activeDeadlineSeconds</code>, <code>spec.terminationGracePeriodSeconds</code>,
<code>spec.tolerations</code> or <code>spec.schedulingGates</code>. For <code>spec.tolerations</code>, you can only add new entries.</p></li><li><p>When updating the <code>spec.activeDeadlineSeconds</code> field, two types of updates
are allowed:</p><ol><li>setting the unassigned field to a positive number;</li><li>updating the field from a positive number to a smaller, non-negative
number.</li></ol></li></ul><h3 id="pod-subresources">Pod subresources</h3><p>The above update rules apply to regular pod updates, but other pod fields can be updated through <em>subresources</em>.</p><ul><li><strong>Resize:</strong> The <code>resize</code> subresource allows container resources (<code>spec.containers[*].resources</code>) to be updated.
See <a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize Container Resources</a> for more details.</li><li><strong>Ephemeral Containers:</strong> The <code>ephemeralContainers</code> subresource allows
<a class="glossary-tooltip" title="A type of container type that you can temporarily run inside a Pod" href="/docs/concepts/workloads/pods/ephemeral-containers/" target="_blank">ephemeral containers</a>
to be added to a Pod.
See <a href="/docs/concepts/workloads/pods/ephemeral-containers/">Ephemeral Containers</a> for more details.</li><li><strong>Status:</strong> The <code>status</code> subresource allows the pod status to be updated.
This is typically only used by the Kubelet and other system controllers.</li><li><strong>Binding:</strong> The <code>binding</code> subresource allows setting the pod's <code>spec.nodeName</code> via a <code>Binding</code> request.
This is typically only used by the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">scheduler</a>.</li></ul><h3 id="pod-generation">Pod generation</h3><ul><li>The <code>metadata.generation</code> field is unique. It will be automatically set by the
system such that new pods have a <code>metadata.generation</code> of 1, and every update to
mutable fields in the pod's spec will increment the <code>metadata.generation</code> by 1.</li></ul><div class="feature-state-notice feature-beta" title="Feature Gate: PodObservedGenerationTracking"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><ul><li><code>observedGeneration</code> is a field that is captured in the <code>status</code> section of the Pod
object. If the feature gate <code>PodObservedGenerationTracking</code> is set, the Kubelet will set <code>status.observedGeneration</code>
to track the pod state to the current pod status. The pod's <code>status.observedGeneration</code> will reflect the
<code>metadata.generation</code> of the pod at the point that the pod status is being reported.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>status.observedGeneration</code> field is managed by the kubelet and external controllers should <strong>not</strong> modify this field.</div><p>Different status fields may either be associated with the <code>metadata.generation</code> of the current sync loop, or with the
<code>metadata.generation</code> of the previous sync loop. The key distinction is whether a change in the <code>spec</code> is reflected
directly in the <code>status</code> or is an indirect result of a running process.</p><h4 id="direct-status-updates">Direct Status Updates</h4><p>For status fields where the allocated spec is directly reflected, the <code>observedGeneration</code> will
be associated with the current <code>metadata.generation</code> (Generation N).</p><p>This behavior applies to:</p><ul><li><strong>Resize Status</strong>: The status of a resource resize operation.</li><li><strong>Allocated Resources</strong>: The resources allocated to the Pod after a resize.</li><li><strong>Ephemeral Containers</strong>: When a new ephemeral container is added, and it is in <code>Waiting</code> state.</li></ul><h4 id="indirect-status-updates">Indirect Status Updates</h4><p>For status fields that are an indirect result of running the spec, the <code>observedGeneration</code> will be associated
with the <code>metadata.generation</code> of the previous sync loop (Generation N-1).</p><p>This behavior applies to:</p><ul><li><strong>Container Image</strong>: The <code>ContainerStatus.ImageID</code> reflects the image from the previous generation until the new image
is pulled and the container is updated.</li><li><strong>Actual Resources</strong>: During an in-progress resize, the actual resources in use still belong to the previous generation's
request.</li><li><strong>Container state</strong>: During an in-progress resize, with require restart policy reflects the previous generation's
request.</li><li><strong>activeDeadlineSeconds</strong> &amp; <strong>terminationGracePeriodSeconds</strong> &amp; <strong>deletionTimestamp</strong>: The effects of these fields on the
Pod's status are a result of the previously observed specification.</li></ul><h2 id="resource-sharing-and-communication">Resource sharing and communication</h2><p>Pods enable data sharing and communication among their constituent
containers.</p><h3 id="pod-storage">Storage in Pods</h3><p>A Pod can specify a set of shared storage
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">volumes</a>. All containers
in the Pod can access the shared volumes, allowing those containers to
share data. Volumes also allow persistent data in a Pod to survive
in case one of the containers within needs to be restarted. See
<a href="/docs/concepts/storage/">Storage</a> for more information on how
Kubernetes implements shared storage and makes it available to Pods.</p><h3 id="pod-networking">Pod networking</h3><p>Each Pod is assigned a unique IP address for each address family. Every
container in a Pod shares the network namespace, including the IP address and
network ports. Inside a Pod (and <strong>only</strong> then), the containers that belong to the Pod
can communicate with one another using <code>localhost</code>. When containers in a Pod communicate
with entities <em>outside the Pod</em>,
they must coordinate how they use the shared network resources (such as ports).
Within a Pod, containers share an IP address and port space, and
can find each other via <code>localhost</code>. The containers in a Pod can also communicate
with each other using standard inter-process communications like SystemV semaphores
or POSIX shared memory. Containers in different Pods have distinct IP addresses
and can not communicate by OS-level IPC without special configuration.
Containers that want to interact with a container running in a different Pod can
use IP networking to communicate.</p><p>Containers within the Pod see the system hostname as being the same as the configured
<code>name</code> for the Pod. There's more about this in the <a href="/docs/concepts/cluster-administration/networking/">networking</a>
section.</p><h2 id="pod-security">Pod security settings</h2><p>To set security constraints on Pods and containers, you use the
<code>securityContext</code> field in the Pod specification. This field gives you
granular control over what a Pod or individual containers can do. For example:</p><ul><li>Drop specific Linux capabilities to avoid the impact of a CVE.</li><li>Force all processes in the Pod to run as a non-root user or as a specific
user or group ID.</li><li>Set a specific seccomp profile.</li><li>Set Windows security options, such as whether containers run as HostProcess.</li></ul><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>You can also use the Pod securityContext to enable
<a href="/docs/concepts/security/linux-kernel-security-constraints/#privileged-containers"><em>privileged mode</em></a>
in Linux containers. Privileged mode overrides many of the other security
settings in the securityContext. Avoid using this setting unless you can't grant
the equivalent permissions by using other fields in the securityContext.
In Kubernetes 1.26 and later, you can run Windows containers in a similarly
privileged mode by setting the <code>windowsOptions.hostProcess</code> flag on the
security context of the Pod spec. For details and instructions, see
<a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create a Windows HostProcess Pod</a>.</div><ul><li>To learn about kernel-level security constraints that you can use,
see <a href="/docs/concepts/security/linux-kernel-security-constraints/">Linux kernel security constraints for Pods and containers</a>.</li><li>To learn more about the Pod security context, see
<a href="/docs/tasks/configure-pod-container/security-context/">Configure a Security Context for a Pod or Container</a>.</li></ul><h2 id="static-pods">Static Pods</h2><p><em>Static Pods</em> are managed directly by the kubelet daemon on a specific node,
without the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a>
observing them.
Whereas most Pods are managed by the control plane (for example, a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a>), for static
Pods, the kubelet directly supervises each static Pod (and restarts it if it fails).</p><p>Static Pods are always bound to one <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">Kubelet</a> on a specific node.
The main use for static Pods is to run a self-hosted control plane: in other words,
using the kubelet to supervise the individual <a href="/docs/concepts/architecture/#control-plane-components">control plane components</a>.</p><p>The kubelet automatically tries to create a <a class="glossary-tooltip" title="An object in the API server that tracks a static pod on a kubelet." href="/docs/reference/glossary/?all=true#term-mirror-pod" target="_blank">mirror Pod</a>
on the Kubernetes API server for each static Pod.
This means that the Pods running on a node are visible on the API server,
but cannot be controlled from there. See the guide <a href="/docs/tasks/configure-pod-container/static-pod/">Create static Pods</a>
for more information.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>spec</code> of a static Pod cannot refer to other API objects
(e.g., <a class="glossary-tooltip" title="Provides an identity for processes that run in a Pod." href="/docs/tasks/configure-pod-container/configure-service-account/" target="_blank">ServiceAccount</a>,
<a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." href="/docs/concepts/configuration/configmap/" target="_blank">ConfigMap</a>,
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a>, etc).</div><h2 id="how-pods-manage-multiple-containers">Pods with multiple containers</h2><p>Pods are designed to support multiple cooperating processes (as containers) that form
a cohesive unit of service. The containers in a Pod are automatically co-located and
co-scheduled on the same physical or virtual machine in the cluster. The containers
can share resources and dependencies, communicate with one another, and coordinate
when and how they are terminated.</p><p>Pods in a Kubernetes cluster are used in two main ways:</p><ul><li><strong>Pods that run a single container</strong>. The "one-container-per-Pod" model is the
most common Kubernetes use case; in this case, you can think of a Pod as a
wrapper around a single container; Kubernetes manages Pods rather than managing
the containers directly.</li><li><strong>Pods that run multiple containers that need to work together</strong>. A Pod can
encapsulate an application composed of
multiple co-located containers that are
tightly coupled and need to share resources. These co-located containers
form a single cohesive unit of service&#8212;for example, one container serving data
stored in a shared volume to the public, while a separate
<a class="glossary-tooltip" title="An auxilliary container that stays running throughout the lifecycle of a Pod." href="/docs/concepts/workloads/pods/sidecar-containers/" target="_blank">sidecar container</a>
refreshes or updates those files.
The Pod wraps these containers, storage resources, and an ephemeral network
identity together as a single unit.</li></ul><p>For example, you might have a container that
acts as a web server for files in a shared volume, and a separate
<a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar container</a>
that updates those files from a remote source, as in the following diagram:</p><figure class="diagram-medium"><img src="/images/docs/pod.svg" alt="Pod creation diagram"></figure><p>Some Pods have <a class="glossary-tooltip" title="One or more initialization containers that must run to completion before any app containers run." href="/docs/concepts/workloads/pods/init-containers/" target="_blank">init containers</a>
as well as <a class="glossary-tooltip" title="A container used to run part of a workload. Compare with init container." href="/docs/reference/glossary/?all=true#term-app-container" target="_blank">app containers</a>.
By default, init containers run and complete before the app containers are started.</p><p>You can also have <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar containers</a>
that provide auxiliary services to the main application Pod (for example: a service mesh).</p><div class="feature-state-notice feature-stable" title="Feature Gate: SidecarContainers"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>Enabled by default, the <code>SidecarContainers</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
allows you to specify <code>restartPolicy: Always</code> for init containers.
Setting the <code>Always</code> restart policy ensures that the containers where you set it are
treated as <em>sidecars</em> that are kept running during the entire lifetime of the Pod.
Containers that you explicitly define as sidecar containers
start up before the main application Pod and remain running until the Pod is
shut down.</p><h2 id="container-probes">Container probes</h2><p>A <em>probe</em> is a diagnostic performed periodically by the kubelet on a container.
To perform a diagnostic, the kubelet can invoke different actions:</p><ul><li><code>ExecAction</code> (performed with the help of the container runtime)</li><li><code>TCPSocketAction</code> (checked directly by the kubelet)</li><li><code>HTTPGetAction</code> (checked directly by the kubelet)</li></ul><p>You can read more about <a href="/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">probes</a>
in the Pod Lifecycle documentation.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about the <a href="/docs/concepts/workloads/pods/pod-lifecycle/">lifecycle of a Pod</a>.</li><li>Learn about <a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a> and how you can use it to
configure different Pods with different container runtime configurations.</li><li>Read about <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>
and how you can use it to manage application availability during disruptions.</li><li>Pod is a top-level resource in the Kubernetes REST API.
The
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/">Pod</a>
object definition describes the object in detail.</li><li><a href="/blog/2015/06/the-distributed-system-toolkit-patterns/">The Distributed System Toolkit: Patterns for Composite Containers</a> explains common layouts for Pods with more than one container.</li><li>Read about <a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod topology spread constraints</a></li></ul><p>To understand the context for why Kubernetes wraps a common Pod API in other resources
(such as <a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSets</a> or
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployments</a>),
you can read about the prior art, including:</p><ul><li><a href="https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema">Aurora</a></li><li><a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/">Borg</a></li><li><a href="https://github.com/d2iq-archive/marathon">Marathon</a></li><li><a href="https://research.google/pubs/pub41684/">Omega</a></li><li><a href="https://engineering.fb.com/data-center-engineering/tupperware/">Tupperware</a>.</li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Pod Lifecycle</h1><p>This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting
in the <code>Pending</code> <a href="#pod-phase">phase</a>, moving through <code>Running</code> if at least one
of its primary containers starts OK, and then through either the <code>Succeeded</code> or
<code>Failed</code> phases depending on whether any container in the Pod terminated in failure.</p><p>Like individual application containers, Pods are considered to be relatively
ephemeral (rather than durable) entities. Pods are created, assigned a unique
ID (<a href="/docs/concepts/overview/working-with-objects/names/#uids">UID</a>), and scheduled
to run on nodes where they remain until termination (according to restart policy) or
deletion.
If a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">Node</a> dies, the Pods running on (or scheduled
to run on) that node are <a href="#pod-garbage-collection">marked for deletion</a>. The control
plane marks the Pods for removal after a timeout period.</p><h2 id="pod-lifetime">Pod lifetime</h2><p>Whilst a Pod is running, the kubelet is able to restart containers to handle some
kind of faults. Within a Pod, Kubernetes tracks different container
<a href="#container-states">states</a> and determines what action to take to make the Pod
healthy again.</p><p>In the Kubernetes API, Pods have both a specification and an actual status. The
status for a Pod object consists of a set of <a href="#pod-conditions">Pod conditions</a>.
You can also inject <a href="#pod-readiness-gate">custom readiness information</a> into the
condition data for a Pod, if that is useful to your application.</p><p>Pods are only <a href="/docs/concepts/scheduling-eviction/">scheduled</a> once in their lifetime;
assigning a Pod to a specific node is called <em>binding</em>, and the process of selecting
which node to use is called <em>scheduling</em>.
Once a Pod has been scheduled and is bound to a node, Kubernetes tries
to run that Pod on the node. The Pod runs on that node until it stops, or until the Pod
is <a href="#pod-termination">terminated</a>; if Kubernetes isn't able to start the Pod on the selected
node (for example, if the node crashes before the Pod starts), then that particular Pod
never starts.</p><p>You can use <a href="/docs/concepts/scheduling-eviction/pod-scheduling-readiness/">Pod Scheduling Readiness</a>
to delay scheduling for a Pod until all its <em>scheduling gates</em> are removed. For example,
you might want to define a set of Pods but only trigger scheduling once all the Pods
have been created.</p><h3 id="pod-fault-recovery">Pods and fault recovery</h3><p>If one of the containers in the Pod fails, then Kubernetes may try to restart that
specific container.
Read <a href="#container-restarts">How Pods handle problems with containers</a> to learn more.</p><p>Pods can however fail in a way that the cluster cannot recover from, and in that case
Kubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the
Pod and relies on other components to provide automatic healing.</p><p>If a Pod is scheduled to a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a> and that
node then fails, the Pod is treated as unhealthy and Kubernetes eventually deletes the Pod.
A Pod won't survive an <a class="glossary-tooltip" title="Process of terminating one or more Pods on Nodes" href="/docs/concepts/scheduling-eviction/" target="_blank">eviction</a> due to
a lack of resources or Node maintenance.</p><p>Kubernetes uses a higher-level abstraction, called a
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>, that handles the work of
managing the relatively disposable Pod instances.</p><p>A given Pod (as defined by a UID) is never "rescheduled" to a different node; instead,
that Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can
even have same name (as in <code>.metadata.name</code>) that the old Pod had, but the replacement
would have a different <code>.metadata.uid</code> from the old Pod.</p><p>Kubernetes does not guarantee that a replacement for an existing Pod would be scheduled to
the same node as the old Pod that was being replaced.</p><h3 id="associated-lifetimes">Associated lifetimes</h3><p>When something is said to have the same lifetime as a Pod, such as a
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">volume</a>,
that means that the thing exists as long as that specific Pod (with that exact UID)
exists. If that Pod is deleted for any reason, and even if an identical replacement
is created, the related thing (a volume, in this example) is also destroyed and
created anew.</p><figure class="diagram-medium"><img src="/images/docs/pod.svg" alt="A multi-container Pod that contains a file puller sidecar and a web server. The Pod uses an ephemeral emptyDir volume for shared storage between the containers."><figcaption><h4>Figure 1.</h4><p>A multi-container Pod that contains a file puller <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar</a> and a web server. The Pod uses an <a href="/docs/concepts/storage/volumes/#emptydir">ephemeral <code>emptyDir</code> volume</a> for shared storage between the containers.</p></figcaption></figure><h2 id="pod-phase">Pod phase</h2><p>A Pod's <code>status</code> field is a
<a href="/docs/reference/generated/kubernetes-api/v1.34/#podstatus-v1-core">PodStatus</a>
object, which has a <code>phase</code> field.</p><p>The phase of a Pod is a simple, high-level summary of where the Pod is in its
lifecycle. The phase is not intended to be a comprehensive rollup of observations
of container or Pod state, nor is it intended to be a comprehensive state machine.</p><p>The number and meanings of Pod phase values are tightly guarded.
Other than what is documented here, nothing should be assumed about Pods that
have a given <code>phase</code> value.</p><p>Here are the possible values for <code>phase</code>:</p><table><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td><code>Pending</code></td><td>The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.</td></tr><tr><td><code>Running</code></td><td>The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.</td></tr><tr><td><code>Succeeded</code></td><td>All containers in the Pod have terminated in success, and will not be restarted.</td></tr><tr><td><code>Failed</code></td><td>All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting.</td></tr><tr><td><code>Unknown</code></td><td>For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.</td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>When a pod is failing to start repeatedly, <code>CrashLoopBackOff</code> may appear in the <code>Status</code> field of some kubectl commands.
Similarly, when a pod is being deleted, <code>Terminating</code> may appear in the <code>Status</code> field of some kubectl commands.</p><p>Make sure not to confuse <em>Status</em>, a kubectl display field for user intuition, with the pod's <code>phase</code>.
Pod phase is an explicit part of the Kubernetes data model and of the
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/">Pod API</a>.</p><pre tabindex="0"><code>  NAMESPACE               NAME               READY   STATUS             RESTARTS   AGE
  alessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h
</code></pre><hr><p>A Pod is granted a term to terminate gracefully, which defaults to 30 seconds.
You can use the flag <code>--force</code> to <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced">terminate a Pod by force</a>.</p></div><p>Since Kubernetes 1.27, the kubelet transitions deleted Pods, except for
<a href="/docs/tasks/configure-pod-container/static-pod/">static Pods</a> and
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced">force-deleted Pods</a>
without a finalizer, to a terminal phase (<code>Failed</code> or <code>Succeeded</code> depending on
the exit statuses of the pod containers) before their deletion from the API server.</p><p>If a node dies or is disconnected from the rest of the cluster, Kubernetes
applies a policy for setting the <code>phase</code> of all Pods on the lost node to Failed.</p><h2 id="container-states">Container states</h2><p>As well as the <a href="#pod-phase">phase</a> of the Pod overall, Kubernetes tracks the state of
each container inside a Pod. You can use
<a href="/docs/concepts/containers/container-lifecycle-hooks/">container lifecycle hooks</a> to
trigger events to run at certain points in a container's lifecycle.</p><p>Once the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">scheduler</a>
assigns a Pod to a Node, the kubelet starts creating containers for that Pod
using a <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>.
There are three possible container states: <code>Waiting</code>, <code>Running</code>, and <code>Terminated</code>.</p><p>To check the state of a Pod's containers, you can use
<code>kubectl describe pod &lt;name-of-pod&gt;</code>. The output shows the state for each container
within that Pod.</p><p>Each state has a specific meaning:</p><h3 id="container-state-waiting"><code>Waiting</code></h3><p>If a container is not in either the <code>Running</code> or <code>Terminated</code> state, it is <code>Waiting</code>.
A container in the <code>Waiting</code> state is still running the operations it requires in
order to complete start up: for example, pulling the container image from a container
image registry, or applying <a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a>
data.
When you use <code>kubectl</code> to query a Pod with a container that is <code>Waiting</code>, you also see
a Reason field to summarize why the container is in that state.</p><h3 id="container-state-running"><code>Running</code></h3><p>The <code>Running</code> status indicates that a container is executing without issues. If there
was a <code>postStart</code> hook configured, it has already executed and finished. When you use
<code>kubectl</code> to query a Pod with a container that is <code>Running</code>, you also see information
about when the container entered the <code>Running</code> state.</p><h3 id="container-state-terminated"><code>Terminated</code></h3><p>A container in the <code>Terminated</code> state began execution and then either ran to
completion or failed for some reason. When you use <code>kubectl</code> to query a Pod with
a container that is <code>Terminated</code>, you see a reason, an exit code, and the start and
finish time for that container's period of execution.</p><p>If a container has a <code>preStop</code> hook configured, this hook runs before the container enters
the <code>Terminated</code> state.</p><h2 id="container-restarts">How Pods handle problems with containers</h2><p>Kubernetes manages container failures within Pods using a <a href="#restart-policy"><code>restartPolicy</code></a> defined in the Pod <code>spec</code>. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:</p><ol><li><strong>Initial crash</strong>: Kubernetes attempts an immediate restart based on the Pod <code>restartPolicy</code>.</li><li><strong>Repeated crashes</strong>: After the initial crash Kubernetes applies an exponential
backoff delay for subsequent restarts, described in <a href="#restart-policy"><code>restartPolicy</code></a>.
This prevents rapid, repeated restart attempts from overloading the system.</li><li><strong>CrashLoopBackOff state</strong>: This indicates that the backoff delay mechanism is currently
in effect for a given container that is in a crash loop, failing and restarting repeatedly.</li><li><strong>Backoff reset</strong>: If a container runs successfully for a certain duration
(e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash
as the first one.</li></ol><p>In practice, a <code>CrashLoopBackOff</code> is a condition or event that might be seen as output
from the <code>kubectl</code> command, while describing or listing Pods, when a container in the Pod
fails to start properly and then continually tries and fails in a loop.</p><p>In other words, when a container enters the crash loop, Kubernetes applies the
exponential backoff delay mentioned in the <a href="#restart-policy">Container restart policy</a>.
This mechanism prevents a faulty container from overwhelming the system with continuous
failed start attempts.</p><p>The <code>CrashLoopBackOff</code> can be caused by issues like the following:</p><ul><li>Application errors that cause the container to exit.</li><li>Configuration errors, such as incorrect environment variables or missing
configuration files.</li><li>Resource constraints, where the container might not have enough memory or CPU
to start properly.</li><li>Health checks failing if the application doesn't start serving within the
expected time.</li><li>Container liveness probes or startup probes returning a <code>Failure</code> result
as mentioned in the <a href="#container-probes">probes section</a>.</li></ul><p>To investigate the root cause of a <code>CrashLoopBackOff</code> issue, a user can:</p><ol><li><strong>Check logs</strong>: Use <code>kubectl logs &lt;name-of-pod&gt;</code> to check the logs of the container.
This is often the most direct way to diagnose the issue causing the crashes.</li><li><strong>Inspect events</strong>: Use <code>kubectl describe pod &lt;name-of-pod&gt;</code> to see events
for the Pod, which can provide hints about configuration or resource issues.</li><li><strong>Review configuration</strong>: Ensure that the Pod configuration, including
environment variables and mounted volumes, is correct and that all required
external resources are available.</li><li><strong>Check resource limits</strong>: Make sure that the container has enough CPU
and memory allocated. Sometimes, increasing the resources in the Pod definition
can resolve the issue.</li><li><strong>Debug application</strong>: There might exist bugs or misconfigurations in the
application code. Running this container image locally or in a development
environment can help diagnose application specific issues.</li></ol><h3 id="restart-policy">Container restarts</h3><p>When a container in your Pod stops, or experiences failure, Kubernetes can restart it.
A restart isn't always appropriate; for example,
<a class="glossary-tooltip" title="One or more initialization containers that must run to completion before any app containers run." href="/docs/concepts/workloads/pods/init-containers/" target="_blank">init containers</a> run only once,
during Pod startup.</p><p>You can configure restarts as a policy that applies to all Pods, or using container-level configuration (for example: when you define a
<a class="glossary-tooltip" title="An auxilliary container that stays running throughout the lifecycle of a Pod." href="/docs/concepts/workloads/pods/sidecar-containers/" target="_blank">sidecar container</a>).</p><h4 id="container-restart-resilience">Container restarts and resilience</h4><p>The Kubernetes project recommends following cloud-native principles, including resilient
design that accounts for unannounced or arbitrary restarts. You can achieve this either
by failing the Pod and relying on automatic
<a href="/docs/concepts/workloads/controllers/">replacement</a>, or you can design for container-level resilience.
Either approach helps to ensure that your overall workload remains available despite
partial failure.</p><h4 id="pod-level-container-restart-policy">Pod-level container restart policy</h4><p>The <code>spec</code> of a Pod has a <code>restartPolicy</code> field with possible values Always, OnFailure,
and Never. The default value is Always.</p><p>The <code>restartPolicy</code> for a Pod applies to <a class="glossary-tooltip" title="A container used to run part of a workload. Compare with init container." href="/docs/reference/glossary/?all=true#term-app-container" target="_blank">app containers</a>
in the Pod and to regular <a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>.
<a href="/docs/concepts/workloads/pods/sidecar-containers/">Sidecar containers</a>
ignore the Pod-level <code>restartPolicy</code> field: in Kubernetes, a sidecar is defined as an
entry inside <code>initContainers</code> that has its container-level <code>restartPolicy</code> set to <code>Always</code>.
For init containers that exit with an error, the kubelet restarts the init container if
the Pod level <code>restartPolicy</code> is either <code>OnFailure</code> or <code>Always</code>:</p><ul><li><code>Always</code>: Automatically restarts the container after any termination.</li><li><code>OnFailure</code>: Only restarts the container if it exits with an error (non-zero exit status).</li><li><code>Never</code>: Does not automatically restart the terminated container.</li></ul><p>When the kubelet is handling container restarts according to the configured restart
policy, that only applies to restarts that make replacement containers inside the
same Pod and running on the same node. After containers in a Pod exit, the kubelet
restarts them with an exponential backoff delay (10s, 20s, 40s, &#8230;), that is capped at
300 seconds (5 minutes). Once a container has executed for 10 minutes without any
problems, the kubelet resets the restart backoff timer for that container.
<a href="/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle">Sidecar containers and Pod lifecycle</a>
explains the behaviour of <code>init containers</code> when specify <code>restartpolicy</code> field on it.</p><h4 id="container-restart-rules">Individual container restart policy and rules</h4><div class="feature-state-notice feature-alpha" title="Feature Gate: ContainerRestartRules"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>If your cluster has the feature gate <code>ContainerRestartRules</code> enabled, you can specify
<code>restartPolicy</code> and <code>restartPolicyRules</code> on <em>individual containers</em> to override the Pod
restart policy. Container restart policy and rules applies to <a class="glossary-tooltip" title="A container used to run part of a workload. Compare with init container." href="/docs/reference/glossary/?all=true#term-app-container" target="_blank">app containers</a>
in the Pod and to regular <a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>.</p><p>A Kubernetes-native <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar container</a>
has its container-level <code>restartPolicy</code> set to <code>Always</code>, and does not support <code>restartPolicyRules</code>.</p><p>The container restarts will follow the same exponential backoff as pod restart policy described above.
Supported container restart policies:</p><ul><li><code>Always</code>: Automatically restarts the container after any termination.</li><li><code>OnFailure</code>: Only restarts the container if it exits with an error (non-zero exit status).</li><li><code>Never</code>: Does not automatically restart the terminated container.</li></ul><p>Additionally, <em>individual containers</em> can specify <code>restartPolicyRules</code>. If the <code>restartPolicyRules</code>
field is specified, then container <code>restartPolicy</code> <strong>must</strong> also be specified. The <code>restartPolicyRules</code>
define a list of rules to apply on container exit. Each rule will consist of a condition
and an action. The supported condition is <code>exitCodes</code>, which compares the exit code of the container
with a list of given values. The supported action is <code>Restart</code>, which means the container will be
restarted. The rules will be evaluated in order. On the first match, the action will be applied.
If none of the rules&#8217; conditions matched, Kubernetes fallback to container&#8217;s configured
<code>restartPolicy</code>.</p><p>For example, a Pod with OnFailure restart policy that have a <code>try-once</code> container. This allows
Pod to only restart certain containers:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span><span>on</span>-failure-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>OnFailure<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>try-once-container   <span> </span><span># This container will run only once because the restartPolicy is Never.</span><span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>docker.io/library/busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'echo "Only running once" &amp;&amp; sleep 10 &amp;&amp; exit 1'</span>]<span>
</span></span></span><span><span><span>    </span><span>restartPolicy</span>:<span> </span>Never     <span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span><span>on</span>-failure-container <span> </span><span># This container will be restarted on failure.</span><span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>docker.io/library/busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'echo "Keep restarting" &amp;&amp; sleep 1800 &amp;&amp; exit 1'</span>]<span>
</span></span></span></code></pre></div><p>A Pod with Always restart policy with an init container that only execute once. If the init
container fails, the Pod fails. This allows the Pod to fail if the initialization failed,
but also keep running once the initialization succeeds:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>fail-pod-if-init-fails<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>  </span><span>initContainers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>init-once     <span> </span><span># This init container will only try once. If it fails, the pod will fail.</span><span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>docker.io/library/busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'echo "Failing initialization" &amp;&amp; sleep 10 &amp;&amp; exit 1'</span>]<span>
</span></span></span><span><span><span>    </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>main-container<span> </span><span># This container will always be restarted once initialization succeeds.</span><span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>docker.io/library/busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'sleep 1800 &amp;&amp; exit 0'</span>]<span>
</span></span></span></code></pre></div><p>A Pod with Never restart policy with a container that ignores and restarts on specific exit codes.
This is useful to differentiate between restartable errors and non-restartable errors:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span>metadata:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>restart-on-exit-codes<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>restart-on-exit-codes<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>docker.io/library/busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'sleep 60 &amp;&amp; exit 0'</span>]<span>
</span></span></span><span><span><span>    </span><span>restartPolicy</span>:<span> </span>Never    <span> </span><span># Container restart policy must be specified if rules are specified</span><span>
</span></span></span><span><span><span>    </span><span>restartPolicyRules</span>:<span>      </span><span># Only restart the container if it exits with code 42</span><span>
</span></span></span><span><span><span>    </span>- <span>action</span>:<span> </span>Restart<span>
</span></span></span><span><span><span>      </span><span>exitCodes</span>:<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>        </span><span>values</span>:<span> </span>[<span>42</span>]<span>
</span></span></span></code></pre></div><p>Restart rules can be used for many more advanced lifecycle management scenarios. Note, restart rules
are affected by the same inconsistencies as the regular restart policy. Kubelet restarts, container
runtime garbage collection, intermitted connectivity issues with the control plane may cause the state
loss and containers may be re-run even when you expect a container not to be restarted.</p><h3 id="reduced-container-restart-delay">Reduced container restart delay</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: ReduceDefaultCrashLoopBackOffDecay"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>With the alpha feature gate <code>ReduceDefaultCrashLoopBackOffDecay</code> enabled,
container start retries across your cluster will be reduced to begin at 1s
(instead of 10s) and increase exponentially by 2x each restart until a maximum
delay of 60s (instead of 300s which is 5 minutes).</p><p>If you use this feature along with the alpha feature
<code>KubeletCrashLoopBackOffMax</code> (described below), individual nodes may have
different maximum delays.</p><h3 id="configurable-container-restart-delay">Configurable container restart delay</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: KubeletCrashLoopBackOffMax"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>With the alpha feature gate <code>KubeletCrashLoopBackOffMax</code> enabled, you can
reconfigure the maximum delay between container start retries from the default
of 300s (5 minutes). This configuration is set per node using kubelet
configuration. In your <a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet
configuration</a>, under
<code>crashLoopBackOff</code> set the <code>maxContainerRestartPeriod</code> field between <code>"1s"</code> and
<code>"300s"</code>. As described above in <a href="#restart-policy">Container restart policy</a>,
delays on that node will still start at 10s and increase exponentially by 2x
each restart, but will now be capped at your configured maximum. If the
<code>maxContainerRestartPeriod</code> you configure is less than the default initial value
of 10s, the initial delay will instead be set to the configured maximum.</p><p>See the following kubelet configuration examples:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># container restart delays will start at 10s, increasing</span><span>
</span></span></span><span><span><span></span><span># 2x each time they are restarted, to a maximum of 100s</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeletConfiguration<span>
</span></span></span><span><span><span></span><span>crashLoopBackOff</span>:<span>
</span></span></span><span><span><span>    </span><span>maxContainerRestartPeriod</span>:<span> </span><span>"100s"</span><span>
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># delays between container restarts will always be 2s</span><span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeletConfiguration<span>
</span></span></span><span><span><span></span><span>crashLoopBackOff</span>:<span>
</span></span></span><span><span><span>    </span><span>maxContainerRestartPeriod</span>:<span> </span><span>"2s"</span><span>
</span></span></span></code></pre></div><p>If you use this feature along with the alpha feature
<code>ReduceDefaultCrashLoopBackOffDecay</code> (described above), your cluster defaults
for initial backoff and maximum backoff will no longer be 10s and 300s, but 1s
and 60s. Per node configuration takes precedence over the defaults set by
<code>ReduceDefaultCrashLoopBackOffDecay</code>, even if this would result in a node having
a longer maximum backoff than other nodes in the cluster.</p><h2 id="pod-conditions">Pod conditions</h2><p>A Pod has a PodStatus, which has an array of
<a href="/docs/reference/generated/kubernetes-api/v1.34/#podcondition-v1-core">PodConditions</a>
through which the Pod has or has not passed. Kubelet manages the following
PodConditions:</p><ul><li><code>PodScheduled</code>: the Pod has been scheduled to a node.</li><li><code>PodReadyToStartContainers</code>: (beta feature; enabled by <a href="#pod-has-network">default</a>) the
Pod sandbox has been successfully created and networking configured.</li><li><code>ContainersReady</code>: all containers in the Pod are ready.</li><li><code>Initialized</code>: all <a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>
have completed successfully.</li><li><code>Ready</code>: the Pod is able to serve requests and should be added to the load
balancing pools of all matching Services.</li><li><code>DisruptionTarget</code>: the pod is about to be terminated due to a disruption (such as preemption, eviction or garbage-collection).</li><li><code>PodResizePending</code>: a pod resize was requested but cannot be applied. See <a href="/docs/tasks/configure-pod-container/resize-container-resources/#pod-resize-status">Pod resize status</a>.</li><li><code>PodResizeInProgress</code>: the pod is in the process of resizing. See <a href="/docs/tasks/configure-pod-container/resize-container-resources/#pod-resize-status">Pod resize status</a>.</li></ul><table><thead><tr><th>Field name</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td>Name of this Pod condition.</td></tr><tr><td><code>status</code></td><td>Indicates whether that condition is applicable, with possible values "<code>True</code>", "<code>False</code>", or "<code>Unknown</code>".</td></tr><tr><td><code>lastProbeTime</code></td><td>Timestamp of when the Pod condition was last probed.</td></tr><tr><td><code>lastTransitionTime</code></td><td>Timestamp for when the Pod last transitioned from one status to another.</td></tr><tr><td><code>reason</code></td><td>Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.</td></tr><tr><td><code>message</code></td><td>Human-readable message indicating details about the last status transition.</td></tr></tbody></table><h3 id="pod-readiness-gate">Pod readiness</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.14 [stable]</code></div><p>Your application can inject extra feedback or signals into PodStatus:
<em>Pod readiness</em>. To use this, set <code>readinessGates</code> in the Pod's <code>spec</code> to
specify a list of additional conditions that the kubelet evaluates for Pod readiness.</p><p>Readiness gates are determined by the current state of <code>status.condition</code>
fields for the Pod. If Kubernetes cannot find such a condition in the
<code>status.conditions</code> field of a Pod, the status of the condition
is defaulted to "<code>False</code>".</p><p>Here is an example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>readinessGates</span>:<span>
</span></span></span><span><span><span>    </span>- <span>conditionType</span>:<span> </span><span>"www.example.com/feature-1"</span><span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>conditions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span>Ready                             <span> </span><span># a built-in PodCondition</span><span>
</span></span></span><span><span><span>      </span><span>status</span>:<span> </span><span>"False"</span><span>
</span></span></span><span><span><span>      </span><span>lastProbeTime</span>:<span> </span><span>null</span><span>
</span></span></span><span><span><span>      </span><span>lastTransitionTime</span>:<span> </span>2018-01-01T00:00:00Z<span>
</span></span></span><span><span><span>    </span>- <span>type</span>:<span> </span><span>"www.example.com/feature-1"</span><span>        </span><span># an extra PodCondition</span><span>
</span></span></span><span><span><span>      </span><span>status</span>:<span> </span><span>"False"</span><span>
</span></span></span><span><span><span>      </span><span>lastProbeTime</span>:<span> </span><span>null</span><span>
</span></span></span><span><span><span>      </span><span>lastTransitionTime</span>:<span> </span>2018-01-01T00:00:00Z<span>
</span></span></span><span><span><span>  </span><span>containerStatuses</span>:<span>
</span></span></span><span><span><span>    </span>- <span>containerID</span>:<span> </span>docker://abcd...<span>
</span></span></span><span><span><span>      </span><span>ready</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>The Pod conditions you add must have names that meet the Kubernetes
<a href="/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">label key format</a>.</p><h3 id="pod-readiness-status">Status for Pod readiness</h3><p>The <code>kubectl patch</code> command does not support patching object status.
To set these <code>status.conditions</code> for the Pod, applications and
<a class="glossary-tooltip" title="A specialized controller used to manage a custom resource" href="/docs/concepts/extend-kubernetes/operator/" target="_blank">operators</a> should use
the <code>PATCH</code> action.
You can use a <a href="/docs/reference/using-api/client-libraries/">Kubernetes client library</a> to
write code that sets custom Pod conditions for Pod readiness.</p><p>For a Pod that uses custom conditions, that Pod is evaluated to be ready <strong>only</strong>
when both the following statements apply:</p><ul><li>All containers in the Pod are ready.</li><li>All conditions specified in <code>readinessGates</code> are <code>True</code>.</li></ul><p>When a Pod's containers are Ready but at least one custom condition is missing or
<code>False</code>, the kubelet sets the Pod's <a href="#pod-conditions">condition</a> to <code>ContainersReady</code>.</p><h3 id="pod-has-network">Pod network readiness</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [beta]</code></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>During its early development, this condition was named <code>PodHasNetwork</code>.</div><p>After a Pod gets scheduled on a node, it needs to be admitted by the kubelet and
to have any required storage volumes mounted. Once these phases are complete,
the kubelet works with
a container runtime (using <a class="glossary-tooltip" title="Protocol for communication between the kubelet and the local container runtime." href="/docs/concepts/architecture/cri" target="_blank">Container Runtime Interface (CRI)</a>) to set up a
runtime sandbox and configure networking for the Pod. If the
<code>PodReadyToStartContainersCondition</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> is enabled
(it is enabled by default for Kubernetes 1.34), the
<code>PodReadyToStartContainers</code> condition will be added to the <code>status.conditions</code> field of a Pod.</p><p>The <code>PodReadyToStartContainers</code> condition is set to <code>False</code> by the Kubelet when it detects a
Pod does not have a runtime sandbox with networking configured. This occurs in
the following scenarios:</p><ul><li>Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for
the Pod using the container runtime.</li><li>Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:<ul><li>the node rebooting, without the Pod getting evicted</li><li>for container runtimes that use virtual machines for isolation, the Pod
sandbox virtual machine rebooting, which then requires creating a new sandbox and
fresh container network configuration.</li></ul></li></ul><p>The <code>PodReadyToStartContainers</code> condition is set to <code>True</code> by the kubelet after the
successful completion of sandbox creation and network configuration for the Pod
by the runtime plugin. The kubelet can start pulling container images and create
containers after <code>PodReadyToStartContainers</code> condition has been set to <code>True</code>.</p><p>For a Pod with init containers, the kubelet sets the <code>Initialized</code> condition to
<code>True</code> after the init containers have successfully completed (which happens
after successful sandbox creation and network configuration by the runtime
plugin). For a Pod without init containers, the kubelet sets the <code>Initialized</code>
condition to <code>True</code> before sandbox creation and network configuration starts.</p><h2 id="container-probes">Container probes</h2><p>A <em>probe</em> is a diagnostic performed periodically by the <a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a>
on a container. To perform a diagnostic, the kubelet either executes code within the container,
or makes a network request.</p><h3 id="probe-check-methods">Check mechanisms</h3><p>There are four different ways to check a container using a probe.
Each probe must define exactly one of these four mechanisms:</p><dl><dt><code>exec</code></dt><dd>Executes a specified command inside the container. The diagnostic
is considered successful if the command exits with a status code of 0.</dd><dt><code>grpc</code></dt><dd>Performs a remote procedure call using <a href="https://grpc.io/">gRPC</a>.
The target should implement
<a href="https://grpc.io/grpc/core/md_doc_health-checking.html">gRPC health checks</a>.
The diagnostic is considered successful if the <code>status</code>
of the response is <code>SERVING</code>.</dd><dt><code>httpGet</code></dt><dd>Performs an HTTP <code>GET</code> request against the Pod's IP
address on a specified port and path. The diagnostic is
considered successful if the response has a status code
greater than or equal to 200 and less than 400.</dd><dt><code>tcpSocket</code></dt><dd>Performs a TCP check against the Pod's IP address on
a specified port. The diagnostic is considered successful if
the port is open. If the remote system (the container) closes
the connection immediately after it opens, this counts as healthy.</dd></dl><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Unlike the other mechanisms, <code>exec</code> probe's implementation involves
the creation/forking of multiple processes each time when executed.
As a result, in case of the clusters having higher pod densities,
lower intervals of <code>initialDelaySeconds</code>, <code>periodSeconds</code>,
configuring any probe with exec mechanism might introduce an overhead on the cpu usage of the node.
In such scenarios, consider using the alternative probe mechanisms to avoid the overhead.</div><h3 id="probe-outcome">Probe outcome</h3><p>Each probe has one of three results:</p><dl><dt><code>Success</code></dt><dd>The container passed the diagnostic.</dd><dt><code>Failure</code></dt><dd>The container failed the diagnostic.</dd><dt><code>Unknown</code></dt><dd>The diagnostic failed (no action should be taken, and the kubelet
will make further checks).</dd></dl><h3 id="types-of-probe">Types of probe</h3><p>The kubelet can optionally perform and react to three kinds of probes on running
containers:</p><dl><dt><code>livenessProbe</code></dt><dd>Indicates whether the container is running. If
the liveness probe fails, the kubelet kills the container, and the container
is subjected to its <a href="#restart-policy">restart policy</a>. If a container does not
provide a liveness probe, the default state is <code>Success</code>.</dd><dt><code>readinessProbe</code></dt><dd>Indicates whether the container is ready to respond to requests.
If the readiness probe fails, the <a class="glossary-tooltip" title="EndpointSlices track the IP addresses of Pods for Services." href="/docs/concepts/services-networking/endpoint-slices/" target="_blank">EndpointSlice</a>
controller removes the Pod's IP address from the EndpointSlices of all Services that match the Pod.
The default state of readiness before the initial delay is <code>Failure</code>. If a container does
not provide a readiness probe, the default state is <code>Success</code>.</dd><dt><code>startupProbe</code></dt><dd>Indicates whether the application within the container is started.
All other probes are disabled if a startup probe is provided, until it succeeds.
If the startup probe fails, the kubelet kills the container, and the container
is subjected to its <a href="#restart-policy">restart policy</a>. If a container does not
provide a startup probe, the default state is <code>Success</code>.</dd></dl><p>For more information about how to set up a liveness, readiness, or startup probe,
see <a href="/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Configure Liveness, Readiness and Startup Probes</a>.</p><h4 id="when-should-you-use-a-liveness-probe">When should you use a liveness probe?</h4><p>If the process in your container is able to crash on its own whenever it
encounters an issue or becomes unhealthy, you do not necessarily need a liveness
probe; the kubelet will automatically perform the correct action in accordance
with the Pod's <code>restartPolicy</code>.</p><p>If you'd like your container to be killed and restarted if a probe fails, then
specify a liveness probe, and specify a <code>restartPolicy</code> of Always or OnFailure.</p><h4 id="when-should-you-use-a-readiness-probe">When should you use a readiness probe?</h4><p>If you'd like to start sending traffic to a Pod only when a probe succeeds,
specify a readiness probe. In this case, the readiness probe might be the same
as the liveness probe, but the existence of the readiness probe in the spec means
that the Pod will start without receiving any traffic and only start receiving
traffic after the probe starts succeeding.</p><p>If you want your container to be able to take itself down for maintenance, you
can specify a readiness probe that checks an endpoint specific to readiness that
is different from the liveness probe.</p><p>If your app has a strict dependency on back-end services, you can implement both
a liveness and a readiness probe. The liveness probe passes when the app itself
is healthy, but the readiness probe additionally checks that each required
back-end service is available. This helps you avoid directing traffic to Pods
that can only respond with error messages.</p><p>If your container needs to work on loading large data, configuration files, or
migrations during startup, you can use a
<a href="#when-should-you-use-a-startup-probe">startup probe</a>. However, if you want to
detect the difference between an app that has failed and an app that is still
processing its startup data, you might prefer a readiness probe.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you want to be able to drain requests when the Pod is deleted, you do not
necessarily need a readiness probe; when the Pod is deleted, the corresponding endpoint
in the <code>EndpointSlice</code> will update its <a href="/docs/concepts/services-networking/endpoint-slices/#conditions">conditions</a>:
the endpoint <code>ready</code> condition will be set to <code>false</code>, so load balancers
will not use the Pod for regular traffic. See <a href="#pod-termination">Pod termination</a>
for more information about how the kubelet handles Pod deletion.</div><h4 id="when-should-you-use-a-startup-probe">When should you use a startup probe?</h4><p>Startup probes are useful for Pods that have containers that take a long time to
come into service. Rather than set a long liveness interval, you can configure
a separate configuration for probing the container as it starts up, allowing
a time longer than the liveness interval would allow.</p><p>If your container usually starts in more than
\( initialDelaySeconds + failureThreshold \times periodSeconds \), you should specify a
startup probe that checks the same endpoint as the liveness probe. The default for
<code>periodSeconds</code> is 10s. You should then set its <code>failureThreshold</code> high enough to
allow the container to start, without changing the default values of the liveness
probe. This helps to protect against deadlocks.</p><h2 id="pod-termination">Termination of Pods</h2><p>Because Pods represent processes running on nodes in the cluster, it is important to
allow those processes to gracefully terminate when they are no longer needed (rather
than being abruptly stopped with a <code>KILL</code> signal and having no chance to clean up).</p><p>The design aim is for you to be able to request deletion and know when processes
terminate, but also be able to ensure that deletes eventually complete.
When you request deletion of a Pod, the cluster records and tracks the intended grace period
before the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in
place, the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> attempts graceful
shutdown.</p><p>Typically, with this graceful termination of the pod, kubelet makes requests to the container runtime
to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal,
with a grace period timeout, to the main process in each container.
The requests to stop the containers are processed by the container runtime asynchronously.
There is no guarantee to the order of processing for these requests.
Many container runtimes respect the <code>STOPSIGNAL</code> value defined in the container image and,
if different, send the container image configured STOPSIGNAL instead of TERM.
Once the grace period has expired, the KILL signal is sent to any remaining
processes, and the Pod is then deleted from the
<a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API Server</a>. If the kubelet or the
container runtime's management service is restarted while waiting for processes to terminate, the
cluster retries from the start including the full original grace period.</p><h3 id="pod-termination-stop-signals">Stop Signals</h3><p>The stop signal used to kill the container can be defined in the container image with the <code>STOPSIGNAL</code> instruction.
If no stop signal is defined in the image, the default signal of the container runtime
(SIGTERM for both containerd and CRI-O) would be used to kill the container.</p><h3 id="defining-custom-stop-signals">Defining custom stop signals</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: ContainerStopSignals"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>If the <code>ContainerStopSignals</code> feature gate is enabled, you can configure a custom stop signal
for your containers from the container Lifecycle. We require the Pod's <code>spec.os.name</code> field
to be present as a requirement for defining stop signals in the container lifecycle.
The list of signals that are valid depends on the OS the Pod is scheduled to.
For Pods scheduled to Windows nodes, we only support SIGTERM and SIGKILL as valid signals.</p><p>Here is an example Pod spec defining a custom stop signal:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>os</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>linux<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>my-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>container-image:latest<span>
</span></span></span><span><span><span>      </span><span>lifecycle</span>:<span>
</span></span></span><span><span><span>        </span><span>stopSignal</span>:<span> </span>SIGUSR1<span>
</span></span></span></code></pre></div><p>If a stop signal is defined in the lifecycle, this will override the signal defined in the container image.
If no stop signal is defined in the container spec, the container would fall back to the default behavior.</p><h3 id="pod-termination-flow">Pod Termination Flow</h3><p>Pod termination flow, illustrated with an example:</p><ol><li><p>You use the <code>kubectl</code> tool to manually delete a specific Pod, with the default grace period
(30 seconds).</p></li><li><p>The Pod in the API server is updated with the time beyond which the Pod is considered "dead"
along with the grace period.
If you use <code>kubectl describe</code> to check the Pod you're deleting, that Pod shows up as "Terminating".
On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked
as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod
shutdown process.</p><ol><li><p>If one of the Pod's containers has defined a <code>preStop</code>
<a href="/docs/concepts/containers/container-lifecycle-hooks/">hook</a> and the <code>terminationGracePeriodSeconds</code>
in the Pod spec is not set to 0, the kubelet runs that hook inside of the container.
The default <code>terminationGracePeriodSeconds</code> setting is 30 seconds.</p><p>If the <code>preStop</code> hook is still running after the grace period expires, the kubelet requests
a small, one-off grace period extension of 2 seconds.<div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If the <code>preStop</code> hook needs longer to complete than the default grace period allows,
you must modify <code>terminationGracePeriodSeconds</code> to suit this.</div></p></li><li><p>The kubelet triggers the container runtime to send a TERM signal to process 1 inside each
container.</p><p>There is <a href="#termination-with-sidecars">special ordering</a> if the Pod has any
<a class="glossary-tooltip" title="An auxilliary container that stays running throughout the lifecycle of a Pod." href="/docs/concepts/workloads/pods/sidecar-containers/" target="_blank">sidecar containers</a> defined.
Otherwise, the containers in the Pod receive the TERM signal at different times and in
an arbitrary order. If the order of shutdowns matters, consider using a <code>preStop</code> hook
to synchronize (or switch to using sidecar containers).</p></li></ol></li><li><p>At the same time as the kubelet is starting graceful shutdown of the Pod, the control plane
evaluates whether to remove that shutting-down Pod from EndpointSlice objects,
where those objects represent a <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a>
with a configured <a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." href="/docs/concepts/overview/working-with-objects/labels/" target="_blank">selector</a>.
<a class="glossary-tooltip" title="ReplicaSet ensures that a specified number of Pod replicas are running at one time" href="/docs/concepts/workloads/controllers/replicaset/" target="_blank">ReplicaSets</a> and other workload resources
no longer treat the shutting-down Pod as a valid, in-service replica.</p><p>Pods that shut down slowly should not continue to serve regular traffic and should start
terminating and finish processing open connections. Some applications need to go beyond
finishing open connections and need more graceful termination, for example, session draining
and completion.</p><p>Any endpoints that represent the terminating Pods are not immediately removed from
EndpointSlices, and a status indicating <a href="/docs/concepts/services-networking/endpoint-slices/#conditions">terminating state</a>
is exposed from the EndpointSlice API.
Terminating endpoints always have their <code>ready</code> status as <code>false</code> (for backward compatibility
with versions before 1.26), so load balancers will not use it for regular traffic.</p><p>If traffic draining on terminating Pod is needed, the actual readiness can be checked as a
condition <code>serving</code>. You can find more details on how to implement connections draining in the
tutorial <a href="/docs/tutorials/services/pods-and-endpoint-termination-flow/">Pods And Endpoints Termination Flow</a></p><a id="pod-termination-beyond-grace-period"></a></li><li><p>The kubelet ensures the Pod is shut down and terminated</p><ol><li>When the grace period expires, if there is still any container running in the Pod, the
kubelet triggers forcible shutdown.
The container runtime sends <code>SIGKILL</code> to any processes still running in any container in the Pod.
The kubelet also cleans up a hidden <code>pause</code> container if that container runtime uses one.</li><li>The kubelet transitions the Pod into a terminal phase (<code>Failed</code> or <code>Succeeded</code> depending on
the end state of its containers).</li><li>The kubelet triggers forcible removal of the Pod object from the API server, by setting grace period
to 0 (immediate deletion).</li><li>The API server deletes the Pod's API object, which is then no longer visible from any client.</li></ol></li></ol><h3 id="pod-termination-forced">Forced Pod termination</h3><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Forced deletions can be potentially disruptive for some workloads and their Pods.</div><p>By default, all deletes are graceful within 30 seconds. The <code>kubectl delete</code> command supports
the <code>--grace-period=&lt;seconds&gt;</code> option which allows you to override the default and specify your
own value.</p><p>Setting the grace period to <code>0</code> forcibly and immediately deletes the Pod from the API
server. If the Pod was still running on a node, that forcible deletion triggers the kubelet to
begin immediate cleanup.</p><p>Using kubectl, You must specify an additional flag <code>--force</code> along with <code>--grace-period=0</code>
in order to perform force deletions.</p><p>When a force deletion is performed, the API server does not wait for confirmation
from the kubelet that the Pod has been terminated on the node it was running on. It
removes the Pod in the API immediately so a new Pod can be created with the same
name. On the node, Pods that are set to terminate immediately will still be given
a small grace period before being force killed.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Immediate deletion does not wait for confirmation that the running resource has been terminated.
The resource may continue to run on the cluster indefinitely.</div><p>If you need to force-delete Pods that are part of a StatefulSet, refer to the task
documentation for
<a href="/docs/tasks/run-application/force-delete-stateful-set-pod/">deleting Pods from a StatefulSet</a>.</p><h3 id="termination-with-sidecars">Pod shutdown and sidecar containers</h3><p>If your Pod includes one or more
<a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar containers</a>
(init containers with an Always restart policy), the kubelet will delay sending
the TERM signal to these sidecar containers until the last main container has fully terminated.
The sidecar containers will be terminated in the reverse order they are defined in the Pod spec.
This ensures that sidecar containers continue serving the other containers in the Pod until they
are no longer needed.</p><p>This means that slow termination of a main container will also delay the termination of the sidecar containers.
If the grace period expires before the termination process is complete, the Pod may enter <a href="#pod-termination-beyond-grace-period">forced termination</a>.
In this case, all remaining containers in the Pod will be terminated simultaneously with a short grace period.</p><p>Similarly, if the Pod has a <code>preStop</code> hook that exceeds the termination grace period, emergency termination may occur.
In general, if you have used <code>preStop</code> hooks to control the termination order without sidecar containers, you can now
remove them and allow the kubelet to manage sidecar termination automatically.</p><h3 id="pod-garbage-collection">Garbage collection of Pods</h3><p>For failed Pods, the API objects remain in the cluster's API until a human or
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> process
explicitly removes them.</p><p>The Pod garbage collector (PodGC), which is a controller in the control plane, cleans up
terminated Pods (with a phase of <code>Succeeded</code> or <code>Failed</code>), when the number of Pods exceeds the
configured threshold (determined by <code>terminated-pod-gc-threshold</code> in the kube-controller-manager).
This avoids a resource leak as Pods are created and terminated over time.</p><p>Additionally, PodGC cleans up any Pods which satisfy any of the following conditions:</p><ol><li>are orphan Pods - bound to a node which no longer exists,</li><li>are unscheduled terminating Pods,</li><li>are terminating Pods, bound to a non-ready node tainted with
<a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-out-of-service"><code>node.kubernetes.io/out-of-service</code></a>.</li></ol><p>Along with cleaning up the Pods, PodGC will also mark them as failed if they are in a non-terminal
phase. Also, PodGC adds a Pod disruption condition when cleaning up an orphan Pod.
See <a href="/docs/concepts/workloads/pods/disruptions/#pod-disruption-conditions">Pod disruption conditions</a>
for more details.</p><h2 id="what-s-next">What's next</h2><ul><li><p>Get hands-on experience
<a href="/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">attaching handlers to container lifecycle events</a>.</p></li><li><p>Get hands-on experience
<a href="/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">configuring Liveness, Readiness and Startup Probes</a>.</p></li><li><p>Learn more about <a href="/docs/concepts/containers/container-lifecycle-hooks/">container lifecycle hooks</a>.</p></li><li><p>Learn more about <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar containers</a>.</p></li><li><p>For detailed information about Pod and container status in the API, see
the API reference documentation covering
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus"><code>status</code></a> for Pod.</p></li></ul></div></div><div><div class="td-content"><h1>Init Containers</h1><p>This page provides an overview of init containers: specialized containers that run
before app containers in a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>.
Init containers can contain utilities or setup scripts not present in an app image.</p><p>You can specify init containers in the Pod specification alongside the <code>containers</code>
array (which describes app containers).</p><p>In Kubernetes, a <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar container</a> is a container that
starts before the main application container and <em>continues to run</em>. This document is about init containers:
containers that run to completion during Pod initialization.</p><h2 id="understanding-init-containers">Understanding init containers</h2><p>A <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a> can have multiple containers
running apps within it, but it can also have one or more init containers, which are run
before the app containers are started.</p><p>Init containers are exactly like regular containers, except:</p><ul><li>Init containers always run to completion.</li><li>Each init container must complete successfully before the next one starts.</li></ul><p>If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.
However, if the Pod has a <code>restartPolicy</code> of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.</p><p>To specify an init container for a Pod, add the <code>initContainers</code> field into
the <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec">Pod specification</a>,
as an array of <code>container</code> items (similar to the app <code>containers</code> field and its contents).
See <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">Container</a> in the
API reference for more details.</p><p>The status of the init containers is returned in <code>.status.initContainerStatuses</code>
field as an array of the container statuses (similar to the <code>.status.containerStatuses</code>
field).</p><h3 id="differences-from-regular-containers">Differences from regular containers</h3><p>Init containers support all the fields and features of app containers,
including resource limits, <a href="/docs/concepts/storage/volumes/">volumes</a>, and security settings. However, the
resource requests and limits for an init container are handled differently,
as documented in <a href="#resource-sharing-within-containers">Resource sharing within containers</a>.</p><p>Regular init containers (in other words: excluding sidecar containers) do not support the
<code>lifecycle</code>, <code>livenessProbe</code>, <code>readinessProbe</code>, or <code>startupProbe</code> fields. Init containers
must run to completion before the Pod can be ready; sidecar containers continue running
during a Pod's lifetime, and <em>do</em> support some probes. See <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar container</a>
for further details about sidecar containers.</p><p>If you specify multiple init containers for a Pod, kubelet runs each init
container sequentially. Each init container must succeed before the next can run.
When all of the init containers have run to completion, kubelet initializes
the application containers for the Pod and runs them as usual.</p><h3 id="differences-from-sidecar-containers">Differences from sidecar containers</h3><p>Init containers run and complete their tasks before the main application container starts.
Unlike <a href="/docs/concepts/workloads/pods/sidecar-containers/">sidecar containers</a>,
init containers are not continuously running alongside the main containers.</p><p>Init containers run to completion sequentially, and the main container does not start
until all the init containers have successfully completed.</p><p>init containers do not support <code>lifecycle</code>, <code>livenessProbe</code>, <code>readinessProbe</code>, or
<code>startupProbe</code> whereas sidecar containers support all these <a href="/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe">probes</a> to control their lifecycle.</p><p>Init containers share the same resources (CPU, memory, network) with the main application
containers but do not interact directly with them. They can, however, use shared volumes
for data exchange.</p><h2 id="using-init-containers">Using init containers</h2><p>Because init containers have separate images from app containers, they
have some advantages for start-up related code:</p><ul><li>Init containers can contain utilities or custom code for setup that are not present in an app
image. For example, there is no need to make an image <code>FROM</code> another image just to use a tool like
<code>sed</code>, <code>awk</code>, <code>python</code>, or <code>dig</code> during setup.</li><li>The application image builder and deployer roles can work independently without
the need to jointly build a single app image.</li><li>Init containers can run with a different view of the filesystem than app containers in the
same Pod. Consequently, they can be given access to
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secrets</a> that app containers cannot access.</li><li>Because init containers run to completion before any app containers start, init containers offer
a mechanism to block or delay app container startup until a set of preconditions are met. Once
preconditions are met, all of the app containers in a Pod can start in parallel.</li><li>Init containers can securely run utilities or custom code that would otherwise make an app
container image less secure. By keeping unnecessary tools separate you can limit the attack
surface of your app container image.</li></ul><h3 id="examples">Examples</h3><p>Here are some ideas for how to use init containers:</p><ul><li><p>Wait for a <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a> to
be created, using a shell one-line command like:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>for</span> i in <span>{</span>1..100<span>}</span>; <span>do</span> sleep 1; <span>if</span> nslookup myservice; <span>then</span> <span>exit</span> 0; <span>fi</span>; <span>done</span>; <span>exit</span> <span>1</span>
</span></span></code></pre></div></li><li><p>Register this Pod with a remote server from the downward API with a command like:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl -X POST http://<span>$MANAGEMENT_SERVICE_HOST</span>:<span>$MANAGEMENT_SERVICE_PORT</span>/register -d <span>'instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)'</span>
</span></span></code></pre></div></li><li><p>Wait for some time before starting the app container with a command like</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>sleep <span>60</span>
</span></span></code></pre></div></li><li><p>Clone a Git repository into a <a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">Volume</a></p></li><li><p>Place values into a configuration file and run a template tool to dynamically
generate a configuration file for the main app container. For example,
place the <code>POD_IP</code> value in a configuration and generate the main app
configuration file using Jinja.</p></li></ul><h4 id="init-containers-in-use">Init containers in use</h4><p>This example defines a simple Pod that has two init containers.
The first waits for <code>myservice</code>, and the second waits for <code>mydb</code>. Once both
init containers complete, the Pod runs the app container from its <code>spec</code> section.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>myapp-pod<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>myapp-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'echo The app is running! &amp;&amp; sleep 3600'</span>]<span>
</span></span></span><span><span><span>  </span><span>initContainers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>init-myservice<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"</span>]<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>init-mydb<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"</span>]<span>
</span></span></span></code></pre></div><p>You can start this Pod by running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>pod/myapp-pod created
</code></pre><p>And check on its status with:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
</code></pre><p>or for more details:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:          myapp-pod
Namespace:     default
[...]
Labels:        app.kubernetes.io/name=MyApp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image "busybox"
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image "busybox"
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice
</code></pre><p>To see logs for the init containers in this Pod, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs myapp-pod -c init-myservice <span># Inspect the first init container</span>
</span></span><span><span>kubectl logs myapp-pod -c init-mydb      <span># Inspect the second init container</span>
</span></span></code></pre></div><p>At this point, those init containers will be waiting to discover <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Services</a> named
<code>mydb</code> and <code>myservice</code>.</p><p>Here's a configuration you can use to make those Services appear:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>myservice<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span><span>9376</span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mydb<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span><span>9377</span><span>
</span></span></span></code></pre></div><p>To create the <code>mydb</code> and <code>myservice</code> services:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f services.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>service/myservice created
service/mydb created
</code></pre><p>You'll then see that those init containers complete, and that the <code>myapp-pod</code>
Pod moves into the Running state:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
</code></pre><p>This simple example should provide some inspiration for you to create your own
init containers. <a href="#what-s-next">What's next</a> contains a link to a more detailed example.</p><h2 id="detailed-behavior">Detailed behavior</h2><p>During Pod startup, the kubelet delays running init containers until the networking
and storage are ready. Then the kubelet runs the Pod's init containers in the order
they appear in the Pod's spec.</p><p>Each init container must exit successfully before
the next container starts. If a container fails to start due to the runtime or
exits with failure, it is retried according to the Pod <code>restartPolicy</code>. However,
if the Pod <code>restartPolicy</code> is set to Always, the init containers use
<code>restartPolicy</code> OnFailure.</p><p>A Pod cannot be <code>Ready</code> until all init containers have succeeded. The ports on an
init container are not aggregated under a Service. A Pod that is initializing
is in the <code>Pending</code> state but should have a condition <code>Initialized</code> set to false.</p><p>If the Pod <a href="#pod-restart-reasons">restarts</a>, or is restarted, all init containers
must execute again.</p><p>Changes to the init container spec are limited to the container image field.
Directly altering the <code>image</code> field of an init container does <em>not</em> restart the
Pod or trigger its recreation. If the Pod has yet to start, that change may
have an effect on how the Pod boots up.</p><p>For a <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a>
you can typically change any field for an init container; the impact of making
that change depends on where the pod template is used.</p><p>Because init containers can be restarted, retried, or re-executed, init container
code should be idempotent. In particular, code that writes into any <code>emptyDir</code> volume
should be prepared for the possibility that an output file already exists.</p><p>Init containers have all of the fields of an app container. However, Kubernetes
prohibits <code>readinessProbe</code> from being used because init containers cannot
define readiness distinct from completion. This is enforced during validation.</p><p>Use <code>activeDeadlineSeconds</code> on the Pod to prevent init containers from failing forever.
The active deadline includes init containers.
However it is recommended to use <code>activeDeadlineSeconds</code> only if teams deploy their application
as a Job, because <code>activeDeadlineSeconds</code> has an effect even after initContainer finished.
The Pod which is already running correctly would be killed by <code>activeDeadlineSeconds</code> if you set.</p><p>The name of each app and init container in a Pod must be unique; a
validation error is thrown for any container sharing a name with another.</p><h3 id="resource-sharing-within-containers">Resource sharing within containers</h3><p>Given the order of execution for init, sidecar and app containers, the following rules
for resource usage apply:</p><ul><li>The highest of any particular resource request or limit defined on all init
containers is the <em>effective init request/limit</em>. If any resource has no
resource limit specified this is considered as the highest limit.</li><li>The Pod's <em>effective request/limit</em> for a resource is the higher of:<ul><li>the sum of all app containers request/limit for a resource</li><li>the effective init request/limit for a resource</li></ul></li><li>Scheduling is done based on effective requests/limits, which means
init containers can reserve resources for initialization that are not used
during the life of the Pod.</li><li>The QoS (quality of service) tier of the Pod's <em>effective QoS tier</em> is the
QoS tier for init containers and app containers alike.</li></ul><p>Quota and limits are applied based on the effective Pod request and
limit.</p><h3 id="cgroups">Init containers and Linux cgroups</h3><p>On Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod
request and limit, the same as the scheduler.</p><h3 id="pod-restart-reasons">Pod restart reasons</h3><p>A Pod can restart, causing re-execution of init containers, for the following
reasons:</p><ul><li>The Pod infrastructure container is restarted. This is uncommon and would
have to be done by someone with root access to nodes.</li><li>All containers in a Pod are terminated while <code>restartPolicy</code> is set to Always,
forcing a restart, and the init container completion record has been lost due
to <a class="glossary-tooltip" title="A collective term for the various mechanisms Kubernetes uses to clean up cluster resources." href="/docs/concepts/architecture/garbage-collection/" target="_blank">garbage collection</a>.</li></ul><p>The Pod will not be restarted when the init container image is changed, or the
init container completion record has been lost due to garbage collection. This
applies for Kubernetes v1.20 and later. If you are using an earlier version of
Kubernetes, consult the documentation for the version you are using.</p><h2 id="what-s-next">What's next</h2><p>Learn more about the following:</p><ul><li><a href="/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container">Creating a Pod that has an init container</a>.</li><li><a href="/docs/tasks/debug/debug-application/debug-init-containers/">Debug init containers</a>.</li><li>Overview of <a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> and <a href="/docs/reference/kubectl/">kubectl</a>.</li><li><a href="/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe">Types of probes</a>: liveness, readiness, startup probe.</li><li><a href="/docs/concepts/workloads/pods/sidecar-containers/">Sidecar containers</a>.</li></ul></div></div><div><div class="td-content"><h1>Sidecar Containers</h1><div class="feature-state-notice feature-stable" title="Feature Gate: SidecarContainers"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>Sidecar containers are the secondary containers that run along with the main
application container within the same <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>.
These containers are used to enhance or to extend the functionality of the primary <em>app
container</em> by providing additional services, or functionality such as logging, monitoring,
security, or data synchronization, without directly altering the primary application code.</p><p>Typically, you only have one app container in a Pod. For example, if you have a web
application that requires a local webserver, the local webserver is a sidecar and the
web application itself is the app container.</p><h2 id="pod-sidecar-containers">Sidecar containers in Kubernetes</h2><p>Kubernetes implements sidecar containers as a special case of
<a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>; sidecar containers remain
running after Pod startup. This document uses the term <em>regular init containers</em> to clearly
refer to containers that only run during Pod startup.</p><p>Provided that your cluster has the <code>SidecarContainers</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> enabled
(the feature is active by default since Kubernetes v1.29), you can specify a <code>restartPolicy</code>
for containers listed in a Pod's <code>initContainers</code> field.
These restartable <em>sidecar</em> containers are independent from other init containers and from
the main application container(s) within the same pod.
These can be started, stopped, or restarted without affecting the main application container
and other init containers.</p><p>You can also run a Pod with multiple containers that are not marked as init or sidecar
containers. This is appropriate if the containers within the Pod are required for the
Pod to work overall, but you don't need to control which containers start or stop first.
You could also do this if you need to support older versions of Kubernetes that don't
support a container-level <code>restartPolicy</code> field.</p><h3 id="sidecar-example">Example application</h3><p>Here's an example of a Deployment with two containers, one of which is a sidecar:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/deployment-sidecar.yaml"><code>application/deployment-sidecar.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/deployment-sidecar.yaml to clipboard"></div><div class="includecode" id="application-deployment-sidecar-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>myapp<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>myapp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>myapp<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>myapp<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>myapp<span>
</span></span></span><span><span><span>          </span><span>image</span>:<span> </span>alpine:latest<span>
</span></span></span><span><span><span>          </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'while true; do echo "logging" &gt;&gt; /opt/logs.txt; sleep 1; done'</span>]<span>
</span></span></span><span><span><span>          </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>              </span><span>mountPath</span>:<span> </span>/opt<span>
</span></span></span><span><span><span>      </span><span>initContainers</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>logshipper<span>
</span></span></span><span><span><span>          </span><span>image</span>:<span> </span>alpine:latest<span>
</span></span></span><span><span><span>          </span><span>restartPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>          </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'tail -F /opt/logs.txt'</span>]<span>
</span></span></span><span><span><span>          </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>              </span><span>mountPath</span>:<span> </span>/opt<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>          </span><span>emptyDir</span>:<span> </span>{}</span></span></code></pre></div></div></div><h2 id="sidecar-containers-and-pod-lifecycle">Sidecar containers and Pod lifecycle</h2><p>If an init container is created with its <code>restartPolicy</code> set to <code>Always</code>, it will
start and remain running during the entire life of the Pod. This can be helpful for
running supporting services separated from the main application containers.</p><p>If a <code>readinessProbe</code> is specified for this init container, its result will be used
to determine the <code>ready</code> state of the Pod.</p><p>Since these containers are defined as init containers, they benefit from the same
ordering and sequential guarantees as regular init containers, allowing you to mix
sidecar containers with regular init containers for complex Pod initialization flows.</p><p>Compared to regular init containers, sidecars defined within <code>initContainers</code> continue to
run after they have started. This is important when there is more than one entry inside
<code>.spec.initContainers</code> for a Pod. After a sidecar-style init container is running (the kubelet
has set the <code>started</code> status for that init container to true), the kubelet then starts the
next init container from the ordered <code>.spec.initContainers</code> list.
That status either becomes true because there is a process running in the
container and no startup probe defined, or as a result of its <code>startupProbe</code> succeeding.</p><p>Upon Pod <a href="/docs/concepts/workloads/pods/pod-lifecycle/#termination-with-sidecars">termination</a>,
the kubelet postpones terminating sidecar containers until the main application container has fully stopped.
The sidecar containers are then shut down in the opposite order of their appearance in the Pod specification.
This approach ensures that the sidecars remain operational, supporting other containers within the Pod,
until their service is no longer required.</p><h3 id="jobs-with-sidecar-containers">Jobs with sidecar containers</h3><p>If you define a Job that uses sidecar using Kubernetes-style init containers,
the sidecar container in each Pod does not prevent the Job from completing after the
main container has finished.</p><p>Here's an example of a Job with two containers, one of which is a sidecar:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/job-sidecar.yaml"><code>application/job/job-sidecar.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/job/job-sidecar.yaml to clipboard"></div><div class="includecode" id="application-job-job-sidecar-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>myjob<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>myjob<span>
</span></span></span><span><span><span>          </span><span>image</span>:<span> </span>alpine:latest<span>
</span></span></span><span><span><span>          </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'echo "logging" &gt; /opt/logs.txt'</span>]<span>
</span></span></span><span><span><span>          </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>              </span><span>mountPath</span>:<span> </span>/opt<span>
</span></span></span><span><span><span>      </span><span>initContainers</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>logshipper<span>
</span></span></span><span><span><span>          </span><span>image</span>:<span> </span>alpine:latest<span>
</span></span></span><span><span><span>          </span><span>restartPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>          </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'tail -F /opt/logs.txt'</span>]<span>
</span></span></span><span><span><span>          </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>            </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>              </span><span>mountPath</span>:<span> </span>/opt<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>          </span><span>emptyDir</span>:<span> </span>{}</span></span></code></pre></div></div></div><h2 id="differences-from-application-containers">Differences from application containers</h2><p>Sidecar containers run alongside <em>app containers</em> in the same pod. However, they do not
execute the primary application logic; instead, they provide supporting functionality to
the main application.</p><p>Sidecar containers have their own independent lifecycles. They can be started, stopped,
and restarted independently of app containers. This means you can update, scale, or
maintain sidecar containers without affecting the primary application.</p><p>Sidecar containers share the same network and storage namespaces with the primary
container. This co-location allows them to interact closely and share resources.</p><p>From a Kubernetes perspective, the sidecar container's graceful termination is less important.
When other containers take all allotted graceful termination time, the sidecar containers
will receive the <code>SIGTERM</code> signal, followed by the <code>SIGKILL</code> signal, before they have time to terminate gracefully.
So exit codes different from <code>0</code> (<code>0</code> indicates successful exit), for sidecar containers are normal
on Pod termination and should be generally ignored by the external tooling.</p><h2 id="differences-from-init-containers">Differences from init containers</h2><p>Sidecar containers work alongside the main container, extending its functionality and
providing additional services.</p><p>Sidecar containers run concurrently with the main application container. They are active
throughout the lifecycle of the pod and can be started and stopped independently of the
main container. Unlike <a href="/docs/concepts/workloads/pods/init-containers/">init containers</a>,
sidecar containers support <a href="/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe">probes</a> to control their lifecycle.</p><p>Sidecar containers can interact directly with the main application containers, because
like init containers they always share the same network, and can optionally also share
volumes (filesystems).</p><p>Init containers stop before the main containers start up, so init containers cannot
exchange messages with the app container in a Pod. Any data passing is one-way
(for example, an init container can put information inside an <code>emptyDir</code> volume).</p><p>Changing the image of a sidecar container will not cause the Pod to restart, but will
trigger a container restart.</p><h2 id="resource-sharing-within-containers">Resource sharing within containers</h2><p>Given the order of execution for init, sidecar and app containers, the following rules
for resource usage apply:</p><ul><li>The highest of any particular resource request or limit defined on all init
containers is the <em>effective init request/limit</em>. If any resource has no
resource limit specified this is considered as the highest limit.</li><li>The Pod's <em>effective request/limit</em> for a resource is the sum of
<a href="/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</a> and the higher of:<ul><li>the sum of all non-init containers(app and sidecar containers) request/limit for a
resource</li><li>the effective init request/limit for a resource</li></ul></li><li>Scheduling is done based on effective requests/limits, which means
init containers can reserve resources for initialization that are not used
during the life of the Pod.</li><li>The QoS (quality of service) tier of the Pod's <em>effective QoS tier</em> is the
QoS tier for all init, sidecar and app containers alike.</li></ul><p>Quota and limits are applied based on the effective Pod request and
limit.</p><h3 id="cgroups">Sidecar containers and Linux cgroups</h3><p>On Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod
request and limit, the same as the scheduler.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn how to <a href="/docs/tutorials/configuration/pod-sidecar-containers/">Adopt Sidecar Containers</a></li><li>Read a blog post on <a href="/blog/2023/08/25/native-sidecar-containers/">native sidecar containers</a>.</li><li>Read about <a href="/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container">creating a Pod that has an init container</a>.</li><li>Learn about the <a href="/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe">types of probes</a>: liveness, readiness, startup probe.</li><li>Learn about <a href="/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</a>.</li></ul></div></div><div><div class="td-content"><h1>Ephemeral Containers</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>This page provides an overview of ephemeral containers: a special type of container
that runs temporarily in an existing <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a> to
accomplish user-initiated actions such as troubleshooting. You use ephemeral
containers to inspect services rather than to build applications.</p><h2 id="understanding-ephemeral-containers">Understanding ephemeral containers</h2><p><a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> are the fundamental building
block of Kubernetes applications. Since Pods are intended to be disposable and
replaceable, you cannot add a container to a Pod once it has been created.
Instead, you usually delete and replace Pods in a controlled fashion using
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">deployments</a>.</p><p>Sometimes it's necessary to inspect the state of an existing Pod, however, for
example to troubleshoot a hard-to-reproduce bug. In these cases you can run
an ephemeral container in an existing Pod to inspect its state and run
arbitrary commands.</p><h3 id="what-is-an-ephemeral-container">What is an ephemeral container?</h3><p>Ephemeral containers differ from other containers in that they lack guarantees
for resources or execution, and they will never be automatically restarted, so
they are not appropriate for building applications. Ephemeral containers are
described using the same <code>ContainerSpec</code> as regular containers, but many fields
are incompatible and disallowed for ephemeral containers.</p><ul><li>Ephemeral containers may not have ports, so fields such as <code>ports</code>,
<code>livenessProbe</code>, <code>readinessProbe</code> are disallowed.</li><li>Pod resource allocations are immutable, so setting <code>resources</code> is disallowed.</li><li>For a complete list of allowed fields, see the <a href="/docs/reference/generated/kubernetes-api/v1.34/#ephemeralcontainer-v1-core">EphemeralContainer reference
documentation</a>.</li></ul><p>Ephemeral containers are created using a special <code>ephemeralcontainers</code> handler
in the API rather than by adding them directly to <code>pod.spec</code>, so it's not
possible to add an ephemeral container using <code>kubectl edit</code>.</p><p>Like regular containers, you may not change or remove an ephemeral container
after you have added it to a Pod.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Ephemeral containers are not supported by <a href="/docs/tasks/configure-pod-container/static-pod/">static pods</a>.</div><h2 id="uses-for-ephemeral-containers">Uses for ephemeral containers</h2><p>Ephemeral containers are useful for interactive troubleshooting when <code>kubectl exec</code> is insufficient because a container has crashed or a container image
doesn't include debugging utilities.</p><p>In particular, <a href="https://github.com/GoogleContainerTools/distroless">distroless images</a>
enable you to deploy minimal container images that reduce attack surface
and exposure to bugs and vulnerabilities. Since distroless images do not include a
shell or any debugging utilities, it's difficult to troubleshoot distroless
images using <code>kubectl exec</code> alone.</p><p>When using ephemeral containers, it's helpful to enable <a href="/docs/tasks/configure-pod-container/share-process-namespace/">process namespace
sharing</a> so
you can view processes in other containers.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn how to <a href="/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container">debug pods using ephemeral containers</a>.</li></ul></div></div><div><div class="td-content"><h1>Disruptions</h1><p>This guide is for application owners who want to build
highly available applications, and thus need to understand
what types of disruptions can happen to Pods.</p><p>It is also for cluster administrators who want to perform automated
cluster actions, like upgrading and autoscaling clusters.</p><h2 id="voluntary-and-involuntary-disruptions">Voluntary and involuntary disruptions</h2><p>Pods do not disappear until someone (a person or a controller) destroys them, or
there is an unavoidable hardware or system software error.</p><p>We call these unavoidable cases <em>involuntary disruptions</em> to
an application. Examples are:</p><ul><li>a hardware failure of the physical machine backing the node</li><li>cluster administrator deletes VM (instance) by mistake</li><li>cloud provider or hypervisor failure makes VM disappear</li><li>a kernel panic</li><li>the node disappears from the cluster due to cluster network partition</li><li>eviction of a pod due to the node being <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">out-of-resources</a>.</li></ul><p>Except for the out-of-resources condition, all these conditions
should be familiar to most users; they are not specific
to Kubernetes.</p><p>We call other cases <em>voluntary disruptions</em>. These include both
actions initiated by the application owner and those initiated by a Cluster
Administrator. Typical application owner actions include:</p><ul><li>deleting the deployment or other controller that manages the pod</li><li>updating a deployment's pod template causing a restart</li><li>directly deleting a pod (e.g. by accident)</li></ul><p>Cluster administrator actions include:</p><ul><li><a href="/docs/tasks/administer-cluster/safely-drain-node/">Draining a node</a> for repair or upgrade.</li><li>Draining a node from a cluster to scale the cluster down (learn about
<a href="/docs/concepts/cluster-administration/node-autoscaling/">Node Autoscaling</a>).</li><li>Removing a pod from a node to permit something else to fit on that node.</li></ul><p>These actions might be taken directly by the cluster administrator, or by automation
run by the cluster administrator, or by your cluster hosting provider.</p><p>Ask your cluster administrator or consult your cloud provider or distribution documentation
to determine if any sources of voluntary disruptions are enabled for your cluster.
If none are enabled, you can skip creating Pod Disruption Budgets.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example,
deleting deployments or pods bypasses Pod Disruption Budgets.</div><h2 id="dealing-with-disruptions">Dealing with disruptions</h2><p>Here are some ways to mitigate involuntary disruptions:</p><ul><li>Ensure your pod <a href="/docs/tasks/configure-pod-container/assign-memory-resource/">requests the resources</a> it needs.</li><li>Replicate your application if you need higher availability. (Learn about running replicated
<a href="/docs/tasks/run-application/run-stateless-application-deployment/">stateless</a>
and <a href="/docs/tasks/run-application/run-replicated-stateful-application/">stateful</a> applications.)</li><li>For even higher availability when running replicated applications,
spread applications across racks (using
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">anti-affinity</a>)
or across zones (if using a
<a href="/docs/setup/multiple-zones">multi-zone cluster</a>.)</li></ul><p>The frequency of voluntary disruptions varies. On a basic Kubernetes cluster, there are
no automated voluntary disruptions (only user-triggered ones). However, your cluster administrator or hosting provider
may run some additional services which cause voluntary disruptions. For example,
rolling out node software updates can cause voluntary disruptions. Also, some implementations
of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.
Your cluster administrator or hosting provider should have documented what level of voluntary
disruptions, if any, to expect. Certain configuration options, such as
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">using PriorityClasses</a>
in your pod spec can also cause voluntary (and involuntary) disruptions.</p><h2 id="pod-disruption-budgets">Pod disruption budgets</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>Kubernetes offers features to help you run highly available applications even when you
introduce frequent voluntary disruptions.</p><p>As an application owner, you can create a PodDisruptionBudget (PDB) for each application.
A PDB limits the number of Pods of a replicated application that are down simultaneously from
voluntary disruptions. For example, a quorum-based application would
like to ensure that the number of replicas running is never brought below the
number needed for a quorum. A web front end might want to
ensure that the number of replicas serving load never falls below a certain
percentage of the total.</p><p>Cluster managers and hosting providers should use tools which
respect PodDisruptionBudgets by calling the <a href="/docs/tasks/administer-cluster/safely-drain-node/#eviction-api">Eviction API</a>
instead of directly deleting pods or deployments.</p><p>For example, the <code>kubectl drain</code> subcommand lets you mark a node as going out of
service. When you run <code>kubectl drain</code>, the tool tries to evict all of the Pods on
the Node you're taking out of service. The eviction request that <code>kubectl</code> submits on
your behalf may be temporarily rejected, so the tool periodically retries all failed
requests until all Pods on the target node are terminated, or until a configurable timeout
is reached.</p><p>A PDB specifies the number of replicas that an application can tolerate having, relative to how
many it is intended to have. For example, a Deployment which has a <code>.spec.replicas: 5</code> is
supposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time,
then the Eviction API will allow voluntary disruption of one (but not two) pods at a time.</p><p>The group of pods that comprise the application is specified using a label selector, the same
as the one used by the application's controller (deployment, stateful-set, etc).</p><p>The "intended" number of pods is computed from the <code>.spec.replicas</code> of the workload resource
that is managing those pods. The control plane discovers the owning workload resource by
examining the <code>.metadata.ownerReferences</code> of the Pod.</p><p><a href="#voluntary-and-involuntary-disruptions">Involuntary disruptions</a> cannot be prevented by PDBs; however they
do count against the budget.</p><p>Pods which are deleted or unavailable due to a rolling upgrade to an application do count
against the disruption budget, but workload resources (such as Deployment and StatefulSet)
are not limited by PDBs when doing rolling upgrades. Instead, the handling of failures
during application updates is configured in the spec for the specific workload resource.</p><p>It is recommended to set <code>AlwaysAllow</code> <a href="/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">Unhealthy Pod Eviction Policy</a>
to your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.
The default behavior is to wait for the application pods to become <a href="/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod">healthy</a>
before the drain can proceed.</p><p>When a pod is evicted using the eviction API, it is gracefully
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">terminated</a>, honoring the
<code>terminationGracePeriodSeconds</code> setting in its <a href="/docs/reference/generated/kubernetes-api/v1.34/#podspec-v1-core">PodSpec</a>.</p><h2 id="pdb-example">PodDisruptionBudget example</h2><p>Consider a cluster with 3 nodes, <code>node-1</code> through <code>node-3</code>.
The cluster is running several applications. One of them has 3 replicas initially called
<code>pod-a</code>, <code>pod-b</code>, and <code>pod-c</code>. Another, unrelated pod without a PDB, called <code>pod-x</code>, is also shown.
Initially, the pods are laid out as follows:</p><table><thead><tr><th>node-1</th><th>node-2</th><th>node-3</th></tr></thead><tbody><tr><td>pod-a <em>available</em></td><td>pod-b <em>available</em></td><td>pod-c <em>available</em></td></tr><tr><td>pod-x <em>available</em></td><td></td><td></td></tr></tbody></table><p>All 3 pods are part of a deployment, and they collectively have a PDB which requires
there be at least 2 of the 3 pods to be available at all times.</p><p>For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.
The cluster administrator first tries to drain <code>node-1</code> using the <code>kubectl drain</code> command.
That tool tries to evict <code>pod-a</code> and <code>pod-x</code>. This succeeds immediately.
Both pods go into the <code>terminating</code> state at the same time.
This puts the cluster in this state:</p><table><thead><tr><th>node-1 <em>draining</em></th><th>node-2</th><th>node-3</th></tr></thead><tbody><tr><td>pod-a <em>terminating</em></td><td>pod-b <em>available</em></td><td>pod-c <em>available</em></td></tr><tr><td>pod-x <em>terminating</em></td><td></td><td></td></tr></tbody></table><p>The deployment notices that one of the pods is terminating, so it creates a replacement
called <code>pod-d</code>. Since <code>node-1</code> is cordoned, it lands on another node. Something has
also created <code>pod-y</code> as a replacement for <code>pod-x</code>.</p><p>(Note: for a StatefulSet, <code>pod-a</code>, which would be called something like <code>pod-0</code>, would need
to terminate completely before its replacement, which is also called <code>pod-0</code> but has a
different UID, could be created. Otherwise, the example applies to a StatefulSet as well.)</p><p>Now the cluster is in this state:</p><table><thead><tr><th>node-1 <em>draining</em></th><th>node-2</th><th>node-3</th></tr></thead><tbody><tr><td>pod-a <em>terminating</em></td><td>pod-b <em>available</em></td><td>pod-c <em>available</em></td></tr><tr><td>pod-x <em>terminating</em></td><td>pod-d <em>starting</em></td><td>pod-y</td></tr></tbody></table><p>At some point, the pods terminate, and the cluster looks like this:</p><table><thead><tr><th>node-1 <em>drained</em></th><th>node-2</th><th>node-3</th></tr></thead><tbody><tr><td></td><td>pod-b <em>available</em></td><td>pod-c <em>available</em></td></tr><tr><td></td><td>pod-d <em>starting</em></td><td>pod-y</td></tr></tbody></table><p>At this point, if an impatient cluster administrator tries to drain <code>node-2</code> or
<code>node-3</code>, the drain command will block, because there are only 2 available
pods for the deployment, and its PDB requires at least 2. After some time passes, <code>pod-d</code> becomes available.</p><p>The cluster state now looks like this:</p><table><thead><tr><th>node-1 <em>drained</em></th><th>node-2</th><th>node-3</th></tr></thead><tbody><tr><td></td><td>pod-b <em>available</em></td><td>pod-c <em>available</em></td></tr><tr><td></td><td>pod-d <em>available</em></td><td>pod-y</td></tr></tbody></table><p>Now, the cluster administrator tries to drain <code>node-2</code>.
The drain command will try to evict the two pods in some order, say
<code>pod-b</code> first and then <code>pod-d</code>. It will succeed at evicting <code>pod-b</code>.
But, when it tries to evict <code>pod-d</code>, it will be refused because that would leave only
one pod available for the deployment.</p><p>The deployment creates a replacement for <code>pod-b</code> called <code>pod-e</code>.
Because there are not enough resources in the cluster to schedule
<code>pod-e</code> the drain will again block. The cluster may end up in this
state:</p><table><thead><tr><th>node-1 <em>drained</em></th><th>node-2</th><th>node-3</th><th><em>no node</em></th></tr></thead><tbody><tr><td></td><td>pod-b <em>terminating</em></td><td>pod-c <em>available</em></td><td>pod-e <em>pending</em></td></tr><tr><td></td><td>pod-d <em>available</em></td><td>pod-y</td><td></td></tr></tbody></table><p>At this point, the cluster administrator needs to
add a node back to the cluster to proceed with the upgrade.</p><p>You can see how Kubernetes varies the rate at which disruptions
can happen, according to:</p><ul><li>how many replicas an application needs</li><li>how long it takes to gracefully shutdown an instance</li><li>how long it takes a new instance to start up</li><li>the type of controller</li><li>the cluster's resource capacity</li></ul><h2 id="pod-disruption-conditions">Pod disruption conditions</h2><div class="feature-state-notice feature-stable" title="Feature Gate: PodDisruptionConditions"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p>A dedicated Pod <code>DisruptionTarget</code> <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions">condition</a>
is added to indicate
that the Pod is about to be deleted due to a <a class="glossary-tooltip" title="An event that leads to Pod(s) going out of service" href="/docs/concepts/workloads/pods/disruptions/" target="_blank">disruption</a>.
The <code>reason</code> field of the condition additionally
indicates one of the following reasons for the Pod termination:</p><dl><dt><code>PreemptionByScheduler</code></dt><dd>Pod is due to be <a class="glossary-tooltip" title="Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node." href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption" target="_blank">preempted</a> by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod priority preemption</a>.</dd><dt><code>DeletionByTaintManager</code></dt><dd>Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within <code>kube-controller-manager</code>) due to a <code>NoExecute</code> taint that the Pod does not tolerate; see <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">taint</a>-based evictions.</dd><dt><code>EvictionByEvictionAPI</code></dt><dd>Pod has been marked for <a class="glossary-tooltip" title="API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination." href="/docs/concepts/scheduling-eviction/api-eviction/" target="_blank">eviction using the Kubernetes API</a> .</dd><dt><code>DeletionByPodGC</code></dt><dd>Pod, that is bound to a no longer existing Node, is due to be deleted by <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection">Pod garbage collection</a>.</dd><dt><code>TerminationByKubelet</code></dt><dd>Pod has been terminated by the kubelet, because of either <a class="glossary-tooltip" title="Node-pressure eviction is the process by which the kubelet proactively fails pods to reclaim resources on nodes." href="/docs/concepts/scheduling-eviction/node-pressure-eviction/" target="_blank">node pressure eviction</a>,
the <a href="/docs/concepts/architecture/nodes/#graceful-node-shutdown">graceful node shutdown</a>,
or preemption for <a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">system critical pods</a>.</dd></dl><p>In all other disruption scenarios, like eviction due to exceeding
<a href="/docs/concepts/configuration/manage-resources-containers/">Pod container limits</a>,
Pods don't receive the <code>DisruptionTarget</code> condition because the disruptions were
probably caused by the Pod and would reoccur on retry.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A Pod disruption might be interrupted. The control plane might re-attempt to
continue the disruption of the same Pod, but it is not guaranteed. As a result,
the <code>DisruptionTarget</code> condition might be added to a Pod, but that Pod might then not actually be
deleted. In such a situation, after some time, the
Pod disruption condition will be cleared.</div><p>Along with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal
phase (see also <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection">Pod garbage collection</a>).</p><p>When using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's
<a href="/docs/concepts/workloads/controllers/job/#pod-failure-policy">Pod failure policy</a>.</p><h2 id="separating-cluster-owner-and-application-owner-roles">Separating Cluster Owner and Application Owner Roles</h2><p>Often, it is useful to think of the Cluster Manager
and Application Owner as separate roles with limited knowledge
of each other. This separation of responsibilities
may make sense in these scenarios:</p><ul><li>when there are many application teams sharing a Kubernetes cluster, and
there is natural specialization of roles</li><li>when third-party tools or services are used to automate cluster management</li></ul><p>Pod Disruption Budgets support this separation of roles by providing an
interface between the roles.</p><p>If you do not have such a separation of responsibilities in your organization,
you may not need to use Pod Disruption Budgets.</p><h2 id="how-to-perform-disruptive-actions-on-your-cluster">How to perform Disruptive Actions on your Cluster</h2><p>If you are a Cluster Administrator, and you need to perform a disruptive action on all
the nodes in your cluster, such as a node or system software upgrade, here are some options:</p><ul><li>Accept downtime during the upgrade.</li><li>Failover to another complete replica cluster.<ul><li>No downtime, but may be costly both for the duplicated nodes
and for human effort to orchestrate the switchover.</li></ul></li><li>Write disruption tolerant applications and use PDBs.<ul><li>No downtime.</li><li>Minimal resource duplication.</li><li>Allows more automation of cluster administration.</li><li>Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary
disruptions largely overlaps with work to support autoscaling and tolerating
involuntary disruptions.</li></ul></li></ul><h2 id="what-s-next">What's next</h2><ul><li><p>Follow steps to protect your application by <a href="/docs/tasks/run-application/configure-pdb/">configuring a Pod Disruption Budget</a>.</p></li><li><p>Learn more about <a href="/docs/tasks/administer-cluster/safely-drain-node/">draining nodes</a></p></li><li><p>Learn about <a href="/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">updating a deployment</a>
including steps to maintain its availability during the rollout.</p></li></ul></div></div><div><div class="td-content"><h1>Pod Hostname</h1><p>This page explains how to set a Pod's hostname,
potential side effects after configuration, and the underlying mechanics.</p><h2 id="default-pod-hostname">Default Pod hostname</h2><p>When a Pod is created, its hostname (as observed from within the Pod)
is derived from the Pod's metadata.name value.
Both the hostname and its corresponding fully qualified domain name (FQDN)
are set to the metadata.name value (from the Pod's perspective)</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>busybox-1<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>      </span>- sleep<span>
</span></span></span><span><span><span>      </span>- <span>"3600"</span><span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>busybox<span>
</span></span></span></code></pre></div><p>The Pod created by this manifest will have its hostname and fully qualified domain name (FQDN) set to <code>busybox-1</code>.</p><h2 id="hostname-with-pod-s-hostname-and-subdomain-fields">Hostname with pod's hostname and subdomain fields</h2><p>The Pod spec includes an optional <code>hostname</code> field.
When set, this value takes precedence over the Pod's <code>metadata.name</code> as the
hostname (observed from within the Pod).
For example, a Pod with spec.hostname set to <code>my-host</code> will have its hostname set to <code>my-host</code>.</p><p>The Pod spec also includes an optional <code>subdomain</code> field,
indicating the Pod belongs to a subdomain within its namespace.
If a Pod has <code>spec.hostname</code> set to "foo" and spec.subdomain set
to "bar" in the namespace <code>my-namespace</code>, its hostname becomes <code>foo</code> and its
fully qualified domain name (FQDN) becomes
<code>foo.bar.my-namespace.svc.cluster-domain.example</code> (observed from within the Pod).</p><p>When both hostname and subdomain are set, the cluster's DNS server will
create A and/or AAAA records based on these fields.
Refer to: <a href="/docs/concepts/services-networking/dns-pod-service/#pod-hostname-and-subdomain-field">Pod's hostname and subdomain fields</a>.</p><h2 id="hostname-with-pod-s-sethostnameasfqdn-fields">Hostname with pod's setHostnameAsFQDN fields</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [stable]</code></div><p>When a Pod is configured to have fully qualified domain name (FQDN), its
hostname is the short hostname. For example, if you have a Pod with the fully
qualified domain name <code>busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example</code>,
then by default the <code>hostname</code> command inside that Pod returns <code>busybox-1</code> and the
<code>hostname --fqdn</code> command returns the FQDN.</p><p>When both <code>setHostnameAsFQDN: true</code> and the subdomain field is set in the Pod spec,
the kubelet writes the Pod's FQDN
into the hostname for that Pod's namespace. In this case, both <code>hostname</code> and <code>hostname --fqdn</code>
return the Pod's FQDN.</p><p>The Pod's FQDN is constructed in the same manner as previously defined.
It is composed of the Pod's <code>spec.hostname</code> (if specified) or <code>metadata.name</code> field,
the <code>spec.subdomain</code>, the <code>namespace</code> name, and the cluster domain suffix.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>In Linux, the hostname field of the kernel (the <code>nodename</code> field of <code>struct utsname</code>) is limited to 64 characters.</p><p>If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start.
The Pod will remain in <code>Pending</code> status (<code>ContainerCreating</code> as seen by <code>kubectl</code>) generating
error events, such as "Failed to construct FQDN from Pod hostname and cluster domain".</p><p>This means that when using this field,
you must ensure the combined length of the Pod's <code>metadata.name</code> (or <code>spec.hostname</code>)
and <code>spec.subdomain</code> fields results in an FQDN that does not exceed 64 characters.</p></div><h2 id="hostname-with-pod-s-hostnameoverride">Hostname with pod's hostnameOverride</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: HostnameOverride"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>Setting a value for <code>hostnameOverride</code> in the Pod spec causes the kubelet
to unconditionally set both the Pod's hostname and fully qualified domain name (FQDN)
to the <code>hostnameOverride</code> value.</p><p>The <code>hostnameOverride</code> field has a length limitation of 64 characters
and must adhere to the DNS subdomain names standard defined in <a href="https://datatracker.ietf.org/doc/html/rfc1123">RFC 1123</a>.</p><p>Example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>busybox-2-busybox-example-domain<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hostnameOverride</span>:<span> </span>busybox-2.busybox.example.domain<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>      </span>- sleep<span>
</span></span></span><span><span><span>      </span>- <span>"3600"</span><span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>busybox<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This only affects the hostname within the Pod; it does not affect the Pod's A or AAAA records in the cluster DNS server.</div><p>If <code>hostnameOverride</code> is set alongside <code>hostname</code> and <code>subdomain</code> fields:</p><ul><li><p>The hostname inside the Pod is overridden to the <code>hostnameOverride</code> value.</p></li><li><p>The Pod's A and/or AAAA records in the cluster DNS server are still generated based on the <code>hostname</code> and <code>subdomain</code> fields.</p></li></ul><p>Note: If <code>hostnameOverride</code> is set, you cannot simultaneously set the <code>hostNetwork</code> and <code>setHostnameAsFQDN</code> fields.
The API server will explicitly reject any create request attempting this combination.</p><p>For details on behavior when <code>hostnameOverride</code> is set in combination with
other fields (hostname, subdomain, setHostnameAsFQDN, hostNetwork),
see the table in the <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/4762-allow-arbitrary-fqdn-as-pod-hostname/README.md#design-details">KEP-4762 design details</a>.</p></div></div><div><div class="td-content"><h1>Pod Quality of Service Classes</h1><p>This page introduces <em>Quality of Service (QoS) classes</em> in Kubernetes, and explains
how Kubernetes assigns a QoS class to each Pod as a consequence of the resource
constraints that you specify for the containers in that Pod. Kubernetes relies on this
classification to make decisions about which Pods to evict when there are not enough
available resources on a Node.</p><h2 id="quality-of-service-classes">Quality of Service classes</h2><p>Kubernetes classifies the Pods that you run and allocates each Pod into a specific
<em>quality of service (QoS) class</em>. Kubernetes uses that classification to influence how different
pods are handled. Kubernetes does this classification based on the
<a href="/docs/concepts/configuration/manage-resources-containers/">resource requests</a>
of the <a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." href="/docs/concepts/containers/" target="_blank">Containers</a> in that Pod, along with
how those requests relate to resource limits.
This is known as <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." href="/docs/concepts/workloads/pods/pod-qos/" target="_blank">Quality of Service</a>
(QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests
and limits of its component Containers. QoS classes are used by Kubernetes to decide
which Pods to evict from a Node experiencing
<a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node Pressure</a>. The possible
QoS classes are <code>Guaranteed</code>, <code>Burstable</code>, and <code>BestEffort</code>. When a Node runs out of resources,
Kubernetes will first evict <code>BestEffort</code> Pods running on that Node, followed by <code>Burstable</code> and
finally <code>Guaranteed</code> Pods. When this eviction is due to resource pressure, only Pods exceeding
resource requests are candidates for eviction.</p><h3 id="guaranteed">Guaranteed</h3><p>Pods that are <code>Guaranteed</code> have the strictest resource limits and are least likely
to face eviction. They are guaranteed not to be killed until they exceed their limits
or there are no lower-priority Pods that can be preempted from the Node. They may
not acquire resources beyond their specified limits. These Pods can also make
use of exclusive CPUs using the
<a href="/docs/tasks/administer-cluster/cpu-management-policies/#static-policy"><code>static</code></a> CPU management policy.</p><h4 id="criteria">Criteria</h4><p>For a Pod to be given a QoS class of <code>Guaranteed</code>:</p><ul><li>Every Container in the Pod must have a memory limit and a memory request.</li><li>For every Container in the Pod, the memory limit must equal the memory request.</li><li>Every Container in the Pod must have a CPU limit and a CPU request.</li><li>For every Container in the Pod, the CPU limit must equal the CPU request.</li></ul><h3 id="burstable">Burstable</h3><p>Pods that are <code>Burstable</code> have some lower-bound resource guarantees based on the request, but
do not require a specific limit. If a limit is not specified, it defaults to a
limit equivalent to the capacity of the Node, which allows the Pods to flexibly increase
their resources if resources are available. In the event of Pod eviction due to Node
resource pressure, these Pods are evicted only after all <code>BestEffort</code> Pods are evicted.
Because a <code>Burstable</code> Pod can include a Container that has no resource limits or requests, a Pod
that is <code>Burstable</code> can try to use any amount of node resources.</p><h4 id="criteria-1">Criteria</h4><p>A Pod is given a QoS class of <code>Burstable</code> if:</p><ul><li>The Pod does not meet the criteria for QoS class <code>Guaranteed</code>.</li><li>At least one Container in the Pod has a memory or CPU request or limit.</li></ul><h3 id="besteffort">BestEffort</h3><p>Pods in the <code>BestEffort</code> QoS class can use node resources that aren't specifically assigned
to Pods in other QoS classes. For example, if you have a node with 16 CPU cores available to the
kubelet, and you assign 4 CPU cores to a <code>Guaranteed</code> Pod, then a Pod in the <code>BestEffort</code>
QoS class can try to use any amount of the remaining 12 CPU cores.</p><p>The kubelet prefers to evict <code>BestEffort</code> Pods if the node comes under resource pressure.</p><h4 id="criteria-2">Criteria</h4><p>A Pod has a QoS class of <code>BestEffort</code> if it doesn't meet the criteria for either <code>Guaranteed</code>
or <code>Burstable</code>. In other words, a Pod is <code>BestEffort</code> only if none of the Containers in the Pod have a
memory limit or a memory request, and none of the Containers in the Pod have a
CPU limit or a CPU request.
Containers in a Pod can request other resources (not CPU or memory) and still be classified as
<code>BestEffort</code>.</p><h2 id="memory-qos-with-cgroup-v2">Memory QoS with cgroup v2</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: MemoryQoS"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [alpha]</code> (enabled by default: false)</div><p>Memory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes.
Memory requests and limits of containers in pod are used to set specific interfaces <code>memory.min</code>
and <code>memory.high</code> provided by the memory controller. When <code>memory.min</code> is set to memory requests,
memory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures
memory availability for Kubernetes pods. And if memory limits are set in the container,
this means that the system needs to limit container memory usage; Memory QoS uses <code>memory.high</code>
to throttle workload approaching its memory limit, ensuring that the system is not overwhelmed
by instantaneous memory allocation.</p><p>Memory QoS relies on QoS class to determine which settings to apply; however, these are different
mechanisms that both provide controls over quality of service.</p><h2 id="class-independent-behavior">Some behavior is independent of QoS class</h2><p>Certain behavior is independent of the QoS class assigned by Kubernetes. For example:</p><ul><li><p>Any Container exceeding a resource limit will be killed and restarted by the kubelet without
affecting other Containers in that Pod.</p></li><li><p>If a Container exceeds its resource request and the node it runs on faces
resource pressure, the Pod it is in becomes a candidate for <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">eviction</a>.
If this occurs, all Containers in the Pod will be terminated. Kubernetes may create a
replacement Pod, usually on a different node.</p></li><li><p>The resource request of a Pod is equal to the sum of the resource requests of
its component Containers, and the resource limit of a Pod is equal to the sum of
the resource limits of its component Containers.</p></li><li><p>The kube-scheduler does not consider QoS class when selecting which Pods to
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption">preempt</a>.
Preemption can occur when a cluster does not have enough resources to run all the Pods
you defined.</p></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/configuration/manage-resources-containers/">resource management for Pods and Containers</a>.</li><li>Learn about <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node-pressure eviction</a>.</li><li>Learn about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod priority and preemption</a>.</li><li>Learn about <a href="/docs/concepts/workloads/pods/disruptions/">Pod disruptions</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/assign-memory-resource/">assign memory resources to containers and pods</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">assign CPU resources to containers and pods</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/quality-service-pod/">configure Quality of Service for Pods</a>.</li></ul></div></div><div><div class="td-content"><h1>User Namespaces</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [beta]</code></div><p>This page explains how user namespaces are used in Kubernetes pods. A user
namespace isolates the user running inside the container from the one
in the host.</p><p>A process running as root in a container can run as a different (non-root) user
in the host; in other words, the process has full privileges for operations
inside the user namespace, but is unprivileged for operations outside the
namespace.</p><p>You can use this feature to reduce the damage a compromised container can do to
the host or other pods in the same node. There are <a href="https://github.com/kubernetes/enhancements/tree/217d790720c5aef09b8bd4d6ca96284a0affe6c2/keps/sig-node/127-user-namespaces#motivation">several security
vulnerabilities</a> rated either <strong>HIGH</strong> or <strong>CRITICAL</strong> that were not
exploitable when user namespaces is active. It is expected user namespace will
mitigate some future vulnerabilities too.</p><h2 id="before-you-begin">Before you begin</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>This is a Linux-only feature and support is needed in Linux for idmap mounts on
the filesystems used. This means:</p><ul><li>On the node, the filesystem you use for <code>/var/lib/kubelet/pods/</code>, or the
custom directory you configure for this, needs idmap mount support.</li><li>All the filesystems used in the pod's volumes must support idmap mounts.</li></ul><p>In practice this means you need at least Linux 6.3, as tmpfs started supporting
idmap mounts in that version. This is usually needed as several Kubernetes
features use tmpfs (the service account token that is mounted by default uses a
tmpfs, Secrets use a tmpfs, etc.)</p><p>Some popular filesystems that support idmap mounts in Linux 6.3 are: btrfs,
ext4, xfs, fat, tmpfs, overlayfs.</p><p>In addition, the container runtime and its underlying OCI runtime must support
user namespaces. The following OCI runtimes offer support:</p><ul><li><a href="https://github.com/containers/crun">crun</a> version 1.9 or greater (it's recommend version 1.13+).</li><li><a href="https://github.com/opencontainers/runc">runc</a> version 1.2 or greater</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Some OCI runtimes do not include the support needed for using user namespaces in
Linux pods. If you use a managed Kubernetes, or have downloaded it from packages
and set it up, it's possible that nodes in your cluster use a runtime that doesn't
include this support.</div><p>To use user namespaces with Kubernetes, you also need to use a CRI
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>
to use this feature with Kubernetes pods:</p><ul><li>containerd: version 2.0 (and later) supports user namespaces for containers.</li><li>CRI-O: version 1.25 (and later) supports user namespaces for containers.</li></ul><p>You can see the status of user namespaces support in cri-dockerd tracked in an <a href="https://github.com/Mirantis/cri-dockerd/issues/74">issue</a>
on GitHub.</p><h2 id="introduction">Introduction</h2><p>User namespaces is a Linux feature that allows to map users in the container to
different users in the host. Furthermore, the capabilities granted to a pod in
a user namespace are valid only in the namespace and void outside of it.</p><p>A pod can opt-in to use user namespaces by setting the <code>pod.spec.hostUsers</code> field
to <code>false</code>.</p><p>The kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way
to guarantee that no two pods on the same node use the same mapping.</p><p>The <code>runAsUser</code>, <code>runAsGroup</code>, <code>fsGroup</code>, etc. fields in the <code>pod.spec</code> always
refer to the user inside the container. These users will be used for volume
mounts (specified in <code>pod.spec.volumes</code>) and therefore the host UID/GID will not
have any effect on writes/reads from volumes the pod can mount. In other words,
the inodes created/read in volumes mounted by the pod will be the same as if the
pod wasn't using user namespaces.</p><p>This way, a pod can easily enable and disable user namespaces (without affecting
its volume's file ownerships) and can also share volumes with pods without user
namespaces by just setting the appropriate users inside the container
(<code>RunAsUser</code>, <code>RunAsGroup</code>, <code>fsGroup</code>, etc.). This applies to any volume the pod
can mount, including <code>hostPath</code> (if the pod is allowed to mount <code>hostPath</code>
volumes).</p><p>By default, the valid UIDs/GIDs when this feature is enabled is the range 0-65535.
This applies to files and processes (<code>runAsUser</code>, <code>runAsGroup</code>, etc.).</p><p>Files using a UID/GID outside this range will be seen as belonging to the
overflow ID, usually 65534 (configured in <code>/proc/sys/kernel/overflowuid</code> and
<code>/proc/sys/kernel/overflowgid</code>). However, it is not possible to modify those
files, even by running as the 65534 user/group.</p><p>If the range 0-65535 is extended with a configuration knob, the aforementioned
restrictions apply to the extended range.</p><p>Most applications that need to run as root but don't access other host
namespaces or resources, should continue to run fine without any changes needed
if user namespaces is activated.</p><h2 id="pods-and-userns">Understanding user namespaces for pods</h2><p>Several container runtimes with their default configuration (like Docker Engine,
containerd, CRI-O) use Linux namespaces for isolation. Other technologies exist
and can be used with those runtimes too (e.g. Kata Containers uses VMs instead of
Linux namespaces). This page is applicable for container runtimes using Linux
namespaces for isolation.</p><p>When creating a pod, by default, several new namespaces are used for isolation:
a network namespace to isolate the network of the container, a PID namespace to
isolate the view of processes, etc. If a user namespace is used, this will
isolate the users in the container from the users in the node.</p><p>This means containers can run as root and be mapped to a non-root user on the
host. Inside the container the process will think it is running as root (and
therefore tools like <code>apt</code>, <code>yum</code>, etc. work fine), while in reality the process
doesn't have privileges on the host. You can verify this, for example, if you
check which user the container process is running by executing <code>ps aux</code> from
the host. The user <code>ps</code> shows is not the same as the user you see if you
execute inside the container the command <code>id</code>.</p><p>This abstraction limits what can happen, for example, if the container manages
to escape to the host. Given that the container is running as a non-privileged
user on the host, it is limited what it can do to the host.</p><p>Furthermore, as users on each pod will be mapped to different non-overlapping
users in the host, it is limited what they can do to other pods too.</p><p>Capabilities granted to a pod are also limited to the pod user namespace and
mostly invalid out of it, some are even completely void. Here are two examples:</p><ul><li><code>CAP_SYS_MODULE</code> does not have any effect if granted to a pod using user
namespaces, the pod isn't able to load kernel modules.</li><li><code>CAP_SYS_ADMIN</code> is limited to the pod's user namespace and invalid outside
of it.</li></ul><p>Without using a user namespace a container running as root, in the case of a
container breakout, has root privileges on the node. And if some capability were
granted to the container, the capabilities are valid on the host too. None of
this is true when we use user namespaces.</p><p>If you want to know more details about what changes when user namespaces are in
use, see <code>man 7 user_namespaces</code>.</p><h2 id="set-up-a-node-to-support-user-namespaces">Set up a node to support user namespaces</h2><p>By default, the kubelet assigns pods UIDs/GIDs above the range 0-65535, based on
the assumption that the host's files and processes use UIDs/GIDs within this
range, which is standard for most Linux distributions. This approach prevents
any overlap between the UIDs/GIDs of the host and those of the pods.</p><p>Avoiding the overlap is important to mitigate the impact of vulnerabilities such
as <a href="https://github.com/kubernetes/kubernetes/issues/104980">CVE-2021-25741</a>, where a pod can potentially read arbitrary
files in the host. If the UIDs/GIDs of the pod and the host don't overlap, it is
limited what a pod would be able to do: the pod UID/GID won't match the host's
file owner/group.</p><p>The kubelet can use a custom range for user IDs and group IDs for pods. To
configure a custom range, the node needs to have:</p><ul><li>A user <code>kubelet</code> in the system (you cannot use any other username here)</li><li>The binary <code>getsubids</code> installed (part of <a href="https://github.com/shadow-maint/shadow">shadow-utils</a>) and
in the <code>PATH</code> for the kubelet binary.</li><li>A configuration of subordinate UIDs/GIDs for the <code>kubelet</code> user (see
<a href="https://man7.org/linux/man-pages/man5/subuid.5.html"><code>man 5 subuid</code></a> and
<a href="https://man7.org/linux/man-pages/man5/subgid.5.html"><code>man 5 subgid</code></a>).</li></ul><p>This setting only gathers the UID/GID range configuration and does not change
the user executing the <code>kubelet</code>.</p><p>You must follow some constraints for the subordinate ID range that you assign
to the <code>kubelet</code> user:</p><ul><li><p>The subordinate user ID, that starts the UID range for Pods, <strong>must</strong> be a
multiple of 65536 and must also be greater than or equal to 65536. In other
words, you cannot use any ID from the range 0-65535 for Pods; the kubelet
imposes this restriction to make it difficult to create an accidentally insecure
configuration.</p></li><li><p>The subordinate ID count must be a multiple of 65536</p></li><li><p>The subordinate ID count must be at least <code>65536 x &lt;maxPods&gt;</code> where <code>&lt;maxPods&gt;</code>
is the maximum number of pods that can run on the node.</p></li><li><p>You must assign the same range for both user IDs and for group IDs, It doesn't
matter if other users have user ID ranges that don't align with the group ID
ranges.</p></li><li><p>None of the assigned ranges should overlap with any other assignment.</p></li><li><p>The subordinate configuration must be only one line. In other words, you can't
have multiple ranges.</p></li></ul><p>For example, you could define <code>/etc/subuid</code> and <code>/etc/subgid</code> to both have
these entries for the <code>kubelet</code> user:</p><pre tabindex="0"><code># The format is
#   name:firstID:count of IDs
# where
# - firstID is 65536 (the minimum value possible)
# - count of IDs is 110 * 65536
#   (110 is the default limit for number of pods on the node)

kubelet:65536:7208960
</code></pre><h2 id="id-count-for-each-of-pods">ID count for each of Pods</h2><p>Starting with Kubernetes v1.33, the ID count for each of Pods can be set in
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubelet.config.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeletConfiguration<span>
</span></span></span><span><span><span></span><span>userNamespaces</span>:<span>
</span></span></span><span><span><span>  </span><span>idsPerPod</span>:<span> </span><span>1048576</span><span>
</span></span></span></code></pre></div><p>The value of <code>idsPerPod</code> (uint32) must be a multiple of 65536.
The default value is 65536.
This value only applies to containers created after the kubelet was started with
this <code>KubeletConfiguration</code>.
Running containers are not affected by this config.</p><p>In Kubernetes prior to v1.33, the ID count for each of Pods was hard-coded to
65536.</p><h2 id="integration-with-pod-security-admission-checks">Integration with Pod security admission checks</h2><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [alpha]</code></div><p>For Linux Pods that enable user namespaces, Kubernetes relaxes the application of
<a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a> in a controlled way.
This behavior can be controlled by the <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature
gate</a>
<code>UserNamespacesPodSecurityStandards</code>, which allows an early opt-in for end
users. Admins have to ensure that user namespaces are enabled by all nodes
within the cluster if using the feature gate.</p><p>If you enable the associated feature gate and create a Pod that uses user
namespaces, the following fields won't be constrained even in contexts that enforce the
<em>Baseline</em> or <em>Restricted</em> pod security standard. This behavior does not
present a security concern because <code>root</code> inside a Pod with user namespaces
actually refers to the user inside the container, that is never mapped to a
privileged user on the host. Here's the list of fields that are <strong>not</strong> checks for Pods in those
circumstances:</p><ul><li><code>spec.securityContext.runAsNonRoot</code></li><li><code>spec.containers[*].securityContext.runAsNonRoot</code></li><li><code>spec.initContainers[*].securityContext.runAsNonRoot</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsNonRoot</code></li><li><code>spec.securityContext.runAsUser</code></li><li><code>spec.containers[*].securityContext.runAsUser</code></li><li><code>spec.initContainers[*].securityContext.runAsUser</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsUser</code></li></ul><h2 id="limitations">Limitations</h2><p>When using a user namespace for the pod, it is disallowed to use other host
namespaces. In particular, if you set <code>hostUsers: false</code> then you are not
allowed to set any of:</p><ul><li><code>hostNetwork: true</code></li><li><code>hostIPC: true</code></li><li><code>hostPID: true</code></li></ul><p>No container can use <code>volumeDevices</code> (raw block volumes, like /dev/sda) either.
This includes all the container arrays in the pod spec:</p><ul><li><code>containers</code></li><li><code>initContainers</code></li><li><code>ephemeralContainers</code></li></ul><h2 id="metrics-and-observability">Metrics and observability</h2><p>The kubelet exports two prometheus metrics specific to user-namespaces:</p><ul><li><code>started_user_namespaced_pods_total</code>: a counter that tracks the number of user namespaced pods that are attempted to be created.</li><li><code>started_user_namespaced_pods_errors_total</code>: a counter that tracks the number of errors creating user namespaced pods.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Take a look at <a href="/docs/tasks/configure-pod-container/user-namespaces/">Use a User Namespace With a Pod</a></li></ul></div></div><div><div class="td-content"><h1>Downward API</h1><div class="lead">There are two ways to expose Pod and container fields to a running container: environment variables, and as files that are populated by a special volume type. Together, these two ways of exposing Pod and container fields are called the downward API.</div><p>It is sometimes useful for a container to have information about itself, without
being overly coupled to Kubernetes. The <em>downward API</em> allows containers to consume
information about themselves or the cluster without using the Kubernetes client
or API server.</p><p>An example is an existing application that assumes a particular well-known
environment variable holds a unique identifier. One possibility is to wrap the
application, but that is tedious and error-prone, and it violates the goal of low
coupling. A better option would be to use the Pod's name as an identifier, and
inject the Pod's name into the well-known environment variable.</p><p>In Kubernetes, there are two ways to expose Pod and container fields to a running container:</p><ul><li>as <a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">environment variables</a></li><li>as <a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">files in a <code>downwardAPI</code> volume</a></li></ul><p>Together, these two ways of exposing Pod and container fields are called the
<em>downward API</em>.</p><h2 id="available-fields">Available fields</h2><p>Only some Kubernetes API fields are available through the downward API. This
section lists which fields you can make available.</p><p>You can pass information from available Pod-level fields using <code>fieldRef</code>.
At the API level, the <code>spec</code> for a Pod always defines at least one
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">Container</a>.
You can pass information from available Container-level fields using
<code>resourceFieldRef</code>.</p><h3 id="downwardapi-fieldRef">Information available via <code>fieldRef</code></h3><p>For some Pod-level fields, you can provide them to a container either as
an environment variable or using a <code>downwardAPI</code> volume. The fields available
via either mechanism are:</p><dl><dt><code>metadata.name</code></dt><dd>the pod's name</dd><dt><code>metadata.namespace</code></dt><dd>the pod's <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a></dd><dt><code>metadata.uid</code></dt><dd>the pod's unique ID</dd><dt><code>metadata.annotations['&lt;KEY&gt;']</code></dt><dd>the value of the pod's <a class="glossary-tooltip" title="A key-value pair that is used to attach arbitrary non-identifying metadata to objects." href="/docs/concepts/overview/working-with-objects/annotations" target="_blank">annotation</a> named <code>&lt;KEY&gt;</code> (for example, <code>metadata.annotations['myannotation']</code>)</dd><dt><code>metadata.labels['&lt;KEY&gt;']</code></dt><dd>the text value of the pod's <a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">label</a> named <code>&lt;KEY&gt;</code> (for example, <code>metadata.labels['mylabel']</code>)</dd></dl><p>The following information is available through environment variables
<strong>but not as a downwardAPI volume fieldRef</strong>:</p><dl><dt><code>spec.serviceAccountName</code></dt><dd>the name of the pod's <a class="glossary-tooltip" title="Provides an identity for processes that run in a Pod." href="/docs/tasks/configure-pod-container/configure-service-account/" target="_blank">service account</a></dd><dt><code>spec.nodeName</code></dt><dd>the name of the <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a> where the Pod is executing</dd><dt><code>status.hostIP</code></dt><dd>the primary IP address of the node to which the Pod is assigned</dd><dt><code>status.hostIPs</code></dt><dd>the IP addresses is a dual-stack version of <code>status.hostIP</code>, the first is always the same as <code>status.hostIP</code>.</dd><dt><code>status.podIP</code></dt><dd>the pod's primary IP address (usually, its IPv4 address)</dd><dt><code>status.podIPs</code></dt><dd>the IP addresses is a dual-stack version of <code>status.podIP</code>, the first is always the same as <code>status.podIP</code></dd></dl><p>The following information is available through a <code>downwardAPI</code> volume
<code>fieldRef</code>, <strong>but not as environment variables</strong>:</p><dl><dt><code>metadata.labels</code></dt><dd>all of the pod's labels, formatted as <code>label-key="escaped-label-value"</code> with one label per line</dd><dt><code>metadata.annotations</code></dt><dd>all of the pod's annotations, formatted as <code>annotation-key="escaped-annotation-value"</code> with one annotation per line</dd></dl><h3 id="downwardapi-resourceFieldRef">Information available via <code>resourceFieldRef</code></h3><p>These container-level fields allow you to provide information about
<a href="/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">requests and limits</a>
for resources such as CPU and memory.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><div class="feature-state-notice feature-beta" title="Feature Gate: InPlacePodVerticalScaling"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>Container CPU and memory resources can be resized while the container is running.
If this happens, a downward API volume will be updated,
but environment variables will not be updated unless the container restarts.
See <a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize CPU and Memory Resources assigned to Containers</a>
for more details.</p></div><dl><dt><code>resource: limits.cpu</code></dt><dd>A container's CPU limit</dd><dt><code>resource: requests.cpu</code></dt><dd>A container's CPU request</dd><dt><code>resource: limits.memory</code></dt><dd>A container's memory limit</dd><dt><code>resource: requests.memory</code></dt><dd>A container's memory request</dd><dt><code>resource: limits.hugepages-*</code></dt><dd>A container's hugepages limit</dd><dt><code>resource: requests.hugepages-*</code></dt><dd>A container's hugepages request</dd><dt><code>resource: limits.ephemeral-storage</code></dt><dd>A container's ephemeral-storage limit</dd><dt><code>resource: requests.ephemeral-storage</code></dt><dd>A container's ephemeral-storage request</dd></dl><h4 id="fallback-information-for-resource-limits">Fallback information for resource limits</h4><p>If CPU and memory limits are not specified for a container, and you use the
downward API to try to expose that information, then the
kubelet defaults to exposing the maximum allocatable value for CPU and memory
based on the <a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">node allocatable</a>
calculation.</p><h2 id="what-s-next">What's next</h2><p>You can read about <a href="/docs/concepts/storage/volumes/#downwardapi"><code>downwardAPI</code> volumes</a>.</p><p>You can try using the downward API to expose container- or Pod-level information:</p><ul><li>as <a href="/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">environment variables</a></li><li>as <a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">files in <code>downwardAPI</code> volume</a></li></ul></div></div><div><div class="td-content"><h1>Workload Management</h1><p>Kubernetes provides several built-in APIs for declarative management of your
<a class="glossary-tooltip" title="A workload is an application running on Kubernetes." href="/docs/concepts/workloads/" target="_blank">workloads</a>
and the components of those workloads.</p><p>Ultimately, your applications run as containers inside
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>; however, managing individual
Pods would be a lot of effort. For example, if a Pod fails, you probably want to
run a new Pod to replace it. Kubernetes can do that for you.</p><p>You use the Kubernetes API to create a workload
<a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">object</a> that represents a higher abstraction level
than a Pod, and then the Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> automatically manages
Pod objects on your behalf, based on the specification for the workload object you defined.</p><p>The built-in APIs for managing workloads are:</p><p><a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> (and, indirectly, <a href="/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>),
the most common way to run an application on your cluster.
Deployment is a good fit for managing a stateless application workload on your cluster, where
any Pod in the Deployment is interchangeable and can be replaced if needed.
(Deployments are a replacement for the legacy
<a class="glossary-tooltip" title="A (deprecated) API object that manages a replicated application." href="/docs/reference/glossary/?all=true#term-replication-controller" target="_blank">ReplicationController</a> API).</p><p>A <a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> lets you
manage one or more Pods &#8211; all running the same application code &#8211; where the Pods rely
on having a distinct identity. This is different from a Deployment where the Pods are
expected to be interchangeable.
The most common use for a StatefulSet is to be able to make a link between its Pods and
their persistent storage. For example, you can run a StatefulSet that associates each Pod
with a <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolume</a>. If one of the Pods
in the StatefulSet fails, Kubernetes makes a replacement Pod that is connected to the
same PersistentVolume.</p><p>A <a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> defines Pods that provide
facilities that are local to a specific <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a>;
for example, a driver that lets containers on that node access a storage system. You use a DaemonSet
when the driver, or other node-level service, has to run on the node where it's useful.
Each Pod in a DaemonSet performs a role similar to a system daemon on a classic Unix / POSIX
server.
A DaemonSet might be fundamental to the operation of your cluster,
such as a plugin to let that node access
<a href="/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model">cluster networking</a>,
it might help you to manage the node,
or it could provide less essential facilities that enhance the container platform you are running.
You can run DaemonSets (and their pods) across every node in your cluster, or across just a subset (for example,
only install the GPU accelerator driver on nodes that have a GPU installed).</p><p>You can use a <a href="/docs/concepts/workloads/controllers/job/">Job</a> and / or
a <a href="/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a> to
define tasks that run to completion and then stop. A Job represents a one-off task,
whereas each CronJob repeats according to a schedule.</p><p>Other topics in this section:</p><div class="section-index"><ul><li><a href="/docs/concepts/workloads/controllers/ttlafterfinished/">Automatic Cleanup for Finished Jobs</a></li><li><a href="/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a></li></ul></div></div></div><div><div class="td-content"><h1>Deployments</h1><div class="lead">A Deployment manages a set of Pods to run an application workload, usually one that doesn't maintain state.</div><p>A <em>Deployment</em> provides declarative updates for <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> and
<a class="glossary-tooltip" title="ReplicaSet ensures that a specified number of Pod replicas are running at one time" href="/docs/concepts/workloads/controllers/replicaset/" target="_blank">ReplicaSets</a>.</p><p>You describe a <em>desired state</em> in a Deployment, and the Deployment <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">Controller</a> changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.</div><h2 id="use-case">Use Case</h2><p>The following are typical use cases for Deployments:</p><ul><li><a href="#creating-a-deployment">Create a Deployment to rollout a ReplicaSet</a>. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.</li><li><a href="#updating-a-deployment">Declare the new state of the Pods</a> by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created, and the Deployment gradually scales it up while scaling down the old ReplicaSet, ensuring Pods are replaced at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.</li><li><a href="#rolling-back-a-deployment">Rollback to an earlier Deployment revision</a> if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.</li><li><a href="#scaling-a-deployment">Scale up the Deployment to facilitate more load</a>.</li><li><a href="#pausing-and-resuming-a-deployment">Pause the rollout of a Deployment</a> to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.</li><li><a href="#deployment-status">Use the status of the Deployment</a> as an indicator that a rollout has stuck.</li><li><a href="#clean-up-policy">Clean up older ReplicaSets</a> that you don't need anymore.</li></ul><h2 id="creating-a-deployment">Creating a Deployment</h2><p>The following is an example of a Deployment. It creates a ReplicaSet to bring up three <code>nginx</code> Pods:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/nginx-deployment.yaml"><code>controllers/nginx-deployment.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy controllers/nginx-deployment.yaml to clipboard"></div><div class="includecode" id="controllers-nginx-deployment-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>In this example:</p><ul><li><p>A Deployment named <code>nginx-deployment</code> is created, indicated by the
<code>.metadata.name</code> field. This name will become the basis for the ReplicaSets
and Pods which are created later. See <a href="#writing-a-deployment-spec">Writing a Deployment Spec</a>
for more details.</p></li><li><p>The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the <code>.spec.replicas</code> field.</p></li><li><p>The <code>.spec.selector</code> field defines how the created ReplicaSet finds which Pods to manage.
In this case, you select a label that is defined in the Pod template (<code>app: nginx</code>).
However, more sophisticated selection rules are possible,
as long as the Pod template itself satisfies the rule.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>.spec.selector.matchLabels</code> field is a map of {key,value} pairs.
A single {key,value} in the <code>matchLabels</code> map is equivalent to an element of <code>matchExpressions</code>,
whose <code>key</code> field is "key", the <code>operator</code> is "In", and the <code>values</code> array contains only "value".
All of the requirements, from both <code>matchLabels</code> and <code>matchExpressions</code>, must be satisfied in order to match.</div></li><li><p>The <code>.spec.template</code> field contains the following sub-fields:</p><ul><li>The Pods are labeled <code>app: nginx</code>using the <code>.metadata.labels</code> field.</li><li>The Pod template's specification, or <code>.spec</code> field, indicates that
the Pods run one container, <code>nginx</code>, which runs the <code>nginx</code>
<a href="https://hub.docker.com/">Docker Hub</a> image at version 1.14.2.</li><li>Create one container and name it <code>nginx</code> using the <code>.spec.containers[0].name</code> field.</li></ul></li></ul><p>Before you begin, make sure your Kubernetes cluster is up and running.
Follow the steps given below to create the above Deployment:</p><ol><li><p>Create the Deployment by running the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
</span></span></code></pre></div></li><li><p>Run <code>kubectl get deployments</code> to check if the Deployment was created.</p><p>If the Deployment is still being created, the output is similar to the following:</p><pre tabindex="0"><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/3     0            0           1s
</code></pre><p>When you inspect the Deployments in your cluster, the following fields are displayed:</p><ul><li><code>NAME</code> lists the names of the Deployments in the namespace.</li><li><code>READY</code> displays how many replicas of the application are available to your users. It follows the pattern ready/desired.</li><li><code>UP-TO-DATE</code> displays the number of replicas that have been updated to achieve the desired state.</li><li><code>AVAILABLE</code> displays how many replicas of the application are available to your users.</li><li><code>AGE</code> displays the amount of time that the application has been running.</li></ul><p>Notice how the number of desired replicas is 3 according to <code>.spec.replicas</code> field.</p></li><li><p>To see the Deployment rollout status, run <code>kubectl rollout status deployment/nginx-deployment</code>.</p><p>The output is similar to:</p><pre tabindex="0"><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment "nginx-deployment" successfully rolled out
</code></pre></li><li><p>Run the <code>kubectl get deployments</code> again a few seconds later.
The output is similar to this:</p><pre tabindex="0"><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           18s
</code></pre><p>Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.</p></li><li><p>To see the ReplicaSet (<code>rs</code>) created by the Deployment, run <code>kubectl get rs</code>. The output is similar to this:</p><pre tabindex="0"><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75675f5897   3         3         3       18s
</code></pre><p>ReplicaSet output shows the following fields:</p><ul><li><code>NAME</code> lists the names of the ReplicaSets in the namespace.</li><li><code>DESIRED</code> displays the desired number of <em>replicas</em> of the application, which you define when you create the Deployment. This is the <em>desired state</em>.</li><li><code>CURRENT</code> displays how many replicas are currently running.</li><li><code>READY</code> displays how many replicas of the application are available to your users.</li><li><code>AGE</code> displays the amount of time that the application has been running.</li></ul><p>Notice that the name of the ReplicaSet is always formatted as
<code>[DEPLOYMENT-NAME]-[HASH]</code>. This name will become the basis for the Pods
which are created.</p><p>The <code>HASH</code> string is the same as the <code>pod-template-hash</code> label on the ReplicaSet.</p></li><li><p>To see the labels automatically generated for each Pod, run <code>kubectl get pods --show-labels</code>.
The output is similar to:</p><pre tabindex="0"><code>NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897
nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897
nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897
</code></pre><p>The created ReplicaSet ensures that there are three <code>nginx</code> Pods.</p></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>You must specify an appropriate selector and Pod template labels in a Deployment
(in this case, <code>app: nginx</code>).</p><p>Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.</p></div><h3 id="pod-template-hash-label">Pod-template-hash label</h3><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Do not change this label.</div><p>The <code>pod-template-hash</code> label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.</p><p>This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the <code>PodTemplate</code> of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,
and in any existing Pods that the ReplicaSet might have.</p><h2 id="updating-a-deployment">Updating a Deployment</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, <code>.spec.template</code>)
is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.</div><p>Follow the steps given below to update your Deployment:</p><ol><li><p>Let's update the nginx Pods to use the <code>nginx:1.16.1</code> image instead of the <code>nginx:1.14.2</code> image.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>set</span> image deployment.v1.apps/nginx-deployment <span>nginx</span><span>=</span>nginx:1.16.1
</span></span></code></pre></div><p>or use the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>set</span> image deployment/nginx-deployment <span>nginx</span><span>=</span>nginx:1.16.1
</span></span></code></pre></div><p>where <code>deployment/nginx-deployment</code> indicates the Deployment,
<code>nginx</code> indicates the Container the update will take place and
<code>nginx:1.16.1</code> indicates the new image and its tag.</p><p>The output is similar to:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment image updated
</code></pre><p>Alternatively, you can <code>edit</code> the Deployment and change <code>.spec.template.spec.containers[0].image</code> from <code>nginx:1.14.2</code> to <code>nginx:1.16.1</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment edited
</code></pre></li><li><p>To see the rollout status, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
</code></pre><p>or</p><pre tabindex="0"><code>deployment "nginx-deployment" successfully rolled out
</code></pre></li></ol><p>Get more details on your updated Deployment:</p><ul><li><p>After the rollout succeeds, you can view the Deployment by running <code>kubectl get deployments</code>.
The output is similar to this:</p><pre tabindex="0"><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           36s
</code></pre></li><li><p>Run <code>kubectl get rs</code> to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it
up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s
</code></pre></li><li><p>Running <code>get pods</code> should now show only the new Pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
</code></pre><p>Next time you want to update these Pods, you only need to update the Deployment's Pod template again.</p><p>Deployment ensures that only a certain number of Pods are down while they are being updated. By default,
it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).</p><p>Deployment also ensures that only a certain number of Pods are created above the desired number of Pods.
By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).</p><p>For example, if you look at the above Deployment closely, you will see that it first creates a new Pod,
then deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of
new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.
It makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of
a Deployment with 4 replicas, the number of Pods would be between 3 and 5.</p></li><li><p>Get details of your Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe deployments
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
   Containers:
    nginx:
      Image:        nginx:1.16.1
      Port:         80/TCP
      Environment:  &lt;none&gt;
      Mounts:       &lt;none&gt;
    Volumes:        &lt;none&gt;
  Conditions:
    Type           Status  Reason
    ----           ------  ------
    Available      True    MinimumReplicasAvailable
    Progressing    True    NewReplicaSetAvailable
  OldReplicaSets:  &lt;none&gt;
  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
  Events:
    Type    Reason             Age   From                   Message
    ----    ------             ----  ----                   -------
    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
</code></pre><p>Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)
and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet
(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet
to 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.
It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.
Finally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.</p></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubernetes doesn't count terminating Pods when calculating the number of <code>availableReplicas</code>, which must be between
<code>replicas - maxUnavailable</code> and <code>replicas + maxSurge</code>. As a result, you might notice that there are more Pods than
expected during a rollout, and that the total resources consumed by the Deployment is more than <code>replicas + maxSurge</code>
until the <code>terminationGracePeriodSeconds</code> of the terminating Pods expires.</div><h3 id="rollover-aka-multiple-updates-in-flight">Rollover (aka multiple updates in-flight)</h3><p>Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up
the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels
match <code>.spec.selector</code> but whose template does not match <code>.spec.template</code> is scaled down. Eventually, the new
ReplicaSet is scaled to <code>.spec.replicas</code> and all old ReplicaSets is scaled to 0.</p><p>If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet
as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously
-- it will add it to its list of old ReplicaSets and start scaling it down.</p><p>For example, suppose you create a Deployment to create 5 replicas of <code>nginx:1.14.2</code>,
but then update the Deployment to create 5 replicas of <code>nginx:1.16.1</code>, when only 3
replicas of <code>nginx:1.14.2</code> had been created. In that case, the Deployment immediately starts
killing the 3 <code>nginx:1.14.2</code> Pods that it had created, and starts creating
<code>nginx:1.16.1</code> Pods. It does not wait for the 5 replicas of <code>nginx:1.14.2</code> to be created
before changing course.</p><h3 id="label-selector-updates">Label selector updates</h3><p>It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.
In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped
all of the implications.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In API version <code>apps/v1</code>, a Deployment's label selector is immutable after it gets created.</div><ul><li>Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,
otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does
not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and
creating a new ReplicaSet.</li><li>Selector updates changes the existing value in a selector key -- result in the same behavior as additions.</li><li>Selector removals removes an existing key from the Deployment selector -- do not require any changes in the
Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the
removed label still exists in any existing Pods and ReplicaSets.</li></ul><h2 id="rolling-back-a-deployment">Rolling Back a Deployment</h2><p>Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.
By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want
(you can change that by modifying revision history limit).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A Deployment's revision is created when a Deployment's rollout is triggered. This means that the
new revision is created if and only if the Deployment's Pod template (<code>.spec.template</code>) is changed,
for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,
do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.
This means that when you roll back to an earlier revision, only the Deployment's Pod template part is
rolled back.</div><ul><li><p>Suppose that you made a typo while updating the Deployment, by putting the image name as <code>nginx:1.161</code> instead of <code>nginx:1.16.1</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>set</span> image deployment/nginx-deployment <span>nginx</span><span>=</span>nginx:1.161
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>The rollout gets stuck. You can verify it by checking the rollout status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
</code></pre></li><li><p>Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,
<a href="#deployment-status">read more here</a>.</p></li><li><p>You see that the number of old replicas (adding the replica count from
<code>nginx-deployment-1564180365</code> and <code>nginx-deployment-2035384211</code>) is 3, and the number of
new replicas (from <code>nginx-deployment-3066724191</code>) is 1.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   1         1         0       6s
</code></pre></li><li><p>Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (<code>maxUnavailable</code> specifically) that you have specified. Kubernetes by default sets the value to 25%.</div></li><li><p>Get the description of the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.161
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
</code></pre><p>To fix this, you need to rollback to a previous revision of Deployment that is stable.</p></li></ul><h3 id="checking-rollout-history-of-a-deployment">Checking Rollout History of a Deployment</h3><p>Follow the steps given below to check the rollout history:</p><ol><li><p>First, check the revisions of this Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout <span>history</span> deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployments "nginx-deployment"
REVISION    CHANGE-CAUSE
1           &lt;none&gt;
2           &lt;none&gt;
3           &lt;none&gt;
</code></pre><p><code>CHANGE-CAUSE</code> is copied from the Deployment annotation <code>kubernetes.io/change-cause</code> to its revisions upon creation. You can specify the<code>CHANGE-CAUSE</code> message by:</p><ul><li>Annotating the Deployment with <code>kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.16.1"</code></li><li>Manually editing the manifest of the resource.</li><li>Using tooling that sets the annotation automatically.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In older versions of Kubernetes, you could use the <code>--record</code> flag with kubectl commands to automatically populate the <code>CHANGE-CAUSE</code> field. This flag is deprecated and will be removed in a future release.</div></li><li><p>To see the details of each revision, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout <span>history</span> deployment/nginx-deployment --revision<span>=</span><span>2</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployments "nginx-deployment" revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Containers:
   nginx:
    Image:      nginx:1.16.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      &lt;none&gt;
  No volumes.
</code></pre></li></ol><h3 id="rolling-back-to-a-previous-revision">Rolling Back to a Previous Revision</h3><p>Follow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.</p><ol><li><p>Now you've decided to undo the current rollout and rollback to the previous revision:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout undo deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment rolled back
</code></pre><p>Alternatively, you can rollback to a specific revision by specifying it with <code>--to-revision</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout undo deployment/nginx-deployment --to-revision<span>=</span><span>2</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment rolled back
</code></pre><p>For more details about rollout related commands, read <a href="/docs/reference/generated/kubectl/kubectl-commands#rollout"><code>kubectl rollout</code></a>.</p><p>The Deployment is now rolled back to a previous stable revision. As you can see, a <code>DeploymentRollback</code> event
for rolling back to revision 2 is generated from Deployment controller.</p></li><li><p>Check if the rollback was successful and the Deployment is running as expected, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30m
</code></pre></li><li><p>Get the description of the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.16.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment "nginx-deployment" to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
</code></pre></li></ol><h2 id="scaling-a-deployment">Scaling a Deployment</h2><p>You can scale a Deployment by using the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale deployment/nginx-deployment --replicas<span>=</span><span>10</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment scaled
</code></pre><p>Assuming <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">horizontal Pod autoscaling</a> is enabled
in your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of
Pods you want to run based on the CPU utilization of your existing Pods.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl autoscale deployment/nginx-deployment --min<span>=</span><span>10</span> --max<span>=</span><span>15</span> --cpu-percent<span>=</span><span>80</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment scaled
</code></pre><h3 id="proportional-scaling">Proportional scaling</h3><p>RollingUpdate Deployments support running multiple versions of an application at the same time. When you
or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress
or paused), the Deployment controller balances the additional replicas in the existing active
ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called <em>proportional scaling</em>.</p><p>For example, you are running a Deployment with 10 replicas, <a href="#max-surge">maxSurge</a>=3, and <a href="#max-unavailable">maxUnavailable</a>=2.</p><ul><li><p>Ensure that the 10 replicas in your Deployment are running.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s
</code></pre></li><li><p>You update to a new image which happens to be unresolvable from inside the cluster.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>set</span> image deployment/nginx-deployment <span>nginx</span><span>=</span>nginx:sometag
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the
<code>maxUnavailable</code> requirement that you mentioned above. Check out the rollout status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
</code></pre></li><li><p>Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas
to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using
proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you
spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the
most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the
ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.</p></li></ul><p>In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the
new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming
the new replicas become healthy. To confirm this, run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
</code></pre><p>The rollout status confirms how the replicas were added to each ReplicaSet.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
</code></pre><h2 id="pausing-and-resuming-a-deployment">Pausing and Resuming a rollout of a Deployment</h2><p>When you update a Deployment, or plan to, you can pause rollouts
for that Deployment before you trigger one or more updates. When
you're ready to apply those changes, you resume rollouts for the
Deployment. This approach allows you to
apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.</p><ul><li><p>For example, with a Deployment that was created:</p><p>Get the Deployment details:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m
</code></pre><p>Get the rollout status:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
</code></pre></li><li><p>Pause by running the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout pause deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment paused
</code></pre></li><li><p>Then update the image of the Deployment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>set</span> image deployment/nginx-deployment <span>nginx</span><span>=</span>nginx:1.16.1
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>Notice that no new rollout started:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout <span>history</span> deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployments "nginx"
REVISION  CHANGE-CAUSE
1   &lt;none&gt;
</code></pre></li><li><p>Get the rollout status to verify that the existing ReplicaSet has not changed:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m
</code></pre></li><li><p>You can make as many updates as you wish, for example, update the resources that will be used:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>set</span> resources deployment/nginx-deployment -c<span>=</span>nginx --limits<span>=</span><span>cpu</span><span>=</span>200m,memory<span>=</span>512Mi
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment resource requirements updated
</code></pre><p>The initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to
the Deployment will not have any effect as long as the Deployment rollout is paused.</p></li><li><p>Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout resume deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment resumed
</code></pre></li><li><p><a class="glossary-tooltip" title="A verb that is used to track changes to an object in Kubernetes as a stream." href="/docs/reference/using-api/api-concepts/#api-verbs" target="_blank">Watch</a> the status of the rollout until it's done.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get rs --watch
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
</code></pre></li><li><p>Get the status of the latest rollout:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
</code></pre></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You cannot rollback a paused Deployment until you resume it.</div><h2 id="deployment-status">Deployment status</h2><p>A Deployment enters various states during its lifecycle. It can be <a href="#progressing-deployment">progressing</a> while
rolling out a new ReplicaSet, it can be <a href="#complete-deployment">complete</a>, or it can <a href="#failed-deployment">fail to progress</a>.</p><h3 id="progressing-deployment">Progressing Deployment</h3><p>Kubernetes marks a Deployment as <em>progressing</em> when one of the following tasks is performed:</p><ul><li>The Deployment creates a new ReplicaSet.</li><li>The Deployment is scaling up its newest ReplicaSet.</li><li>The Deployment is scaling down its older ReplicaSet(s).</li><li>New Pods become ready or available (ready for at least <a href="#min-ready-seconds">MinReadySeconds</a>).</li></ul><p>When the rollout becomes &#8220;progressing&#8221;, the Deployment controller adds a condition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "True"</code></li><li><code>reason: NewReplicaSetCreated</code> | <code>reason: FoundNewReplicaSet</code> | <code>reason: ReplicaSetUpdated</code></li></ul><p>You can monitor the progress for a Deployment by using <code>kubectl rollout status</code>.</p><h3 id="complete-deployment">Complete Deployment</h3><p>Kubernetes marks a Deployment as <em>complete</em> when it has the following characteristics:</p><ul><li>All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any
updates you've requested have been completed.</li><li>All of the replicas associated with the Deployment are available.</li><li>No old replicas for the Deployment are running.</li></ul><p>When the rollout becomes &#8220;complete&#8221;, the Deployment controller sets a condition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "True"</code></li><li><code>reason: NewReplicaSetAvailable</code></li></ul><p>This <code>Progressing</code> condition will retain a status value of <code>"True"</code> until a new rollout
is initiated. The condition holds even when availability of replicas changes (which
does instead affect the <code>Available</code> condition).</p><p>You can check if a Deployment has completed by using <code>kubectl rollout status</code>. If the rollout completed
successfully, <code>kubectl rollout status</code> returns a zero exit code.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment "nginx-deployment" successfully rolled out
</code></pre><p>and the exit status from <code>kubectl rollout</code> is 0 (success):</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>echo</span> <span>$?</span>
</span></span></code></pre></div><pre tabindex="0"><code>0
</code></pre><h3 id="failed-deployment">Failed Deployment</h3><p>Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur
due to some of the following factors:</p><ul><li>Insufficient quota</li><li>Readiness probe failures</li><li>Image pull errors</li><li>Insufficient permissions</li><li>Limit ranges</li><li>Application runtime misconfiguration</li></ul><p>One way you can detect this condition is to specify a deadline parameter in your Deployment spec:
(<a href="#progress-deadline-seconds"><code>.spec.progressDeadlineSeconds</code></a>). <code>.spec.progressDeadlineSeconds</code> denotes the
number of seconds the Deployment controller waits before indicating (in the Deployment status) that the
Deployment progress has stalled.</p><p>The following <code>kubectl</code> command sets the spec with <code>progressDeadlineSeconds</code> to make the controller report
lack of progress of a rollout for a Deployment after 10 minutes:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch deployment/nginx-deployment -p <span>'{"spec":{"progressDeadlineSeconds":600}}'</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>deployment.apps/nginx-deployment patched
</code></pre><p>Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "False"</code></li><li><code>reason: ProgressDeadlineExceeded</code></li></ul><p>This condition can also fail early and is then set to status value of <code>"False"</code> due to reasons as <code>ReplicaSetCreateError</code>.
Also, the deadline is not taken into account anymore once the Deployment rollout completes.</p><p>See the <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties">Kubernetes API conventions</a> for more information on status conditions.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubernetes takes no action on a stalled Deployment other than to report a status condition with
<code>reason: ProgressDeadlineExceeded</code>. Higher level orchestrators can take advantage of it and act accordingly, for
example, rollback the Deployment to its previous version.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline.
You can safely pause a Deployment rollout in the middle of a rollout and resume without triggering
the condition for exceeding the deadline.</div><p>You may experience transient errors with your Deployments, either due to a low timeout that you have set or
due to any other kind of error that can be treated as transient. For example, let's suppose you have
insufficient quota. If you describe the Deployment you will notice the following section:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>&lt;...&gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&lt;...&gt;
</code></pre><p>If you run <code>kubectl get deployment nginx-deployment -o yaml</code>, the Deployment status is similar to this:</p><pre tabindex="0"><code>status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set "nginx-deployment-4262182780" is progressing.
    reason: ReplicaSetUpdated
    status: "True"
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: 'Error creating: pods "nginx-deployment-4262182780-" is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2'
    reason: FailedCreate
    status: "True"
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
</code></pre><p>Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the
reason for the Progressing condition:</p><pre tabindex="0"><code>Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
</code></pre><p>You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other
controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota
conditions and the Deployment controller then completes the Deployment rollout, you'll see the
Deployment's status update with a successful condition (<code>status: "True"</code> and <code>reason: NewReplicaSetAvailable</code>).</p><pre tabindex="0"><code>Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
</code></pre><p><code>type: Available</code> with <code>status: "True"</code> means that your Deployment has minimum availability. Minimum availability is dictated
by the parameters specified in the deployment strategy. <code>type: Progressing</code> with <code>status: "True"</code> means that your Deployment
is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum
required new replicas are available (see the Reason of the condition for the particulars - in our case
<code>reason: NewReplicaSetAvailable</code> means that the Deployment is complete).</p><p>You can check if a Deployment has failed to progress by using <code>kubectl rollout status</code>. <code>kubectl rollout status</code>
returns a non-zero exit code if the Deployment has exceeded the progression deadline.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment "nginx" exceeded its progress deadline
</code></pre><p>and the exit status from <code>kubectl rollout</code> is 1 (indicating an error):</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>echo</span> <span>$?</span>
</span></span></code></pre></div><pre tabindex="0"><code>1
</code></pre><h3 id="operating-on-a-failed-deployment">Operating on a failed deployment</h3><p>All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back
to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.</p><h2 id="clean-up-policy">Clean up Policy</h2><p>You can set <code>.spec.revisionHistoryLimit</code> field in a Deployment to specify how many old ReplicaSets for
this Deployment you want to retain. The rest will be garbage-collected in the background. By default,
it is 10.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment
thus that Deployment will not be able to roll back.</div><p>The cleanup only starts <strong>after</strong> a Deployment reaches a
<a href="/docs/concepts/workloads/controllers/deployment/#complete-deployment">complete state</a>.
If you set <code>.spec.revisionHistoryLimit</code> to 0, any rollout nonetheless triggers creation of a new
ReplicaSet before Kubernetes removes the old one.</p><p>Even with a non-zero revision history limit, you can have more ReplicaSets than the limit
you configure. For example, if pods are crash looping, and there are multiple rolling updates
events triggered over time, you might end up with more ReplicaSets than the
<code>.spec.revisionHistoryLimit</code> because the Deployment never reaches a complete state.</p><h2 id="canary-deployment">Canary Deployment</h2><p>If you want to roll out releases to a subset of users or servers using the Deployment, you
can create multiple Deployments, one for each release, following the canary pattern described in
<a href="/docs/concepts/workloads/management/#canary-deployments">managing resources</a>.</p><h2 id="writing-a-deployment-spec">Writing a Deployment Spec</h2><p>As with all other Kubernetes configs, a Deployment needs <code>.apiVersion</code>, <code>.kind</code>, and <code>.metadata</code> fields.
For general information about working with config files, see
<a href="/docs/tasks/run-application/run-stateless-application-deployment/">deploying applications</a>,
configuring containers, and <a href="/docs/concepts/overview/working-with-objects/object-management/">using kubectl to manage resources</a> documents.</p><p>When the control plane creates new Pods for a Deployment, the <code>.metadata.name</code> of the
Deployment is part of the basis for naming those Pods. The name of a Deployment must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostnames. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><p>A Deployment also needs a <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> section</a>.</p><h3 id="pod-template">Pod Template</h3><p>The <code>.spec.template</code> and <code>.spec.selector</code> are the only required fields of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href="/docs/concepts/workloads/pods/#pod-templates">Pod template</a>. It has exactly the same schema as a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>, except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See <a href="#selector">selector</a>.</p><p>Only a <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>.spec.template.spec.restartPolicy</code></a> equal to <code>Always</code> is
allowed, which is the default if not specified.</p><h3 id="replicas">Replicas</h3><p><code>.spec.replicas</code> is an optional field that specifies the number of desired Pods. It defaults to 1.</p><p>Should you manually scale a Deployment, example via <code>kubectl scale deployment deployment --replicas=X</code>, and then you update that Deployment based on a manifest
(for example: by running <code>kubectl apply -f deployment.yaml</code>),
then applying that manifest overwrites the manual scaling that you previously did.</p><p>If a <a href="/docs/tasks/run-application/horizontal-pod-autoscale/">HorizontalPodAutoscaler</a> (or any
similar API for horizontal scaling) is managing scaling for a Deployment, don't set <code>.spec.replicas</code>.</p><p>Instead, allow the Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> to manage the
<code>.spec.replicas</code> field automatically.</p><h3 id="selector">Selector</h3><p><code>.spec.selector</code> is a required field that specifies a <a href="/docs/concepts/overview/working-with-objects/labels/">label selector</a>
for the Pods targeted by this Deployment.</p><p><code>.spec.selector</code> must match <code>.spec.template.metadata.labels</code>, or it will be rejected by the API.</p><p>In API version <code>apps/v1</code>, <code>.spec.selector</code> and <code>.metadata.labels</code> do not default to <code>.spec.template.metadata.labels</code> if not set. So they must be set explicitly. Also note that <code>.spec.selector</code> is immutable after creation of the Deployment in <code>apps/v1</code>.</p><p>A Deployment may terminate Pods whose labels match the selector if their template is different
from <code>.spec.template</code> or if the total number of such Pods exceeds <code>.spec.replicas</code>. It brings up new
Pods with <code>.spec.template</code> if the number of Pods is less than the desired number.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You should not create other Pods whose labels match this selector, either directly, by creating
another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you
do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.</div><p>If you have multiple controllers that have overlapping selectors, the controllers will fight with each
other and won't behave correctly.</p><h3 id="strategy">Strategy</h3><p><code>.spec.strategy</code> specifies the strategy used to replace old Pods by new ones.
<code>.spec.strategy.type</code> can be "Recreate" or "RollingUpdate". "RollingUpdate" is
the default value.</p><h4 id="recreate-deployment">Recreate Deployment</h4><p>All existing Pods are killed before new ones are created when <code>.spec.strategy.type==Recreate</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods
of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new
revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the
replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an
"at most" guarantee for your Pods, you should consider using a
<a href="/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>.</div><h4 id="rolling-update-deployment">Rolling Update Deployment</h4><p>The Deployment updates Pods in a rolling update
fashion (gradually scale down the old ReplicaSets and scale up the new one) when <code>.spec.strategy.type==RollingUpdate</code>. You can specify <code>maxUnavailable</code> and <code>maxSurge</code> to control
the rolling update process.</p><h5 id="max-unavailable">Max Unavailable</h5><p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> is an optional field that specifies the maximum number
of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)
or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by
rounding down. The value cannot be 0 if <code>.spec.strategy.rollingUpdate.maxSurge</code> is 0. The default value is 25%.</p><p>For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired
Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled
down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available
at all times during the update is at least 70% of the desired Pods.</p><h5 id="max-surge">Max Surge</h5><p><code>.spec.strategy.rollingUpdate.maxSurge</code> is an optional field that specifies the maximum number of Pods
that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a
percentage of desired Pods (for example, 10%). The value cannot be 0 if <code>maxUnavailable</code> is 0. The absolute number
is calculated from the percentage by rounding up. The default value is 25%.</p><p>For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the
rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired
Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the
total number of Pods running at any time during the update is at most 130% of desired Pods.</p><p>Here are some Rolling Update Deployment examples that use the <code>maxUnavailable</code> and <code>maxSurge</code>:</p><ul class="nav nav-tabs" id="tab-with-md"><li class="nav-item"><a class="nav-link active" href="#tab-with-md-0">Max Unavailable</a></li><li class="nav-item"><a class="nav-link" href="#tab-with-md-1">Max Surge</a></li><li class="nav-item"><a class="nav-link" href="#tab-with-md-2">Hybrid</a></li></ul><div class="tab-content" id="tab-with-md"><div id="tab-with-md-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span> </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span> </span><span>labels</span>:<span>
</span></span></span><span><span><span>   </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span> </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span> </span><span>selector</span>:<span>
</span></span></span><span><span><span>   </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>     </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span> </span><span>template</span>:<span>
</span></span></span><span><span><span>   </span><span>metadata</span>:<span>
</span></span></span><span><span><span>     </span><span>labels</span>:<span>
</span></span></span><span><span><span>       </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>   </span><span>spec</span>:<span>
</span></span></span><span><span><span>     </span><span>containers</span>:<span>
</span></span></span><span><span><span>     </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>       </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>       </span><span>ports</span>:<span>
</span></span></span><span><span><span>       </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span> </span><span>strategy</span>:<span>
</span></span></span><span><span><span>   </span><span>type</span>:<span> </span>RollingUpdate<span>
</span></span></span><span><span><span>   </span><span>rollingUpdate</span>:<span>
</span></span></span><span><span><span>     </span><span>maxUnavailable</span>:<span> </span><span>1</span><span>
</span></span></span></code></pre></div></p></div><div id="tab-with-md-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span> </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span> </span><span>labels</span>:<span>
</span></span></span><span><span><span>   </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span> </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span> </span><span>selector</span>:<span>
</span></span></span><span><span><span>   </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>     </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span> </span><span>template</span>:<span>
</span></span></span><span><span><span>   </span><span>metadata</span>:<span>
</span></span></span><span><span><span>     </span><span>labels</span>:<span>
</span></span></span><span><span><span>       </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>   </span><span>spec</span>:<span>
</span></span></span><span><span><span>     </span><span>containers</span>:<span>
</span></span></span><span><span><span>     </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>       </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>       </span><span>ports</span>:<span>
</span></span></span><span><span><span>       </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span> </span><span>strategy</span>:<span>
</span></span></span><span><span><span>   </span><span>type</span>:<span> </span>RollingUpdate<span>
</span></span></span><span><span><span>   </span><span>rollingUpdate</span>:<span>
</span></span></span><span><span><span>     </span><span>maxSurge</span>:<span> </span><span>1</span><span>
</span></span></span></code></pre></div></p></div><div id="tab-with-md-2" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span> </span><span>name</span>:<span> </span>nginx-deployment<span>
</span></span></span><span><span><span> </span><span>labels</span>:<span>
</span></span></span><span><span><span>   </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span> </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span> </span><span>selector</span>:<span>
</span></span></span><span><span><span>   </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>     </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span> </span><span>template</span>:<span>
</span></span></span><span><span><span>   </span><span>metadata</span>:<span>
</span></span></span><span><span><span>     </span><span>labels</span>:<span>
</span></span></span><span><span><span>       </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>   </span><span>spec</span>:<span>
</span></span></span><span><span><span>     </span><span>containers</span>:<span>
</span></span></span><span><span><span>     </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>       </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>       </span><span>ports</span>:<span>
</span></span></span><span><span><span>       </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span> </span><span>strategy</span>:<span>
</span></span></span><span><span><span>   </span><span>type</span>:<span> </span>RollingUpdate<span>
</span></span></span><span><span><span>   </span><span>rollingUpdate</span>:<span>
</span></span></span><span><span><span>     </span><span>maxSurge</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>     </span><span>maxUnavailable</span>:<span> </span><span>1</span><span>
</span></span></span></code></pre></div></p></div></div><h3 id="progress-deadline-seconds">Progress Deadline Seconds</h3><p><code>.spec.progressDeadlineSeconds</code> is an optional field that specifies the number of seconds you want
to wait for your Deployment to progress before the system reports back that the Deployment has
<a href="#failed-deployment">failed progressing</a> - surfaced as a condition with <code>type: Progressing</code>, <code>status: "False"</code>.
and <code>reason: ProgressDeadlineExceeded</code> in the status of the resource. The Deployment controller will keep
retrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment
controller will roll back a Deployment as soon as it observes such a condition.</p><p>If specified, this field needs to be greater than <code>.spec.minReadySeconds</code>.</p><h3 id="min-ready-seconds">Min Ready Seconds</h3><p><code>.spec.minReadySeconds</code> is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be ready without any of its containers crashing, for it to be considered available.
This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see <a href="/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">Container Probes</a>.</p><h3 id="terminating-pods">Terminating Pods</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DeploymentReplicaSetTerminatingReplicas"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>You can enable this feature by setting the <code>DeploymentReplicaSetTerminatingReplicas</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
on the <a href="/docs/reference/command-line-tools-reference/kube-apiserver/">API server</a>
and on the <a href="/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a></p><p>Pods that become terminating due to deletion or scale down may take a long time to terminate, and may consume
additional resources during that period. As a result, the total number of all pods can temporarily exceed
<code>.spec.replicas</code>. Terminating pods can be tracked using the <code>.status.terminatingReplicas</code> field of the Deployment.</p><h3 id="revision-history-limit">Revision History Limit</h3><p>A Deployment's revision history is stored in the ReplicaSets it controls.</p><p><code>.spec.revisionHistoryLimit</code> is an optional field that specifies the number of old ReplicaSets to retain
to allow rollback. These old ReplicaSets consume resources in <code>etcd</code> and crowd the output of <code>kubectl get rs</code>. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.</p><p>More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.
In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.</p><h3 id="paused">Paused</h3><p><code>.spec.paused</code> is an optional boolean field for pausing and resuming a Deployment. The only difference between
a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused
Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when
it is created.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/workloads/pods/">Pods</a>.</li><li><a href="/docs/tasks/run-application/run-stateless-application-deployment/">Run a stateless application using a Deployment</a>.</li><li>Read the
<a href="/docs/reference/kubernetes-api/workload-resources/deployment-v1/">Deployment</a> to understand the Deployment API.</li><li>Read about <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li><li>Use kubectl to <a href="/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/">create a Deployment</a>.</li></ul></div></div><div><div class="td-content"><h1>ReplicaSet</h1><div class="lead">A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. Usually, you define a Deployment and let that Deployment manage ReplicaSets automatically.</div><p>A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often
used to guarantee the availability of a specified number of identical Pods.</p><h2 id="how-a-replicaset-works">How a ReplicaSet works</h2><p>A ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number
of replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods
it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating
and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod
template.</p><p>A ReplicaSet is linked to its Pods via the Pods' <a href="/docs/concepts/architecture/garbage-collection/#owners-dependents">metadata.ownerReferences</a>
field, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning
ReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet
knows of the state of the Pods it is maintaining and plans accordingly.</p><p>A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no
OwnerReference or the OwnerReference is not a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">Controller</a> and it
matches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.</p><h2 id="when-to-use-a-replicaset">When to use a ReplicaSet</h2><p>A ReplicaSet ensures that a specified number of pod replicas are running at any given
time. However, a Deployment is a higher-level concept that manages ReplicaSets and
provides declarative updates to Pods along with a lot of other useful features.
Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless
you require custom update orchestration or don't require updates at all.</p><p>This actually means that you may never need to manipulate ReplicaSet objects:
use a Deployment instead, and define your application in the spec section.</p><h2 id="example">Example</h2><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/frontend.yaml"><code>controllers/frontend.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy controllers/frontend.yaml to clipboard"></div><div class="includecode" id="controllers-frontend-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ReplicaSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>guestbook<span>
</span></span></span><span><span><span>    </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span># modify replicas according to your case</span><span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>php-redis<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5<span>
</span></span></span></code></pre></div></div></div><p>Saving this manifest into <code>frontend.yaml</code> and submitting it to a Kubernetes cluster will
create the defined ReplicaSet and the Pods that it manages.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>You can then get the current ReplicaSets deployed:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get rs
</span></span></code></pre></div><p>And see the frontend one you created:</p><pre tabindex="0"><code>NAME       DESIRED   CURRENT   READY   AGE
frontend   3         3         3       6s
</code></pre><p>You can also check on the state of the ReplicaSet:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe rs/frontend
</span></span></code></pre></div><p>And you will see output similar to:</p><pre tabindex="0"><code>Name:         frontend
Namespace:    default
Selector:     tier=frontend
Labels:       app=guestbook
              tier=frontend
Annotations:  &lt;none&gt;
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  tier=frontend
  Containers:
   php-redis:
    Image:        us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-gbgfx
  Normal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-rwz57
  Normal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-wkl7w
</code></pre><p>And lastly you can check for the Pods brought up:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>You should see Pod information similar to:</p><pre tabindex="0"><code>NAME             READY   STATUS    RESTARTS   AGE
frontend-gbgfx   1/1     Running   0          10m
frontend-rwz57   1/1     Running   0          10m
frontend-wkl7w   1/1     Running   0          10m
</code></pre><p>You can also verify that the owner reference of these pods is set to the frontend ReplicaSet.
To do this, get the yaml of one of the Pods running:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods frontend-gbgfx -o yaml
</span></span></code></pre></div><p>The output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>creationTimestamp</span>:<span> </span><span>"2024-02-28T22:30:44Z"</span><span>
</span></span></span><span><span><span>  </span><span>generateName</span>:<span> </span>frontend-<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>frontend-gbgfx<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>ownerReferences</span>:<span>
</span></span></span><span><span><span>  </span>- <span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span>    </span><span>blockOwnerDeletion</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>controller</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>ReplicaSet<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>    </span><span>uid</span>:<span> </span>e129deca-f864-481b-bb16-b27abfd92292<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><h2 id="non-template-pod-acquisitions">Non-Template Pod acquisitions</h2><p>While you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have
labels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited
to owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.</p><p>Take the previous frontend ReplicaSet example, and the Pods specified in the following manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-rs.yaml"><code>pods/pod-rs.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-rs.yaml to clipboard"></div><div class="includecode" id="pods-pod-rs-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod1<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>hello1<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>gcr.io/google-samples/hello-app:2.0<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod2<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>hello2<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>gcr.io/google-samples/hello-app:1.0<span>
</span></span></span></code></pre></div></div></div><p>As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend
ReplicaSet, they will immediately be acquired by it.</p><p>Suppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to
fulfill its replica count requirement:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over
its desired count.</p><p>Fetching the Pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>The output shows that the new Pods are either already terminated, or in the process of being terminated:</p><pre tabindex="0"><code>NAME             READY   STATUS        RESTARTS   AGE
frontend-b2zdv   1/1     Running       0          10m
frontend-vcmts   1/1     Running       0          10m
frontend-wtsmm   1/1     Running       0          10m
pod1             0/1     Terminating   0          1s
pod2             0/1     Terminating   0          1s
</code></pre><p>If you create the Pods first:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>And then create the ReplicaSet however:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the
number of its new Pods and the original matches its desired count. As fetching the Pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods
</span></span></code></pre></div><p>Will reveal in its output:</p><pre tabindex="0"><code>NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   0          9s
pod1             1/1     Running   0          36s
pod2             1/1     Running   0          36s
</code></pre><p>In this manner, a ReplicaSet can own a non-homogeneous set of Pods</p><h2 id="writing-a-replicaset-manifest">Writing a ReplicaSet manifest</h2><p>As with all other Kubernetes API objects, a ReplicaSet needs the <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.
For ReplicaSets, the <code>kind</code> is always a ReplicaSet.</p><p>When the control plane creates new Pods for a ReplicaSet, the <code>.metadata.name</code> of the
ReplicaSet is part of the basis for naming those Pods. The name of a ReplicaSet must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostnames. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><p>A ReplicaSet also needs a <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> section</a>.</p><h3 id="pod-template">Pod Template</h3><p>The <code>.spec.template</code> is a <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a> which is also
required to have labels in place. In our <code>frontend.yaml</code> example we had one label: <code>tier: frontend</code>.
Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.</p><p>For the template's <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">restart policy</a> field,
<code>.spec.template.spec.restartPolicy</code>, the only allowed value is <code>Always</code>, which is the default.</p><h3 id="pod-selector">Pod Selector</h3><p>The <code>.spec.selector</code> field is a <a href="/docs/concepts/overview/working-with-objects/labels/">label selector</a>. As discussed
<a href="#how-a-replicaset-works">earlier</a> these are the labels used to identify potential Pods to acquire. In our
<code>frontend.yaml</code> example, the selector was:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>  </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span></code></pre></div><p>In the ReplicaSet, <code>.spec.template.metadata.labels</code> must match <code>spec.selector</code>, or it will
be rejected by the API.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>For 2 ReplicaSets specifying the same <code>.spec.selector</code> but different
<code>.spec.template.metadata.labels</code> and <code>.spec.template.spec</code> fields, each ReplicaSet ignores the
Pods created by the other ReplicaSet.</div><h3 id="replicas">Replicas</h3><p>You can specify how many Pods should run concurrently by setting <code>.spec.replicas</code>. The ReplicaSet will create/delete
its Pods to match this number.</p><p>If you do not specify <code>.spec.replicas</code>, then it defaults to 1.</p><h2 id="working-with-replicasets">Working with ReplicaSets</h2><h3 id="deleting-a-replicaset-and-its-pods">Deleting a ReplicaSet and its Pods</h3><p>To delete a ReplicaSet and all of its Pods, use
<a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>. The
<a href="/docs/concepts/architecture/garbage-collection/">Garbage collector</a> automatically deletes all of
the dependent Pods by default.</p><p>When using the REST API or the <code>client-go</code> library, you must set <code>propagationPolicy</code> to
<code>Background</code> or <code>Foreground</code> in the <code>-d</code> option. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl proxy --port<span>=</span><span>8080</span>
</span></span><span><span>curl -X DELETE  <span>'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend'</span> <span>\
</span></span></span><span><span><span></span>  -d <span>'{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}'</span> <span>\
</span></span></span><span><span><span></span>  -H <span>"Content-Type: application/json"</span>
</span></span></code></pre></div><h3 id="deleting-just-a-replicaset">Deleting just a ReplicaSet</h3><p>You can delete a ReplicaSet without affecting any of its Pods using
<a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>
with the <code>--cascade=orphan</code> option.
When using the REST API or the <code>client-go</code> library, you must set <code>propagationPolicy</code> to <code>Orphan</code>.
For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl proxy --port<span>=</span><span>8080</span>
</span></span><span><span>curl -X DELETE  <span>'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend'</span> <span>\
</span></span></span><span><span><span></span>  -d <span>'{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}'</span> <span>\
</span></span></span><span><span><span></span>  -H <span>"Content-Type: application/json"</span>
</span></span></code></pre></div><p>Once the original is deleted, you can create a new ReplicaSet to replace it. As long
as the old and new <code>.spec.selector</code> are the same, then the new one will adopt the old Pods.
However, it will not make any effort to make existing Pods match a new, different pod template.
To update Pods to a new spec in a controlled way, use a
<a href="/docs/concepts/workloads/controllers/deployment/#creating-a-deployment">Deployment</a>, as
ReplicaSets do not support a rolling update directly.</p><h3 id="terminating-pods">Terminating Pods</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DeploymentReplicaSetTerminatingReplicas"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>You can enable this feature by setting the <code>DeploymentReplicaSetTerminatingReplicas</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
on the <a href="/docs/reference/command-line-tools-reference/kube-apiserver/">API server</a>
and on the <a href="/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a></p><p>Pods that become terminating due to deletion or scale down may take a long time to terminate, and may consume
additional resources during that period. As a result, the total number of all pods can temporarily exceed
<code>.spec.replicas</code>. Terminating pods can be tracked using the <code>.status.terminatingReplicas</code> field of the ReplicaSet.</p><h3 id="isolating-pods-from-a-replicaset">Isolating Pods from a ReplicaSet</h3><p>You can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods
from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (
assuming that the number of replicas is not also changed).</p><h3 id="scaling-a-replicaset">Scaling a ReplicaSet</h3><p>A ReplicaSet can be easily scaled up or down by simply updating the <code>.spec.replicas</code> field. The ReplicaSet controller
ensures that a desired number of Pods with a matching label selector are available and operational.</p><p>When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to
prioritize scaling down pods based on the following general algorithm:</p><ol><li>Pending (and unschedulable) pods are scaled down first</li><li>If <code>controller.kubernetes.io/pod-deletion-cost</code> annotation is set, then
the pod with the lower value will come first.</li><li>Pods on nodes with more replicas come before pods on nodes with fewer replicas.</li><li>If the pods' creation times differ, the pod that was created more recently
comes before the older pod (the creation times are bucketed on an integer log scale).</li></ol><p>If all of the above match, then selection is random.</p><h3 id="pod-deletion-cost">Pod deletion cost</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [beta]</code></div><p>Using the <a href="/docs/reference/labels-annotations-taints/#pod-deletion-cost"><code>controller.kubernetes.io/pod-deletion-cost</code></a>
annotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.</p><p>The annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents the cost of
deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion
cost are preferred to be deleted before pods with higher deletion cost.</p><p>The implicit value for this annotation for pods that don't set it is 0; negative values are permitted.
Invalid values will be rejected by the API server.</p><p>This feature is beta and enabled by default. You can disable it using the
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
<code>PodDeletionCost</code> in both kube-apiserver and kube-controller-manager.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li>This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.</li><li>Users should avoid updating the annotation frequently, such as updating it based on a metric value,
because doing so will generate a significant number of pod updates on the apiserver.</li></ul></div><h4 id="example-use-case">Example Use Case</h4><p>The different pods of an application could have different utilization levels. On scale down, the application
may prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application
should update <code>controller.kubernetes.io/pod-deletion-cost</code> once before issuing a scale down (setting the
annotation to a value proportional to pod utilization level). This works if the application itself controls
the down scaling; for example, the driver pod of a Spark deployment.</p><h3 id="replicaset-as-a-horizontal-pod-autoscaler-target">ReplicaSet as a Horizontal Pod Autoscaler Target</h3><p>A ReplicaSet can also be a target for
<a href="/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscalers (HPA)</a>. That is,
a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting
the ReplicaSet we created in the previous example.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/hpa-rs.yaml"><code>controllers/hpa-rs.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy controllers/hpa-rs.yaml to clipboard"></div><div class="includecode" id="controllers-hpa-rs-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>autoscaling/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>HorizontalPodAutoscaler<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>frontend-scaler<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>scaleTargetRef</span>:<span>
</span></span></span><span><span><span>    </span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>ReplicaSet<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>  </span><span>minReplicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>maxReplicas</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span><span>targetCPUUtilizationPercentage</span>:<span> </span><span>50</span><span>
</span></span></span></code></pre></div></div></div><p>Saving this manifest into <code>hpa-rs.yaml</code> and submitting it to a Kubernetes cluster should
create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage
of the replicated Pods.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
</span></span></code></pre></div><p>Alternatively, you can use the <code>kubectl autoscale</code> command to accomplish the same
(and it's easier!)</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl autoscale rs frontend --max<span>=</span><span>10</span> --min<span>=</span><span>3</span> --cpu<span>=</span>50%
</span></span></code></pre></div><h2 id="alternatives-to-replicaset">Alternatives to ReplicaSet</h2><h3 id="deployment-recommended">Deployment (recommended)</h3><p><a href="/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> is an object which can own ReplicaSets and update
them and their Pods via declarative, server-side rolling updates.
While ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Pod
creation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that
they create. Deployments own and manage their ReplicaSets.
As such, it is recommended to use Deployments when you want ReplicaSets.</p><h3 id="bare-pods">Bare Pods</h3><p>Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or
terminated for any reason, such as in the case of node failure or disruptive node maintenance,
such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your
application requires only a single Pod. Think of it similarly to a process supervisor, only it
supervises multiple Pods across multiple nodes instead of individual processes on a single node. A
ReplicaSet delegates local container restarts to some agent on the node such as Kubelet.</p><h3 id="job">Job</h3><p>Use a <a href="/docs/concepts/workloads/controllers/job/"><code>Job</code></a> instead of a ReplicaSet for Pods that are
expected to terminate on their own (that is, batch jobs).</p><h3 id="daemonset">DaemonSet</h3><p>Use a <a href="/docs/concepts/workloads/controllers/daemonset/"><code>DaemonSet</code></a> instead of a ReplicaSet for Pods that provide a
machine-level function, such as machine monitoring or machine logging. These Pods have a lifetime that is tied
to a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.</p><h3 id="replicationcontroller">ReplicationController</h3><p>ReplicaSets are the successors to <a href="/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationControllers</a>.
The two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based
selector requirements as described in the <a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">labels user guide</a>.
As such, ReplicaSets are preferred over ReplicationControllers</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a>.</li><li>Learn about <a href="/docs/concepts/workloads/controllers/deployment/">Deployments</a>.</li><li><a href="/docs/tasks/run-application/run-stateless-application-deployment/">Run a Stateless Application Using a Deployment</a>,
which relies on ReplicaSets to work.</li><li><code>ReplicaSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/replica-set-v1/">ReplicaSet</a>
object definition to understand the API for replica sets.</li><li>Read about <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li></ul></div></div><div><div class="td-content"><h1>StatefulSets</h1><div class="lead">A StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for managing applications that need persistent storage or a stable, unique network identity.</div><p>StatefulSet is the workload API object used to manage stateful applications.</p><p>Manages the deployment and scaling of a set of <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>, <em>and provides guarantees about the ordering and uniqueness</em> of these Pods.</p><p>Like a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a>, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p><p>If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.</p><h2 id="using-statefulsets">Using StatefulSets</h2><p>StatefulSets are valuable for applications that require one or more of the
following:</p><ul><li>Stable, unique network identifiers.</li><li>Stable, persistent storage.</li><li>Ordered, graceful deployment and scaling.</li><li>Ordered, automated rolling updates.</li></ul><p>In the above, stable is synonymous with persistence across Pod (re)scheduling.
If an application doesn't require any stable identifiers or ordered deployment,
deletion, or scaling, you should deploy your application using a workload object
that provides a set of stateless replicas.
<a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> or
<a href="/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> may be better suited to your stateless needs.</p><h2 id="limitations">Limitations</h2><ul><li>The storage for a given Pod must either be provisioned by a
<a href="/docs/concepts/storage/dynamic-provisioning/">PersistentVolume Provisioner</a>
based on the requested <em>storage class</em>, or pre-provisioned by an admin.</li><li>Deleting and/or scaling a StatefulSet down will <em>not</em> delete the volumes associated with the
StatefulSet. This is done to ensure data safety, which is generally more valuable than an
automatic purge of all related StatefulSet resources.</li><li>StatefulSets currently require a <a href="/docs/concepts/services-networking/service/#headless-services">Headless Service</a>
to be responsible for the network identity of the Pods. You are responsible for creating this
Service.</li><li>StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is
deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is
possible to scale the StatefulSet down to 0 prior to deletion.</li><li>When using <a href="#rolling-updates">Rolling Updates</a> with the default
<a href="#pod-management-policies">Pod Management Policy</a> (<code>OrderedReady</code>),
it's possible to get into a broken state that requires
<a href="#forced-rollback">manual intervention to repair</a>.</li></ul><h2 id="components">Components</h2><p>The example below demonstrates the components of a StatefulSet.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>web<span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span>None<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StatefulSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>web<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span> </span><span># has to match .spec.template.metadata.labels</span><span>
</span></span></span><span><span><span>  </span><span>serviceName</span>:<span> </span><span>"nginx"</span><span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span> </span><span># by default is 1</span><span>
</span></span></span><span><span><span>  </span><span>minReadySeconds</span>:<span> </span><span>10</span><span> </span><span># by default is 0</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span> </span><span># has to match .spec.selector.matchLabels</span><span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>terminationGracePeriodSeconds</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>registry.k8s.io/nginx-slim:0.24<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>web<span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>www<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/usr/share/nginx/html<span>
</span></span></span><span><span><span>  </span><span>volumeClaimTemplates</span>:<span>
</span></span></span><span><span><span>  </span>- <span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>www<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>accessModes</span>:<span> </span>[<span> </span><span>"ReadWriteOnce"</span><span> </span>]<span>
</span></span></span><span><span><span>      </span><span>storageClassName</span>:<span> </span><span>"my-storage-class"</span><span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span>
</span></span></span><span><span><span>        </span><span>requests</span>:<span>
</span></span></span><span><span><span>          </span><span>storage</span>:<span> </span>1Gi<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This example uses the <code>ReadWriteOnce</code> access mode, for simplicity. For
production use, the Kubernetes project recommends using the <code>ReadWriteOncePod</code>
access mode instead.</div><p>In the above example:</p><ul><li>A Headless Service, named <code>nginx</code>, is used to control the network domain.</li><li>The StatefulSet, named <code>web</code>, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.</li><li>The <code>volumeClaimTemplates</code> will provide stable storage using
<a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a> provisioned by a
PersistentVolume Provisioner.</li></ul><p>The name of a StatefulSet object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><h3 id="pod-selector">Pod Selector</h3><p>You must set the <code>.spec.selector</code> field of a StatefulSet to match the labels of its
<code>.spec.template.metadata.labels</code>. Failing to specify a matching Pod Selector will result in a
validation error during StatefulSet creation.</p><h3 id="volume-claim-templates">Volume Claim Templates</h3><p>You can set the <code>.spec.volumeClaimTemplates</code> field to create a
<a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim</a>.
This will provide stable storage to the StatefulSet if either:</p><ul><li>The StorageClass specified for the volume claim is set up to use <a href="/docs/concepts/storage/dynamic-provisioning/">dynamic
provisioning</a>.</li><li>The cluster already contains a PersistentVolume with the correct StorageClass
and sufficient available storage space.</li></ul><h3 id="minimum-ready-seconds">Minimum ready seconds</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p><code>.spec.minReadySeconds</code> is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be running and ready without any of its containers crashing, for it to be considered available.
This is used to check progression of a rollout when using a <a href="#rolling-updates">Rolling Update</a> strategy.
This field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see <a href="/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">Container Probes</a>.</p><h2 id="pod-identity">Pod Identity</h2><p>StatefulSet Pods have a unique identity that consists of an ordinal, a
stable network identity, and stable storage. The identity sticks to the Pod,
regardless of which node it's (re)scheduled on.</p><h3 id="ordinal-index">Ordinal Index</h3><p>For a StatefulSet with N <a href="#replicas">replicas</a>, each Pod in the StatefulSet
will be assigned an integer ordinal, that is unique over the Set. By default,
pods will be assigned ordinals from 0 up through N-1. The StatefulSet controller
will also add a pod label with this index: <code>apps.kubernetes.io/pod-index</code>.</p><h3 id="start-ordinal">Start ordinal</h3><div class="feature-state-notice feature-stable" title="Feature Gate: StatefulSetStartOrdinal"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p><code>.spec.ordinals</code> is an optional field that allows you to configure the integer
ordinals assigned to each Pod. It defaults to nil. Within the field, you can
configure the following options:</p><ul><li><code>.spec.ordinals.start</code>: If the <code>.spec.ordinals.start</code> field is set, Pods will
be assigned ordinals from <code>.spec.ordinals.start</code> up through
<code>.spec.ordinals.start + .spec.replicas - 1</code>.</li></ul><h3 id="stable-network-id">Stable Network ID</h3><p>Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet
and the ordinal of the Pod. The pattern for the constructed hostname
is <code>$(statefulset name)-$(ordinal)</code>. The example above will create three Pods
named <code>web-0,web-1,web-2</code>.
A StatefulSet can use a <a href="/docs/concepts/services-networking/service/#headless-services">Headless Service</a>
to control the domain of its Pods. The domain managed by this Service takes the form:
<code>$(service name).$(namespace).svc.cluster.local</code>, where "cluster.local" is the
cluster domain.
As each Pod is created, it gets a matching DNS subdomain, taking the form:
<code>$(podname).$(governing service domain)</code>, where the governing service is defined
by the <code>serviceName</code> field on the StatefulSet.</p><p>Depending on how DNS is configured in your cluster, you may not be able to look up the DNS
name for a newly-run Pod immediately. This behavior can occur when other clients in the
cluster have already sent queries for the hostname of the Pod before it was created.
Negative caching (normal in DNS) means that the results of previous failed lookups are
remembered and reused, even after the Pod is running, for at least a few seconds.</p><p>If you need to discover Pods promptly after they are created, you have a few options:</p><ul><li>Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.</li><li>Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the
config map for CoreDNS, which currently caches for 30 seconds).</li></ul><p>As mentioned in the <a href="#limitations">limitations</a> section, you are responsible for
creating the <a href="/docs/concepts/services-networking/service/#headless-services">Headless Service</a>
responsible for the network identity of the pods.</p><p>Here are some examples of choices for Cluster Domain, Service name,
StatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.</p><table><thead><tr><th>Cluster Domain</th><th>Service (ns/name)</th><th>StatefulSet (ns/name)</th><th>StatefulSet Domain</th><th>Pod DNS</th><th>Pod Hostname</th></tr></thead><tbody><tr><td>cluster.local</td><td>default/nginx</td><td>default/web</td><td>nginx.default.svc.cluster.local</td><td>web-{0..N-1}.nginx.default.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>cluster.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.cluster.local</td><td>web-{0..N-1}.nginx.foo.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>kube.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.kube.local</td><td>web-{0..N-1}.nginx.foo.svc.kube.local</td><td>web-{0..N-1}</td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Cluster Domain will be set to <code>cluster.local</code> unless
<a href="/docs/concepts/services-networking/dns-pod-service/">otherwise configured</a>.</div><h3 id="stable-storage">Stable Storage</h3><p>For each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one
PersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume
with a StorageClass of <code>my-storage-class</code> and 1 GiB of provisioned storage. If no StorageClass
is specified, then the default StorageClass will be used. When a Pod is (re)scheduled
onto a node, its <code>volumeMounts</code> mount the PersistentVolumes associated with its
PersistentVolume Claims. Note that, the PersistentVolumes associated with the
Pods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.
This must be done manually.</p><h3 id="pod-name-label">Pod Name Label</h3><p>When the StatefulSet <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> creates a Pod,
it adds a label, <code>statefulset.kubernetes.io/pod-name</code>, that is set to the name of
the Pod. This label allows you to attach a Service to a specific Pod in
the StatefulSet.</p><h3 id="pod-index-label">Pod index label</h3><div class="feature-state-notice feature-stable" title="Feature Gate: PodIndexLabel"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>When the StatefulSet <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> creates a Pod,
the new Pod is labelled with <code>apps.kubernetes.io/pod-index</code>. The value of this label is the ordinal index of
the Pod. This label allows you to route traffic to a particular pod index, filter logs/metrics
using the pod index label, and more. Note the feature gate <code>PodIndexLabel</code> is enabled and locked by default for this
feature, in order to disable it, users will have to use server emulated version v1.31.</p><h2 id="deployment-and-scaling-guarantees">Deployment and Scaling Guarantees</h2><ul><li>For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.</li><li>When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.</li><li>Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.</li><li>Before a Pod is terminated, all of its successors must be completely shutdown.</li></ul><p>The StatefulSet should not specify a <code>pod.Spec.TerminationGracePeriodSeconds</code> of 0. This practice
is unsafe and strongly discouraged. For further explanation, please refer to
<a href="/docs/tasks/run-application/force-delete-stateful-set-pod/">force deleting StatefulSet Pods</a>.</p><p>When the nginx example above is created, three Pods will be deployed in the order
web-0, web-1, web-2. web-1 will not be deployed before web-0 is
<a href="/docs/concepts/workloads/pods/pod-lifecycle/">Running and Ready</a>, and web-2 will not be deployed until
web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before
web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and
becomes Running and Ready.</p><p>If a user were to scale the deployed example by patching the StatefulSet such that
<code>replicas=1</code>, web-2 would be terminated first. web-1 would not be terminated until web-2
is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and
is completely shutdown, but prior to web-1's termination, web-1 would not be terminated
until web-0 is Running and Ready.</p><h3 id="pod-management-policies">Pod Management Policies</h3><p>StatefulSet allows you to relax its ordering guarantees while
preserving its uniqueness and identity guarantees via its <code>.spec.podManagementPolicy</code> field.</p><h4 id="orderedready-pod-management">OrderedReady Pod Management</h4><p><code>OrderedReady</code> pod management is the default for StatefulSets. It implements the behavior
described in <a href="#deployment-and-scaling-guarantees">Deployment and Scaling Guarantees</a>.</p><h4 id="parallel-pod-management">Parallel Pod Management</h4><p><code>Parallel</code> pod management tells the StatefulSet controller to launch or
terminate all Pods in parallel, and to not wait for Pods to become Running
and Ready or completely terminated prior to launching or terminating another
Pod. This option only affects the behavior for scaling operations. Updates are not
affected.</p><h2 id="update-strategies">Update strategies</h2><p>A StatefulSet's <code>.spec.updateStrategy</code> field allows you to configure
and disable automated rolling updates for containers, labels, resource request/limits, and
annotations for the Pods in a StatefulSet. There are two possible values:</p><dl><dt><code>OnDelete</code></dt><dd>When a StatefulSet's <code>.spec.updateStrategy.type</code> is set to <code>OnDelete</code>,
the StatefulSet controller will not automatically update the Pods in a
StatefulSet. Users must manually delete Pods to cause the controller to
create new Pods that reflect modifications made to a StatefulSet's <code>.spec.template</code>.</dd><dt><code>RollingUpdate</code></dt><dd>The <code>RollingUpdate</code> update strategy implements automated, rolling updates for the Pods in a
StatefulSet. This is the default update strategy.</dd></dl><h2 id="rolling-updates">Rolling Updates</h2><p>When a StatefulSet's <code>.spec.updateStrategy.type</code> is set to <code>RollingUpdate</code>, the
StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed
in the same order as Pod termination (from the largest ordinal to the smallest), updating
each Pod one at a time.</p><p>The Kubernetes control plane waits until an updated Pod is Running and Ready prior
to updating its predecessor. If you have set <code>.spec.minReadySeconds</code> (see
<a href="#minimum-ready-seconds">Minimum Ready Seconds</a>), the control plane additionally waits that
amount of time after the Pod turns ready, before moving on.</p><h3 id="partitions">Partitioned rolling updates</h3><p>The <code>RollingUpdate</code> update strategy can be partitioned, by specifying a
<code>.spec.updateStrategy.rollingUpdate.partition</code>. If a partition is specified, all Pods with an
ordinal that is greater than or equal to the partition will be updated when the StatefulSet's
<code>.spec.template</code> is updated. All Pods with an ordinal that is less than the partition will not
be updated, and, even if they are deleted, they will be recreated at the previous version. If a
StatefulSet's <code>.spec.updateStrategy.rollingUpdate.partition</code> is greater than its <code>.spec.replicas</code>,
updates to its <code>.spec.template</code> will not be propagated to its Pods.
In most cases you will not need to use a partition, but they are useful if you want to stage an
update, roll out a canary, or perform a phased roll out.</p><h3 id="maximum-unavailable-pods">Maximum unavailable Pods</h3><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [alpha]</code></div><p>You can control the maximum number of Pods that can be unavailable during an update
by specifying the <code>.spec.updateStrategy.rollingUpdate.maxUnavailable</code> field.
The value can be an absolute number (for example, <code>5</code>) or a percentage of desired
Pods (for example, <code>10%</code>). Absolute number is calculated from the percentage value
by rounding it up. This field cannot be 0. The default setting is 1.</p><p>This field applies to all Pods in the range <code>0</code> to <code>replicas - 1</code>. If there is any
unavailable Pod in the range <code>0</code> to <code>replicas - 1</code>, it will be counted towards
<code>maxUnavailable</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>maxUnavailable</code> field is in Alpha stage and it is honored only by API servers
that are running with the <code>MaxUnavailableStatefulSet</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
enabled.</div><h3 id="forced-rollback">Forced rollback</h3><p>When using <a href="#rolling-updates">Rolling Updates</a> with the default
<a href="#pod-management-policies">Pod Management Policy</a> (<code>OrderedReady</code>),
it's possible to get into a broken state that requires manual intervention to repair.</p><p>If you update the Pod template to a configuration that never becomes Running and
Ready (for example, due to a bad binary or application-level configuration error),
StatefulSet will stop the rollout and wait.</p><p>In this state, it's not enough to revert the Pod template to a good configuration.
Due to a <a href="https://github.com/kubernetes/kubernetes/issues/67250">known issue</a>,
StatefulSet will continue to wait for the broken Pod to become Ready
(which never happens) before it will attempt to revert it back to the working
configuration.</p><p>After reverting the template, you must also delete any Pods that StatefulSet had
already attempted to run with the bad configuration.
StatefulSet will then begin to recreate the Pods using the reverted template.</p><h2 id="revision-history">Revision history</h2><p>ControllerRevision is a Kubernetes API resource used by controllers, such as the StatefulSet controller, to track historical configuration changes.</p><p>StatefulSets use ControllerRevisions to maintain a revision history, enabling rollbacks and version tracking.</p><h3 id="how-statefulsets-track-changes-using-controllerrevisions">How StatefulSets track changes using ControllerRevisions</h3><p>When you update a StatefulSet's Pod template (<code>spec.template</code>), the StatefulSet controller:</p><ol><li>Prepares a new ControllerRevision object</li><li>Stores a snapshot of the Pod template and metadata</li><li>Assigns an incremental revision number</li></ol><h4 id="key-properties">Key Properties</h4><p>See <a href="/docs/reference/kubernetes-api/workload-resources/controller-revision-v1/">ControllerRevision</a> to learn more about key properties and other details.</p><hr><h3 id="managing-revision-history">Managing Revision History</h3><p>Control retained revisions with <code>.spec.revisionHistoryLimit</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StatefulSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>webapp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>revisionHistoryLimit</span>:<span> </span><span>5</span><span>  </span><span># Keep last 5 revisions</span><span>
</span></span></span><span><span><span>  </span><span># ... other spec fields ...</span><span>
</span></span></span></code></pre></div><ul><li><strong>Default</strong>: 10 revisions retained if unspecified</li><li><strong>Cleanup</strong>: Oldest revisions are garbage-collected when exceeding the limit</li></ul><h4 id="performing-rollbacks">Performing Rollbacks</h4><p>You can revert to a previous configuration using:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># View revision history</span>
</span></span><span><span>kubectl rollout <span>history</span> statefulset/webapp
</span></span><span><span>
</span></span><span><span><span># Rollback to a specific revision</span>
</span></span><span><span>kubectl rollout undo statefulset/webapp --to-revision<span>=</span><span>3</span>
</span></span></code></pre></div><p>This will:</p><ul><li>Apply the Pod template from revision 3</li><li>Create a new ControllerRevision with an updated revision number</li></ul><h4 id="inspecting-controllerrevisions">Inspecting ControllerRevisions</h4><p>To view associated ControllerRevisions:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># List all revisions for the StatefulSet</span>
</span></span><span><span>kubectl get controllerrevisions -l app.kubernetes.io/name<span>=</span>webapp
</span></span><span><span>
</span></span><span><span><span># View detailed configuration of a specific revision</span>
</span></span><span><span>kubectl get controllerrevision/webapp-3 -o yaml
</span></span></code></pre></div><h4 id="best-practices">Best Practices</h4><h5 id="retention-policy">Retention Policy</h5><ul><li>Set <code>revisionHistoryLimit</code> between <strong>5&#8211;10</strong> for most workloads.</li><li>Increase only if <strong>deep rollback history</strong> is required.</li></ul><h5 id="monitoring">Monitoring</h5><ul><li><p>Regularly check revisions with:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get controllerrevisions
</span></span></code></pre></div></li><li><p>Alert on <strong>rapid revision count growth</strong>.</p></li></ul><h5 id="avoid">Avoid</h5><ul><li>Manual edits to ControllerRevision objects.</li><li>Using revisions as a backup mechanism (use actual backup tools).</li><li>Setting <code>revisionHistoryLimit: 0</code> (disables rollback capability).</li></ul><h2 id="persistentvolumeclaim-retention">PersistentVolumeClaim retention</h2><div class="feature-state-notice feature-stable" title="Feature Gate: StatefulSetAutoDeletePVC"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>The optional <code>.spec.persistentVolumeClaimRetentionPolicy</code> field controls if
and how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the
<code>StatefulSetAutoDeletePVC</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
on the API server and the controller manager to use this field.
Once enabled, there are two policies you can configure for each StatefulSet:</p><dl><dt><code>whenDeleted</code></dt><dd>Configures the volume retention behavior that applies when the StatefulSet is deleted.</dd><dt><code>whenScaled</code></dt><dd>Configures the volume retention behavior that applies when the replica count of
the StatefulSet is reduced; for example, when scaling down the set.</dd></dl><p>For each policy that you can configure, you can set the value to either <code>Delete</code> or <code>Retain</code>.</p><dl><dt><code>Delete</code></dt><dd>The PVCs created from the StatefulSet <code>volumeClaimTemplate</code> are deleted for each Pod
affected by the policy. With the <code>whenDeleted</code> policy all PVCs from the
<code>volumeClaimTemplate</code> are deleted after their Pods have been deleted. With the
<code>whenScaled</code> policy, only PVCs corresponding to Pod replicas being scaled down are
deleted, after their Pods have been deleted.</dd><dt><code>Retain</code> (default)</dt><dd>PVCs from the <code>volumeClaimTemplate</code> are not affected when their Pod is
deleted. This is the behavior before this new feature.</dd></dl><p>Bear in mind that these policies <strong>only</strong> apply when Pods are being removed due to the
StatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet
fails due to node failure, and the control plane creates a replacement Pod, the StatefulSet
retains the existing PVC. The existing volume is unaffected, and the cluster will attach it to
the node where the new Pod is about to launch.</p><p>The default for policies is <code>Retain</code>, matching the StatefulSet behavior before this new feature.</p><p>Here is an example policy:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StatefulSet<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>persistentVolumeClaimRetentionPolicy</span>:<span>
</span></span></span><span><span><span>    </span><span>whenDeleted</span>:<span> </span>Retain<span>
</span></span></span><span><span><span>    </span><span>whenScaled</span>:<span> </span>Delete<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><p>The StatefulSet <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> adds
<a href="/docs/concepts/overview/working-with-objects/owners-dependents/#owner-references-in-object-specifications">owner references</a>
to its PVCs, which are then deleted by the <a class="glossary-tooltip" title="A collective term for the various mechanisms Kubernetes uses to clean up cluster resources." href="/docs/concepts/architecture/garbage-collection/" target="_blank">garbage collector</a> after the Pod is terminated. This enables the Pod to
cleanly unmount all volumes before the PVCs are deleted (and before the backing PV and
volume are deleted, depending on the retain policy). When you set the <code>whenDeleted</code>
policy to <code>Delete</code>, an owner reference to the StatefulSet instance is placed on all PVCs
associated with that StatefulSet.</p><p>The <code>whenScaled</code> policy must delete PVCs only when a Pod is scaled down, and not when a
Pod is deleted for another reason. When reconciling, the StatefulSet controller compares
its desired replica count to the actual Pods present on the cluster. Any StatefulSet Pod
whose id greater than the replica count is condemned and marked for deletion. If the
<code>whenScaled</code> policy is <code>Delete</code>, the condemned Pods are first set as owners to the
associated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs
to be garbage collected after only the condemned Pods have terminated.</p><p>This means that if the controller crashes and restarts, no Pod will be deleted before its
owner reference has been updated appropriate to the policy. If a condemned Pod is
force-deleted while the controller is down, the owner reference may or may not have been
set up, depending on when the controller crashed. It may take several reconcile loops to
update the owner references, so some condemned Pods may have set up owner references and
others may not. For this reason we recommend waiting for the controller to come back up,
which will verify owner references before terminating Pods. If that is not possible, the
operator should verify the owner references on PVCs to ensure the expected objects are
deleted when Pods are force-deleted.</p><h3 id="replicas">Replicas</h3><p><code>.spec.replicas</code> is an optional field that specifies the number of desired Pods. It defaults to 1.</p><p>Should you manually scale a deployment, example via <code>kubectl scale statefulset statefulset --replicas=X</code>, and then you update that StatefulSet
based on a manifest (for example: by running <code>kubectl apply -f statefulset.yaml</code>), then applying that manifest overwrites the manual scaling
that you previously did.</p><p>If a <a href="/docs/tasks/run-application/horizontal-pod-autoscale/">HorizontalPodAutoscaler</a>
(or any similar API for horizontal scaling) is managing scaling for a
Statefulset, don't set <code>.spec.replicas</code>. Instead, allow the Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> to manage
the <code>.spec.replicas</code> field automatically.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a>.</li><li>Find out how to use StatefulSets<ul><li>Follow an example of <a href="/docs/tutorials/stateful-application/basic-stateful-set/">deploying a stateful application</a>.</li><li>Follow an example of <a href="/docs/tutorials/stateful-application/cassandra/">deploying Cassandra with Stateful Sets</a>.</li><li>Follow an example of <a href="/docs/tasks/run-application/run-replicated-stateful-application/">running a replicated stateful application</a>.</li><li>Learn how to <a href="/docs/tasks/run-application/scale-stateful-set/">scale a StatefulSet</a>.</li><li>Learn what's involved when you <a href="/docs/tasks/run-application/delete-stateful-set/">delete a StatefulSet</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/configure-volume-storage/">configure a Pod to use a volume for storage</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">configure a Pod to use a PersistentVolume for storage</a>.</li></ul></li><li><code>StatefulSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/">StatefulSet</a>
object definition to understand the API for stateful sets.</li><li>Read about <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li></ul></div></div><div><div class="td-content"><h1>DaemonSet</h1><div class="lead">A DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.</div><p>A <em>DaemonSet</em> ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the
cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage
collected. Deleting a DaemonSet will clean up the Pods it created.</p><p>Some typical uses of a DaemonSet are:</p><ul><li>running a cluster storage daemon on every node</li><li>running a logs collection daemon on every node</li><li>running a node monitoring daemon on every node</li></ul><p>In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.
A more complex setup might use multiple DaemonSets for a single type of daemon, but with
different flags and/or different memory and cpu requests for different hardware types.</p><h2 id="writing-a-daemonset-spec">Writing a DaemonSet Spec</h2><h3 id="create-a-daemonset">Create a DaemonSet</h3><p>You can describe a DaemonSet in a YAML file. For example, the <code>daemonset.yaml</code> file below
describes a DaemonSet that runs the fluentd-elasticsearch Docker image:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/daemonset.yaml"><code>controllers/daemonset.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy controllers/daemonset.yaml to clipboard"></div><div class="includecode" id="controllers-daemonset-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DaemonSet<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>k8s-app</span>:<span> </span>fluentd-logging<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>      </span><span># these tolerations are to have the daemonset runnable on control plane nodes</span><span>
</span></span></span><span><span><span>      </span><span># remove them if your control plane nodes should not run pods</span><span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>node-role.kubernetes.io/control-plane<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>        </span><span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>node-role.kubernetes.io/master<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>        </span><span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>fluentd-elasticsearch<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>quay.io/fluentd_elasticsearch/fluentd:v5.0.1<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>limits</span>:<span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span>          </span><span>requests</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span>100m<span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span>200Mi<span>
</span></span></span><span><span><span>        </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>      </span><span># it may be desirable to set a high priority class to ensure that a DaemonSet Pod</span><span>
</span></span></span><span><span><span>      </span><span># preempts running Pods</span><span>
</span></span></span><span><span><span>      </span><span># priorityClassName: important</span><span>
</span></span></span><span><span><span>      </span><span>terminationGracePeriodSeconds</span>:<span> </span><span>30</span><span>
</span></span></span><span><span><span>      </span><span>volumes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>        </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>/var/log<span>
</span></span></span></code></pre></div></div></div><p>Create a DaemonSet based on the YAML file:</p><pre tabindex="0"><code>kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
</code></pre><h3 id="required-fields">Required Fields</h3><p>As with all other Kubernetes config, a DaemonSet needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields. For
general information about working with config files, see
<a href="/docs/tasks/run-application/run-stateless-application-deployment/">running stateless applications</a>
and <a href="/docs/concepts/overview/working-with-objects/object-management/">object management using kubectl</a>.</p><p>The name of a DaemonSet object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>A DaemonSet also needs a
<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code></a>
section.</p><h3 id="pod-template">Pod Template</h3><p>The <code>.spec.template</code> is one of the required fields in <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a>.
It has exactly the same schema as a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>,
except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate
labels (see <a href="#pod-selector">pod selector</a>).</p><p>A Pod Template in a DaemonSet must have a <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>RestartPolicy</code></a>
equal to <code>Always</code>, or be unspecified, which defaults to <code>Always</code>.</p><h3 id="pod-selector">Pod Selector</h3><p>The <code>.spec.selector</code> field is a pod selector. It works the same as the <code>.spec.selector</code> of
a <a href="/docs/concepts/workloads/controllers/job/">Job</a>.</p><p>You must specify a pod selector that matches the labels of the
<code>.spec.template</code>.
Also, once a DaemonSet is created,
its <code>.spec.selector</code> can not be mutated. Mutating the pod selector can lead to the
unintentional orphaning of Pods, and it was found to be confusing to users.</p><p>The <code>.spec.selector</code> is an object consisting of two fields:</p><ul><li><code>matchLabels</code> - works the same as the <code>.spec.selector</code> of a
<a href="/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>.</li><li><code>matchExpressions</code> - allows to build more sophisticated selectors by specifying key,
list of values and an operator that relates the key and values.</li></ul><p>When the two are specified the result is ANDed.</p><p>The <code>.spec.selector</code> must match the <code>.spec.template.metadata.labels</code>.
Config with these two not matching will be rejected by the API.</p><h3 id="running-pods-on-select-nodes">Running Pods on select Nodes</h3><p>If you specify a <code>.spec.template.spec.nodeSelector</code>, then the DaemonSet controller will
create Pods on nodes which match that <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">node selector</a>.
Likewise if you specify a <code>.spec.template.spec.affinity</code>,
then DaemonSet controller will create Pods on nodes which match that
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/">node affinity</a>.
If you do not specify either, then the DaemonSet controller will create Pods on all nodes.</p><h2 id="how-daemon-pods-are-scheduled">How Daemon Pods are scheduled</h2><p>A DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod.
The DaemonSet controller creates a Pod for each eligible node and adds the
<code>spec.affinity.nodeAffinity</code> field of the Pod to match the target host. After
the Pod is created, the default scheduler typically takes over and then binds
the Pod to the target host by setting the <code>.spec.nodeName</code> field. If the new
Pod cannot fit on the node, the default scheduler may preempt (evict) some of
the existing Pods based on the
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority">priority</a>
of the new Pod.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If it's important that the DaemonSet pod run on each node, it's often desirable
to set the <code>.spec.template.spec.priorityClassName</code> of the DaemonSet to a
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">PriorityClass</a>
with a higher priority to ensure that this eviction occurs.</div><p>The user can specify a different scheduler for the Pods of the DaemonSet, by
setting the <code>.spec.template.spec.schedulerName</code> field of the DaemonSet.</p><p>The original node affinity specified at the
<code>.spec.template.spec.affinity.nodeAffinity</code> field (if specified) is taken into
consideration by the DaemonSet controller when evaluating the eligible nodes,
but is replaced on the created Pod with the node affinity that matches the name
of the eligible node.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>nodeAffinity</span>:<span>
</span></span></span><span><span><span>  </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>    </span><span>nodeSelectorTerms</span>:<span>
</span></span></span><span><span><span>    </span>- <span>matchFields</span>:<span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>metadata.name<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>        </span><span>values</span>:<span>
</span></span></span><span><span><span>        </span>- target-host-name<span>
</span></span></span></code></pre></div><h3 id="taints-and-tolerations">Taints and tolerations</h3><p>The DaemonSet controller automatically adds a set of <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint." href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">tolerations</a> to DaemonSet Pods:</p><table><caption>Tolerations for DaemonSet pods</caption><thead><tr><th>Toleration key</th><th>Effect</th><th>Details</th></tr></thead><tbody><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-not-ready"><code>node.kubernetes.io/not-ready</code></a></td><td><code>NoExecute</code></td><td>DaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-unreachable"><code>node.kubernetes.io/unreachable</code></a></td><td><code>NoExecute</code></td><td>DaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-disk-pressure"><code>node.kubernetes.io/disk-pressure</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes with disk pressure issues.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-memory-pressure"><code>node.kubernetes.io/memory-pressure</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes with memory pressure issues.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-pid-pressure"><code>node.kubernetes.io/pid-pressure</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes with process pressure issues.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-unschedulable"><code>node.kubernetes.io/unschedulable</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes that are unschedulable.</td></tr><tr><td><a href="/docs/reference/labels-annotations-taints/#node-kubernetes-io-network-unavailable"><code>node.kubernetes.io/network-unavailable</code></a></td><td><code>NoSchedule</code></td><td><strong>Only added for DaemonSet Pods that request host networking</strong>, i.e., Pods having <code>spec.hostNetwork: true</code>. Such DaemonSet Pods can be scheduled onto nodes with unavailable network.</td></tr></tbody></table><p>You can add your own tolerations to the Pods of a DaemonSet as well, by
defining these in the Pod template of the DaemonSet.</p><p>Because the DaemonSet controller sets the
<code>node.kubernetes.io/unschedulable:NoSchedule</code> toleration automatically,
Kubernetes can run DaemonSet Pods on nodes that are marked as <em>unschedulable</em>.</p><p>If you use a DaemonSet to provide an important node-level function, such as
<a href="/docs/concepts/cluster-administration/networking/">cluster networking</a>, it is
helpful that Kubernetes places DaemonSet Pods on nodes before they are ready.
For example, without that special toleration, you could end up in a deadlock
situation where the node is not marked as ready because the network plugin is
not running there, and at the same time the network plugin is not running on
that node because the node is not yet ready.</p><h2 id="communicating-with-daemon-pods">Communicating with Daemon Pods</h2><p>Some possible patterns for communicating with Pods in a DaemonSet are:</p><ul><li><strong>Push</strong>: Pods in the DaemonSet are configured to send updates to another service, such
as a stats database. They do not have clients.</li><li><strong>NodeIP and Known Port</strong>: Pods in the DaemonSet can use a <code>hostPort</code>, so that the pods
are reachable via the node IPs.
Clients know the list of node IPs somehow, and know the port by convention.</li><li><strong>DNS</strong>: Create a <a href="/docs/concepts/services-networking/service/#headless-services">headless service</a>
with the same pod selector, and then discover DaemonSets using the <code>endpoints</code>
resource or retrieve multiple A records from DNS.</li><li><strong>Service</strong>: Create a service with the same Pod selector, and use the service to reach a
daemon on a random node. Use <a href="/docs/concepts/services-networking/service-traffic-policy/">Service Internal Traffic Policy</a>
to limit to pods on the same node.</li></ul><h2 id="updating-a-daemonset">Updating a DaemonSet</h2><p>If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete
Pods from newly not-matching nodes.</p><p>You can modify the Pods that a DaemonSet creates. However, Pods do not allow all
fields to be updated. Also, the DaemonSet controller will use the original template the next
time a node (even with the same name) is created.</p><p>You can delete a DaemonSet. If you specify <code>--cascade=orphan</code> with <code>kubectl</code>, then the Pods
will be left on the nodes. If you subsequently create a new DaemonSet with the same selector,
the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces
them according to its <code>updateStrategy</code>.</p><p>You can <a href="/docs/tasks/manage-daemon/update-daemon-set/">perform a rolling update</a> on a DaemonSet.</p><h2 id="alternatives-to-daemonset">Alternatives to DaemonSet</h2><h3 id="init-scripts">Init scripts</h3><p>It is certainly possible to run daemon processes by directly starting them on a node (e.g. using
<code>init</code>, <code>upstartd</code>, or <code>systemd</code>). This is perfectly fine. However, there are several advantages to
running such processes via a DaemonSet:</p><ul><li>Ability to monitor and manage logs for daemons in the same way as applications.</li><li>Same config language and tools (e.g. Pod templates, <code>kubectl</code>) for daemons and applications.</li><li>Running daemons in containers with resource limits increases isolation between daemons from app
containers. However, this can also be accomplished by running the daemons in a container but not in a Pod.</li></ul><h3 id="bare-pods">Bare Pods</h3><p>It is possible to create Pods directly which specify a particular node to run on. However,
a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of
node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should
use a DaemonSet rather than creating individual Pods.</p><h3 id="static-pods">Static Pods</h3><p>It is possible to create Pods by writing a file to a certain directory watched by Kubelet. These
are called <a href="/docs/tasks/configure-pod-container/static-pod/">static pods</a>.
Unlike DaemonSet, static Pods cannot be managed with kubectl
or other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful
in cluster bootstrapping cases. Also, static Pods may be deprecated in the future.</p><h3 id="deployments">Deployments</h3><p>DaemonSets are similar to <a href="/docs/concepts/workloads/controllers/deployment/">Deployments</a> in that
they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,
storage servers).</p><p>Use a Deployment for stateless services, like frontends, where scaling up and down the
number of replicas and rolling out updates are more important than controlling exactly which host
the Pod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on
all or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.</p><p>For example, <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugins</a> often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a>:<ul><li>Learn about <a href="/docs/tasks/configure-pod-container/static-pod/">static Pods</a>, which are useful for running Kubernetes
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> components.</li></ul></li><li>Find out how to use DaemonSets:<ul><li><a href="/docs/tasks/manage-daemon/update-daemon-set/">Perform a rolling update on a DaemonSet</a>.</li><li><a href="/docs/tasks/manage-daemon/rollback-daemon-set/">Perform a rollback on a DaemonSet</a>
(for example, if a roll out didn't work how you expected).</li></ul></li><li>Understand <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">how Kubernetes assigns Pods to Nodes</a>.</li><li>Learn about <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">device plugins</a> and
<a href="/docs/concepts/cluster-administration/addons/">add ons</a>, which often run as DaemonSets.</li><li><code>DaemonSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/daemon-set-v1/">DaemonSet</a>
object definition to understand the API for daemon sets.</li></ul></div></div><div><div class="td-content"><h1>Jobs</h1><div class="lead">Jobs represent one-off tasks that run to completion and then stop.</div><p>A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.
As pods successfully complete, the Job tracks the successful completions. When a specified number
of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up
the Pods it created. Suspending a Job will delete its active Pods until the Job
is resumed again.</p><p>A simple case is to create one Job object in order to reliably run one Pod to completion.
The Job object will start a new Pod if the first Pod fails or is deleted (for example
due to a node hardware failure or a node reboot).</p><p>You can also use a Job to run multiple Pods in parallel.</p><p>If you want to run a Job (either a single task, or several in parallel) on a schedule,
see <a href="/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a>.</p><h2 id="running-an-example-job">Running an example Job</h2><p>Here is an example Job config. It computes &#960; to 2000 places and prints it out.
It takes around 10s to complete.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/job.yaml"><code>controllers/job.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy controllers/job.yaml to clipboard"></div><div class="includecode" id="controllers-job-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pi<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>pi<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>perl:5.34.0<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>"perl"</span>,<span>  </span><span>"-Mbignum=bpi"</span>,<span> </span><span>"-wle"</span>,<span> </span><span>"print bpi(2000)"</span>]<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>  </span><span>backoffLimit</span>:<span> </span><span>4</span><span>
</span></span></span></code></pre></div></div></div><p>You can run the example with this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>job.batch/pi created
</code></pre><p>Check on the status of the Job with <code>kubectl</code>:</p><ul class="nav nav-tabs" id="check-status-of-job"><li class="nav-item"><a class="nav-link active" href="#check-status-of-job-0">kubectl describe job pi</a></li><li class="nav-item"><a class="nav-link" href="#check-status-of-job-1">kubectl get job pi -o yaml</a></li></ul><div class="tab-content" id="check-status-of-job"><div id="check-status-of-job-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>Name:           pi
</span></span><span><span>Namespace:      default
</span></span><span><span>Selector:       batch.kubernetes.io/controller-uid<span>=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span><span>Labels:         batch.kubernetes.io/controller-uid<span>=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span><span>                batch.kubernetes.io/job-name<span>=</span>pi
</span></span><span><span>                ...
</span></span><span><span>Annotations:    batch.kubernetes.io/job-tracking: <span>""</span>
</span></span><span><span>Parallelism:    <span>1</span>
</span></span><span><span>Completions:    <span>1</span>
</span></span><span><span>Start Time:     Mon, <span>02</span> Dec <span>2019</span> 15:20:11 +0200
</span></span><span><span>Completed At:   Mon, <span>02</span> Dec <span>2019</span> 15:21:16 +0200
</span></span><span><span>Duration:       65s
</span></span><span><span>Pods Statuses:  <span>0</span> Running / <span>1</span> Succeeded / <span>0</span> Failed
</span></span><span><span>Pod Template:
</span></span><span><span>  Labels:  batch.kubernetes.io/controller-uid<span>=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span><span>           batch.kubernetes.io/job-name<span>=</span>pi
</span></span><span><span>  Containers:
</span></span><span><span>   pi:
</span></span><span><span>    Image:      perl:5.34.0
</span></span><span><span>    Port:       &lt;none&gt;
</span></span><span><span>    Host Port:  &lt;none&gt;
</span></span><span><span>    Command:
</span></span><span><span>      perl
</span></span><span><span>      -Mbignum<span>=</span>bpi
</span></span><span><span>      -wle
</span></span><span><span>      print bpi<span>(</span>2000<span>)</span>
</span></span><span><span>    Environment:  &lt;none&gt;
</span></span><span><span>    Mounts:       &lt;none&gt;
</span></span><span><span>  Volumes:        &lt;none&gt;
</span></span><span><span>Events:
</span></span><span><span>  Type    Reason            Age   From            Message
</span></span><span><span>  ----    ------            ----  ----            -------
</span></span><span><span>  Normal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4
</span></span><span><span>  Normal  Completed         18s   job-controller  Job completed
</span></span></code></pre></div></p></div><div id="check-status-of-job-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>
</span></span><span><span>apiVersion: batch/v1
</span></span><span><span>kind: Job
</span></span><span><span>metadata:
</span></span><span><span>  annotations: batch.kubernetes.io/job-tracking: <span>""</span>
</span></span><span><span>             ...  
</span></span><span><span>  creationTimestamp: <span>"2022-11-10T17:53:53Z"</span>
</span></span><span><span>  generation: <span>1</span>
</span></span><span><span>  labels:
</span></span><span><span>    batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span><span>    batch.kubernetes.io/job-name: pi
</span></span><span><span>  name: pi
</span></span><span><span>  namespace: default
</span></span><span><span>  resourceVersion: <span>"4751"</span>
</span></span><span><span>  uid: 204fb678-040b-497f-9266-35ffa8716d14
</span></span><span><span>spec:
</span></span><span><span>  backoffLimit: <span>4</span>
</span></span><span><span>  completionMode: NonIndexed
</span></span><span><span>  completions: <span>1</span>
</span></span><span><span>  parallelism: <span>1</span>
</span></span><span><span>  selector:
</span></span><span><span>    matchLabels:
</span></span><span><span>      batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span><span>  suspend: <span>false</span>
</span></span><span><span>  template:
</span></span><span><span>    metadata:
</span></span><span><span>      creationTimestamp: null
</span></span><span><span>      labels:
</span></span><span><span>        batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span><span>        batch.kubernetes.io/job-name: pi
</span></span><span><span>    spec:
</span></span><span><span>      containers:
</span></span><span><span>      - command:
</span></span><span><span>        - perl
</span></span><span><span>        - -Mbignum<span>=</span>bpi
</span></span><span><span>        - -wle
</span></span><span><span>        - print bpi<span>(</span>2000<span>)</span>
</span></span><span><span>        image: perl:5.34.0
</span></span><span><span>        imagePullPolicy: IfNotPresent
</span></span><span><span>        name: pi
</span></span><span><span>        resources: <span>{}</span>
</span></span><span><span>        terminationMessagePath: /dev/termination-log
</span></span><span><span>        terminationMessagePolicy: File
</span></span><span><span>      dnsPolicy: ClusterFirst
</span></span><span><span>      restartPolicy: Never
</span></span><span><span>      schedulerName: default-scheduler
</span></span><span><span>      securityContext: <span>{}</span>
</span></span><span><span>      terminationGracePeriodSeconds: <span>30</span>
</span></span><span><span>status:
</span></span><span><span>  active: <span>1</span>
</span></span><span><span>  ready: <span>0</span>
</span></span><span><span>  startTime: <span>"2022-11-10T17:53:57Z"</span>
</span></span><span><span>  uncountedTerminatedPods: <span>{}</span>
</span></span></code></pre></div></p></div></div><p>To view completed Pods of a Job, use <code>kubectl get pods</code>.</p><p>To list all the Pods that belong to a Job in a machine readable form, you can use a command like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>pods</span><span>=</span><span>$(</span>kubectl get pods --selector<span>=</span>batch.kubernetes.io/job-name<span>=</span>pi --output<span>=</span><span>jsonpath</span><span>=</span><span>'{.items[*].metadata.name}'</span><span>)</span>
</span></span><span><span><span>echo</span> <span>$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>pi-5rwd7
</code></pre><p>Here, the selector is the same as the selector for the Job. The <code>--output=jsonpath</code> option specifies an expression
with the name from each Pod in the returned list.</p><p>View the standard output of one of the pods:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs <span>$pods</span>
</span></span></code></pre></div><p>Another way to view the logs of a Job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs jobs/pi
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
</code></pre><h2 id="writing-a-job-spec">Writing a Job spec</h2><p>As with all other Kubernetes config, a Job needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.</p><p>When the control plane creates new Pods for a Job, the <code>.metadata.name</code> of the
Job is part of the basis for naming those Pods. The name of a Job must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostnames. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.
Even when the name is a DNS subdomain, the name must be no longer than 63
characters.</p><p>A Job also needs a <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> section</a>.</p><h3 id="job-labels">Job Labels</h3><p>Job labels will have <code>batch.kubernetes.io/</code> prefix for <code>job-name</code> and <code>controller-uid</code>.</p><h3 id="pod-template">Pod Template</h3><p>The <code>.spec.template</code> is the only required field of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a>.
It has exactly the same schema as a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>,
except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a pod template in a Job must specify appropriate
labels (see <a href="#pod-selector">pod selector</a>) and an appropriate restart policy.</p><p>Only a <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>RestartPolicy</code></a>
equal to <code>Never</code> or <code>OnFailure</code> is allowed.</p><h3 id="pod-selector">Pod selector</h3><p>The <code>.spec.selector</code> field is optional. In almost all cases you should not specify it.
See section <a href="#specifying-your-own-pod-selector">specifying your own pod selector</a>.</p><h3 id="parallel-jobs">Parallel execution for Jobs</h3><p>There are three main types of task suitable to run as a Job:</p><ol><li>Non-parallel Jobs<ul><li>normally, only one Pod is started, unless the Pod fails.</li><li>the Job is complete as soon as its Pod terminates successfully.</li></ul></li><li>Parallel Jobs with a <em>fixed completion count</em>:<ul><li>specify a non-zero positive value for <code>.spec.completions</code>.</li><li>the Job represents the overall task, and is complete when there are <code>.spec.completions</code> successful Pods.</li><li>when using <code>.spec.completionMode="Indexed"</code>, each Pod gets a different index in the range 0 to <code>.spec.completions-1</code>.</li></ul></li><li>Parallel Jobs with a <em>work queue</em>:<ul><li>do not specify <code>.spec.completions</code>, default to <code>.spec.parallelism</code>.</li><li>the Pods must coordinate amongst themselves or an external service to determine
what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.</li><li>each Pod is independently capable of determining whether or not all its peers are done,
and thus that the entire Job is done.</li><li>when <em>any</em> Pod from the Job terminates with success, no new Pods are created.</li><li>once at least one Pod has terminated with success and all Pods are terminated,
then the Job is completed with success.</li><li>once any Pod has exited with success, no other Pod should still be doing any work
for this task or writing any output. They should all be in the process of exiting.</li></ul></li></ol><p>For a <em>non-parallel</em> Job, you can leave both <code>.spec.completions</code> and <code>.spec.parallelism</code> unset.
When both are unset, both are defaulted to 1.</p><p>For a <em>fixed completion count</em> Job, you should set <code>.spec.completions</code> to the number of completions needed.
You can set <code>.spec.parallelism</code>, or leave it unset and it will default to 1.</p><p>For a <em>work queue</em> Job, you must leave <code>.spec.completions</code> unset, and set <code>.spec.parallelism</code> to
a non-negative integer.</p><p>For more information about how to make use of the different types of job,
see the <a href="#job-patterns">job patterns</a> section.</p><h4 id="controlling-parallelism">Controlling parallelism</h4><p>The requested parallelism (<code>.spec.parallelism</code>) can be set to any non-negative value.
If it is unspecified, it defaults to 1.
If it is specified as 0, then the Job is effectively paused until it is increased.</p><p>Actual parallelism (number of pods running at any instant) may be more or less than requested
parallelism, for a variety of reasons:</p><ul><li>For <em>fixed completion count</em> Jobs, the actual number of pods running in parallel will not exceed the number of
remaining completions. Higher values of <code>.spec.parallelism</code> are effectively ignored.</li><li>For <em>work queue</em> Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.</li><li>If the Job <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">Controller</a> has not had time to react.</li><li>If the Job controller failed to create Pods for any reason (lack of <code>ResourceQuota</code>, lack of permission, etc.),
then there may be fewer pods than requested.</li><li>The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.</li><li>When a Pod is gracefully shut down, it takes time to stop.</li></ul><h3 id="completion-mode">Completion mode</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>Jobs with <em>fixed completion count</em> - that is, jobs that have non null
<code>.spec.completions</code> - can have a completion mode that is specified in <code>.spec.completionMode</code>:</p><ul><li><p><code>NonIndexed</code> (default): the Job is considered complete when there have been
<code>.spec.completions</code> successfully completed Pods. In other words, each Pod
completion is homologous to each other. Note that Jobs that have null
<code>.spec.completions</code> are implicitly <code>NonIndexed</code>.</p></li><li><p><code>Indexed</code>: the Pods of a Job get an associated completion index from 0 to
<code>.spec.completions-1</code>. The index is available through four mechanisms:</p><ul><li>The Pod annotation <code>batch.kubernetes.io/job-completion-index</code>.</li><li>The Pod label <code>batch.kubernetes.io/job-completion-index</code> (for v1.28 and later). Note
the feature gate <code>PodIndexLabel</code> must be enabled to use this label, and it is enabled
by default.</li><li>As part of the Pod hostname, following the pattern <code>$(job-name)-$(index)</code>.
When you use an Indexed Job in combination with a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a>, Pods within the Job can use
the deterministic hostnames to address each other via DNS. For more information about
how to configure this, see <a href="/docs/tasks/job/job-with-pod-to-pod-communication/">Job with Pod-to-Pod Communication</a>.</li><li>From the containerized task, in the environment variable <code>JOB_COMPLETION_INDEX</code>.</li></ul><p>The Job is considered complete when there is one successfully completed Pod
for each index. For more information about how to use this mode, see
<a href="/docs/tasks/job/indexed-parallel-processing-static/">Indexed Job for Parallel Processing with Static Work Assignment</a>.</p></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Although rare, more than one Pod could be started for the same index (due to various reasons such as node failures,
kubelet restarts, or Pod evictions). In this case, only the first Pod that completes successfully will
count towards the completion count and update the status of the Job. The other Pods that are running
or completed for the same index will be deleted by the Job controller once they are detected.</div><h2 id="handling-pod-and-container-failures">Handling Pod and container failures</h2><p>A container in a Pod may fail for a number of reasons, such as because the process in it exited with
a non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this
happens, and the <code>.spec.template.spec.restartPolicy = "OnFailure"</code>, then the Pod stays
on the node, but the container is re-run. Therefore, your program needs to handle the case when it is
restarted locally, or else specify <code>.spec.template.spec.restartPolicy = "Never"</code>.
See <a href="/docs/concepts/workloads/pods/pod-lifecycle/#example-states">pod lifecycle</a> for more information on <code>restartPolicy</code>.</p><p>An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node
(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the
<code>.spec.template.spec.restartPolicy = "Never"</code>. When a Pod fails, then the Job controller
starts a new Pod. This means that your application needs to handle the case when it is restarted in a new
pod. In particular, it needs to handle temporary files, locks, incomplete output and the like
caused by previous runs.</p><p>By default, each pod failure is counted towards the <code>.spec.backoffLimit</code> limit,
see <a href="#pod-backoff-failure-policy">pod backoff failure policy</a>. However, you can
customize handling of pod failures by setting the Job's <a href="#pod-failure-policy">pod failure policy</a>.</p><p>Additionally, you can choose to count the pod failures independently for each
index of an <a href="#completion-mode">Indexed</a> Job by setting the <code>.spec.backoffLimitPerIndex</code> field
(for more information, see <a href="#backoff-limit-per-index">backoff limit per index</a>).</p><p>Note that even if you specify <code>.spec.parallelism = 1</code> and <code>.spec.completions = 1</code> and
<code>.spec.template.spec.restartPolicy = "Never"</code>, the same program may
sometimes be started twice.</p><p>If you do specify <code>.spec.parallelism</code> and <code>.spec.completions</code> both greater than 1, then there may be
multiple pods running at once. Therefore, your pods must also be tolerant of concurrency.</p><p>If you specify the <code>.spec.podFailurePolicy</code> field, the Job controller does not consider a terminating
Pod (a pod that has a <code>.metadata.deletionTimestamp</code> field set) as a failure until that Pod is
terminal (its <code>.status.phase</code> is <code>Failed</code> or <code>Succeeded</code>). However, the Job controller
creates a replacement Pod as soon as the termination becomes apparent. Once the
pod terminates, the Job controller evaluates <code>.backoffLimit</code> and <code>.podFailurePolicy</code>
for the relevant Job, taking this now-terminated Pod into consideration.</p><p>If either of these requirements is not satisfied, the Job controller counts
a terminating Pod as an immediate failure, even if that Pod later terminates
with <code>phase: "Succeeded"</code>.</p><h3 id="pod-backoff-failure-policy">Pod backoff failure policy</h3><p>There are situations where you want to fail a Job after some amount of retries
due to a logical error in configuration etc.
To do so, set <code>.spec.backoffLimit</code> to specify the number of retries before
considering a Job as failed.</p><p>The <code>.spec.backoffLimit</code> is set by default to 6, unless the
<a href="#backoff-limit-per-index">backoff limit per index</a> (only Indexed Job) is specified.
When <code>.spec.backoffLimitPerIndex</code> is specified, then <code>.spec.backoffLimit</code> defaults
to 2147483647 (MaxInt32).</p><p>Failed Pods associated with the Job are recreated by the Job controller with an
exponential back-off delay (10s, 20s, 40s ...) capped at six minutes.</p><p>The number of retries is calculated in two ways:</p><ul><li>The number of Pods with <code>.status.phase = "Failed"</code>.</li><li>When using <code>restartPolicy = "OnFailure"</code>, the number of retries in all the
containers of Pods with <code>.status.phase</code> equal to <code>Pending</code> or <code>Running</code>.</li></ul><p>If either of the calculations reaches the <code>.spec.backoffLimit</code>, the Job is
considered failed.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If your Job has <code>restartPolicy = "OnFailure"</code>, keep in mind that your Pod running the job
will be terminated once the job backoff limit has been reached. This can make debugging
the Job's executable more difficult. We suggest setting
<code>restartPolicy = "Never"</code> when debugging the Job or using a logging system to ensure output
from failed Jobs is not lost inadvertently.</div><h3 id="backoff-limit-per-index">Backoff limit per index</h3><div class="feature-state-notice feature-stable" title="Feature Gate: JobBackoffLimitPerIndex"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>When you run an <a href="#completion-mode">indexed</a> Job, you can choose to handle retries
for pod failures independently for each index. To do so, set the
<code>.spec.backoffLimitPerIndex</code> to specify the maximal number of pod failures
per index.</p><p>When the per-index backoff limit is exceeded for an index, Kubernetes considers the index as failed and adds it to the
<code>.status.failedIndexes</code> field. The succeeded indexes, those with a successfully
executed pods, are recorded in the <code>.status.completedIndexes</code> field, regardless of whether you set
the <code>backoffLimitPerIndex</code> field.</p><p>Note that a failing index does not interrupt execution of other indexes.
Once all indexes finish for a Job where you specified a backoff limit per index,
if at least one of those indexes did fail, the Job controller marks the overall
Job as failed, by setting the Failed condition in the status. The Job gets
marked as failed even if some, potentially nearly all, of the indexes were
processed successfully.</p><p>You can additionally limit the maximal number of indexes marked failed by
setting the <code>.spec.maxFailedIndexes</code> field.
When the number of failed indexes exceeds the <code>maxFailedIndexes</code> field, the
Job controller triggers termination of all remaining running Pods for that Job.
Once all pods are terminated, the entire Job is marked failed by the Job
controller, by setting the Failed condition in the Job status.</p><p>Here is an example manifest for a Job that defines a <code>backoffLimitPerIndex</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-backoff-limit-per-index-example.yaml"><code>/controllers/job-backoff-limit-per-index-example.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy /controllers/job-backoff-limit-per-index-example.yaml to clipboard"></div><div class="includecode" id="controllers-job-backoff-limit-per-index-example-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>job-backoff-limit-per-index-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>completionMode</span>:<span> </span>Indexed <span> </span><span># required for the feature</span><span>
</span></span></span><span><span><span>  </span><span>backoffLimitPerIndex</span>:<span> </span><span>1</span><span>  </span><span># maximal number of failures per index</span><span>
</span></span></span><span><span><span>  </span><span>maxFailedIndexes</span>:<span> </span><span>5</span><span>      </span><span># maximal number of failed indexes before terminating the Job execution</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span> </span><span># required for the feature</span><span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>example<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>python<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>           </span><span># The jobs fails as there is at least one failed index</span><span>
</span></span></span><span><span><span>                           </span><span># (all even indexes fail in here), yet all indexes</span><span>
</span></span></span><span><span><span>                           </span><span># are executed as maxFailedIndexes is not exceeded.</span><span>
</span></span></span><span><span><span>        </span>- python3<span>
</span></span></span><span><span><span>        </span>- -c<span>
</span></span></span><span><span><span>        </span>- |<span>
</span></span></span><span><span><span>          import os, sys
</span></span></span><span><span><span>          print("Hello world")
</span></span></span><span><span><span>          if int(os.environ.get("JOB_COMPLETION_INDEX")) % 2 == 0:
</span></span></span><span><span><span>            sys.exit(1)</span><span>          
</span></span></span></code></pre></div></div></div><p>In the example above, the Job controller allows for one restart for each
of the indexes. When the total number of failed indexes exceeds 5, then
the entire Job is terminated.</p><p>Once the job is finished, the Job status looks as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-sh"><span><span>kubectl get -o yaml job job-backoff-limit-per-index-example
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span>status</span>:<span>
</span></span></span><span><span><span>    </span><span>completedIndexes</span>:<span> </span><span>1</span>,<span>3</span>,<span>5</span>,<span>7</span>,<span>9</span><span>
</span></span></span><span><span><span>    </span><span>failedIndexes</span>:<span> </span><span>0</span>,<span>2</span>,<span>4</span>,<span>6</span>,<span>8</span><span>
</span></span></span><span><span><span>    </span><span>succeeded</span>:<span> </span><span>5</span><span>          </span><span># 1 succeeded pod for each of 5 succeeded indexes</span><span>
</span></span></span><span><span><span>    </span><span>failed</span>:<span> </span><span>10</span><span>            </span><span># 2 failed pods (1 retry) for each of 5 failed indexes</span><span>
</span></span></span><span><span><span>    </span><span>conditions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>message</span>:<span> </span>Job has failed indexes<span>
</span></span></span><span><span><span>      </span><span>reason</span>:<span> </span>FailedIndexes<span>
</span></span></span><span><span><span>      </span><span>status</span>:<span> </span><span>"True"</span><span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>FailureTarget<span>
</span></span></span><span><span><span>    </span>- <span>message</span>:<span> </span>Job has failed indexes<span>
</span></span></span><span><span><span>      </span><span>reason</span>:<span> </span>FailedIndexes<span>
</span></span></span><span><span><span>      </span><span>status</span>:<span> </span><span>"True"</span><span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>Failed<span>
</span></span></span></code></pre></div><p>The Job controller adds the <code>FailureTarget</code> Job condition to trigger
<a href="#job-termination-and-cleanup">Job termination and cleanup</a>. When all of the
Job Pods are terminated, the Job controller adds the <code>Failed</code> condition
with the same values for <code>reason</code> and <code>message</code> as the <code>FailureTarget</code> Job
condition. For details, see <a href="#termination-of-job-pods">Termination of Job Pods</a>.</p><p>Additionally, you may want to use the per-index backoff along with a
<a href="#pod-failure-policy">pod failure policy</a>. When using
per-index backoff, there is a new <code>FailIndex</code> action available which allows you to
avoid unnecessary retries within an index.</p><h3 id="pod-failure-policy">Pod failure policy</h3><div class="feature-state-notice feature-stable" title="Feature Gate: JobPodFailurePolicy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p>A Pod failure policy, defined with the <code>.spec.podFailurePolicy</code> field, enables
your cluster to handle Pod failures based on the container exit codes and the
Pod conditions.</p><p>In some situations, you may want to have a better control when handling Pod
failures than the control provided by the <a href="#pod-backoff-failure-policy">Pod backoff failure policy</a>,
which is based on the Job's <code>.spec.backoffLimit</code>. These are some examples of use cases:</p><ul><li>To optimize costs of running workloads by avoiding unnecessary Pod restarts,
you can terminate a Job as soon as one of its Pods fails with an exit code
indicating a software bug.</li><li>To guarantee that your Job finishes even if there are disruptions, you can
ignore Pod failures caused by disruptions (such as <a class="glossary-tooltip" title="Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node." href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption" target="_blank">preemption</a>,
<a class="glossary-tooltip" title="API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination." href="/docs/concepts/scheduling-eviction/api-eviction/" target="_blank">API-initiated eviction</a>
or <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">taint</a>-based eviction) so
that they don't count towards the <code>.spec.backoffLimit</code> limit of retries.</li></ul><p>You can configure a Pod failure policy, in the <code>.spec.podFailurePolicy</code> field,
to meet the above use cases. This policy can handle Pod failures based on the
container exit codes and the Pod conditions.</p><p>Here is a manifest for a Job that defines a <code>podFailurePolicy</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-pod-failure-policy-example.yaml"><code>/controllers/job-pod-failure-policy-example.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy /controllers/job-pod-failure-policy-example.yaml to clipboard"></div><div class="includecode" id="controllers-job-pod-failure-policy-example-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>job-pod-failure-policy-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>12</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>main<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>docker.io/library/bash:5<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>"bash"</span>]<span>        </span><span># example command simulating a bug which triggers the FailJob action</span><span>
</span></span></span><span><span><span>        </span><span>args</span>:<span>
</span></span></span><span><span><span>        </span>- -c<span>
</span></span></span><span><span><span>        </span>- echo "Hello world!" &amp;&amp; sleep 5 &amp;&amp; exit 42<span>
</span></span></span><span><span><span>  </span><span>backoffLimit</span>:<span> </span><span>6</span><span>
</span></span></span><span><span><span>  </span><span>podFailurePolicy</span>:<span>
</span></span></span><span><span><span>    </span><span>rules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>action</span>:<span> </span>FailJob<span>
</span></span></span><span><span><span>      </span><span>onExitCodes</span>:<span>
</span></span></span><span><span><span>        </span><span>containerName</span>:<span> </span>main     <span> </span><span># optional</span><span>
</span></span></span><span><span><span>        </span><span>operator: In             # one of</span>:<span> </span>In, NotIn<span>
</span></span></span><span><span><span>        </span><span>values</span>:<span> </span>[<span>42</span>]<span>
</span></span></span><span><span><span>    </span>- <span>action: Ignore             # one of</span>:<span> </span>Ignore, FailJob, Count<span>
</span></span></span><span><span><span>      </span><span>onPodConditions</span>:<span>
</span></span></span><span><span><span>      </span>- <span>type</span>:<span> </span>DisruptionTarget  <span> </span><span># indicates Pod disruption</span><span>
</span></span></span></code></pre></div></div></div><p>In the example above, the first rule of the Pod failure policy specifies that
the Job should be marked failed if the <code>main</code> container fails with the 42 exit
code. The following are the rules for the <code>main</code> container specifically:</p><ul><li>an exit code of 0 means that the container succeeded</li><li>an exit code of 42 means that the <strong>entire Job</strong> failed</li><li>any other exit code represents that the container failed, and hence the entire
Pod. The Pod will be re-created if the total number of restarts is
below <code>backoffLimit</code>. If the <code>backoffLimit</code> is reached the <strong>entire Job</strong> failed.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Because the Pod template specifies a <code>restartPolicy: Never</code>,
the kubelet does not restart the <code>main</code> container in that particular Pod.</div><p>The second rule of the Pod failure policy, specifying the <code>Ignore</code> action for
failed Pods with condition <code>DisruptionTarget</code> excludes Pod disruptions from
being counted towards the <code>.spec.backoffLimit</code> limit of retries.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If the Job failed, either by the Pod failure policy or Pod backoff
failure policy, and the Job is running multiple Pods, Kubernetes terminates all
the Pods in that Job that are still Pending or Running.</div><p>These are some requirements and semantics of the API:</p><ul><li>if you want to use a <code>.spec.podFailurePolicy</code> field for a Job, you must
also define that Job's pod template with <code>.spec.restartPolicy</code> set to <code>Never</code>.</li><li>the Pod failure policy rules you specify under <code>spec.podFailurePolicy.rules</code>
are evaluated in order. Once a rule matches a Pod failure, the remaining rules
are ignored. When no rule matches the Pod failure, the default
handling applies.</li><li>you may want to restrict a rule to a specific container by specifying its name
in<code>spec.podFailurePolicy.rules[*].onExitCodes.containerName</code>. When not specified the rule
applies to all containers. When specified, it should match one the container
or <code>initContainer</code> names in the Pod template.</li><li>you may specify the action taken when a Pod failure policy is matched by
<code>spec.podFailurePolicy.rules[*].action</code>. Possible values are:<ul><li><code>FailJob</code>: use to indicate that the Pod's job should be marked as Failed and
all running Pods should be terminated.</li><li><code>Ignore</code>: use to indicate that the counter towards the <code>.spec.backoffLimit</code>
should not be incremented and a replacement Pod should be created.</li><li><code>Count</code>: use to indicate that the Pod should be handled in the default way.
The counter towards the <code>.spec.backoffLimit</code> should be incremented.</li><li><code>FailIndex</code>: use this action along with <a href="#backoff-limit-per-index">backoff limit per index</a>
to avoid unnecessary retries within the index of a failed pod.</li></ul></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When you use a <code>podFailurePolicy</code>, the job controller only matches Pods in the
<code>Failed</code> phase. Pods with a deletion timestamp that are not in a terminal phase
(<code>Failed</code> or <code>Succeeded</code>) are considered still terminating. This implies that
terminating pods retain a <a href="#job-tracking-with-finalizers">tracking finalizer</a>
until they reach a terminal phase.
Since Kubernetes 1.27, Kubelet transitions deleted pods to a terminal phase
(see: <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase">Pod Phase</a>). This
ensures that deleted pods have their finalizers removed by the Job controller.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Starting with Kubernetes v1.28, when Pod failure policy is used, the Job controller recreates
terminating Pods only once these Pods reach the terminal <code>Failed</code> phase. This behavior is similar
to <code>podReplacementPolicy: Failed</code>. For more information, see <a href="#pod-replacement-policy">Pod replacement policy</a>.</div><p>When you use the <code>podFailurePolicy</code>, and the Job fails due to the pod
matching the rule with the <code>FailJob</code> action, then the Job controller triggers
the Job termination process by adding the <code>FailureTarget</code> condition.
For more details, see <a href="#job-termination-and-cleanup">Job termination and cleanup</a>.</p><h2 id="success-policy">Success policy</h2><p>When creating an Indexed Job, you can define when a Job can be declared as succeeded using a <code>.spec.successPolicy</code>,
based on the pods that succeeded.</p><p>By default, a Job succeeds when the number of succeeded Pods equals <code>.spec.completions</code>.
These are some situations where you might want additional control for declaring a Job succeeded:</p><ul><li>When running simulations with different parameters,
you might not need all the simulations to succeed for the overall Job to be successful.</li><li>When following a leader-worker pattern, only the success of the leader determines the success or
failure of a Job. Examples of this are frameworks like MPI and PyTorch etc.</li></ul><p>You can configure a success policy, in the <code>.spec.successPolicy</code> field,
to meet the above use cases. This policy can handle Job success based on the
succeeded pods. After the Job meets the success policy, the job controller terminates the lingering Pods.
A success policy is defined by rules. Each rule can take one of the following forms:</p><ul><li>When you specify the <code>succeededIndexes</code> only,
once all indexes specified in the <code>succeededIndexes</code> succeed, the job controller marks the Job as succeeded.
The <code>succeededIndexes</code> must be a list of intervals between 0 and <code>.spec.completions-1</code>.</li><li>When you specify the <code>succeededCount</code> only,
once the number of succeeded indexes reaches the <code>succeededCount</code>, the job controller marks the Job as succeeded.</li><li>When you specify both <code>succeededIndexes</code> and <code>succeededCount</code>,
once the number of succeeded indexes from the subset of indexes specified in the <code>succeededIndexes</code> reaches the <code>succeededCount</code>,
the job controller marks the Job as succeeded.</li></ul><p>Note that when you specify multiple rules in the <code>.spec.successPolicy.rules</code>,
the job controller evaluates the rules in order. Once the Job meets a rule, the job controller ignores remaining rules.</p><p>Here is a manifest for a Job with <code>successPolicy</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-success-policy.yaml"><code>/controllers/job-success-policy.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy /controllers/job-success-policy.yaml to clipboard"></div><div class="includecode" id="controllers-job-success-policy-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>job-success<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span><span>completionMode</span>:<span> </span>Indexed<span> </span><span># Required for the success policy</span><span>
</span></span></span><span><span><span>  </span><span>successPolicy</span>:<span>
</span></span></span><span><span><span>    </span><span>rules</span>:<span>
</span></span></span><span><span><span>      </span>- <span>succeededIndexes</span>:<span> </span><span>0</span>,<span>2-3</span><span>
</span></span></span><span><span><span>        </span><span>succeededCount</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>main<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>python<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>          </span><span># Provided that at least one of the Pods with 0, 2, and 3 indexes has succeeded,</span><span>
</span></span></span><span><span><span>                          </span><span># the overall Job is a success.</span><span>
</span></span></span><span><span><span>          </span>- python3<span>
</span></span></span><span><span><span>          </span>- -c<span>
</span></span></span><span><span><span>          </span>- |<span>
</span></span></span><span><span><span>            import os, sys
</span></span></span><span><span><span>            if os.environ.get("JOB_COMPLETION_INDEX") == "2":
</span></span></span><span><span><span>              sys.exit(0)
</span></span></span><span><span><span>            else:
</span></span></span><span><span><span>              sys.exit(1)</span><span>            
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div></div></div><p>In the example above, both <code>succeededIndexes</code> and <code>succeededCount</code> have been specified.
Therefore, the job controller will mark the Job as succeeded and terminate the lingering Pods
when either of the specified indexes, 0, 2, or 3, succeed.
The Job that meets the success policy gets the <code>SuccessCriteriaMet</code> condition with a <code>SuccessPolicy</code> reason.
After the removal of the lingering Pods is issued, the Job gets the <code>Complete</code> condition.</p><p>Note that the <code>succeededIndexes</code> is represented as intervals separated by a hyphen.
The number are listed in represented by the first and last element of the series, separated by a hyphen.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When you specify both a success policy and some terminating policies such as <code>.spec.backoffLimit</code> and <code>.spec.podFailurePolicy</code>,
once the Job meets either policy, the job controller respects the terminating policy and ignores the success policy.</div><h2 id="job-termination-and-cleanup">Job termination and cleanup</h2><p>When a Job completes, no more Pods are created, but the Pods are <a href="#pod-backoff-failure-policy">usually</a> not deleted either.
Keeping them around allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.
The job object also remains after it is completed so that you can view its status. It is up to the user to delete
old jobs after noting their status. Delete the job with <code>kubectl</code> (e.g. <code>kubectl delete jobs/pi</code> or <code>kubectl delete -f ./job.yaml</code>).
When you delete the job using <code>kubectl</code>, all the pods it created are deleted too.</p><p>By default, a Job will run uninterrupted unless a Pod fails (<code>restartPolicy=Never</code>)
or a Container exits in error (<code>restartPolicy=OnFailure</code>), at which point the Job defers to the
<code>.spec.backoffLimit</code> described above. Once <code>.spec.backoffLimit</code> has been reached the Job will
be marked as failed and any running Pods will be terminated.</p><p>Another way to terminate a Job is by setting an active deadline.
Do this by setting the <code>.spec.activeDeadlineSeconds</code> field of the Job to a number of seconds.
The <code>activeDeadlineSeconds</code> applies to the duration of the job, no matter how many Pods are created.
Once a Job reaches <code>activeDeadlineSeconds</code>, all of its running Pods are terminated and the Job status
will become <code>type: Failed</code> with <code>reason: DeadlineExceeded</code>.</p><p>Note that a Job's <code>.spec.activeDeadlineSeconds</code> takes precedence over its <code>.spec.backoffLimit</code>.
Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once
it reaches the time limit specified by <code>activeDeadlineSeconds</code>, even if the <code>backoffLimit</code> is not yet reached.</p><p>Example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pi-with-timeout<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>backoffLimit</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>activeDeadlineSeconds</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>pi<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>perl:5.34.0<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>"perl"</span>,<span> </span><span>"-Mbignum=bpi"</span>,<span> </span><span>"-wle"</span>,<span> </span><span>"print bpi(2000)"</span>]<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div><p>Note that both the Job spec and the <a href="/docs/concepts/workloads/pods/init-containers/#detailed-behavior">Pod template spec</a>
within the Job have an <code>activeDeadlineSeconds</code> field. Ensure that you set this field at the proper level.</p><p>Keep in mind that the <code>restartPolicy</code> applies to the Pod, and not to the Job itself:
there is no automatic Job restart once the Job status is <code>type: Failed</code>.
That is, the Job termination mechanisms activated with <code>.spec.activeDeadlineSeconds</code>
and <code>.spec.backoffLimit</code> result in a permanent Job failure that requires manual intervention to resolve.</p><h3 id="terminal-job-conditions">Terminal Job conditions</h3><p>A Job has two possible terminal states, each of which has a corresponding Job
condition:</p><ul><li>Succeeded: Job condition <code>Complete</code></li><li>Failed: Job condition <code>Failed</code></li></ul><p>Jobs fail for the following reasons:</p><ul><li>The number of Pod failures exceeded the specified <code>.spec.backoffLimit</code> in the Job
specification. For details, see <a href="#pod-backoff-failure-policy">Pod backoff failure policy</a>.</li><li>The Job runtime exceeded the specified <code>.spec.activeDeadlineSeconds</code></li><li>An indexed Job that used <code>.spec.backoffLimitPerIndex</code> has failed indexes.
For details, see <a href="#backoff-limit-per-index">Backoff limit per index</a>.</li><li>The number of failed indexes in the Job exceeded the specified
<code>spec.maxFailedIndexes</code>. For details, see <a href="#backoff-limit-per-index">Backoff limit per index</a></li><li>A failed Pod matches a rule in <code>.spec.podFailurePolicy</code> that has the <code>FailJob</code>
action. For details about how Pod failure policy rules might affect failure
evaluation, see <a href="#pod-failure-policy">Pod failure policy</a>.</li></ul><p>Jobs succeed for the following reasons:</p><ul><li>The number of succeeded Pods reached the specified <code>.spec.completions</code></li><li>The criteria specified in <code>.spec.successPolicy</code> are met. For details, see
<a href="#success-policy">Success policy</a>.</li></ul><p>In Kubernetes v1.31 and later the Job controller delays the addition of the
terminal conditions,<code>Failed</code> or <code>Complete</code>, until all of the Job Pods are terminated.</p><p>In Kubernetes v1.30 and earlier, the Job controller added the <code>Complete</code> or the
<code>Failed</code> Job terminal conditions as soon as the Job termination process was
triggered and all Pod finalizers were removed. However, some Pods would still
be running or terminating at the moment that the terminal condition was added.</p><p>In Kubernetes v1.31 and later, the controller only adds the Job terminal conditions
<em>after</em> all of the Pods are terminated. You can control this behavior by using the
<code>JobManagedBy</code> and the <code>JobPodReplacementPolicy</code> (both enabled by default)
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gates</a>.</p><h3 id="termination-of-job-pods">Termination of Job pods</h3><p>The Job controller adds the <code>FailureTarget</code> condition or the <code>SuccessCriteriaMet</code>
condition to the Job to trigger Pod termination after a Job meets either the
success or failure criteria.</p><p>Factors like <code>terminationGracePeriodSeconds</code> might increase the amount of time
from the moment that the Job controller adds the <code>FailureTarget</code> condition or the
<code>SuccessCriteriaMet</code> condition to the moment that all of the Job Pods terminate
and the Job controller adds a <a href="#terminal-job-conditions">terminal condition</a>
(<code>Failed</code> or <code>Complete</code>).</p><p>You can use the <code>FailureTarget</code> or the <code>SuccessCriteriaMet</code> condition to evaluate
whether the Job has failed or succeeded without having to wait for the controller
to add a terminal condition.</p><p>For example, you might want to decide when to create a replacement Job
that replaces a failed Job. If you replace the failed Job when the <code>FailureTarget</code>
condition appears, your replacement Job runs sooner, but could result in Pods
from the failed and the replacement Job running at the same time, using
extra compute resources.</p><p>Alternatively, if your cluster has limited resource capacity, you could choose to
wait until the <code>Failed</code> condition appears on the Job, which would delay your
replacement Job but would ensure that you conserve resources by waiting
until all of the failed Pods are removed.</p><h2 id="clean-up-finished-jobs-automatically">Clean up finished jobs automatically</h2><p>Finished Jobs are usually no longer needed in the system. Keeping them around in
the system will put pressure on the API server. If the Jobs are managed directly
by a higher level controller, such as
<a href="/docs/concepts/workloads/controllers/cron-jobs/">CronJobs</a>, the Jobs can be
cleaned up by CronJobs based on the specified capacity-based cleanup policy.</p><h3 id="ttl-mechanism-for-finished-jobs">TTL mechanism for finished Jobs</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>Another way to clean up finished Jobs (either <code>Complete</code> or <code>Failed</code>)
automatically is to use a TTL mechanism provided by a
<a href="/docs/concepts/workloads/controllers/ttlafterfinished/">TTL controller</a> for
finished resources, by specifying the <code>.spec.ttlSecondsAfterFinished</code> field of
the Job.</p><p>When the TTL controller cleans up the Job, it will delete the Job cascadingly,
i.e. delete its dependent objects, such as Pods, together with the Job. Note
that when the Job is deleted, its lifecycle guarantees, such as finalizers, will
be honored.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pi-with-ttl<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ttlSecondsAfterFinished</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>pi<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>perl:5.34.0<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span> </span>[<span>"perl"</span>,<span> </span><span>"-Mbignum=bpi"</span>,<span> </span><span>"-wle"</span>,<span> </span><span>"print bpi(2000)"</span>]<span>
</span></span></span><span><span><span>      </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span></code></pre></div><p>The Job <code>pi-with-ttl</code> will be eligible to be automatically deleted, <code>100</code>
seconds after it finishes.</p><p>If the field is set to <code>0</code>, the Job will be eligible to be automatically deleted
immediately after it finishes. If the field is unset, this Job won't be cleaned
up by the TTL controller after it finishes.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>It is recommended to set <code>ttlSecondsAfterFinished</code> field because unmanaged jobs
(Jobs that you created directly, and not indirectly through other workload APIs
such as CronJob) have a default deletion
policy of <code>orphanDependents</code> causing Pods created by an unmanaged Job to be left around
after that Job is fully deleted.
Even though the <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> eventually
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection">garbage collects</a>
the Pods from a deleted Job after they either fail or complete, sometimes those
lingering pods may cause cluster performance degradation or in worst case cause the
cluster to go offline due to this degradation.</p><p>You can use <a href="/docs/concepts/policy/limit-range/">LimitRanges</a> and
<a href="/docs/concepts/policy/resource-quotas/">ResourceQuotas</a> to place a
cap on the amount of resources that a particular namespace can
consume.</p></div><h2 id="job-patterns">Job patterns</h2><p>The Job object can be used to process a set of independent but related <em>work items</em>.
These might be emails to be sent, frames to be rendered, files to be transcoded,
ranges of keys in a NoSQL database to scan, and so on.</p><p>In a complex system, there may be multiple different sets of work items. Here we are just
considering one set of work items that the user wants to manage together &#8212; a <em>batch job</em>.</p><p>There are several different patterns for parallel computation, each with strengths and weaknesses.
The tradeoffs are:</p><ul><li>One Job object for each work item, versus a single Job object for all work items.
One Job per work item creates some overhead for the user and for the system to manage
large numbers of Job objects.
A single Job for all work items is better for large numbers of items.</li><li>Number of Pods created equals number of work items, versus each Pod can process multiple work items.
When the number of Pods equals the number of work items, the Pods typically
requires less modification to existing code and containers. Having each Pod
process multiple work items is better for large numbers of items.</li><li>Several approaches use a work queue. This requires running a queue service,
and modifications to the existing program or container to make it use the work queue.
Other approaches are easier to adapt to an existing containerised application.</li><li>When the Job is associated with a
<a href="/docs/concepts/services-networking/service/#headless-services">headless Service</a>,
you can enable the Pods within a Job to communicate with each other to
collaborate in a computation.</li></ul><p>The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.
The pattern names are also links to examples and more detailed description.</p><table><thead><tr><th>Pattern</th><th>Single Job object</th><th>Fewer pods than work items?</th><th>Use app unmodified?</th></tr></thead><tbody><tr><td><a href="/docs/tasks/job/coarse-parallel-processing-work-queue/">Queue with Pod Per Work Item</a></td><td>&#10003;</td><td></td><td>sometimes</td></tr><tr><td><a href="/docs/tasks/job/fine-parallel-processing-work-queue/">Queue with Variable Pod Count</a></td><td>&#10003;</td><td>&#10003;</td><td></td></tr><tr><td><a href="/docs/tasks/job/indexed-parallel-processing-static/">Indexed Job with Static Work Assignment</a></td><td>&#10003;</td><td></td><td>&#10003;</td></tr><tr><td><a href="/docs/tasks/job/job-with-pod-to-pod-communication/">Job with Pod-to-Pod Communication</a></td><td>&#10003;</td><td>sometimes</td><td>sometimes</td></tr><tr><td><a href="/docs/tasks/job/parallel-processing-expansion/">Job Template Expansion</a></td><td></td><td></td><td>&#10003;</td></tr></tbody></table><p>When you specify completions with <code>.spec.completions</code>, each Pod created by the Job controller
has an identical <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>spec</code></a>.
This means that all pods for a task will have the same command line and the same
image, the same volumes, and (almost) the same environment variables. These patterns
are different ways to arrange for pods to work on different things.</p><p>This table shows the required settings for <code>.spec.parallelism</code> and <code>.spec.completions</code> for each of the patterns.
Here, <code>W</code> is the number of work items.</p><table><thead><tr><th>Pattern</th><th><code>.spec.completions</code></th><th><code>.spec.parallelism</code></th></tr></thead><tbody><tr><td><a href="/docs/tasks/job/coarse-parallel-processing-work-queue/">Queue with Pod Per Work Item</a></td><td>W</td><td>any</td></tr><tr><td><a href="/docs/tasks/job/fine-parallel-processing-work-queue/">Queue with Variable Pod Count</a></td><td>null</td><td>any</td></tr><tr><td><a href="/docs/tasks/job/indexed-parallel-processing-static/">Indexed Job with Static Work Assignment</a></td><td>W</td><td>any</td></tr><tr><td><a href="/docs/tasks/job/job-with-pod-to-pod-communication/">Job with Pod-to-Pod Communication</a></td><td>W</td><td>W</td></tr><tr><td><a href="/docs/tasks/job/parallel-processing-expansion/">Job Template Expansion</a></td><td>1</td><td>should be 1</td></tr></tbody></table><h2 id="advanced-usage">Advanced usage</h2><h3 id="suspending-a-job">Suspending a Job</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>When a Job is created, the Job controller will immediately begin creating Pods
to satisfy the Job's requirements and will continue to do so until the Job is
complete. However, you may want to temporarily suspend a Job's execution and
resume it later, or start Jobs in suspended state and have a custom controller
decide later when to start them.</p><p>To suspend a Job, you can update the <code>.spec.suspend</code> field of
the Job to true; later, when you want to resume it again, update it to false.
Creating a Job with <code>.spec.suspend</code> set to true will create it in the suspended
state.</p><p>When a Job is resumed from suspension, its <code>.status.startTime</code> field will be
reset to the current time. This means that the <code>.spec.activeDeadlineSeconds</code>
timer will be stopped and reset when a Job is suspended and resumed.</p><p>When you suspend a Job, any running Pods that don't have a status of <code>Completed</code>
will be <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">terminated</a>
with a SIGTERM signal. The Pod's graceful termination period will be honored and
your Pod must handle this signal in this period. This may involve saving
progress for later or undoing changes. Pods terminated this way will not count
towards the Job's <code>completions</code> count.</p><p>An example Job definition in the suspended state can be like so:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get job myjob -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>myjob<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>suspend</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>parallelism</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>completions</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span>...<span>
</span></span></span></code></pre></div><p>You can also toggle Job suspension by patching the Job using the command line.</p><p>Suspend an active Job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch job/myjob --type<span>=</span>strategic --patch <span>'{"spec":{"suspend":true}}'</span>
</span></span></code></pre></div><p>Resume a suspended Job:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch job/myjob --type<span>=</span>strategic --patch <span>'{"spec":{"suspend":false}}'</span>
</span></span></code></pre></div><p>The Job's status can be used to determine if a Job is suspended or has been
suspended in the past:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get jobs/myjob -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span># .metadata and .spec omitted</span><span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>conditions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>lastProbeTime</span>:<span> </span><span>"2021-02-05T13:14:33Z"</span><span>
</span></span></span><span><span><span>    </span><span>lastTransitionTime</span>:<span> </span><span>"2021-02-05T13:14:33Z"</span><span>
</span></span></span><span><span><span>    </span><span>status</span>:<span> </span><span>"True"</span><span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Suspended<span>
</span></span></span><span><span><span>  </span><span>startTime</span>:<span> </span><span>"2021-02-05T13:13:48Z"</span><span>
</span></span></span></code></pre></div><p>The Job condition of type "Suspended" with status "True" means the Job is
suspended; the <code>lastTransitionTime</code> field can be used to determine how long the
Job has been suspended for. If the status of that condition is "False", then the
Job was previously suspended and is now running. If such a condition does not
exist in the Job's status, the Job has never been stopped.</p><p>Events are also created when the Job is suspended and resumed:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe jobs/myjob
</span></span></code></pre></div><pre tabindex="0"><code>Name:           myjob
...
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl
  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl
  Normal  Suspended         11m   job-controller  Job suspended
  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44
  Normal  Resumed           3s    job-controller  Job resumed
</code></pre><p>The last four events, particularly the "Suspended" and "Resumed" events, are
directly a result of toggling the <code>.spec.suspend</code> field. In the time between
these two events, we see that no Pods were created, but Pod creation restarted
as soon as the Job was resumed.</p><h3 id="mutable-scheduling-directives">Mutable Scheduling Directives</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [stable]</code></div><p>In most cases, a parallel job will want the pods to run with constraints,
like all in the same zone, or all either on GPU model x or y but not a mix of both.</p><p>The <a href="#suspending-a-job">suspend</a> field is the first step towards achieving those semantics. Suspend allows a
custom queue controller to decide when a job should start; However, once a job is unsuspended,
a custom queue controller has no influence on where the pods of a job will actually land.</p><p>This feature allows updating a Job's scheduling directives before it starts, which gives custom queue
controllers the ability to influence pod placement while at the same time offloading actual
pod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never
been unsuspended before.</p><p>The fields in a Job's pod template that can be updated are node affinity, node selector,
tolerations, labels, annotations and <a href="/docs/concepts/scheduling-eviction/pod-scheduling-readiness/">scheduling gates</a>.</p><h3 id="specifying-your-own-pod-selector">Specifying your own Pod selector</h3><p>Normally, when you create a Job object, you do not specify <code>.spec.selector</code>.
The system defaulting logic adds this field when the Job is created.
It picks a selector value that will not overlap with any other jobs.</p><p>However, in some cases, you might need to override this automatically set selector.
To do this, you can specify the <code>.spec.selector</code> of the Job.</p><p>Be very careful when doing this. If you specify a label selector which is not
unique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated
job may be deleted, or this Job may count other Pods as completing it, or one or both
Jobs may refuse to create Pods or run to completion. If a non-unique selector is
chosen, then other controllers (e.g. ReplicationController) and their Pods may behave
in unpredictable ways too. Kubernetes will not stop you from making a mistake when
specifying <code>.spec.selector</code>.</p><p>Here is an example of a case when you might want to use this feature.</p><p>Say Job <code>old</code> is already running. You want existing Pods
to keep running, but you want the rest of the Pods it creates
to use a different pod template and for the Job to have a new name.
You cannot update the Job because these fields are not updatable.
Therefore, you delete Job <code>old</code> but <em>leave its pods
running</em>, using <code>kubectl delete jobs/old --cascade=orphan</code>.
Before deleting it, you make a note of what selector it uses:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get job old -o yaml
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>old<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>batch.kubernetes.io/controller-uid</span>:<span> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><p>Then you create a new Job with name <code>new</code> and you explicitly specify the same selector.
Since the existing Pods have label <code>batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002</code>,
they are controlled by Job <code>new</code> as well.</p><p>You need to specify <code>manualSelector: true</code> in the new Job since you are not using
the selector that the system normally generates for you automatically.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>new<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>manualSelector</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>batch.kubernetes.io/controller-uid</span>:<span> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><p>The new Job itself will have a different uid from <code>a8f3d00d-c6d2-11e5-9f87-42010af00002</code>. Setting
<code>manualSelector: true</code> tells the system that you know what you are doing and to allow this
mismatch.</p><h3 id="job-tracking-with-finalizers">Job tracking with finalizers</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>The control plane keeps track of the Pods that belong to any Job and notices if
any such Pod is removed from the API server. To do that, the Job controller
creates Pods with the finalizer <code>batch.kubernetes.io/job-tracking</code>. The
controller removes the finalizer only after the Pod has been accounted for in
the Job status, allowing the Pod to be removed by other controllers or users.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>See <a href="/docs/tasks/debug/debug-application/debug-pods/">My pod stays terminating</a> if you
observe that pods from a Job are stuck with the tracking finalizer.</div><h3 id="elastic-indexed-jobs">Elastic Indexed Jobs</h3><div class="feature-state-notice feature-stable" title="Feature Gate: ElasticIndexedJob"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p>You can scale Indexed Jobs up or down by mutating both <code>.spec.parallelism</code>
and <code>.spec.completions</code> together such that <code>.spec.parallelism == .spec.completions</code>.
When scaling down, Kubernetes removes the Pods with higher indexes.</p><p>Use cases for elastic Indexed Jobs include batch workloads which require
scaling an indexed Job, such as MPI, Horovod, Ray, and PyTorch training jobs.</p><h3 id="pod-replacement-policy">Delayed creation of replacement pods</h3><div class="feature-state-notice feature-stable" title="Feature Gate: JobPodReplacementPolicy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>By default, the Job controller recreates Pods as soon they either fail or are terminating (have a deletion timestamp).
This means that, at a given time, when some of the Pods are terminating, the number of running Pods for a Job
can be greater than <code>parallelism</code> or greater than one Pod per index (if you are using an Indexed Job).</p><p>You may choose to create replacement Pods only when the terminating Pod is fully terminal (has <code>status.phase: Failed</code>).
To do this, set the <code>.spec.podReplacementPolicy: Failed</code>.
The default replacement policy depends on whether the Job has a <code>podFailurePolicy</code> set.
With no Pod failure policy defined for a Job, omitting the <code>podReplacementPolicy</code> field selects the
<code>TerminatingOrFailed</code> replacement policy:
the control plane creates replacement Pods immediately upon Pod deletion
(as soon as the control plane sees that a Pod for this Job has <code>deletionTimestamp</code> set).
For Jobs with a Pod failure policy set, the default <code>podReplacementPolicy</code> is <code>Failed</code>, and no other
value is permitted.
See <a href="#pod-failure-policy">Pod failure policy</a> to learn more about Pod failure policies for Jobs.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>new<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>podReplacementPolicy</span>:<span> </span>Failed<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><p>Provided your cluster has the feature gate enabled, you can inspect the <code>.status.terminating</code> field of a Job.
The value of the field is the number of Pods owned by the Job that are currently terminating.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get jobs/myjob -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Job<span>
</span></span></span><span><span><span></span><span># .metadata and .spec omitted</span><span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>terminating</span>:<span> </span><span>3</span><span> </span><span># three Pods are terminating and have not yet reached the Failed phase</span><span>
</span></span></span></code></pre></div><h3 id="delegation-of-managing-a-job-object-to-external-controller">Delegation of managing a Job object to external controller</h3><div class="feature-state-notice feature-beta" title="Feature Gate: JobManagedBy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [beta]</code> (enabled by default: true)</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You can only set the <code>managedBy</code> field on Jobs if you enable the <code>JobManagedBy</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
(enabled by default).</div><p>This feature allows you to disable the built-in Job controller, for a specific
Job, and delegate reconciliation of the Job to an external controller.</p><p>You indicate the controller that reconciles the Job by setting a custom value
for the <code>spec.managedBy</code> field - any value
other than <code>kubernetes.io/job-controller</code>. The value of the field is immutable.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When using this feature, make sure the controller indicated by the field is
installed, otherwise the Job may not be reconciled at all.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>When developing an external Job controller be aware that your controller needs
to operate in a fashion conformant with the definitions of the API spec and
status fields of the Job object.</p><p>Please review these in detail in the <a href="/docs/reference/kubernetes-api/workload-resources/job-v1/">Job API</a>.
We also recommend that you run the e2e conformance tests for the Job object to
verify your implementation.</p><p>Finally, when developing an external Job controller make sure it does not use the
<code>batch.kubernetes.io/job-tracking</code> finalizer, reserved for the built-in controller.</p></div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>If you are considering to disable the <code>JobManagedBy</code> feature gate, or to
downgrade the cluster to a version without the feature gate enabled, check if
there are jobs with a custom value of the <code>spec.managedBy</code> field. If there
are such jobs, there is a risk that they might be reconciled by two controllers
after the operation: the built-in Job controller and the external controller
indicated by the field value.</div><h2 id="alternatives">Alternatives</h2><h3 id="bare-pods">Bare Pods</h3><p>When the node that a Pod is running on reboots or fails, the pod is terminated
and will not be restarted. However, a Job will create new Pods to replace terminated ones.
For this reason, we recommend that you use a Job rather than a bare Pod, even if your application
requires only a single Pod.</p><h3 id="replication-controller">Replication Controller</h3><p>Jobs are complementary to <a href="/docs/concepts/workloads/controllers/replicationcontroller/">Replication Controllers</a>.
A Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job
manages Pods that are expected to terminate (e.g. batch tasks).</p><p>As discussed in <a href="/docs/concepts/workloads/pods/pod-lifecycle/">Pod Lifecycle</a>, <code>Job</code> is <em>only</em> appropriate
for pods with <code>RestartPolicy</code> equal to <code>OnFailure</code> or <code>Never</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If <code>RestartPolicy</code> is not set, the default value is <code>Always</code>.</div><h3 id="single-job-starts-controller-pod">Single Job starts controller Pod</h3><p>Another pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort
of custom controller for those Pods. This allows the most flexibility, but may be somewhat
complicated to get started with and offers less integration with Kubernetes.</p><p>An advantage of this approach is that the overall process gets the completion guarantee of a Job
object, but maintains complete control over what Pods are created and how work is assigned to them.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a>.</li><li>Read about different ways of running Jobs:<ul><li><a href="/docs/tasks/job/coarse-parallel-processing-work-queue/">Coarse Parallel Processing Using a Work Queue</a></li><li><a href="/docs/tasks/job/fine-parallel-processing-work-queue/">Fine Parallel Processing Using a Work Queue</a></li><li>Use an <a href="/docs/tasks/job/indexed-parallel-processing-static/">indexed Job for parallel processing with static work assignment</a></li><li>Create multiple Jobs based on a template: <a href="/docs/tasks/job/parallel-processing-expansion/">Parallel Processing using Expansions</a></li></ul></li><li>Follow the links within <a href="#clean-up-finished-jobs-automatically">Clean up finished jobs automatically</a>
to learn more about how your cluster can clean up completed and / or failed tasks.</li><li><code>Job</code> is part of the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/job-v1/">Job</a>
object definition to understand the API for jobs.</li><li>Read about <a href="/docs/concepts/workloads/controllers/cron-jobs/"><code>CronJob</code></a>, which you
can use to define a series of Jobs that will run based on a schedule, similar to
the UNIX tool <code>cron</code>.</li><li>Practice how to configure handling of retriable and non-retriable pod failures
using <code>podFailurePolicy</code>, based on the step-by-step <a href="/docs/tasks/job/pod-failure-policy/">examples</a>.</li></ul></div></div><div><div class="td-content"><h1>Automatic Cleanup for Finished Jobs</h1><div class="lead">A time-to-live mechanism to clean up old Jobs that have finished execution.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>When your Job has finished, it's useful to keep that Job in the API (and not immediately delete the Job)
so that you can tell whether the Job succeeded or failed.</p><p>Kubernetes' TTL-after-finished <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> provides a
TTL (time to live) mechanism to limit the lifetime of Job objects that
have finished execution.</p><h2 id="cleanup-for-finished-jobs">Cleanup for finished Jobs</h2><p>The TTL-after-finished controller is only supported for Jobs. You can use this mechanism to clean
up finished Jobs (either <code>Complete</code> or <code>Failed</code>) automatically by specifying the
<code>.spec.ttlSecondsAfterFinished</code> field of a Job, as in this
<a href="/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">example</a>.</p><p>The TTL-after-finished controller assumes that a Job is eligible to be cleaned up
TTL seconds after the Job has finished. The timer starts once the
status condition of the Job changes to show that the Job is either <code>Complete</code> or <code>Failed</code>; once the TTL has
expired, that Job becomes eligible for
<a href="/docs/concepts/architecture/garbage-collection/#cascading-deletion">cascading</a> removal. When the
TTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete
its dependent objects together with it.</p><p>Kubernetes honors object lifecycle guarantees on the Job, such as waiting for
<a href="/docs/concepts/overview/working-with-objects/finalizers/">finalizers</a>.</p><p>You can set the TTL seconds at any time. Here are some examples for setting the
<code>.spec.ttlSecondsAfterFinished</code> field of a Job:</p><ul><li>Specify this field in the Job manifest, so that a Job can be cleaned up
automatically some time after it finishes.</li><li>Manually set this field of existing, already finished Jobs, so that they become eligible
for cleanup.</li><li>Use a
<a href="/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">mutating admission webhook</a>
to set this field dynamically at Job creation time. Cluster administrators can
use this to enforce a TTL policy for finished jobs.</li><li>Use a
<a href="/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">mutating admission webhook</a>
to set this field dynamically after the Job has finished, and choose
different TTL values based on job status, labels. For this case, the webhook needs
to detect changes to the <code>.status</code> of the Job and only set a TTL when the Job
is being marked as completed.</li><li>Write your own controller to manage the cleanup TTL for Jobs that match a particular
<a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." href="/docs/concepts/overview/working-with-objects/labels/" target="_blank">selector</a>.</li></ul><h2 id="caveats">Caveats</h2><h3 id="updating-ttl-for-finished-jobs">Updating TTL for finished Jobs</h3><p>You can modify the TTL period, e.g. <code>.spec.ttlSecondsAfterFinished</code> field of Jobs,
after the job is created or has finished. If you extend the TTL period after the
existing <code>ttlSecondsAfterFinished</code> period has expired, Kubernetes doesn't guarantee
to retain that Job, even if an update to extend the TTL returns a successful API
response.</p><h3 id="time-skew">Time skew</h3><p>Because the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to
determine whether the TTL has expired or not, this feature is sensitive to time
skew in your cluster, which may cause the control plane to clean up Job objects
at the wrong time.</p><p>Clocks aren't always correct, but the difference should be
very small. Please be aware of this risk when setting a non-zero TTL.</p><h2 id="what-s-next">What's next</h2><ul><li><p>Read <a href="/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">Clean up Jobs automatically</a></p></li><li><p>Refer to the <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md">Kubernetes Enhancement Proposal</a>
(KEP) for adding this mechanism.</p></li></ul></div></div><div><div class="td-content"><h1>CronJob</h1><div class="lead">A CronJob starts one-time Jobs on a repeating schedule.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>A <em>CronJob</em> creates <a class="glossary-tooltip" title="A finite or batch task that runs to completion." href="/docs/concepts/workloads/controllers/job/" target="_blank">Jobs</a> on a repeating schedule.</p><p>CronJob is meant for performing regular scheduled actions such as backups, report generation,
and so on. One CronJob object is like one line of a <em>crontab</em> (cron table) file on a
Unix system. It runs a Job periodically on a given schedule, written in
<a href="https://en.wikipedia.org/wiki/Cron">Cron</a> format.</p><p>CronJobs have limitations and idiosyncrasies.
For example, in certain circumstances, a single CronJob can create multiple concurrent Jobs. See the <a href="#cron-job-limitations">limitations</a> below.</p><p>When the control plane creates new Jobs and (indirectly) Pods for a CronJob, the <code>.metadata.name</code>
of the CronJob is part of the basis for naming those Pods. The name of a CronJob must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostnames. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.
Even when the name is a DNS subdomain, the name must be no longer than 52
characters. This is because the CronJob controller will automatically append
11 characters to the name you provide and there is a constraint that the
length of a Job name is no more than 63 characters.</p><h2 id="example">Example</h2><p>This example CronJob manifest prints the current time and a hello message every minute:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/cronjob.yaml"><code>application/job/cronjob.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/job/cronjob.yaml to clipboard"></div><div class="includecode" id="application-job-cronjob-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>batch/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CronJob<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hello<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>schedule</span>:<span> </span><span>"* * * * *"</span><span>
</span></span></span><span><span><span>  </span><span>jobTemplate</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>template</span>:<span>
</span></span></span><span><span><span>        </span><span>spec</span>:<span>
</span></span></span><span><span><span>          </span><span>containers</span>:<span>
</span></span></span><span><span><span>          </span>- <span>name</span>:<span> </span>hello<span>
</span></span></span><span><span><span>            </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>            </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span><span><span><span>            </span><span>command</span>:<span>
</span></span></span><span><span><span>            </span>- /bin/sh<span>
</span></span></span><span><span><span>            </span>- -c<span>
</span></span></span><span><span><span>            </span>- date; echo Hello from the Kubernetes cluster<span>
</span></span></span><span><span><span>          </span><span>restartPolicy</span>:<span> </span>OnFailure<span>
</span></span></span></code></pre></div></div></div><p>(<a href="/docs/tasks/job/automated-tasks-with-cron-jobs/">Running Automated Tasks with a CronJob</a>
takes you through this example in more detail).</p><h2 id="writing-a-cronjob-spec">Writing a CronJob spec</h2><h3 id="schedule-syntax">Schedule syntax</h3><p>The <code>.spec.schedule</code> field is required. The value of that field follows the <a href="https://en.wikipedia.org/wiki/Cron">Cron</a> syntax:</p><pre tabindex="0"><code># &#9484;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472; minute (0 - 59)
# &#9474; &#9484;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472; hour (0 - 23)
# &#9474; &#9474; &#9484;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472; day of the month (1 - 31)
# &#9474; &#9474; &#9474; &#9484;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472; month (1 - 12)
# &#9474; &#9474; &#9474; &#9474; &#9484;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472; day of the week (0 - 6) (Sunday to Saturday)
# &#9474; &#9474; &#9474; &#9474; &#9474;                                   OR sun, mon, tue, wed, thu, fri, sat
# &#9474; &#9474; &#9474; &#9474; &#9474;
# &#9474; &#9474; &#9474; &#9474; &#9474;
# * * * * *
</code></pre><p>For example, <code>0 3 * * 1</code> means this task is scheduled to run weekly on a Monday at 3 AM.</p><p>The format also includes extended "Vixie cron" step values. As explained in the
<a href="https://www.freebsd.org/cgi/man.cgi?crontab%285%29">FreeBSD manual</a>:</p><blockquote><p>Step values can be used in conjunction with ranges. Following a range
with <code>/&lt;number&gt;</code> specifies skips of the number's value through the
range. For example, <code>0-23/2</code> can be used in the hours field to specify
command execution every other hour (the alternative in the V7 standard is
<code>0,2,4,6,8,10,12,14,16,18,20,22</code>). Steps are also permitted after an
asterisk, so if you want to say "every two hours", just use <code>*/2</code>.</p></blockquote><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A question mark (<code>?</code>) in the schedule has the same meaning as an asterisk <code>*</code>, that is,
it stands for any of available value for a given field.</div><p>Other than the standard syntax, some macros like <code>@monthly</code> can also be used:</p><table><thead><tr><th>Entry</th><th>Description</th><th>Equivalent to</th></tr></thead><tbody><tr><td>@yearly (or @annually)</td><td>Run once a year at midnight of 1 January</td><td>0 0 1 1 *</td></tr><tr><td>@monthly</td><td>Run once a month at midnight of the first day of the month</td><td>0 0 1 * *</td></tr><tr><td>@weekly</td><td>Run once a week at midnight on Sunday morning</td><td>0 0 * * 0</td></tr><tr><td>@daily (or @midnight)</td><td>Run once a day at midnight</td><td>0 0 * * *</td></tr><tr><td>@hourly</td><td>Run once an hour at the beginning of the hour</td><td>0 * * * *</td></tr></tbody></table><p>To generate CronJob schedule expressions, you can also use web tools like <a href="https://crontab.guru/">crontab.guru</a>.</p><h3 id="job-template">Job template</h3><p>The <code>.spec.jobTemplate</code> defines a template for the Jobs that the CronJob creates, and it is required.
It has exactly the same schema as a <a href="/docs/concepts/workloads/controllers/job/">Job</a>, except that
it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.
You can specify common metadata for the templated Jobs, such as
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">labels</a> or
<a class="glossary-tooltip" title="A key-value pair that is used to attach arbitrary non-identifying metadata to objects." href="/docs/concepts/overview/working-with-objects/annotations" target="_blank">annotations</a>.
For information about writing a Job <code>.spec</code>, see <a href="/docs/concepts/workloads/controllers/job/#writing-a-job-spec">Writing a Job Spec</a>.</p><h3 id="starting-deadline">Deadline for delayed Job start</h3><p>The <code>.spec.startingDeadlineSeconds</code> field is optional.
This field defines a deadline (in whole seconds) for starting the Job, if that Job misses its scheduled time
for any reason.</p><p>After missing the deadline, the CronJob skips that instance of the Job (future occurrences are still scheduled).
For example, if you have a backup Job that runs twice a day, you might allow it to start up to 8 hours late,
but no later, because a backup taken any later wouldn't be useful: you would instead prefer to wait for
the next scheduled run.</p><p>For Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs.
If you don't specify <code>startingDeadlineSeconds</code> for a CronJob, the Job occurrences have no deadline.</p><p>If the <code>.spec.startingDeadlineSeconds</code> field is set (not null), the CronJob
controller measures the time between when a Job is expected to be created and
now. If the difference is higher than that limit, it will skip this execution.</p><p>For example, if it is set to <code>200</code>, it allows a Job to be created for up to 200
seconds after the actual schedule.</p><h3 id="concurrency-policy">Concurrency policy</h3><p>The <code>.spec.concurrencyPolicy</code> field is also optional.
It specifies how to treat concurrent executions of a Job that is created by this CronJob.
The spec may specify only one of the following concurrency policies:</p><ul><li><code>Allow</code> (default): The CronJob allows concurrently running Jobs</li><li><code>Forbid</code>: The CronJob does not allow concurrent runs; if it is time for a new Job run and the
previous Job run hasn't finished yet, the CronJob skips the new Job run. Also note that when the
previous Job run finishes, <code>.spec.startingDeadlineSeconds</code> is still taken into account and may
result in a new Job run.</li><li><code>Replace</code>: If it is time for a new Job run and the previous Job run hasn't finished yet, the
CronJob replaces the currently running Job run with a new Job run</li></ul><p>Note that concurrency policy only applies to the Jobs created by the same CronJob.
If there are multiple CronJobs, their respective Jobs are always allowed to run concurrently.</p><h3 id="schedule-suspension">Schedule suspension</h3><p>You can suspend execution of Jobs for a CronJob, by setting the optional <code>.spec.suspend</code> field
to true. The field defaults to false.</p><p>This setting does <em>not</em> affect Jobs that the CronJob has already started.</p><p>If you do set that field to true, all subsequent executions are suspended (they remain
scheduled, but the CronJob controller does not start the Jobs to run the tasks) until
you unsuspend the CronJob.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Executions that are suspended during their scheduled time count as missed Jobs.
When <code>.spec.suspend</code> changes from <code>true</code> to <code>false</code> on an existing CronJob without a
<a href="#starting-deadline">starting deadline</a>, the missed Jobs are scheduled immediately.</div><h3 id="jobs-history-limits">Jobs history limits</h3><p>The <code>.spec.successfulJobsHistoryLimit</code> and <code>.spec.failedJobsHistoryLimit</code> fields specify
how many completed and failed Jobs should be kept. Both fields are optional.</p><ul><li><p><code>.spec.successfulJobsHistoryLimit</code>: This field specifies the number of successful finished
jobs to keep. The default value is <code>3</code>. Setting this field to <code>0</code> will not keep any successful jobs.</p></li><li><p><code>.spec.failedJobsHistoryLimit</code>: This field specifies the number of failed finished jobs to keep.
The default value is <code>1</code>. Setting this field to <code>0</code> will not keep any failed jobs.</p></li></ul><p>For another way to clean up Jobs automatically, see
<a href="/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">Clean up finished Jobs automatically</a>.</p><h3 id="time-zones">Time zones</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [stable]</code></div><p>For CronJobs with no time zone specified, the <a class="glossary-tooltip" title="Control Plane component that runs controller processes." href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank">kube-controller-manager</a>
interprets schedules relative to its local time zone.</p><p>You can specify a time zone for a CronJob by setting <code>.spec.timeZone</code> to the name
of a valid <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones">time zone</a>.
For example, setting <code>.spec.timeZone: "Etc/UTC"</code> instructs Kubernetes to interpret
the schedule relative to Coordinated Universal Time.</p><p>A time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.</p><h2 id="cron-job-limitations">CronJob limitations</h2><h3 id="unsupported-timezone-specification">Unsupported TimeZone specification</h3><p>Specifying a timezone using <code>CRON_TZ</code> or <code>TZ</code> variables inside <code>.spec.schedule</code>
is <strong>not officially supported</strong> (and never has been). If you try to set a schedule
that includes <code>TZ</code> or <code>CRON_TZ</code> timezone specification, Kubernetes will fail to
create or update the resource with a validation error. You should specify time zones
using the <a href="#time-zones">time zone field</a>, instead.</p><h3 id="modifying-a-cronjob">Modifying a CronJob</h3><p>By design, a CronJob contains a template for <em>new</em> Jobs.
If you modify an existing CronJob, the changes you make will apply to new Jobs that
start to run after your modification is complete. Jobs (and their Pods) that have already
started continue to run without changes.
That is, the CronJob does <em>not</em> update existing Jobs, even if those remain running.</p><h3 id="job-creation">Job creation</h3><p>A CronJob creates a Job object approximately once per execution time of its schedule.
The scheduling is approximate because there
are certain circumstances where two Jobs might be created, or no Job might be created.
Kubernetes tries to avoid those situations, but does not completely prevent them. Therefore,
the Jobs that you define should be <em>idempotent</em>.</p><p>Starting with Kubernetes v1.32, CronJobs apply an annotation
<code>batch.kubernetes.io/cronjob-scheduled-timestamp</code> to their created Jobs. This annotation
indicates the originally scheduled creation time for the Job and is formatted in RFC3339.</p><p>If <code>startingDeadlineSeconds</code> is set to a large value or left unset (the default)
and if <code>concurrencyPolicy</code> is set to <code>Allow</code>, the Jobs will always run
at least once.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>If <code>startingDeadlineSeconds</code> is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.</div><p>For every CronJob, the CronJob <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">Controller</a> checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the Job and logs the error.</p><pre tabindex="0"><code>Cannot determine if job needs to be started. Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
</code></pre><p>It is important to note that if the <code>startingDeadlineSeconds</code> field is set (not <code>nil</code>), the controller counts how many missed Jobs occurred from the value of <code>startingDeadlineSeconds</code> until now rather than from the last scheduled time until now. For example, if <code>startingDeadlineSeconds</code> is <code>200</code>, the controller counts how many missed Jobs occurred in the last 200 seconds.</p><p>A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if <code>concurrencyPolicy</code> is set to <code>Forbid</code> and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.</p><p>For example, suppose a CronJob is set to schedule a new Job every one minute beginning at <code>08:30:00</code>, and its
<code>startingDeadlineSeconds</code> field is not set. If the CronJob controller happens to
be down from <code>08:29:00</code> to <code>10:21:00</code>, the Job will not start as the number of missed Jobs which missed their schedule is greater than 100.</p><p>To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at <code>08:30:00</code>, and its
<code>startingDeadlineSeconds</code> is set to 200 seconds. If the CronJob controller happens to
be down for the same period as the previous example (<code>08:29:00</code> to <code>10:21:00</code>,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now.</p><p>The CronJob is only responsible for creating Jobs that match its schedule, and
the Job in turn is responsible for the management of the Pods it represents.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a> and
<a href="/docs/concepts/workloads/controllers/job/">Jobs</a>, two concepts
that CronJobs rely upon.</li><li>Read about the detailed <a href="https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format">format</a>
of CronJob <code>.spec.schedule</code> fields.</li><li>For instructions on creating and working with CronJobs, and for an example
of a CronJob manifest,
see <a href="/docs/tasks/job/automated-tasks-with-cron-jobs/">Running automated tasks with CronJobs</a>.</li><li><code>CronJob</code> is part of the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/cron-job-v1/">CronJob</a>
API reference for more details.</li></ul></div></div><div><div class="td-content"><h1>ReplicationController</h1><div class="lead">Legacy API for managing workloads that can scale horizontally. Superseded by the Deployment and ReplicaSet APIs.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A <a href="/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> that configures a <a href="/docs/concepts/workloads/controllers/replicaset/"><code>ReplicaSet</code></a> is now the recommended way to set up replication.</div><p>A <em>ReplicationController</em> ensures that a specified number of pod replicas are running at any one
time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is
always up and available.</p><h2 id="how-a-replicationcontroller-works">How a ReplicationController works</h2><p>If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the
ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a
ReplicationController are automatically replaced if they fail, are deleted, or are terminated.
For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.
For this reason, you should use a ReplicationController even if your application requires
only a single pod. A ReplicationController is similar to a process supervisor,
but instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods
across multiple nodes.</p><p>ReplicationController is often abbreviated to "rc" in discussion, and as a shortcut in
kubectl commands.</p><p>A simple case is to create one ReplicationController object to reliably run one instance of
a Pod indefinitely. A more complex use case is to run several identical replicas of a replicated
service, such as web servers.</p><h2 id="running-an-example-replicationcontroller">Running an example ReplicationController</h2><p>This example ReplicationController config runs three copies of the nginx web server.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/replication.yaml"><code>controllers/replication.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy controllers/replication.yaml to clipboard"></div><div class="includecode" id="controllers-replication-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ReplicationController<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Run the example job by downloading the example file and then running this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/controllers/replication.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>replicationcontroller/nginx created
</code></pre><p>Check on the status of the ReplicationController using this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe replicationcontrollers/nginx
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &lt;none&gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
</code></pre><p>Here, three pods are created, but none is running yet, perhaps because the image is being pulled.
A little later, the same command may show:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>Pods Status:    <span>3</span> Running / <span>0</span> Waiting / <span>0</span> Succeeded / <span>0</span> Failed
</span></span></code></pre></div><p>To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>pods</span><span>=</span><span>$(</span>kubectl get pods --selector<span>=</span><span>app</span><span>=</span>nginx --output<span>=</span><span>jsonpath</span><span>={</span>.items..metadata.name<span>}</span><span>)</span>
</span></span><span><span><span>echo</span> <span>$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>nginx-3ntk0 nginx-4ok8v nginx-qrm3m
</code></pre><p>Here, the selector is the same as the selector for the ReplicationController (seen in the
<code>kubectl describe</code> output), and in a different form in <code>replication.yaml</code>. The <code>--output=jsonpath</code> option
specifies an expression with the name from each pod in the returned list.</p><h2 id="writing-a-replicationcontroller-manifest">Writing a ReplicationController Manifest</h2><p>As with all other Kubernetes config, a ReplicationController needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.</p><p>When the control plane creates new Pods for a ReplicationController, the <code>.metadata.name</code> of the
ReplicationController is part of the basis for naming those Pods. The name of a ReplicationController must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain</a>
value, but this can produce unexpected results for the Pod hostnames. For best compatibility,
the name should follow the more restrictive rules for a
<a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS label</a>.</p><p>For general information about working with configuration files, see <a href="/docs/concepts/overview/working-with-objects/object-management/">object management</a>.</p><p>A ReplicationController also needs a <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> section</a>.</p><h3 id="pod-template">Pod Template</h3><p>The <code>.spec.template</code> is the only required field of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href="/docs/concepts/workloads/pods/#pod-templates">pod template</a>. It has exactly the same schema as a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>, except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See <a href="#pod-selector">pod selector</a>.</p><p>Only a <a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>.spec.template.spec.restartPolicy</code></a> equal to <code>Always</code> is allowed, which is the default if not specified.</p><p>For local container restarts, ReplicationControllers delegate to an agent on the node,
for example the <a href="/docs/reference/command-line-tools-reference/kubelet/">Kubelet</a>.</p><h3 id="labels-on-the-replicationcontroller">Labels on the ReplicationController</h3><p>The ReplicationController can itself have labels (<code>.metadata.labels</code>). Typically, you
would set these the same as the <code>.spec.template.metadata.labels</code>; if <code>.metadata.labels</code> is not specified
then it defaults to <code>.spec.template.metadata.labels</code>. However, they are allowed to be
different, and the <code>.metadata.labels</code> do not affect the behavior of the ReplicationController.</p><h3 id="pod-selector">Pod Selector</h3><p>The <code>.spec.selector</code> field is a <a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selector</a>. A ReplicationController
manages all the pods with labels that match the selector. It does not distinguish
between pods that it created or deleted and pods that another person or process created or
deleted. This allows the ReplicationController to be replaced without affecting the running pods.</p><p>If specified, the <code>.spec.template.metadata.labels</code> must be equal to the <code>.spec.selector</code>, or it will
be rejected by the API. If <code>.spec.selector</code> is unspecified, it will be defaulted to
<code>.spec.template.metadata.labels</code>.</p><p>Also you should not normally create any pods whose labels match this selector, either directly, with
another ReplicationController, or with another controller such as Job. If you do so, the
ReplicationController thinks that it created the other pods. Kubernetes does not stop you
from doing this.</p><p>If you do end up with multiple controllers that have overlapping selectors, you
will have to manage the deletion yourself (see <a href="#working-with-replicationcontrollers">below</a>).</p><h3 id="multiple-replicas">Multiple Replicas</h3><p>You can specify how many pods should run concurrently by setting <code>.spec.replicas</code> to the number
of pods you would like to have running concurrently. The number running at any time may be higher
or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully
shutdown, and a replacement starts early.</p><p>If you do not specify <code>.spec.replicas</code>, then it defaults to 1.</p><h2 id="working-with-replicationcontrollers">Working with ReplicationControllers</h2><h3 id="deleting-a-replicationcontroller-and-its-pods">Deleting a ReplicationController and its Pods</h3><p>To delete a ReplicationController and all its pods, use <a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>. Kubectl will scale the ReplicationController to zero and wait
for it to delete each pod before deleting the ReplicationController itself. If this kubectl
command is interrupted, it can be restarted.</p><p>When using the REST API or <a href="/docs/reference/using-api/client-libraries/">client library</a>, you need to do the steps explicitly (scale replicas to
0, wait for pod deletions, then delete the ReplicationController).</p><h3 id="deleting-only-a-replicationcontroller">Deleting only a ReplicationController</h3><p>You can delete a ReplicationController without affecting any of its pods.</p><p>Using kubectl, specify the <code>--cascade=orphan</code> option to <a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>.</p><p>When using the REST API or <a href="/docs/reference/using-api/client-libraries/">client library</a>, you can delete the ReplicationController object.</p><p>Once the original is deleted, you can create a new ReplicationController to replace it. As long
as the old and new <code>.spec.selector</code> are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update pods to a new spec in a controlled way, use a <a href="#rolling-updates">rolling update</a>.</p><h3 id="isolating-pods-from-a-replicationcontroller">Isolating pods from a ReplicationController</h3><p>Pods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).</p><h2 id="common-usage-patterns">Common usage patterns</h2><h3 id="rescheduling">Rescheduling</h3><p>As mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).</p><h3 id="scaling">Scaling</h3><p>The ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by updating the <code>replicas</code> field.</p><h3 id="rolling-updates">Rolling updates</h3><p>The ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.</p><p>As explained in <a href="https://issue.k8s.io/1353">#1353</a>, the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.</p><p>Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.</p><p>The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.</p><h3 id="multiple-release-tracks">Multiple release tracks</h3><p>In addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.</p><p>For instance, a service might target all pods with <code>tier in (frontend), environment in (prod)</code>. Now say you have 10 replicated pods that make up this tier. But you want to be able to 'canary' a new version of this component. You could set up a ReplicationController with <code>replicas</code> set to 9 for the bulk of the replicas, with labels <code>tier=frontend, environment=prod, track=stable</code>, and another ReplicationController with <code>replicas</code> set to 1 for the canary, with labels <code>tier=frontend, environment=prod, track=canary</code>. Now the service is covering both the canary and non-canary pods. But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.</p><h3 id="using-replicationcontrollers-with-services">Using ReplicationControllers with Services</h3><p>Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic
goes to the old version, and some goes to the new version.</p><p>A ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.</p><h2 id="writing-programs-for-replication">Writing programs for Replication</h2><p>Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the <a href="https://www.rabbitmq.com/tutorials/tutorial-two-python.html">RabbitMQ work queues</a>, as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.</p><h2 id="responsibilities-of-the-replicationcontroller">Responsibilities of the ReplicationController</h2><p>The ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, <a href="https://issue.k8s.io/620">readiness</a> and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.</p><p>The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in <a href="https://issue.k8s.io/492">#492</a>), which would change its <code>replicas</code> field. We will not add scheduling policies (for example, <a href="https://issue.k8s.io/367#issuecomment-48428019">spreading</a>) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation (<a href="https://issue.k8s.io/170">#170</a>).</p><p>The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The "macro" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like <a href="https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1">Asgard</a> managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.</p><h2 id="api-object">API Object</h2><p>Replication controller is a top-level resource in the Kubernetes REST API. More details about the
API object can be found at:
<a href="/docs/reference/generated/kubernetes-api/v1.34/#replicationcontroller-v1-core">ReplicationController API object</a>.</p><h2 id="alternatives-to-replicationcontroller">Alternatives to ReplicationController</h2><h3 id="replicaset">ReplicaSet</h3><p><a href="/docs/concepts/workloads/controllers/replicaset/"><code>ReplicaSet</code></a> is the next-generation ReplicationController that supports the new <a href="/docs/concepts/overview/working-with-objects/labels/#set-based-requirement">set-based label selector</a>.
It's mainly used by <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a> as a mechanism to orchestrate pod creation, deletion and updates.
Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.</p><h3 id="deployment-recommended">Deployment (Recommended)</h3><p><a href="/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want the rolling update functionality, because they are declarative, server-side, and have additional features.</p><h3 id="bare-pods">Bare Pods</h3><p>Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.</p><h3 id="job">Job</h3><p>Use a <a href="/docs/concepts/workloads/controllers/job/"><code>Job</code></a> instead of a ReplicationController for pods that are expected to terminate on their own
(that is, batch jobs).</p><h3 id="daemonset">DaemonSet</h3><p>Use a <a href="/docs/concepts/workloads/controllers/daemonset/"><code>DaemonSet</code></a> instead of a ReplicationController for pods that provide a
machine-level function, such as machine monitoring or machine logging. These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/workloads/pods/">Pods</a>.</li><li>Learn about <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a>, the replacement
for ReplicationController.</li><li><code>ReplicationController</code> is part of the Kubernetes REST API.
Read the
<a href="/docs/reference/kubernetes-api/workload-resources/replication-controller-v1/">ReplicationController</a>
object definition to understand the API for replication controllers.</li></ul></div></div><div><div class="td-content"><h1>Autoscaling Workloads</h1><div class="lead">With autoscaling, you can automatically update your workloads in one way or another. This allows your cluster to react to changes in resource demand more elastically and efficiently.</div><p>In Kubernetes, you can <em>scale</em> a workload depending on the current demand of resources.
This allows your cluster to react to changes in resource demand more elastically and efficiently.</p><p>When you scale a workload, you can either increase or decrease the number of replicas managed by
the workload, or adjust the resources available to the replicas in-place.</p><p>The first approach is referred to as <em>horizontal scaling</em>, while the second is referred to as
<em>vertical scaling</em>.</p><p>There are manual and automatic ways to scale your workloads, depending on your use case.</p><h2 id="scaling-workloads-manually">Scaling workloads manually</h2><p>Kubernetes supports <em>manual scaling</em> of workloads. Horizontal scaling can be done
using the <code>kubectl</code> CLI.
For vertical scaling, you need to <em>patch</em> the resource definition of your workload.</p><p>See below for examples of both strategies.</p><ul><li><strong>Horizontal scaling</strong>: <a href="/docs/tutorials/kubernetes-basics/scale/scale-intro/">Running multiple instances of your app</a></li><li><strong>Vertical scaling</strong>: <a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resizing CPU and memory resources assigned to containers</a></li></ul><h2 id="scaling-workloads-automatically">Scaling workloads automatically</h2><p>Kubernetes also supports <em>automatic scaling</em> of workloads, which is the focus of this page.</p><p>The concept of <em>Autoscaling</em> in Kubernetes refers to the ability to automatically update an
object that manages a set of Pods (for example a
<a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a>).</p><h3 id="scaling-workloads-horizontally">Scaling workloads horizontally</h3><p>In Kubernetes, you can automatically scale a workload horizontally using a <em>HorizontalPodAutoscaler</em> (HPA).</p><p>It is implemented as a Kubernetes API resource and a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>
and periodically adjusts the number of <a class="glossary-tooltip" title="Replicas are copies of pods, ensuring availability, scalability, and fault tolerance by maintaining identical instances." href="/docs/reference/glossary/?all=true#term-replica" target="_blank">replicas</a>
in a workload to match observed resource utilization such as CPU or memory usage.</p><p>There is a <a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">walkthrough tutorial</a> of configuring a HorizontalPodAutoscaler for a Deployment.</p><h3 id="scaling-workloads-vertically">Scaling workloads vertically</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>You can automatically scale a workload vertically using a <em>VerticalPodAutoscaler</em> (VPA).
Unlike the HPA, the VPA doesn't come with Kubernetes by default, but is a separate project
that can be found <a href="https://github.com/kubernetes/autoscaler/tree/9f87b78df0f1d6e142234bb32e8acbd71295585a/vertical-pod-autoscaler">on GitHub</a>.</p><p>Once installed, it allows you to create <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinitions</a>
(CRDs) for your workloads which define <em>how</em> and <em>when</em> to scale the resources of the managed replicas.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You will need to have the <a href="https://github.com/kubernetes-sigs/metrics-server">Metrics Server</a>
installed to your cluster for the VPA to work.</div><p>At the moment, the VPA can operate in four different modes:</p><table><caption>Different modes of the VPA</caption><thead><tr><th>Mode</th><th>Description</th></tr></thead><tbody><tr><td><code>Auto</code></td><td>Currently <code>Recreate</code>. This might change to in-place updates in the future.</td></tr><tr><td><code>Recreate</code></td><td>The VPA assigns resource requests on pod creation as well as updates them on existing pods by evicting them when the requested resources differ significantly from the new recommendation</td></tr><tr><td><code>Initial</code></td><td>The VPA only assigns resource requests on pod creation and never changes them later.</td></tr><tr><td><code>Off</code></td><td>The VPA does not automatically change the resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object.</td></tr></tbody></table><h4 id="in-place-pod-vertical-scaling">In-place pod vertical scaling</h4><div class="feature-state-notice feature-beta" title="Feature Gate: InPlacePodVerticalScaling"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>As of Kubernetes 1.34, VPA does not support resizing pods in-place,
but this integration is being worked on.
For manually resizing pods in-place, see <a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize Container Resources In-Place</a>.</p><h3 id="autoscaling-based-on-cluster-size">Autoscaling based on cluster size</h3><p>For workloads that need to be scaled based on the size of the cluster (for example
<code>cluster-dns</code> or other system components), you can use the
<a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler"><em>Cluster Proportional Autoscaler</em></a>.
Just like the VPA, it is not part of the Kubernetes core, but hosted as its
own project on GitHub.</p><p>The Cluster Proportional Autoscaler watches the number of schedulable <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">nodes</a>
and cores and scales the number of replicas of the target workload accordingly.</p><p>If the number of replicas should stay the same, you can scale your workloads vertically according to the cluster size using
the <a href="https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler"><em>Cluster Proportional Vertical Autoscaler</em></a>.
The project is <strong>currently in beta</strong> and can be found on GitHub.</p><p>While the Cluster Proportional Autoscaler scales the number of replicas of a workload,
the Cluster Proportional Vertical Autoscaler adjusts the resource requests for a workload
(for example a Deployment or DaemonSet) based on the number of nodes and/or cores in the cluster.</p><h3 id="event-driven-autoscaling">Event driven Autoscaling</h3><p>It is also possible to scale workloads based on events, for example using the
<a href="https://keda.sh/"><em>Kubernetes Event Driven Autoscaler</em> (<strong>KEDA</strong>)</a>.</p><p>KEDA is a CNCF-graduated project enabling you to scale your workloads based on the number
of events to be processed, for example the amount of messages in a queue. There exists
a wide range of adapters for different event sources to choose from.</p><h3 id="autoscaling-based-on-schedules">Autoscaling based on schedules</h3><p>Another strategy for scaling your workloads is to <strong>schedule</strong> the scaling operations, for example in order to
reduce resource consumption during off-peak hours.</p><p>Similar to event driven autoscaling, such behavior can be achieved using KEDA in conjunction with
its <a href="https://keda.sh/docs/latest/scalers/cron/"><code>Cron</code> scaler</a>.
The <code>Cron</code> scaler allows you to define schedules (and time zones) for scaling your workloads in or out.</p><h2 id="scaling-cluster-infrastructure">Scaling cluster infrastructure</h2><p>If scaling workloads isn't enough to meet your needs, you can also scale your cluster infrastructure itself.</p><p>Scaling the cluster infrastructure normally means adding or removing <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">nodes</a>.
Read <a href="/docs/concepts/cluster-administration/node-autoscaling/">Node autoscaling</a>
for more information.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about scaling horizontally<ul><li><a href="/docs/tasks/run-application/scale-stateful-set/">Scale a StatefulSet</a></li><li><a href="/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">HorizontalPodAutoscaler Walkthrough</a></li></ul></li><li><a href="/docs/tasks/configure-pod-container/resize-container-resources/">Resize Container Resources In-Place</a></li><li><a href="/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">Autoscale the DNS Service in a Cluster</a></li><li>Learn about <a href="/docs/concepts/cluster-administration/node-autoscaling/">Node autoscaling</a></li></ul></div></div><div><div class="td-content"><h1>Managing Workloads</h1><p>You've deployed your application and exposed it via a Service. Now what? Kubernetes provides a
number of tools to help you manage your application deployment, including scaling and updating.</p><h2 id="organizing-resource-configurations">Organizing resource configurations</h2><p>Many applications require multiple resources to be created, such as a Deployment along with a Service.
Management of multiple resources can be simplified by grouping them together in the same file
(separated by <code>---</code> in YAML). For example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/nginx-app.yaml"><code>application/nginx-app.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy application/nginx-app.yaml to clipboard"></div><div class="includecode" id="application-nginx-app-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-nginx-svc<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>LoadBalancer<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-nginx<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.14.2<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>Multiple resources can be created the same way as a single resource:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">service/my-nginx-svc created
deployment.apps/my-nginx created
</code></pre><p>The resources will be created in the order they appear in the manifest. Therefore, it's best to
specify the Service first, since that will ensure the scheduler can spread the pods associated
with the Service as they are created by the controller(s), such as Deployment.</p><p><code>kubectl apply</code> also accepts multiple <code>-f</code> arguments:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml <span>\
</span></span></span><span><span><span></span>  -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><p>It is a recommended practice to put resources related to the same microservice or application tier
into the same file, and to group all of the files associated with your application in the same
directory. If the tiers of your application bind to each other using DNS, you can deploy all of
the components of your stack together.</p><p>A URL can also be specified as a configuration source, which is handy for deploying directly from
manifests in your source control system:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/my-nginx created
</code></pre><p>If you need to define more manifests, such as adding a ConfigMap, you can do that too.</p><h3 id="external-tools">External tools</h3><p>This section lists only the most common tools used for managing workloads on Kubernetes. To see a larger list, view
<a href="https://landscape.cncf.io/guide#app-definition-and-development--application-definition-image-build">Application definition and image build</a>
in the <a class="glossary-tooltip" title="Cloud Native Computing Foundation" href="https://cncf.io/" target="_blank">CNCF</a> Landscape.</p><h4 id="external-tool-helm">Helm</h4><div class="alert alert-secondary callout third-party-content">&#128711; This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><p><a href="https://helm.sh/">Helm</a> is a tool for managing packages of pre-configured
Kubernetes resources. These packages are known as <em>Helm charts</em>.</p><h4 id="external-tool-kustomize">Kustomize</h4><p><a href="https://kustomize.io/">Kustomize</a> traverses a Kubernetes manifest to add, remove or update configuration options.
It is available both as a standalone binary and as a <a href="/docs/tasks/manage-kubernetes-objects/kustomization/">native feature</a>
of kubectl.</p><h2 id="bulk-operations-in-kubectl">Bulk operations in kubectl</h2><p>Resource creation isn't the only operation that <code>kubectl</code> can perform in bulk. It can also extract
resource names from configuration files in order to perform other operations, in particular to
delete the same resources you created:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete -f https://k8s.io/examples/application/nginx-app.yaml
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps "my-nginx" deleted
service "my-nginx-svc" deleted
</code></pre><p>In the case of two resources, you can specify both resources on the command line using the
resource/name syntax:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete deployments/my-nginx services/my-nginx-svc
</span></span></code></pre></div><p>For larger numbers of resources, you'll find it easier to specify the selector (label query)
specified using <code>-l</code> or <code>--selector</code>, to filter resources by their labels:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl delete deployment,services -l <span>app</span><span>=</span>nginx
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps "my-nginx" deleted
service "my-nginx-svc" deleted
</code></pre><h3 id="chaining-and-filtering">Chaining and filtering</h3><p>Because <code>kubectl</code> outputs resource names in the same syntax it accepts, you can chain operations
using <code>$()</code> or <code>xargs</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get <span>$(</span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service/ <span>)</span>
</span></span><span><span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service/ | xargs -i kubectl get <span>'{}'</span>
</span></span></code></pre></div><p>The output might be similar to:</p><pre tabindex="0"><code class="language-none">NAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)      AGE
my-nginx-svc   LoadBalancer   10.0.0.208   &lt;pending&gt;     80/TCP       0s
</code></pre><p>With the above commands, first you create resources under <code>docs/concepts/cluster-administration/nginx/</code> and print
the resources created with <code>-o name</code> output format (print each resource as resource/name).
Then you <code>grep</code> only the Service, and then print it with <a href="/docs/reference/kubectl/generated/kubectl_get/"><code>kubectl get</code></a>.</p><h3 id="recursive-operations-on-local-files">Recursive operations on local files</h3><p>If you happen to organize your resources across several subdirectories within a particular
directory, you can recursively perform the operations on the subdirectories also, by specifying
<code>--recursive</code> or <code>-R</code> alongside the <code>--filename</code>/<code>-f</code> argument.</p><p>For instance, assume there is a directory <code>project/k8s/development</code> that holds all of the
<a class="glossary-tooltip" title="A serialized specification of one or more Kubernetes API objects." href="/docs/reference/glossary/?all=true#term-manifest" target="_blank">manifests</a> needed for the development environment,
organized by resource type:</p><pre tabindex="0"><code class="language-none">project/k8s/development
&#9500;&#9472;&#9472; configmap
&#9474;&#160;&#160; &#9492;&#9472;&#9472; my-configmap.yaml
&#9500;&#9472;&#9472; deployment
&#9474;&#160;&#160; &#9492;&#9472;&#9472; my-deployment.yaml
&#9492;&#9472;&#9472; pvc
    &#9492;&#9472;&#9472; my-pvc.yaml
</code></pre><p>By default, performing a bulk operation on <code>project/k8s/development</code> will stop at the first level
of the directory, not processing any subdirectories. If you had tried to create the resources in
this directory using the following command, we would have encountered an error:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f project/k8s/development
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">error: you must provide one or more resources by argument or filename (.json|.yaml|.yml|stdin)
</code></pre><p>Instead, specify the <code>--recursive</code> or <code>-R</code> command line argument along with the <code>--filename</code>/<code>-f</code> argument:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f project/k8s/development --recursive
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">configmap/my-config created
deployment.apps/my-deployment created
persistentvolumeclaim/my-pvc created
</code></pre><p>The <code>--recursive</code> argument works with any operation that accepts the <code>--filename</code>/<code>-f</code> argument such as:
<code>kubectl create</code>, <code>kubectl get</code>, <code>kubectl delete</code>, <code>kubectl describe</code>, or even <code>kubectl rollout</code>.</p><p>The <code>--recursive</code> argument also works when multiple <code>-f</code> arguments are provided:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">namespace/development created
namespace/staging created
configmap/my-config created
deployment.apps/my-deployment created
persistentvolumeclaim/my-pvc created
</code></pre><p>If you're interested in learning more about <code>kubectl</code>, go ahead and read
<a href="/docs/reference/kubectl/">Command line tool (kubectl)</a>.</p><h2 id="updating-your-application-without-an-outage">Updating your application without an outage</h2><p>At some point, you'll eventually need to update your deployed application, typically by specifying
a new image or image tag. <code>kubectl</code> supports several update operations, each of which is applicable
to different scenarios.</p><p>You can run multiple copies of your app, and use a <em>rollout</em> to gradually shift the traffic to
new healthy Pods. Eventually, all the running Pods would have the new software.</p><p>This section of the page guides you through how to create and update applications with Deployments.</p><p>Let's say you were running version 1.14.2 of nginx:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create deployment my-nginx --image<span>=</span>nginx:1.14.2
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/my-nginx created
</code></pre><p>Ensure that there is 1 replica:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale --replicas <span>1</span> deployments/my-nginx --subresource<span>=</span><span>'scale'</span> --type<span>=</span><span>'merge'</span> -p <span>'{"spec":{"replicas": 1}}'</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/my-nginx scaled
</code></pre><p>and allow Kubernetes to add more temporary replicas during a rollout, by setting a <em>surge maximum</em> of
100%:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl patch --type<span>=</span><span>'merge'</span> -p <span>'{"spec":{"strategy":{"rollingUpdate":{"maxSurge": "100%" }}}}'</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/my-nginx patched
</code></pre><p>To update to version 1.16.1, change <code>.spec.template.spec.containers[0].image</code> from <code>nginx:1.14.2</code>
to <code>nginx:1.16.1</code> using <code>kubectl edit</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit deployment/my-nginx
</span></span><span><span><span># Change the manifest to use the newer container image, then save your changes</span>
</span></span></code></pre></div><p>That's it! The Deployment will declaratively update the deployed nginx application progressively
behind the scene. It ensures that only a certain number of old replicas may be down while they are
being updated, and only a certain number of new replicas may be created above the desired number
of pods. To learn more details about how this happens,
visit <a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a>.</p><p>You can use rollouts with DaemonSets, Deployments, or StatefulSets.</p><h3 id="managing-rollouts">Managing rollouts</h3><p>You can use <a href="/docs/reference/kubectl/generated/kubectl_rollout/"><code>kubectl rollout</code></a> to manage a
progressive update of an existing application.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f my-deployment.yaml
</span></span><span><span>
</span></span><span><span><span># wait for rollout to finish</span>
</span></span><span><span>kubectl rollout status deployment/my-deployment --timeout 10m <span># 10 minute timeout</span>
</span></span></code></pre></div><p>or</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f backing-stateful-component.yaml
</span></span><span><span>
</span></span><span><span><span># don't wait for rollout to finish, just check the status</span>
</span></span><span><span>kubectl rollout status statefulsets/backing-stateful-component --watch<span>=</span><span>false</span>
</span></span></code></pre></div><p>You can also pause, resume or cancel a rollout.
Visit <a href="/docs/reference/kubectl/generated/kubectl_rollout/"><code>kubectl rollout</code></a> to learn more.</p><h2 id="canary-deployments">Canary deployments</h2><p>Another scenario where multiple labels are needed is to distinguish deployments of different
releases or configurations of the same component. It is common practice to deploy a <em>canary</em> of a
new application release (specified via image tag in the pod template) side by side with the
previous release so that the new release can receive live production traffic before fully rolling
it out.</p><p>For instance, you can use a <code>track</code> label to differentiate different releases.</p><p>The primary, stable release would have a <code>track</code> label with value as <code>stable</code>:</p><pre tabindex="0"><code class="language-none">name: frontend
replicas: 3
...
labels:
   app: guestbook
   tier: frontend
   track: stable
...
image: gb-frontend:v3
</code></pre><p>and then you can create a new release of the guestbook frontend that carries the <code>track</code> label
with different value (i.e. <code>canary</code>), so that two sets of pods would not overlap:</p><pre tabindex="0"><code class="language-none">name: frontend-canary
replicas: 1
...
labels:
   app: guestbook
   tier: frontend
   track: canary
...
image: gb-frontend:v4
</code></pre><p>The frontend service would span both sets of replicas by selecting the common subset of their
labels (i.e. omitting the <code>track</code> label), so that the traffic will be redirected to both
applications:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>selector</span>:<span>
</span></span></span><span><span><span>   </span><span>app</span>:<span> </span>guestbook<span>
</span></span></span><span><span><span>   </span><span>tier</span>:<span> </span>frontend<span>
</span></span></span></code></pre></div><p>You can tweak the number of replicas of the stable and canary releases to determine the ratio of
each release that will receive live production traffic (in this case, 3:1).
Once you're confident, you can update the stable track to the new application release and remove
the canary one.</p><h2 id="updating-annotations">Updating annotations</h2><p>Sometimes you would want to attach annotations to resources. Annotations are arbitrary
non-identifying metadata for retrieval by API clients such as tools or libraries.
This can be done with <code>kubectl annotate</code>. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl annotate pods my-nginx-v4-9gw19 <span>description</span><span>=</span><span>'my frontend running nginx'</span>
</span></span><span><span>kubectl get pods my-nginx-v4-9gw19 -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>apiVersion: v1
</span></span><span><span>kind: pod
</span></span><span><span>metadata:
</span></span><span><span>  annotations:
</span></span><span><span>    description: my frontend running nginx
</span></span><span><span>...
</span></span></code></pre></div><p>For more information, see <a href="/docs/concepts/overview/working-with-objects/annotations/">annotations</a>
and <a href="/docs/reference/kubectl/generated/kubectl_annotate/">kubectl annotate</a>.</p><h2 id="scaling-your-application">Scaling your application</h2><p>When load on your application grows or shrinks, use <code>kubectl</code> to scale your application.
For instance, to decrease the number of nginx replicas from 3 to 1, do:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl scale deployment/my-nginx --replicas<span>=</span><span>1</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/my-nginx scaled
</code></pre><p>Now you only have one pod managed by the deployment.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get pods -l <span>app</span><span>=</span>my-nginx
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                        READY     STATUS    RESTARTS   AGE
my-nginx-2035384211-j5fhi   1/1       Running   0          30m
</code></pre><p>To have the system automatically choose the number of nginx replicas as needed,
ranging from 1 to 3, do:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># This requires an existing source of container and Pod metrics</span>
</span></span><span><span>kubectl autoscale deployment/my-nginx --min<span>=</span><span>1</span> --max<span>=</span><span>3</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">horizontalpodautoscaler.autoscaling/my-nginx autoscaled
</code></pre><p>Now your nginx replicas will be scaled up and down as needed, automatically.</p><p>For more information, please see <a href="/docs/reference/kubectl/generated/kubectl_scale/">kubectl scale</a>,
<a href="/docs/reference/kubectl/generated/kubectl_autoscale/">kubectl autoscale</a> and
<a href="/docs/tasks/run-application/horizontal-pod-autoscale/">horizontal pod autoscaler</a> document.</p><h2 id="in-place-updates-of-resources">In-place updates of resources</h2><p>Sometimes it's necessary to make narrow, non-disruptive updates to resources you've created.</p><h3 id="kubectl-apply">kubectl apply</h3><p>It is suggested to maintain a set of configuration files in source control
(see <a href="https://martinfowler.com/bliki/InfrastructureAsCode.html">configuration as code</a>),
so that they can be maintained and versioned along with the code for the resources they configure.
Then, you can use <a href="/docs/reference/kubectl/generated/kubectl_apply/"><code>kubectl apply</code></a>
to push your configuration changes to the cluster.</p><p>This command will compare the version of the configuration that you're pushing with the previous
version and apply the changes you've made, without overwriting any automated changes to properties
you haven't specified.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/my-nginx configured
</code></pre><p>To learn more about the underlying mechanism, read <a href="/docs/reference/using-api/server-side-apply/">server-side apply</a>.</p><h3 id="kubectl-edit">kubectl edit</h3><p>Alternatively, you may also update resources with <a href="/docs/reference/kubectl/generated/kubectl_edit/"><code>kubectl edit</code></a>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit deployment/my-nginx
</span></span></code></pre></div><p>This is equivalent to first <code>get</code> the resource, edit it in text editor, and then <code>apply</code> the
resource with the updated version:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get deployment my-nginx -o yaml &gt; /tmp/nginx.yaml
</span></span><span><span>vi /tmp/nginx.yaml
</span></span><span><span><span># do some edit, and then save the file</span>
</span></span><span><span>
</span></span><span><span>kubectl apply -f /tmp/nginx.yaml
</span></span><span><span>deployment.apps/my-nginx configured
</span></span><span><span>
</span></span><span><span>rm /tmp/nginx.yaml
</span></span></code></pre></div><p>This allows you to do more significant changes more easily. Note that you can specify the editor
with your <code>EDITOR</code> or <code>KUBE_EDITOR</code> environment variables.</p><p>For more information, please see <a href="/docs/reference/kubectl/generated/kubectl_edit/">kubectl edit</a>.</p><h3 id="kubectl-patch">kubectl patch</h3><p>You can use <a href="/docs/reference/kubectl/generated/kubectl_patch/"><code>kubectl patch</code></a> to update API objects in place.
This subcommand supports JSON patch,
JSON merge patch, and strategic merge patch.</p><p>See
<a href="/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">Update API Objects in Place Using kubectl patch</a>
for more details.</p><h2 id="disruptive-updates">Disruptive updates</h2><p>In some cases, you may need to update resource fields that cannot be updated once initialized, or
you may want to make a recursive change immediately, such as to fix broken pods created by a
Deployment. To change such fields, use <code>replace --force</code>, which deletes and re-creates the
resource. In this case, you can modify your original configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">deployment.apps/my-nginx deleted
deployment.apps/my-nginx replaced
</code></pre><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/tasks/debug/debug-application/debug-running-pod/">how to use <code>kubectl</code> for application introspection and debugging</a>.</li></ul></div></div><div><div class="td-content"><h1>Services, Load Balancing, and Networking</h1><div class="lead">Concepts and resources behind networking in Kubernetes.</div><h2 id="the-kubernetes-network-model">The Kubernetes network model</h2><p>The Kubernetes network model is built out of several pieces:</p><ul><li><p>Each <a href="/docs/concepts/workloads/pods/">pod</a> in a cluster gets its
own unique cluster-wide IP address.</p><ul><li>A pod has its own private network namespace which is shared by
all of the containers within the pod. Processes running in
different containers in the same pod can communicate with each
other over <code>localhost</code>.</li></ul></li><li><p>The <em>pod network</em> (also called a cluster network) handles communication
between pods. It ensures that (barring intentional network segmentation):</p><ul><li><p>All pods can communicate with all other pods, whether they are
on the same <a href="/docs/concepts/architecture/nodes/">node</a> or on
different nodes. Pods can communicate with each other
directly, without the use of proxies or address translation (NAT).</p><p>On Windows, this rule does not apply to host-network pods.</p></li><li><p>Agents on a node (such as system daemons, or kubelet) can
communicate with all pods on that node.</p></li></ul></li><li><p>The <a href="/docs/concepts/services-networking/service/">Service</a> API
lets you provide a stable (long lived) IP address or hostname for a service implemented
by one or more backend pods, where the individual pods making up
the service can change over time.</p><ul><li><p>Kubernetes automatically manages
<a href="/docs/concepts/services-networking/endpoint-slices/">EndpointSlice</a>
objects to provide information about the pods currently backing a Service.</p></li><li><p>A service proxy implementation monitors the set of Service and
EndpointSlice objects, and programs the data plane to route
service traffic to its backends, by using operating system or
cloud provider APIs to intercept or rewrite packets.</p></li></ul></li><li><p>The <a href="/docs/concepts/services-networking/gateway/">Gateway</a> API
(or its predecessor, <a href="/docs/concepts/services-networking/ingress/">Ingress</a>)
allows you to make Services accessible to clients that are outside the cluster.</p><ul><li>A simpler, but less-configurable, mechanism for cluster
ingress is available via the Service API's
<a href="/docs/concepts/services-networking/service/#loadbalancer"><code>type: LoadBalancer</code></a>,
when using a supported <a class="glossary-tooltip" title="An organization that offers a cloud computing platform." href="/docs/reference/glossary/?all=true#term-cloud-provider" target="_blank">Cloud Provider</a>.</li></ul></li><li><p><a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy</a> is a built-in
Kubernetes API that allows you to control traffic between pods, or between pods and
the outside world.</p></li></ul><p>In older container systems, there was no automatic connectivity
between containers on different hosts, and so it was often necessary
to explicitly create links between containers, or to map container
ports to host ports to make them reachable by containers on other
hosts. This is not needed in Kubernetes; Kubernetes's model is that
pods can be treated much like VMs or physical hosts from the
perspectives of port allocation, naming, service discovery, load
balancing, application configuration, and migration.</p><p>Only a few parts of this model are implemented by Kubernetes itself.
For the other parts, Kubernetes defines the APIs, but the
corresponding functionality is provided by external components, some
of which are optional:</p><ul><li><p>Pod network namespace setup is handled by system-level software implementing the
<a href="/docs/concepts/containers/cri/">Container Runtime Interface</a>.</p></li><li><p>The pod network itself is managed by a
<a href="/docs/concepts/cluster-administration/addons/#networking-and-network-policy">pod network implementation</a>.
On Linux, most container runtimes use the
<a class="glossary-tooltip" title="Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification." href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank">Container Networking Interface (CNI)</a>
to interact with the pod network implementation, so these
implementations are often called <em>CNI plugins</em>.</p></li><li><p>Kubernetes provides a default implementation of service proxying,
called <a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank">kube-proxy</a>, but some pod
network implementations instead use their own service proxy that
is more tightly integrated with the rest of the implementation.</p></li><li><p>NetworkPolicy is generally also implemented by the pod network
implementation. (Some simpler pod network implementations don't
implement NetworkPolicy, or an administrator may choose to
configure the pod network without NetworkPolicy support. In these
cases, the API will still be present, but it will have no effect.)</p></li><li><p>There are many <a href="https://gateway-api.sigs.k8s.io/implementations/">implementations of the Gateway API</a>,
some of which are specific to particular cloud environments, some more
focused on "bare metal" environments, and others more generic.</p></li></ul><h2 id="what-s-next">What's next</h2><p>The <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a>
tutorial lets you learn about Services and Kubernetes networking with a hands-on example.</p><p><a href="/docs/concepts/cluster-administration/networking/">Cluster Networking</a> explains how to set
up networking for your cluster, and also provides an overview of the technologies involved.</p><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/concepts/services-networking/service/">Service</a></h5><p>Expose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/ingress/">Ingress</a></h5><p>Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/ingress-controllers/">Ingress Controllers</a></h5><p>In order for an <a href="/docs/concepts/services-networking/ingress/">Ingress</a> to work in your cluster, there must be an <em>ingress controller</em> running. You need to select at least one ingress controller and make sure it is set up in your cluster. This page lists common ingress controllers that you can deploy.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/gateway/">Gateway API</a></h5><p>Gateway API is a family of API kinds that provide dynamic infrastructure provisioning and advanced traffic routing.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a></h5><p>The EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to handle large numbers of backends, and allows the cluster to update its list of healthy backends efficiently.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/network-policies/">Network Policies</a></h5><p>If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), NetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between Pods and the outside world. Your cluster must use a network plugin that supports NetworkPolicy enforcement.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/dns-pod-service/">DNS for Services and Pods</a></h5><p>Your workload can discover Services within your cluster using DNS; this page explains how that works.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/dual-stack/">IPv4/IPv6 dual-stack</a></h5><p>Kubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or dual stack networking with both network families active. This page explains how.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/topology-aware-routing/">Topology Aware Routing</a></h5><p><em>Topology Aware Routing</em> provides a mechanism to help keep network traffic within the zone where it originated. Preferring same-zone traffic between Pods in your cluster can help with reliability, performance (network latency and throughput), or cost.</p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/windows-networking/">Networking on Windows</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/cluster-ip-allocation/">Service ClusterIP allocation</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/services-networking/service-traffic-policy/">Service Internal Traffic Policy</a></h5><p>If two Pods in your cluster want to communicate, and both Pods are actually running on the same node, use <em>Service Internal Traffic Policy</em> to keep network traffic within that node. Avoiding a round trip via the cluster network can help with reliability, performance (network latency and throughput), or cost.</p></div></div></div></div><div><div class="td-content"><h1>Service</h1><div class="lead">Expose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends.</div><p>In Kubernetes, a Service is a method for exposing a network application that is running as one or more
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> in your cluster.</p><p>A key aim of Services in Kubernetes is that you don't need to modify your existing
application to use an unfamiliar service discovery mechanism.
You can run code in Pods, whether this is a code designed for a cloud-native world, or
an older app you've containerized. You use a Service to make that set of Pods available
on the network so that clients can interact with it.</p><p>If you use a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a> to run your app,
that Deployment can create and destroy Pods dynamically. From one moment to the next,
you don't know how many of those Pods are working and healthy; you might not even know
what those healthy Pods are named.
Kubernetes <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> are created and destroyed
to match the desired state of your cluster. Pods are ephemeral resources (you should not
expect that an individual Pod is reliable and durable).</p><p>Each Pod gets its own IP address (Kubernetes expects network plugins to ensure this).
For a given Deployment in your cluster, the set of Pods running in one moment in
time could be different from the set of Pods running that application a moment later.</p><p>This leads to a problem: if some set of Pods (call them "backends") provides
functionality to other Pods (call them "frontends") inside your cluster,
how do the frontends find out and keep track of which IP address to connect
to, so that the frontend can use the backend part of the workload?</p><p>Enter <em>Services</em>.</p><h2 id="services-in-kubernetes">Services in Kubernetes</h2><p>The Service API, part of Kubernetes, is an abstraction to help you expose groups of
Pods over a network. Each Service object defines a logical set of endpoints (usually
these endpoints are Pods) along with a policy about how to make those pods accessible.</p><p>For example, consider a stateless image-processing backend which is running with
3 replicas. Those replicas are fungible&#8212;frontends do not care which backend
they use. While the actual Pods that compose the backend set may change, the
frontend clients should not need to be aware of that, nor should they need to keep
track of the set of backends themselves.</p><p>The Service abstraction enables this decoupling.</p><p>The set of Pods targeted by a Service is usually determined
by a <a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." href="/docs/concepts/overview/working-with-objects/labels/" target="_blank">selector</a> that you
define.
To learn about other ways to define Service endpoints,
see <a href="#services-without-selectors">Services <em>without</em> selectors</a>.</p><p>If your workload speaks HTTP, you might choose to use an
<a href="/docs/concepts/services-networking/ingress/">Ingress</a> to control how web traffic
reaches that workload.
Ingress is not a Service type, but it acts as the entry point for your
cluster. An Ingress lets you consolidate your routing rules into a single resource, so
that you can expose multiple components of your workload, running separately in your
cluster, behind a single listener.</p><p>The <a href="https://gateway-api.sigs.k8s.io/#what-is-the-gateway-api">Gateway</a> API for Kubernetes
provides extra capabilities beyond Ingress and Service. You can add Gateway to your cluster -
it is a family of extension APIs, implemented using
<a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinitions</a> -
and then use these to configure access to network services that are running in your cluster.</p><h3 id="cloud-native-service-discovery">Cloud-native service discovery</h3><p>If you're able to use Kubernetes APIs for service discovery in your application,
you can query the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a>
for matching EndpointSlices. Kubernetes updates the EndpointSlices for a Service
whenever the set of Pods in a Service changes.</p><p>For non-native applications, Kubernetes offers ways to place a network port or load
balancer in between your application and the backend Pods.</p><p>Either way, your workload can use these <a href="#discovering-services">service discovery</a>
mechanisms to find the target it wants to connect to.</p><h2 id="defining-a-service">Defining a Service</h2><p>A Service is an <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">object</a>
(the same way that a Pod or a ConfigMap is an object). You can create,
view or modify Service definitions using the Kubernetes API. Usually
you use a tool such as <code>kubectl</code> to make those API calls for you.</p><p>For example, suppose you have a set of Pods that each listen on TCP port 9376
and are labelled as <code>app.kubernetes.io/name=MyApp</code>. You can define a Service to
publish that TCP listener:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/simple-service.yaml"><code>service/simple-service.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/simple-service.yaml to clipboard"></div><div class="includecode" id="service-simple-service-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>9376</span><span>
</span></span></span></code></pre></div></div></div><p>Applying this manifest creates a new Service named "my-service" with the default
ClusterIP <a href="#publishing-services-service-types">service type</a>. The Service
targets TCP port 9376 on any Pod with the <code>app.kubernetes.io/name: MyApp</code> label.</p><p>Kubernetes assigns this Service an IP address (the <em>cluster IP</em>),
that is used by the virtual IP address mechanism. For more details on that mechanism,
read <a href="/docs/reference/networking/virtual-ips/">Virtual IPs and Service Proxies</a>.</p><p>The controller for that Service continuously scans for Pods that
match its selector, and then makes any necessary updates to the set of
EndpointSlices for the Service.</p><p>The name of a Service object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#rfc-1035-label-names">RFC 1035 label name</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A Service can map <em>any</em> incoming <code>port</code> to a <code>targetPort</code>. By default and
for convenience, the <code>targetPort</code> is set to the same value as the <code>port</code>
field.</div><h3 id="relaxed-naming-requirements-for-service-objects">Relaxed naming requirements for Service objects</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: RelaxedServiceNameValidation"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>The <code>RelaxedServiceNameValidation</code> feature gate allows Service object names to start with a digit. When this feature gate is enabled, Service object names must be valid <a href="/docs/concepts/overview/working-with-objects/names/#dns-label-names">RFC 1123 label names</a>.</p><h3 id="field-spec-ports">Port definitions</h3><p>Port definitions in Pods have names, and you can reference these names in the
<code>targetPort</code> attribute of a Service. For example, we can bind the <code>targetPort</code>
of the Service to the Pod port in the following way:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>proxy<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx:stable<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>      </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>http-web-svc<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>proxy<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>name-of-service-port<span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span>http-web-svc<span>
</span></span></span></code></pre></div><p>This works even if there is a mixture of Pods in the Service using a single
configured name, with the same network protocol available via different
port numbers. This offers a lot of flexibility for deploying and evolving
your Services. For example, you can change the port numbers that Pods expose
in the next version of your backend software, without breaking clients.</p><p>The default protocol for Services is
<a href="/docs/reference/networking/service-protocols/#protocol-tcp">TCP</a>; you can also
use any other <a href="/docs/reference/networking/service-protocols/">supported protocol</a>.</p><p>Because many Services need to expose more than one port, Kubernetes supports
<a href="#multi-port-services">multiple port definitions</a> for a single Service.
Each port definition can have the same <code>protocol</code>, or a different one.</p><h3 id="services-without-selectors">Services without selectors</h3><p>Services most commonly abstract access to Kubernetes Pods thanks to the selector,
but when used with a corresponding set of
<a class="glossary-tooltip" title="EndpointSlices track the IP addresses of Pods for Services." href="/docs/concepts/services-networking/endpoint-slices/" target="_blank">EndpointSlices</a>
objects and without a selector, the Service can abstract other kinds of backends,
including ones that run outside the cluster.</p><p>For example:</p><ul><li>You want to have an external database cluster in production, but in your
test environment you use your own databases.</li><li>You want to point your Service to a Service in a different
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">Namespace</a> or on another cluster.</li><li>You are migrating a workload to Kubernetes. While evaluating the approach,
you run only a portion of your backends in Kubernetes.</li></ul><p>In any of these scenarios you can define a Service <em>without</em> specifying a
selector to match Pods. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>http<span>
</span></span></span><span><span><span>      </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>9376</span><span>
</span></span></span></code></pre></div><p>Because this Service has no selector, the corresponding EndpointSlice
objects are not created automatically. You can map the Service
to the network address and port where it's running, by adding an EndpointSlice
object manually. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>discovery.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EndpointSlice<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service-1<span> </span><span># by convention, use the name of the Service</span><span>
</span></span></span><span><span><span>                     </span><span># as a prefix for the name of the EndpointSlice</span><span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span># You should set the "kubernetes.io/service-name" label.</span><span>
</span></span></span><span><span><span>    </span><span># Set its value to match the name of the Service</span><span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/service-name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>addressType</span>:<span> </span>IPv4<span>
</span></span></span><span><span><span></span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>http<span> </span><span># should match with the name of the service port defined above</span><span>
</span></span></span><span><span><span>    </span><span>appProtocol</span>:<span> </span>http<span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>9376</span><span>
</span></span></span><span><span><span></span><span>endpoints</span>:<span>
</span></span></span><span><span><span>  </span>- <span>addresses</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"10.4.5.6"</span><span>
</span></span></span><span><span><span>  </span>- <span>addresses</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"10.1.2.3"</span><span>
</span></span></span></code></pre></div><h4 id="custom-endpointslices">Custom EndpointSlices</h4><p>When you create an <a href="#endpointslices">EndpointSlice</a> object for a Service, you can
use any name for the EndpointSlice. Each EndpointSlice in a namespace must have a
unique name. You link an EndpointSlice to a Service by setting the
<code>kubernetes.io/service-name</code> <a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">label</a>
on that EndpointSlice.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The endpoint IPs <em>must not</em> be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or
link-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6).</p><p>The endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services,
because <a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank">kube-proxy</a> doesn't support virtual IPs
as a destination.</p></div><p>For an EndpointSlice that you create yourself, or in your own code,
you should also pick a value to use for the label
<a href="/docs/reference/labels-annotations-taints/#endpointslicekubernetesiomanaged-by"><code>endpointslice.kubernetes.io/managed-by</code></a>.
If you create your own controller code to manage EndpointSlices, consider using a
value similar to <code>"my-domain.example/name-of-controller"</code>. If you are using a third
party tool, use the name of the tool in all-lowercase and change spaces and other
punctuation to dashes (<code>-</code>).
If people are directly using a tool such as <code>kubectl</code> to manage EndpointSlices,
use a name that describes this manual management, such as <code>"staff"</code> or
<code>"cluster-admins"</code>. You should
avoid using the reserved value <code>"controller"</code>, which identifies EndpointSlices
managed by Kubernetes' own control plane.</p><h4 id="service-no-selector-access">Accessing a Service without a selector</h4><p>Accessing a Service without a selector works the same as if it had a selector.
In the <a href="#services-without-selectors">example</a> for a Service without a selector,
traffic is routed to one of the two endpoints defined in
the EndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The Kubernetes API server does not allow proxying to endpoints that are not mapped to
pods. Actions such as <code>kubectl port-forward service/&lt;service-name&gt; forwardedPort:servicePort</code> where the service has no
selector will fail due to this constraint. This prevents the Kubernetes API server
from being used as a proxy to endpoints the caller may not be authorized to access.</div><p>An <code>ExternalName</code> Service is a special case of Service that does not have
selectors and uses DNS names instead. For more information, see the
<a href="#externalname">ExternalName</a> section.</p><h3 id="endpointslices">EndpointSlices</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p><a href="/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a> are objects that
represent a subset (a <em>slice</em>) of the backing network endpoints for a Service.</p><p>Your Kubernetes cluster tracks how many endpoints each EndpointSlice represents.
If there are so many endpoints for a Service that a threshold is reached, then
Kubernetes adds another empty EndpointSlice and stores new endpoint information
there.
By default, Kubernetes makes a new EndpointSlice once the existing EndpointSlices
all contain at least 100 endpoints. Kubernetes does not make the new EndpointSlice
until an extra endpoint needs to be added.</p><p>See <a href="/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a> for more
information about this API.</p><h3 id="endpoints">Endpoints (deprecated)</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [deprecated]</code></div><p>The EndpointSlice API is the evolution of the older
<a href="/docs/reference/kubernetes-api/service-resources/endpoints-v1/">Endpoints</a>
API. The deprecated Endpoints API has several problems relative to
EndpointSlice:</p><ul><li>It does not support dual-stack clusters.</li><li>It does not contain information needed to support newer features, such as
<a href="/docs/concepts/services-networking/service/#traffic-distribution">trafficDistribution</a>.</li><li>It will truncate the list of endpoints if it is too long to fit in a single object.</li></ul><p>Because of this, it is recommended that all clients use the
EndpointSlice API rather than Endpoints.</p><h4 id="over-capacity-endpoints">Over-capacity endpoints</h4><p>Kubernetes limits the number of endpoints that can fit in a single Endpoints
object. When there are over 1000 backing endpoints for a Service, Kubernetes
truncates the data in the Endpoints object. Because a Service can be linked
with more than one EndpointSlice, the 1000 backing endpoint limit only
affects the legacy Endpoints API.</p><p>In that case, Kubernetes selects at most 1000 possible backend endpoints to store
into the Endpoints object, and sets an
<a class="glossary-tooltip" title="A key-value pair that is used to attach arbitrary non-identifying metadata to objects." href="/docs/concepts/overview/working-with-objects/annotations" target="_blank">annotation</a> on the Endpoints:
<a href="/docs/reference/labels-annotations-taints/#endpoints-kubernetes-io-over-capacity"><code>endpoints.kubernetes.io/over-capacity: truncated</code></a>.
The control plane also removes that annotation if the number of backend Pods drops below 1000.</p><p>Traffic is still sent to backends, but any load balancing mechanism that relies on the
legacy Endpoints API only sends traffic to at most 1000 of the available backing endpoints.</p><p>The same API limit means that you cannot manually update an Endpoints to have more than 1000 endpoints.</p><h3 id="application-protocol">Application protocol</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><p>The <code>appProtocol</code> field provides a way to specify an application protocol for
each Service port. This is used as a hint for implementations to offer
richer behavior for protocols that they understand.
The value of this field is mirrored by the corresponding
Endpoints and EndpointSlice objects.</p><p>This field follows standard Kubernetes label syntax. Valid values are one of:</p><ul><li><p><a href="https://www.iana.org/assignments/service-names">IANA standard service names</a>.</p></li><li><p>Implementation-defined prefixed names such as <code>mycompany.com/my-custom-protocol</code>.</p></li><li><p>Kubernetes-defined prefixed names:</p></li></ul><table><thead><tr><th>Protocol</th><th>Description</th></tr></thead><tbody><tr><td><code>kubernetes.io/h2c</code></td><td>HTTP/2 over cleartext as described in <a href="https://www.rfc-editor.org/rfc/rfc7540">RFC 7540</a></td></tr><tr><td><code>kubernetes.io/ws</code></td><td>WebSocket over cleartext as described in <a href="https://www.rfc-editor.org/rfc/rfc6455">RFC 6455</a></td></tr><tr><td><code>kubernetes.io/wss</code></td><td>WebSocket over TLS as described in <a href="https://www.rfc-editor.org/rfc/rfc6455">RFC 6455</a></td></tr></tbody></table><h3 id="multi-port-services">Multi-port Services</h3><p>For some Services, you need to expose more than one port.
Kubernetes lets you configure multiple port definitions on a Service object.
When using multiple ports for a Service, you must give all of your ports names
so that these are unambiguous.
For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>http<span>
</span></span></span><span><span><span>      </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>9376</span><span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>https<span>
</span></span></span><span><span><span>      </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>443</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>9377</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>As with Kubernetes <a class="glossary-tooltip" title="A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name." href="/docs/concepts/overview/working-with-objects/names" target="_blank">names</a> in general, names for ports
must only contain lowercase alphanumeric characters and <code>-</code>. Port names must
also start and end with an alphanumeric character.</p><p>For example, the names <code>123-abc</code> and <code>web</code> are valid, but <code>123_abc</code> and <code>-web</code> are not.</p></div><h2 id="publishing-services-service-types">Service type</h2><p>For some parts of your application (for example, frontends) you may want to expose a
Service onto an external IP address, one that's accessible from outside of your
cluster.</p><p>Kubernetes Service types allow you to specify what kind of Service you want.</p><p>The available <code>type</code> values and their behaviors are:</p><dl><dt><a href="#type-clusterip"><code>ClusterIP</code></a></dt><dd>Exposes the Service on a cluster-internal IP. Choosing this value
makes the Service only reachable from within the cluster. This is the
default that is used if you don't explicitly specify a <code>type</code> for a Service.
You can expose the Service to the public internet using an
<a href="/docs/concepts/services-networking/ingress/">Ingress</a> or a
<a href="https://gateway-api.sigs.k8s.io/">Gateway</a>.</dd><dt><a href="#type-nodeport"><code>NodePort</code></a></dt><dd>Exposes the Service on each Node's IP at a static port (the <code>NodePort</code>).
To make the node port available, Kubernetes sets up a cluster IP address,
the same as if you had requested a Service of <code>type: ClusterIP</code>.</dd><dt><a href="#loadbalancer"><code>LoadBalancer</code></a></dt><dd>Exposes the Service externally using an external load balancer. Kubernetes
does not directly offer a load balancing component; you must provide one, or
you can integrate your Kubernetes cluster with a cloud provider.</dd><dt><a href="#externalname"><code>ExternalName</code></a></dt><dd>Maps the Service to the contents of the <code>externalName</code> field (for example,
to the hostname <code>api.foo.bar.example</code>). The mapping configures your cluster's
DNS server to return a <code>CNAME</code> record with that external hostname value.
No proxying of any kind is set up.</dd></dl><p>The <code>type</code> field in the Service API is designed as nested functionality - each level
adds to the previous. However there is an exception to this nested design. You can
define a <code>LoadBalancer</code> Service by
<a href="/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation">disabling the load balancer <code>NodePort</code> allocation</a>.</p><h3 id="type-clusterip"><code>type: ClusterIP</code></h3><p>This default Service type assigns an IP address from a pool of IP addresses that
your cluster has reserved for that purpose.</p><p>Several of the other types for Service build on the <code>ClusterIP</code> type as a
foundation.</p><p>If you define a Service that has the <code>.spec.clusterIP</code> set to <code>"None"</code> then
Kubernetes does not assign an IP address. See <a href="#headless-services">headless Services</a>
for more information.</p><h4 id="choosing-your-own-ip-address">Choosing your own IP address</h4><p>You can specify your own cluster IP address as part of a <code>Service</code> creation
request. To do this, set the <code>.spec.clusterIP</code> field. For example, if you
already have an existing DNS entry that you wish to reuse, or legacy systems
that are configured for a specific IP address and difficult to re-configure.</p><p>The IP address that you choose must be a valid IPv4 or IPv6 address from within the
<code>service-cluster-ip-range</code> CIDR range that is configured for the API server.
If you try to create a Service with an invalid <code>clusterIP</code> address value, the API
server will return a 422 HTTP status code to indicate that there's a problem.</p><p>Read <a href="/docs/reference/networking/virtual-ips/#avoiding-collisions">avoiding collisions</a>
to learn how Kubernetes helps reduce the risk and impact of two different Services
both trying to use the same IP address.</p><h3 id="type-nodeport"><code>type: NodePort</code></h3><p>If you set the <code>type</code> field to <code>NodePort</code>, the Kubernetes control plane
allocates a port from a range specified by <code>--service-node-port-range</code> flag (default: 30000-32767).
Each node proxies that port (the same port number on every Node) into your Service.
Your Service reports the allocated port in its <code>.spec.ports[*].nodePort</code> field.</p><p>Using a NodePort gives you the freedom to set up your own load balancing solution,
to configure environments that are not fully supported by Kubernetes, or even
to expose one or more nodes' IP addresses directly.</p><p>For a node port Service, Kubernetes additionally allocates a port (TCP, UDP or
SCTP to match the protocol of the Service). Every node in the cluster configures
itself to listen on that assigned port and to forward traffic to one of the ready
endpoints associated with that Service. You'll be able to contact the <code>type: NodePort</code>
Service, from outside the cluster, by connecting to any node using the appropriate
protocol (for example: TCP), and the appropriate port (as assigned to that Service).</p><h4 id="nodeport-custom-port">Choosing your own port</h4><p>If you want a specific port number, you can specify a value in the <code>nodePort</code>
field. The control plane will either allocate you that port or report that
the API transaction failed.
This means that you need to take care of possible port collisions yourself.
You also have to use a valid port number, one that's inside the range configured
for NodePort use.</p><p>Here is an example manifest for a Service of <code>type: NodePort</code> that specifies
a NodePort value (30007, in this example):</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>NodePort<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span># By default and for convenience, the `targetPort` is set to</span><span>
</span></span></span><span><span><span>      </span><span># the same value as the `port` field.</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span># Optional field</span><span>
</span></span></span><span><span><span>      </span><span># By default and for convenience, the Kubernetes control plane</span><span>
</span></span></span><span><span><span>      </span><span># will allocate a port from a range (default: 30000-32767)</span><span>
</span></span></span><span><span><span>      </span><span>nodePort</span>:<span> </span><span>30007</span><span>
</span></span></span></code></pre></div><h4 id="avoid-nodeport-collisions">Reserve Nodeport ranges to avoid collisions</h4><p>The policy for assigning ports to NodePort services applies to both the auto-assignment and
the manual assignment scenarios. When a user wants to create a NodePort service that
uses a specific port, the target port may conflict with another port that has already been assigned.</p><p>To avoid this problem, the port range for NodePort services is divided into two bands.
Dynamic port assignment uses the upper band by default, and it may use the lower band once the
upper band has been exhausted. Users can then allocate from the lower band with a lower risk of port collision.</p><h4 id="service-nodeport-custom-listen-address">Custom IP address configuration for <code>type: NodePort</code> Services</h4><p>You can set up nodes in your cluster to use a particular IP address for serving node port
services. You might want to do this if each node is connected to multiple networks (for example:
one network for application traffic, and another network for traffic between nodes and the
control plane).</p><p>If you want to specify particular IP address(es) to proxy the port, you can set the
<code>--nodeport-addresses</code> flag for kube-proxy or the equivalent <code>nodePortAddresses</code>
field of the <a href="/docs/reference/config-api/kube-proxy-config.v1alpha1/">kube-proxy configuration file</a>
to particular IP block(s).</p><p>This flag takes a comma-delimited list of IP blocks (e.g. <code>10.0.0.0/8</code>, <code>192.0.2.0/25</code>)
to specify IP address ranges that kube-proxy should consider as local to this node.</p><p>For example, if you start kube-proxy with the <code>--nodeport-addresses=127.0.0.0/8</code> flag,
kube-proxy only selects the loopback interface for NodePort Services.
The default for <code>--nodeport-addresses</code> is an empty list.
This means that kube-proxy should consider all available network interfaces for NodePort.
(That's also compatible with earlier Kubernetes releases.)<div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This Service is visible as <code>&lt;NodeIP&gt;:spec.ports[*].nodePort</code> and <code>.spec.clusterIP:spec.ports[*].port</code>.
If the <code>--nodeport-addresses</code> flag for kube-proxy or the equivalent field
in the kube-proxy configuration file is set, <code>&lt;NodeIP&gt;</code> would be a filtered
node IP address (or possibly IP addresses).</div></p><h3 id="loadbalancer"><code>type: LoadBalancer</code></h3><p>On cloud providers which support external load balancers, setting the <code>type</code>
field to <code>LoadBalancer</code> provisions a load balancer for your Service.
The actual creation of the load balancer happens asynchronously, and
information about the provisioned balancer is published in the Service's
<code>.status.loadBalancer</code> field.
For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>9376</span><span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span><span>10.0.171.239</span><span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>LoadBalancer<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>loadBalancer</span>:<span>
</span></span></span><span><span><span>    </span><span>ingress</span>:<span>
</span></span></span><span><span><span>    </span>- <span>ip</span>:<span> </span><span>192.0.2.127</span><span>
</span></span></span></code></pre></div><p>Traffic from the external load balancer is directed at the backend Pods. The cloud
provider decides how it is load balanced.</p><p>To implement a Service of <code>type: LoadBalancer</code>, Kubernetes typically starts off
by making the changes that are equivalent to you requesting a Service of
<code>type: NodePort</code>. The cloud-controller-manager component then configures the external
load balancer to forward traffic to that assigned node port.</p><p>You can configure a load balanced Service to
<a href="#load-balancer-nodeport-allocation">omit</a> assigning a node port, provided that the
cloud provider implementation supports this.</p><p>Some cloud providers allow you to specify the <code>loadBalancerIP</code>. In those cases, the load-balancer is created
with the user-specified <code>loadBalancerIP</code>. If the <code>loadBalancerIP</code> field is not specified,
the load balancer is set up with an ephemeral IP address. If you specify a <code>loadBalancerIP</code>
but your cloud provider does not support the feature, the <code>loadbalancerIP</code> field that you
set is ignored.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The<code>.spec.loadBalancerIP</code> field for a Service was deprecated in Kubernetes v1.24.</p><p>This field was under-specified and its meaning varies across implementations.
It also cannot support dual-stack networking. This field may be removed in a future API version.</p><p>If you're integrating with a provider that supports specifying the load balancer IP address(es)
for a Service via a (provider specific) annotation, you should switch to doing that.</p><p>If you are writing code for a load balancer integration with Kubernetes, avoid using this field.
You can integrate with <a href="https://gateway-api.sigs.k8s.io/">Gateway</a> rather than Service, or you
can define your own (provider specific) annotations on the Service that specify the equivalent detail.</p></div><h4 id="node-liveness-impact-on-load-balancer-traffic">Node liveness impact on load balancer traffic</h4><p>Load balancer health checks are critical to modern applications. They are used to
determine which server (virtual machine, or IP address) the load balancer should
dispatch traffic to. The Kubernetes APIs do not define how health checks have to be
implemented for Kubernetes managed load balancers, instead it's the cloud providers
(and the people implementing integration code) who decide on the behavior. Load
balancer health checks are extensively used within the context of supporting the
<code>externalTrafficPolicy</code> field for Services.</p><h4 id="load-balancers-with-mixed-protocol-types">Load balancers with mixed protocol types</h4><div class="feature-state-notice feature-stable" title="Feature Gate: MixedProtocolLBService"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code> (enabled by default: true)</div><p>By default, for LoadBalancer type of Services, when there is more than one port defined, all
ports must have the same protocol, and the protocol must be one which is supported
by the cloud provider.</p><p>The feature gate <code>MixedProtocolLBService</code> (enabled by default for the kube-apiserver as of v1.24) allows the use of
different protocols for LoadBalancer type of Services, when there is more than one port defined.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The set of protocols that can be used for load balanced Services is defined by your
cloud provider; they may impose restrictions beyond what the Kubernetes API enforces.</div><h4 id="load-balancer-nodeport-allocation">Disabling load balancer NodePort allocation</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>You can optionally disable node port allocation for a Service of <code>type: LoadBalancer</code>, by setting
the field <code>spec.allocateLoadBalancerNodePorts</code> to <code>false</code>. This should only be used for load balancer implementations
that route traffic directly to pods as opposed to using node ports. By default, <code>spec.allocateLoadBalancerNodePorts</code>
is <code>true</code> and type LoadBalancer Services will continue to allocate node ports. If <code>spec.allocateLoadBalancerNodePorts</code>
is set to <code>false</code> on an existing Service with allocated node ports, those node ports will <strong>not</strong> be de-allocated automatically.
You must explicitly remove the <code>nodePorts</code> entry in every Service port to de-allocate those node ports.</p><h4 id="load-balancer-class">Specifying class of load balancer implementation</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>For a Service with <code>type</code> set to <code>LoadBalancer</code>, the <code>.spec.loadBalancerClass</code> field
enables you to use a load balancer implementation other than the cloud provider default.</p><p>By default, <code>.spec.loadBalancerClass</code> is not set and a <code>LoadBalancer</code>
type of Service uses the cloud provider's default load balancer implementation if the
cluster is configured with a cloud provider using the <code>--cloud-provider</code> component
flag.</p><p>If you specify <code>.spec.loadBalancerClass</code>, it is assumed that a load balancer
implementation that matches the specified class is watching for Services.
Any default load balancer implementation (for example, the one provided by
the cloud provider) will ignore Services that have this field set.
<code>spec.loadBalancerClass</code> can be set on a Service of type <code>LoadBalancer</code> only.
Once set, it cannot be changed.
The value of <code>spec.loadBalancerClass</code> must be a label-style identifier,
with an optional prefix such as "<code>internal-vip</code>" or "<code>example.com/internal-vip</code>".
Unprefixed names are reserved for end-users.</p><h4 id="load-balancer-ip-mode">Load balancer IP address mode</h4><div class="feature-state-notice feature-stable" title="Feature Gate: LoadBalancerIPMode"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>For a Service of <code>type: LoadBalancer</code>, a controller can set <code>.status.loadBalancer.ingress.ipMode</code>.
The <code>.status.loadBalancer.ingress.ipMode</code> specifies how the load-balancer IP behaves.
It may be specified only when the <code>.status.loadBalancer.ingress.ip</code> field is also specified.</p><p>There are two possible values for <code>.status.loadBalancer.ingress.ipMode</code>: "VIP" and "Proxy".
The default value is "VIP" meaning that traffic is delivered to the node
with the destination set to the load-balancer's IP and port.
There are two cases when setting this to "Proxy", depending on how the load-balancer
from the cloud provider delivers the traffics:</p><ul><li>If the traffic is delivered to the node then DNATed to the pod, the destination would be set to the node's IP and node port;</li><li>If the traffic is delivered directly to the pod, the destination would be set to the pod's IP and port.</li></ul><p>Service implementations may use this information to adjust traffic routing.</p><h4 id="internal-load-balancer">Internal load balancer</h4><p>In a mixed environment it is sometimes necessary to route traffic from Services inside the same
(virtual) network address block.</p><p>In a split-horizon DNS environment you would need two Services to be able to route both external
and internal traffic to your endpoints.</p><p>To set an internal load balancer, add one of the following annotations to your Service
depending on the cloud service provider you're using:</p><ul class="nav nav-tabs" id="service-tabs"><li class="nav-item"><a class="nav-link active" href="#service-tabs-0">Default</a></li><li class="nav-item"><a class="nav-link" href="#service-tabs-1">GCP</a></li><li class="nav-item"><a class="nav-link" href="#service-tabs-2">AWS</a></li><li class="nav-item"><a class="nav-link" href="#service-tabs-3">Azure</a></li><li class="nav-item"><a class="nav-link" href="#service-tabs-4">IBM Cloud</a></li><li class="nav-item"><a class="nav-link" href="#service-tabs-5">OpenStack</a></li><li class="nav-item"><a class="nav-link" href="#service-tabs-6">Baidu Cloud</a></li><li class="nav-item"><a class="nav-link" href="#service-tabs-7">Tencent Cloud</a></li><li class="nav-item"><a class="nav-link" href="#service-tabs-8">Alibaba Cloud</a></li><li class="nav-item"><a class="nav-link" href="#service-tabs-9">OCI</a></li></ul><div class="tab-content" id="service-tabs"><div id="service-tabs-0" class="tab-pane show active"><p><p>Select one of the tabs.</p></p></div><div id="service-tabs-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>networking.gke.io/load-balancer-type</span>:<span> </span><span>"Internal"</span><span>
</span></span></span></code></pre></div></p></div><div id="service-tabs-2" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>service.beta.kubernetes.io/aws-load-balancer-scheme</span>:<span> </span><span>"internal"</span><span>
</span></span></span></code></pre></div></p></div><div id="service-tabs-3" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>service.beta.kubernetes.io/azure-load-balancer-internal</span>:<span> </span><span>"true"</span><span>
</span></span></span></code></pre></div></p></div><div id="service-tabs-4" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type</span>:<span> </span><span>"private"</span><span>
</span></span></span></code></pre></div></p></div><div id="service-tabs-5" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>service.beta.kubernetes.io/openstack-internal-load-balancer</span>:<span> </span><span>"true"</span><span>
</span></span></span></code></pre></div></p></div><div id="service-tabs-6" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>service.beta.kubernetes.io/cce-load-balancer-internal-vpc</span>:<span> </span><span>"true"</span><span>
</span></span></span></code></pre></div></p></div><div id="service-tabs-7" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>service.kubernetes.io/qcloud-loadbalancer-internal-subnetid</span>:<span> </span>subnet-xxxxx<span>
</span></span></span></code></pre></div></p></div><div id="service-tabs-8" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type</span>:<span> </span><span>"intranet"</span><span>
</span></span></span></code></pre></div></p></div><div id="service-tabs-9" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>service.beta.kubernetes.io/oci-load-balancer-internal</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div></p></div></div><h3 id="externalname"><code>type: ExternalName</code></h3><p>Services of type ExternalName map a Service to a DNS name, not to a typical selector such as
<code>my-service</code> or <code>cassandra</code>. You specify these Services with the <code>spec.externalName</code> parameter.</p><p>This Service definition, for example, maps
the <code>my-service</code> Service in the <code>prod</code> namespace to <code>my.database.example.com</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>prod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>ExternalName<span>
</span></span></span><span><span><span>  </span><span>externalName</span>:<span> </span>my.database.example.com<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>A Service of <code>type: ExternalName</code> accepts an IPv4 address string,
but treats that string as a DNS name comprised of digits,
not as an IP address (the internet does not however allow such names in DNS).
Services with external names that resemble IPv4
addresses are not resolved by DNS servers.</p><p>If you want to map a Service directly to a specific IP address, consider using
<a href="#headless-services">headless Services</a>.</p></div><p>When looking up the host <code>my-service.prod.svc.cluster.local</code>, the cluster DNS Service
returns a <code>CNAME</code> record with the value <code>my.database.example.com</code>. Accessing
<code>my-service</code> works in the same way as other Services but with the crucial
difference that redirection happens at the DNS level rather than via proxying or
forwarding. Should you later decide to move your database into your cluster, you
can start its Pods, add appropriate selectors or endpoints, and change the
Service's <code>type</code>.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>You may have trouble using ExternalName for some common protocols, including HTTP and HTTPS.
If you use ExternalName then the hostname used by clients inside your cluster is different from
the name that the ExternalName references.</p><p>For protocols that use hostnames this difference may lead to errors or unexpected responses.
HTTP requests will have a <code>Host:</code> header that the origin server does not recognize;
TLS servers will not be able to provide a certificate matching the hostname that the client connected to.</p></div><h2 id="headless-services">Headless Services</h2><p>Sometimes you don't need load-balancing and a single Service IP. In
this case, you can create what are termed <em>headless Services</em>, by explicitly
specifying <code>"None"</code> for the cluster IP address (<code>.spec.clusterIP</code>).</p><p>You can use a headless Service to interface with other service discovery mechanisms,
without being tied to Kubernetes' implementation.</p><p>For headless Services, a cluster IP is not allocated, kube-proxy does not handle
these Services, and there is no load balancing or proxying done by the platform for them.</p><p>A headless Service allows a client to connect to whichever Pod it prefers, directly. Services that are headless don't
configure routes and packet forwarding using
<a href="/docs/reference/networking/virtual-ips/">virtual IP addresses and proxies</a>; instead, headless Services report the
endpoint IP addresses of the individual pods via internal DNS records, served through the cluster's
<a href="/docs/concepts/services-networking/dns-pod-service/">DNS service</a>.
To define a headless Service, you make a Service with <code>.spec.type</code> set to ClusterIP (which is also the default for <code>type</code>),
and you additionally set <code>.spec.clusterIP</code> to None.</p><p>The string value None is a special case and is not the same as leaving the <code>.spec.clusterIP</code> field unset.</p><p>How DNS is automatically configured depends on whether the Service has selectors defined:</p><h3 id="with-selectors">With selectors</h3><p>For headless Services that define selectors, the endpoints controller creates
EndpointSlices in the Kubernetes API, and modifies the DNS configuration to return
A or AAAA records (IPv4 or IPv6 addresses) that point directly to the Pods backing the Service.</p><h3 id="without-selectors">Without selectors</h3><p>For headless Services that do not define selectors, the control plane does
not create EndpointSlice objects. However, the DNS system looks for and configures
either:</p><ul><li>DNS CNAME records for <a href="#externalname"><code>type: ExternalName</code></a> Services.</li><li>DNS A / AAAA records for all IP addresses of the Service's ready endpoints,
for all Service types other than <code>ExternalName</code>.<ul><li>For IPv4 endpoints, the DNS system creates A records.</li><li>For IPv6 endpoints, the DNS system creates AAAA records.</li></ul></li></ul><p>When you define a headless Service without a selector, the <code>port</code> must
match the <code>targetPort</code>.</p><h2 id="discovering-services">Discovering services</h2><p>For clients running inside your cluster, Kubernetes supports two primary modes of
finding a Service: environment variables and DNS.</p><h3 id="environment-variables">Environment variables</h3><p>When a Pod is run on a Node, the kubelet adds a set of environment variables
for each active Service. It adds <code>{SVCNAME}_SERVICE_HOST</code> and <code>{SVCNAME}_SERVICE_PORT</code> variables,
where the Service name is upper-cased and dashes are converted to underscores.</p><p>For example, the Service <code>redis-primary</code> which exposes TCP port 6379 and has been
allocated cluster IP address 10.0.0.11, produces the following environment
variables:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>REDIS_PRIMARY_SERVICE_HOST</span><span>=</span>10.0.0.11
</span></span><span><span><span>REDIS_PRIMARY_SERVICE_PORT</span><span>=</span><span>6379</span>
</span></span><span><span><span>REDIS_PRIMARY_PORT</span><span>=</span>tcp://10.0.0.11:6379
</span></span><span><span><span>REDIS_PRIMARY_PORT_6379_TCP</span><span>=</span>tcp://10.0.0.11:6379
</span></span><span><span><span>REDIS_PRIMARY_PORT_6379_TCP_PROTO</span><span>=</span>tcp
</span></span><span><span><span>REDIS_PRIMARY_PORT_6379_TCP_PORT</span><span>=</span><span>6379</span>
</span></span><span><span><span>REDIS_PRIMARY_PORT_6379_TCP_ADDR</span><span>=</span>10.0.0.11
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>When you have a Pod that needs to access a Service, and you are using
the environment variable method to publish the port and cluster IP to the client
Pods, you must create the Service <em>before</em> the client Pods come into existence.
Otherwise, those client Pods won't have their environment variables populated.</p><p>If you only use DNS to discover the cluster IP for a Service, you don't need to
worry about this ordering issue.</p></div><p>Kubernetes also supports and provides variables that are compatible with Docker
Engine's "<em><a href="https://docs.docker.com/network/links/">legacy container links</a></em>" feature.
You can read <a href="https://github.com/kubernetes/kubernetes/blob/dd2d12f6dc0e654c15d5db57a5f9f6ba61192726/pkg/kubelet/envvars/envvars.go#L72"><code>makeLinkVariables</code></a>
to see how this is implemented in Kubernetes.</p><h3 id="dns">DNS</h3><p>You can (and almost always should) set up a DNS service for your Kubernetes
cluster using an <a href="/docs/concepts/cluster-administration/addons/">add-on</a>.</p><p>A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new
Services and creates a set of DNS records for each one. If DNS has been enabled
throughout your cluster then all Pods should automatically be able to resolve
Services by their DNS name.</p><p>For example, if you have a Service called <code>my-service</code> in a Kubernetes
namespace <code>my-ns</code>, the control plane and the DNS Service acting together
create a DNS record for <code>my-service.my-ns</code>. Pods in the <code>my-ns</code> namespace
should be able to find the service by doing a name lookup for <code>my-service</code>
(<code>my-service.my-ns</code> would also work).</p><p>Pods in other namespaces must qualify the name as <code>my-service.my-ns</code>. These names
will resolve to the cluster IP assigned for the Service.</p><p>Kubernetes also supports DNS SRV (Service) records for named ports. If the
<code>my-service.my-ns</code> Service has a port named <code>http</code> with the protocol set to
<code>TCP</code>, you can do a DNS SRV query for <code>_http._tcp.my-service.my-ns</code> to discover
the port number for <code>http</code>, as well as the IP address.</p><p>The Kubernetes DNS server is the only way to access <code>ExternalName</code> Services.
You can find more information about <code>ExternalName</code> resolution in
<a href="/docs/concepts/services-networking/dns-pod-service/">DNS for Services and Pods</a>.</p><a id="shortcomings"><a id="the-gory-details-of-virtual-ips"><a id="proxy-modes"><a id="proxy-mode-userspace"><a id="proxy-mode-iptables"><a id="proxy-mode-ipvs"><a id="ips-and-vips"><h2 id="virtual-ip-addressing-mechanism">Virtual IP addressing mechanism</h2><p>Read <a href="/docs/reference/networking/virtual-ips/">Virtual IPs and Service Proxies</a> explains the
mechanism Kubernetes provides to expose a Service with a virtual IP address.</p><h3 id="traffic-policies">Traffic policies</h3><p>You can set the <code>.spec.internalTrafficPolicy</code> and <code>.spec.externalTrafficPolicy</code> fields
to control how Kubernetes routes traffic to healthy (&#8220;ready&#8221;) backends.</p><p>See <a href="/docs/reference/networking/virtual-ips/#traffic-policies">Traffic Policies</a> for more details.</p><h3 id="traffic-distribution">Traffic distribution</h3><div class="feature-state-notice feature-stable" title="Feature Gate: ServiceTrafficDistribution"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>The <code>.spec.trafficDistribution</code> field provides another way to influence traffic
routing within a Kubernetes Service. While traffic policies focus on strict
semantic guarantees, traffic distribution allows you to express <em>preferences</em>
(such as routing to topologically closer endpoints). This can help optimize for
performance, cost, or reliability. In Kubernetes 1.34, the
following field value is supported:</p><dl><dt><code>PreferClose</code></dt><dd>Indicates a preference for routing traffic to endpoints that are in the same
zone as the client.</dd></dl><div class="feature-state-notice feature-beta" title="Feature Gate: PreferSameTrafficDistribution"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>In Kubernetes 1.34, two additional values are
available (unless the <code>PreferSameTrafficDistribution</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature
gate</a> is
disabled):</p><dl><dt><code>PreferSameZone</code></dt><dd>This is an alias for <code>PreferClose</code> that is clearer about the intended semantics.</dd><dt><code>PreferSameNode</code></dt><dd>Indicates a preference for routing traffic to endpoints that are on the same
node as the client.</dd></dl><p>If the field is not set, the implementation will apply its default routing strategy.</p><p>See <a href="/docs/reference/networking/virtual-ips/#traffic-distribution">Traffic
Distribution</a> for
more details</p><h3 id="session-stickiness">Session stickiness</h3><p>If you want to make sure that connections from a particular client are passed to
the same Pod each time, you can configure session affinity based on the client's
IP address. Read <a href="/docs/reference/networking/virtual-ips/#session-affinity">session affinity</a>
to learn more.</p><h2 id="external-ips">External IPs</h2><p>If there are external IPs that route to one or more cluster nodes, Kubernetes Services
can be exposed on those <code>externalIPs</code>. When network traffic arrives into the cluster, with
the external IP (as destination IP) and the port matching that Service, rules and routes
that Kubernetes has configured ensure that the traffic is routed to one of the endpoints
for that Service.</p><p>When you define a Service, you can specify <code>externalIPs</code> for any
<a href="#publishing-services-service-types">service type</a>.
In the example below, the Service named <code>"my-service"</code> can be accessed by clients using TCP,
on <code>"198.51.100.32:80"</code> (calculated from <code>.spec.externalIPs[]</code> and <code>.spec.ports[].port</code>).</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>http<span>
</span></span></span><span><span><span>      </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>49152</span><span>
</span></span></span><span><span><span>  </span><span>externalIPs</span>:<span>
</span></span></span><span><span><span>    </span>- <span>198.51.100.32</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubernetes does not manage allocation of <code>externalIPs</code>; these are the responsibility
of the cluster administrator.</div><h2 id="api-object">API Object</h2><p>Service is a top-level resource in the Kubernetes REST API. You can find more details
about the <a href="/docs/reference/generated/kubernetes-api/v1.34/#service-v1-core">Service API object</a>.</p><h2 id="what-s-next">What's next</h2><p>Learn more about Services and how they fit into Kubernetes:</p><ul><li>Follow the <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a>
tutorial.</li><li>Read about <a href="/docs/concepts/services-networking/ingress/">Ingress</a>, which
exposes HTTP and HTTPS routes from outside the cluster to Services within
your cluster.</li><li>Read about <a href="/docs/concepts/services-networking/gateway/">Gateway</a>, an extension to
Kubernetes that provides more flexibility than Ingress.</li></ul><p>For more context, read the following:</p><ul><li><a href="/docs/reference/networking/virtual-ips/">Virtual IPs and Service Proxies</a></li><li><a href="/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a></li><li><a href="/docs/reference/kubernetes-api/service-resources/service-v1/">Service API reference</a></li><li><a href="/docs/reference/kubernetes-api/service-resources/endpoint-slice-v1/">EndpointSlice API reference</a></li><li><a href="/docs/reference/kubernetes-api/service-resources/endpoints-v1/">Endpoint API reference (legacy)</a></li></ul></a></a></a></a></a></a></a></div></div><div><div class="td-content"><h1>Ingress</h1><div class="lead">Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API.</div><p><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.19 [stable]</code></div><p>An API object that manages external access to the services in a cluster, typically HTTP.</p><p>Ingress may provide load balancing, SSL termination and name-based virtual hosting.</p></p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Ingress is frozen. New features are being added to the <a href="/docs/concepts/services-networking/gateway/">Gateway API</a>.</div><h2 id="terminology">Terminology</h2><p>For clarity, this guide defines the following terms:</p><ul><li>Node: A worker machine in Kubernetes, part of a cluster.</li><li>Cluster: A set of Nodes that run containerized applications managed by Kubernetes.
For this example, and in most common Kubernetes deployments, nodes in the cluster
are not part of the public internet.</li><li>Edge router: A router that enforces the firewall policy for your cluster. This
could be a gateway managed by a cloud provider or a physical piece of hardware.</li><li>Cluster network: A set of links, logical or physical, that facilitate communication
within a cluster according to the Kubernetes <a href="/docs/concepts/cluster-administration/networking/">networking model</a>.</li><li>Service: A Kubernetes <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a> that identifies
a set of Pods using <a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">label</a> selectors.
Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.</li></ul><h2 id="what-is-ingress">What is Ingress?</h2><p><a href="/docs/reference/generated/kubernetes-api/v1.34/#ingress-v1-networking-k8s-io">Ingress</a>
exposes HTTP and HTTPS routes from outside the cluster to
<a href="/docs/concepts/services-networking/service/" target="_blank">services</a> within the cluster.
Traffic routing is controlled by rules defined on the Ingress resource.</p><p>Here is a simple example where an Ingress sends all its traffic to one Service:</p><figure class="diagram-large"><a href="https://mermaid.live/edit#pako:eNqNkstuwyAQRX8F4U0r2VHqPlSRKqt0UamLqlnaWWAYJygYLB59KMm_Fxcix-qmGwbuXA7DwAEzzQETXKutof0Ovb4vaoUQkwKUu6pi3FwXM_QSHGBt0VFFt8DRU2OWSGrKUUMlVQwMmhVLEV1Vcm9-aUksiuXRaO_CEhkv4WjBfAgG1TrGaLa-iaUw6a0DcwGI-WgOsF7zm-pN881fvRx1UDzeiFq7ghb1kgqFWiElyTjnuXVG74FkbdumefEpuNuRu_4rZ1pqQ7L5fL6YQPaPNiFuywcG9_-ihNyUkm6YSONWkjVNM8WUIyaeOJLO3clTB_KhL8NQDmVe-OJjxgZM5FhFiiFTK5zjDkxHBQ9_4zB4a-x20EGNSZhyaKmXrg7f5hSsvufUwTMXThtMWiot5Jh6p9ffimHijIezaSVoeN0uiqcfMJvf7w"><img src="/docs/images/ingress.svg" alt="ingress-diagram"></a><figcaption><p>Figure. Ingress</p></figcaption></figure><p>An Ingress may be configured to give Services externally-reachable URLs,
load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting.
An <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a>
is responsible for fulfilling the Ingress, usually with a load balancer, though
it may also configure your edge router or additional frontends to help handle the traffic.</p><p>An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically
uses a service of type <a href="/docs/concepts/services-networking/service/#type-nodeport">Service.Type=NodePort</a> or
<a href="/docs/concepts/services-networking/service/#loadbalancer">Service.Type=LoadBalancer</a>.</p><h2 id="prerequisites">Prerequisites</h2><p>You must have an <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a>
to satisfy an Ingress. Only creating an Ingress resource has no effect.</p><p>You may need to deploy an Ingress controller such as <a href="https://kubernetes.github.io/ingress-nginx/deploy/">ingress-nginx</a>.
You can choose from a number of <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controllers</a>.</p><p>Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress
controllers operate slightly differently.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Make sure you review your Ingress controller's documentation to understand the caveats of choosing it.</div><h2 id="the-ingress-resource">The Ingress resource</h2><p>A minimal Ingress resource example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/minimal-ingress.yaml"><code>service/networking/minimal-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/minimal-ingress.yaml to clipboard"></div><div class="includecode" id="service-networking-minimal-ingress-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>minimal-ingress<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>nginx.ingress.kubernetes.io/rewrite-target</span>:<span> </span>/<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ingressClassName</span>:<span> </span>nginx-example<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>path</span>:<span> </span>/testpath<span>
</span></span></span><span><span><span>        </span><span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>test<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>An Ingress needs <code>apiVersion</code>, <code>kind</code>, <code>metadata</code> and <code>spec</code> fields.
The name of an Ingress object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.
For general information about working with config files, see
<a href="/docs/tasks/run-application/run-stateless-application-deployment/">deploying applications</a>,
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">configuring containers</a>,
<a href="/docs/concepts/workloads/management/">managing resources</a>.
Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which
is the <a href="https://github.com/kubernetes/ingress-nginx/blob/main/docs/examples/rewrite/README.md">rewrite-target annotation</a>.
Different <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controllers</a> support different annotations.
Review the documentation for your choice of Ingress controller to learn which annotations are supported.</p><p>The <a href="/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec">Ingress spec</a>
has all the information needed to configure a load balancer or proxy server. Most importantly, it
contains a list of rules matched against all incoming requests. Ingress resource only supports rules
for directing HTTP(S) traffic.</p><p>If the <code>ingressClassName</code> is omitted, a <a href="#default-ingress-class">default Ingress class</a>
should be defined.</p><p>There are some ingress controllers, that work without the definition of a
default <code>IngressClass</code>. For example, the Ingress-NGINX controller can be
configured with a <a href="https://kubernetes.github.io/ingress-nginx/user-guide/k8s-122-migration/#what-is-the-flag-watch-ingress-without-class">flag</a>
<code>--watch-ingress-without-class</code>. It is <a href="https://kubernetes.github.io/ingress-nginx/user-guide/k8s-122-migration/#i-have-only-one-ingress-controller-in-my-cluster-what-should-i-do">recommended</a> though, to specify the
default <code>IngressClass</code> as shown <a href="#default-ingress-class">below</a>.</p><h3 id="ingress-rules">Ingress rules</h3><p>Each HTTP rule contains the following information:</p><ul><li>An optional host. In this example, no host is specified, so the rule applies to all inbound
HTTP traffic through the IP address specified. If a host is provided (for example,
foo.bar.com), the rules apply to that host.</li><li>A list of paths (for example, <code>/testpath</code>), each of which has an associated
backend defined with a <code>service.name</code> and a <code>service.port.name</code> or
<code>service.port.number</code>. Both the host and path must match the content of an
incoming request before the load balancer directs traffic to the referenced
Service.</li><li>A backend is a combination of Service and port names as described in the
<a href="/docs/concepts/services-networking/service/">Service doc</a> or a <a href="#resource-backend">custom resource backend</a>
by way of a <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CRD</a>. HTTP (and HTTPS) requests to the
Ingress that match the host and path of the rule are sent to the listed backend.</li></ul><p>A <code>defaultBackend</code> is often configured in an Ingress controller to service any requests that do not
match a path in the spec.</p><h3 id="default-backend">DefaultBackend</h3><p>An Ingress with no rules sends all traffic to a single default backend and <code>.spec.defaultBackend</code>
is the backend that should handle requests in that case.
The <code>defaultBackend</code> is conventionally a configuration option of the
<a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a> and
is not specified in your Ingress resources.
If no <code>.spec.rules</code> are specified, <code>.spec.defaultBackend</code> must be specified.
If <code>defaultBackend</code> is not set, the handling of requests that do not match any of the rules will be up to the
ingress controller (consult the documentation for your ingress controller to find out how it handles this case).</p><p>If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is
routed to your default backend.</p><h3 id="resource-backend">Resource backends</h3><p>A <code>Resource</code> backend is an ObjectRef to another Kubernetes resource within the
same namespace as the Ingress object. A <code>Resource</code> is a mutually exclusive
setting with Service, and will fail validation if both are specified. A common
usage for a <code>Resource</code> backend is to ingress data to an object storage backend
with static assets.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/ingress-resource-backend.yaml"><code>service/networking/ingress-resource-backend.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/ingress-resource-backend.yaml to clipboard"></div><div class="includecode" id="service-networking-ingress-resource-backend-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>ingress-resource-backend<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>defaultBackend</span>:<span>
</span></span></span><span><span><span>    </span><span>resource</span>:<span>
</span></span></span><span><span><span>      </span><span>apiGroup</span>:<span> </span>k8s.example.com<span>
</span></span></span><span><span><span>      </span><span>kind</span>:<span> </span>StorageBucket<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>static-assets<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>http</span>:<span>
</span></span></span><span><span><span>        </span><span>paths</span>:<span>
</span></span></span><span><span><span>          </span>- <span>path</span>:<span> </span>/icons<span>
</span></span></span><span><span><span>            </span><span>pathType</span>:<span> </span>ImplementationSpecific<span>
</span></span></span><span><span><span>            </span><span>backend</span>:<span>
</span></span></span><span><span><span>              </span><span>resource</span>:<span>
</span></span></span><span><span><span>                </span><span>apiGroup</span>:<span> </span>k8s.example.com<span>
</span></span></span><span><span><span>                </span><span>kind</span>:<span> </span>StorageBucket<span>
</span></span></span><span><span><span>                </span><span>name</span>:<span> </span>icon-assets<span>
</span></span></span></code></pre></div></div></div><p>After creating the Ingress above, you can view it with the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl describe ingress ingress-resource-backend
</span></span></code></pre></div><pre tabindex="0"><code>Name:             ingress-resource-backend
Namespace:        default
Address:
Default backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets
Annotations:  &lt;none&gt;
Events:       &lt;none&gt;
</code></pre><h3 id="path-types">Path types</h3><p>Each path in an Ingress is required to have a corresponding path type. Paths
that do not include an explicit <code>pathType</code> will fail validation. There are three
supported path types:</p><ul><li><p><code>ImplementationSpecific</code>: With this path type, matching is up to the
IngressClass. Implementations can treat this as a separate <code>pathType</code> or treat
it identically to <code>Prefix</code> or <code>Exact</code> path types.</p></li><li><p><code>Exact</code>: Matches the URL path exactly and with case sensitivity.</p></li><li><p><code>Prefix</code>: Matches based on a URL path prefix split by <code>/</code>. Matching is case
sensitive and done on a path element by element basis. A path element refers
to the list of labels in the path split by the <code>/</code> separator. A request is a
match for path <em>p</em> if every <em>p</em> is an element-wise prefix of <em>p</em> of the
request path.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If the last element of the path is a substring of the last
element in request path, it is not a match (for example: <code>/foo/bar</code>
matches <code>/foo/bar/baz</code>, but does not match <code>/foo/barbaz</code>).</div></li></ul><h3 id="examples">Examples</h3><table><thead><tr><th>Kind</th><th>Path(s)</th><th>Request path(s)</th><th>Matches?</th></tr></thead><tbody><tr><td>Prefix</td><td><code>/</code></td><td>(all paths)</td><td>Yes</td></tr><tr><td>Exact</td><td><code>/foo</code></td><td><code>/foo</code></td><td>Yes</td></tr><tr><td>Exact</td><td><code>/foo</code></td><td><code>/bar</code></td><td>No</td></tr><tr><td>Exact</td><td><code>/foo</code></td><td><code>/foo/</code></td><td>No</td></tr><tr><td>Exact</td><td><code>/foo/</code></td><td><code>/foo</code></td><td>No</td></tr><tr><td>Prefix</td><td><code>/foo</code></td><td><code>/foo</code>, <code>/foo/</code></td><td>Yes</td></tr><tr><td>Prefix</td><td><code>/foo/</code></td><td><code>/foo</code>, <code>/foo/</code></td><td>Yes</td></tr><tr><td>Prefix</td><td><code>/aaa/bb</code></td><td><code>/aaa/bbb</code></td><td>No</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbb</code></td><td>Yes</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb/</code></td><td><code>/aaa/bbb</code></td><td>Yes, ignores trailing slash</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbb/</code></td><td>Yes, matches trailing slash</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbb/ccc</code></td><td>Yes, matches subpath</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbbxyz</code></td><td>No, does not match string prefix</td></tr><tr><td>Prefix</td><td><code>/</code>, <code>/aaa</code></td><td><code>/aaa/ccc</code></td><td>Yes, matches <code>/aaa</code> prefix</td></tr><tr><td>Prefix</td><td><code>/</code>, <code>/aaa</code>, <code>/aaa/bbb</code></td><td><code>/aaa/bbb</code></td><td>Yes, matches <code>/aaa/bbb</code> prefix</td></tr><tr><td>Prefix</td><td><code>/</code>, <code>/aaa</code>, <code>/aaa/bbb</code></td><td><code>/ccc</code></td><td>Yes, matches <code>/</code> prefix</td></tr><tr><td>Prefix</td><td><code>/aaa</code></td><td><code>/ccc</code></td><td>No, uses default backend</td></tr><tr><td>Mixed</td><td><code>/foo</code> (Prefix), <code>/foo</code> (Exact)</td><td><code>/foo</code></td><td>Yes, prefers Exact</td></tr></tbody></table><h4 id="multiple-matches">Multiple matches</h4><p>In some cases, multiple paths within an Ingress will match a request. In those
cases precedence will be given first to the longest matching path. If two paths
are still equally matched, precedence will be given to paths with an exact path
type over prefix path type.</p><h2 id="hostname-wildcards">Hostname wildcards</h2><p>Hosts can be precise matches (for example &#8220;<code>foo.bar.com</code>&#8221;) or a wildcard (for
example &#8220;<code>*.foo.com</code>&#8221;). Precise matches require that the HTTP <code>host</code> header
matches the <code>host</code> field. Wildcard matches require the HTTP <code>host</code> header is
equal to the suffix of the wildcard rule.</p><table><thead><tr><th>Host</th><th>Host header</th><th>Match?</th></tr></thead><tbody><tr><td><code>*.foo.com</code></td><td><code>bar.foo.com</code></td><td>Matches based on shared suffix</td></tr><tr><td><code>*.foo.com</code></td><td><code>baz.bar.foo.com</code></td><td>No match, wildcard only covers a single DNS label</td></tr><tr><td><code>*.foo.com</code></td><td><code>foo.com</code></td><td>No match, wildcard only covers a single DNS label</td></tr></tbody></table><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/ingress-wildcard-host.yaml"><code>service/networking/ingress-wildcard-host.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/ingress-wildcard-host.yaml to clipboard"></div><div class="includecode" id="service-networking-ingress-wildcard-host-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>ingress-wildcard-host<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>host</span>:<span> </span><span>"foo.bar.com"</span><span>
</span></span></span><span><span><span>    </span><span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span><span>"/bar"</span><span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service1<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span>- <span>host</span>:<span> </span><span>"*.foo.com"</span><span>
</span></span></span><span><span><span>    </span><span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span><span>"/foo"</span><span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service2<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><h2 id="ingress-class">Ingress class</h2><p>Ingresses can be implemented by different controllers, often with different
configuration. Each Ingress should specify a class, a reference to an
IngressClass resource that contains additional configuration including the name
of the controller that should implement the class.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/external-lb.yaml"><code>service/networking/external-lb.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/external-lb.yaml to clipboard"></div><div class="includecode" id="service-networking-external-lb-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>IngressClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>external-lb<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>controller</span>:<span> </span>example.com/ingress-controller<span>
</span></span></span><span><span><span>  </span><span>parameters</span>:<span>
</span></span></span><span><span><span>    </span><span>apiGroup</span>:<span> </span>k8s.example.com<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>IngressParameters<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>external-lb<span>
</span></span></span></code></pre></div></div></div><p>The <code>.spec.parameters</code> field of an IngressClass lets you reference another
resource that provides configuration related to that IngressClass.</p><p>The specific type of parameters to use depends on the ingress controller
that you specify in the <code>.spec.controller</code> field of the IngressClass.</p><h3 id="ingressclass-scope">IngressClass scope</h3><p>Depending on your ingress controller, you may be able to use parameters
that you set cluster-wide, or just for one namespace.</p><ul class="nav nav-tabs" id="tabs-ingressclass-parameter-scope"><li class="nav-item"><a class="nav-link active" href="#tabs-ingressclass-parameter-scope-0">Cluster</a></li><li class="nav-item"><a class="nav-link" href="#tabs-ingressclass-parameter-scope-1">Namespaced</a></li></ul><div class="tab-content" id="tabs-ingressclass-parameter-scope"><div id="tabs-ingressclass-parameter-scope-0" class="tab-pane show active"><p><p>The default scope for IngressClass parameters is cluster-wide.</p><p>If you set the <code>.spec.parameters</code> field and don't set
<code>.spec.parameters.scope</code>, or if you set <code>.spec.parameters.scope</code> to
<code>Cluster</code>, then the IngressClass refers to a cluster-scoped resource.
The <code>kind</code> (in combination the <code>apiGroup</code>) of the parameters
refers to a cluster-scoped API (possibly a custom resource), and
the <code>name</code> of the parameters identifies a specific cluster scoped
resource for that API.</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>IngressClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>external-lb-1<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>controller</span>:<span> </span>example.com/ingress-controller<span>
</span></span></span><span><span><span>  </span><span>parameters</span>:<span>
</span></span></span><span><span><span>    </span><span># The parameters for this IngressClass are specified in a</span><span>
</span></span></span><span><span><span>    </span><span># ClusterIngressParameter (API group k8s.example.net) named</span><span>
</span></span></span><span><span><span>    </span><span># "external-config-1". This definition tells Kubernetes to</span><span>
</span></span></span><span><span><span>    </span><span># look for a cluster-scoped parameter resource.</span><span>
</span></span></span><span><span><span>    </span><span>scope</span>:<span> </span>Cluster<span>
</span></span></span><span><span><span>    </span><span>apiGroup</span>:<span> </span>k8s.example.net<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>ClusterIngressParameter<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>external-config-1<span>
</span></span></span></code></pre></div></p></div><div id="tabs-ingressclass-parameter-scope-1" class="tab-pane"><p><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>If you set the <code>.spec.parameters</code> field and set
<code>.spec.parameters.scope</code> to <code>Namespace</code>, then the IngressClass refers
to a namespaced-scoped resource. You must also set the <code>namespace</code>
field within <code>.spec.parameters</code> to the namespace that contains
the parameters you want to use.</p><p>The <code>kind</code> (in combination the <code>apiGroup</code>) of the parameters
refers to a namespaced API (for example: ConfigMap), and
the <code>name</code> of the parameters identifies a specific resource
in the namespace you specified in <code>namespace</code>.</p><p>Namespace-scoped parameters help the cluster operator delegate control over the
configuration (for example: load balancer settings, API gateway definition)
that is used for a workload. If you used a cluster-scoped parameter then either:</p><ul><li>the cluster operator team needs to approve a different team's changes every
time there's a new configuration change being applied.</li><li>the cluster operator must define specific access controls, such as
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a> roles and bindings, that let
the application team make changes to the cluster-scoped parameters resource.</li></ul><p>The IngressClass API itself is always cluster-scoped.</p><p>Here is an example of an IngressClass that refers to parameters that are
namespaced:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>IngressClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>external-lb-2<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>controller</span>:<span> </span>example.com/ingress-controller<span>
</span></span></span><span><span><span>  </span><span>parameters</span>:<span>
</span></span></span><span><span><span>    </span><span># The parameters for this IngressClass are specified in an</span><span>
</span></span></span><span><span><span>    </span><span># IngressParameter (API group k8s.example.com) named "external-config",</span><span>
</span></span></span><span><span><span>    </span><span># that's in the "external-configuration" namespace.</span><span>
</span></span></span><span><span><span>    </span><span>scope</span>:<span> </span>Namespace<span>
</span></span></span><span><span><span>    </span><span>apiGroup</span>:<span> </span>k8s.example.com<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>IngressParameter<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>external-configuration<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>external-config<span>
</span></span></span></code></pre></div></p></div></div><h3 id="deprecated-annotation">Deprecated annotation</h3><p>Before the IngressClass resource and <code>ingressClassName</code> field were added in
Kubernetes 1.18, Ingress classes were specified with a
<code>kubernetes.io/ingress.class</code> annotation on the Ingress. This annotation was
never formally defined, but was widely supported by Ingress controllers.</p><p>The newer <code>ingressClassName</code> field on Ingresses is a replacement for that
annotation, but is not a direct equivalent. While the annotation was generally
used to reference the name of the Ingress controller that should implement the
Ingress, the field is a reference to an IngressClass resource that contains
additional Ingress configuration, including the name of the Ingress controller.</p><h3 id="default-ingress-class">Default IngressClass</h3><p>You can mark a particular IngressClass as default for your cluster. Setting the
<code>ingressclass.kubernetes.io/is-default-class</code> annotation to <code>true</code> on an
IngressClass resource will ensure that new Ingresses without an
<code>ingressClassName</code> field specified will be assigned this default IngressClass.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>If you have more than one IngressClass marked as the default for your cluster,
the admission controller prevents creating new Ingress objects that don't have
an <code>ingressClassName</code> specified. You can resolve this by ensuring that at most 1
IngressClass is marked as default in your cluster.</div><p>There are some ingress controllers, that work without the definition of a
default <code>IngressClass</code>. For example, the Ingress-NGINX controller can be
configured with a <a href="https://kubernetes.github.io/ingress-nginx/#what-is-the-flag-watch-ingress-without-class">flag</a>
<code>--watch-ingress-without-class</code>. It is <a href="https://kubernetes.github.io/ingress-nginx/#i-have-only-one-instance-of-the-ingresss-nginx-controller-in-my-cluster-what-should-i-do">recommended</a> though, to specify the
default <code>IngressClass</code>:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/default-ingressclass.yaml"><code>service/networking/default-ingressclass.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/default-ingressclass.yaml to clipboard"></div><div class="includecode" id="service-networking-default-ingressclass-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>IngressClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/component</span>:<span> </span>controller<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx-example<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>ingressclass.kubernetes.io/is-default-class</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>controller</span>:<span> </span>k8s.io/ingress-nginx<span>
</span></span></span></code></pre></div></div></div><h2 id="types-of-ingress">Types of Ingress</h2><h3 id="single-service-ingress">Ingress backed by a single Service</h3><p>There are existing Kubernetes concepts that allow you to expose a single Service
(see <a href="#alternatives">alternatives</a>). You can also do this with an Ingress by specifying a
<em>default backend</em> with no rules.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/test-ingress.yaml"><code>service/networking/test-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/test-ingress.yaml to clipboard"></div><div class="includecode" id="service-networking-test-ingress-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-ingress<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>defaultBackend</span>:<span>
</span></span></span><span><span><span>    </span><span>service</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>test<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span>
</span></span></span><span><span><span>        </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>If you create it using <code>kubectl apply -f</code> you should be able to view the state
of the Ingress you added:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get ingress test-ingress
</span></span></code></pre></div><pre tabindex="0"><code>NAME           CLASS         HOSTS   ADDRESS         PORTS   AGE
test-ingress   external-lb   *       203.0.113.123   80      59s
</code></pre><p>Where <code>203.0.113.123</code> is the IP allocated by the Ingress controller to satisfy
this Ingress.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Ingress controllers and load balancers may take a minute or two to allocate an IP address.
Until that time, you often see the address listed as <code>&lt;pending&gt;</code>.</div><h3 id="simple-fanout">Simple fanout</h3><p>A fanout configuration routes traffic from a single IP address to more than one Service,
based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers
down to a minimum. For example, a setup like:</p><figure class="diagram-large"><a href="https://mermaid.live/edit#pako:eNqNUslOwzAQ_RXLvYCUhMQpUFzUUzkgcUBwbHpw4klr4diR7bCo8O8k2FFbFomLPZq3jP00O1xpDpjijWHtFt09zAuFUCUFKHey8vf6NE7QrdoYsDZumGIb4Oi6NAskNeOoZJKpCgxK4oXwrFVgRyi7nCVXWZKRPMlysv5yD6Q4Xryf1Vq_WzDPooJs9egLNDbolKTpT03JzKgh3zWEztJZ0Niu9L-qZGcdmAMfj4cxvWmreba613z9C0B-AMQD-V_AdA-A4j5QZu0SatRKJhSqhZR0wjmPrDP6CeikrutQxy-Cuy2dtq9RpaU2dJKm6fzI5Glmg0VOLio4_5dLjx27hFSC015KJ2VZHtuQvY2fuHcaE43G0MaCREOow_FV5cMxHZ5-oPX75UM5avuXhXuOI9yAaZjg_aLuBl6B3RYaKDDtSw4166QrcKE-emrXcubghgunDaY1kxYizDqnH99UhakzHYykpWD9hjS--fEJoIELqQ"><img src="/docs/images/ingressFanOut.svg" alt="ingress-fanout-diagram"></a><figcaption><p>Figure. Ingress Fan Out</p></figcaption></figure><p>It would require an Ingress such as:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/simple-fanout-example.yaml"><code>service/networking/simple-fanout-example.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/simple-fanout-example.yaml to clipboard"></div><div class="includecode" id="service-networking-simple-fanout-example-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>simple-fanout-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>host</span>:<span> </span>foo.bar.com<span>
</span></span></span><span><span><span>    </span><span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>path</span>:<span> </span>/foo<span>
</span></span></span><span><span><span>        </span><span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service1<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>4200</span><span>
</span></span></span><span><span><span>      </span>- <span>path</span>:<span> </span>/bar<span>
</span></span></span><span><span><span>        </span><span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service2<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>8080</span><span>
</span></span></span></code></pre></div></div></div><p>When you create the Ingress with <code>kubectl apply -f</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe ingress simple-fanout-example
</span></span></code></pre></div><pre tabindex="0"><code>Name:             simple-fanout-example
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:4200 (10.8.0.90:4200)
               /bar   service2:8080 (10.8.0.91:8080)
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     22s                loadbalancer-controller  default/test
</code></pre><p>The Ingress controller provisions an implementation-specific load balancer
that satisfies the Ingress, as long as the Services (<code>service1</code>, <code>service2</code>) exist.
When it has done so, you can see the address of the load balancer at the
Address field.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Depending on the <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a>
you are using, you may need to create a default-http-backend
<a href="/docs/concepts/services-networking/service/">Service</a>.</div><h3 id="name-based-virtual-hosting">Name based virtual hosting</h3><p>Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.</p><figure class="diagram-large"><a href="https://mermaid.live/edit#pako:eNqNkl9PwyAUxb8KYS-atM1Kp05m9qSJJj4Y97jugcLtRqTQAPVPdN_dVlq3qUt8gZt7zvkBN7xjbgRgiteW1Rt0_zjLNUJcSdD-ZBn21WmcoDu9tuBcXDHN1iDQVWHnSBkmUMEU0xwsSuK5DK5l745QejFNLtMkJVmSZmT1Re9NcTz_uDXOU1QakxTMJtxUHw7ss-SQLhehQEODTsdH4l20Q-zFyc84-Y67pghv5apxHuweMuj9eS2_NiJdPhix-kMgvwQShOyYMNkJoEUYM3PuGkpUKyY1KqVSdCSEiJy35gnoqCzLvo5fpPAbOqlfI26UsXQ0Ho9nB5CnqesRGTnncPYvSqsdUvqp9KRdlI6KojjEkB0mnLgjDRONhqENBYm6oXbLV5V1y6S7-l42_LowlIN2uFm_twqOcAW2YlK0H_i9c-bYb6CCHNO2FFCyRvkc53rbWptaMA83QnpjMS2ZchBh1nizeNMcU28bGEzXkrV_pArN7Sc0rBTu"><img src="/docs/images/ingressNameBased.svg" alt="ingress-namebase-diagram"></a><figcaption><p>Figure. Ingress Name Based Virtual hosting</p></figcaption></figure><p>The following Ingress tells the backing load balancer to route requests based on
the <a href="https://tools.ietf.org/html/rfc7230#section-5.4">Host header</a>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/name-virtual-host-ingress.yaml"><code>service/networking/name-virtual-host-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/name-virtual-host-ingress.yaml to clipboard"></div><div class="includecode" id="service-networking-name-virtual-host-ingress-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>name-virtual-host-ingress<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>host</span>:<span> </span>foo.bar.com<span>
</span></span></span><span><span><span>    </span><span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span><span>"/"</span><span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service1<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span>- <span>host</span>:<span> </span>bar.foo.com<span>
</span></span></span><span><span><span>    </span><span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span><span>"/"</span><span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service2<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>If you create an Ingress resource without any hosts defined in the rules, then any
web traffic to the IP address of your Ingress controller can be matched without a name based
virtual host being required.</p><p>For example, the following Ingress routes traffic
requested for <code>first.bar.com</code> to <code>service1</code>, <code>second.bar.com</code> to <code>service2</code>,
and any traffic whose request host header doesn't match <code>first.bar.com</code>
and <code>second.bar.com</code> to <code>service3</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/name-virtual-host-ingress-no-third-host.yaml"><code>service/networking/name-virtual-host-ingress-no-third-host.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/name-virtual-host-ingress-no-third-host.yaml to clipboard"></div><div class="includecode" id="service-networking-name-virtual-host-ingress-no-third-host-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>name-virtual-host-ingress-no-third-host<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>host</span>:<span> </span>first.bar.com<span>
</span></span></span><span><span><span>    </span><span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span><span>"/"</span><span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service1<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span>- <span>host</span>:<span> </span>second.bar.com<span>
</span></span></span><span><span><span>    </span><span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span><span>"/"</span><span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service2<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span>- <span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span><span>"/"</span><span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service3<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><h3 id="tls">TLS</h3><p>You can secure an Ingress by specifying a <a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a>
that contains a TLS private key and certificate. The Ingress resource only
supports a single TLS port, 443, and assumes TLS termination at the ingress point
(traffic to the Service and its Pods is in plaintext).
If the TLS configuration section in an Ingress specifies different hosts, they are
multiplexed on the same port according to the hostname specified through the
SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret
must contain keys named <code>tls.crt</code> and <code>tls.key</code> that contain the certificate
and private key to use for TLS. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>testsecret-tls<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>tls.crt</span>:<span> </span>base64 encoded cert<span>
</span></span></span><span><span><span>  </span><span>tls.key</span>:<span> </span>base64 encoded key<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>kubernetes.io/tls<span>
</span></span></span></code></pre></div><p>Referencing this secret in an Ingress tells the Ingress controller to
secure the channel from the client to the load balancer using TLS. You need to make
sure the TLS secret you created came from a certificate that contains a Common
Name (CN), also known as a Fully Qualified Domain Name (FQDN) for <code>https-example.foo.com</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Keep in mind that TLS will not work on the default rule because the
certificates would have to be issued for all the possible sub-domains. Therefore,
<code>hosts</code> in the <code>tls</code> section need to explicitly match the <code>host</code> in the <code>rules</code>
section.</div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/tls-example-ingress.yaml"><code>service/networking/tls-example-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/tls-example-ingress.yaml to clipboard"></div><div class="includecode" id="service-networking-tls-example-ingress-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Ingress<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>tls-example-ingress<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>tls</span>:<span>
</span></span></span><span><span><span>  </span>- <span>hosts</span>:<span>
</span></span></span><span><span><span>      </span>- https-example.foo.com<span>
</span></span></span><span><span><span>    </span><span>secretName</span>:<span> </span>testsecret-tls<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>host</span>:<span> </span>https-example.foo.com<span>
</span></span></span><span><span><span>    </span><span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>path</span>:<span> </span>/<span>
</span></span></span><span><span><span>        </span><span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>        </span><span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service1<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>There is a gap between TLS features supported by various Ingress
controllers. Please refer to documentation on
<a href="https://kubernetes.github.io/ingress-nginx/user-guide/tls/">nginx</a>,
<a href="https://git.k8s.io/ingress-gce/README.md#frontend-https">GCE</a>, or any other
platform specific Ingress controller to understand how TLS works in your environment.</div><h3 id="load-balancing">Load balancing</h3><p>An Ingress controller is bootstrapped with some load balancing policy settings
that it applies to all Ingress, such as the load balancing algorithm, backend
weight scheme, and others. More advanced load balancing concepts
(e.g. persistent sessions, dynamic weights) are not yet exposed through the
Ingress. You can instead get these features through the load balancer used for
a Service.</p><p>It's also worth noting that even though health checks are not exposed directly
through the Ingress, there exist parallel concepts in Kubernetes such as
<a href="/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">readiness probes</a>
that allow you to achieve the same end result. Please review the controller
specific documentation to see how they handle health checks (for example:
<a href="https://git.k8s.io/ingress-nginx/README.md">nginx</a>, or
<a href="https://git.k8s.io/ingress-gce/README.md#health-checks">GCE</a>).</p><h2 id="updating-an-ingress">Updating an Ingress</h2><p>To update an existing Ingress to add a new Host, you can update it by editing the resource:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe ingress <span>test</span>
</span></span></code></pre></div><pre tabindex="0"><code>Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl edit ingress <span>test</span>
</span></span></code></pre></div><p>This pops up an editor with the existing configuration in YAML format.
Modify it to include the new Host:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>host</span>:<span> </span>foo.bar.com<span>
</span></span></span><span><span><span>    </span><span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service1<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span>/foo<span>
</span></span></span><span><span><span>        </span><span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span>  </span>- <span>host</span>:<span> </span>bar.baz.com<span>
</span></span></span><span><span><span>    </span><span>http</span>:<span>
</span></span></span><span><span><span>      </span><span>paths</span>:<span>
</span></span></span><span><span><span>      </span>- <span>backend</span>:<span>
</span></span></span><span><span><span>          </span><span>service</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>service2<span>
</span></span></span><span><span><span>            </span><span>port</span>:<span>
</span></span></span><span><span><span>              </span><span>number</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span>/foo<span>
</span></span></span><span><span><span>        </span><span>pathType</span>:<span> </span>Prefix<span>
</span></span></span><span><span><span></span>..<span>
</span></span></span></code></pre></div><p>After you save your changes, kubectl updates the resource in the API server, which tells the
Ingress controller to reconfigure the load balancer.</p><p>Verify this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe ingress <span>test</span>
</span></span></code></pre></div><pre tabindex="0"><code>Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test
</code></pre><p>You can achieve the same outcome by invoking <code>kubectl replace -f</code> on a modified Ingress YAML file.</p><h2 id="failing-across-availability-zones">Failing across availability zones</h2><p>Techniques for spreading traffic across failure domains differ between cloud providers.
Please check the documentation of the relevant <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controller</a> for details.</p><h2 id="alternatives">Alternatives</h2><p>You can expose a Service in multiple ways that don't directly involve the Ingress resource:</p><ul><li>Use <a href="/docs/concepts/services-networking/service/#loadbalancer">Service.Type=LoadBalancer</a></li><li>Use <a href="/docs/concepts/services-networking/service/#type-nodeport">Service.Type=NodePort</a></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn about the <a href="/docs/reference/kubernetes-api/service-resources/ingress-v1/">Ingress</a> API</li><li>Learn about <a href="/docs/concepts/services-networking/ingress-controllers/">Ingress controllers</a></li><li><a href="/docs/tasks/access-application-cluster/ingress-minikube/">Set up Ingress on Minikube with the NGINX Controller</a></li></ul></div></div><div><div class="td-content"><h1>Ingress Controllers</h1><div class="lead">In order for an <a href="/docs/concepts/services-networking/ingress/">Ingress</a> to work in your cluster, there must be an <em>ingress controller</em> running. You need to select at least one ingress controller and make sure it is set up in your cluster. This page lists common ingress controllers that you can deploy.</div><p>In order for the Ingress resource to work, the cluster must have an ingress controller running.</p><p>Unlike other types of controllers which run as part of the <code>kube-controller-manager</code> binary, Ingress controllers
are not started automatically with a cluster. Use this page to choose the ingress controller implementation
that best fits your cluster.</p><p>Kubernetes as a project supports and maintains <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller#readme">AWS</a>, <a href="https://git.k8s.io/ingress-gce/README.md#readme">GCE</a>, and
<a href="https://git.k8s.io/ingress-nginx/README.md#readme">nginx</a> ingress controllers.</p><h2 id="additional-controllers">Additional controllers</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><ul><li><a href="https://docs.microsoft.com/azure/application-gateway/tutorial-ingress-controller-add-on-existing?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Faks%2Ftoc.json&amp;bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json">AKS Application Gateway Ingress Controller</a> is an ingress controller that configures the <a href="https://docs.microsoft.com/azure/application-gateway/overview">Azure Application Gateway</a>.</li><li><a href="https://www.alibabacloud.com/help/en/mse/user-guide/overview-of-mse-ingress-gateways">Alibaba Cloud MSE Ingress</a> is an ingress controller that configures the <a href="https://www.alibabacloud.com/help/en/mse/product-overview/cloud-native-gateway-overview?spm=a2c63.p38356.0.0.20563003HJK9is">Alibaba Cloud Native Gateway</a>, which is also the commercial version of <a href="https://github.com/alibaba/higress">Higress</a>.</li><li><a href="https://github.com/apache/apisix-ingress-controller">Apache APISIX ingress controller</a> is an <a href="https://github.com/apache/apisix">Apache APISIX</a>-based ingress controller.</li><li><a href="https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes">Avi Kubernetes Operator</a> provides L4-L7 load-balancing using <a href="https://avinetworks.com/">VMware NSX Advanced Load Balancer</a>.</li><li><a href="https://github.com/bfenetworks/ingress-bfe">BFE Ingress Controller</a> is a <a href="https://www.bfe-networks.net">BFE</a>-based ingress controller.</li><li><a href="https://docs.cilium.io/en/stable/network/servicemesh/ingress/">Cilium Ingress Controller</a> is an ingress controller powered by <a href="https://cilium.io/">Cilium</a>.</li><li>The <a href="https://github.com/citrix/citrix-k8s-ingress-controller#readme">Citrix ingress controller</a> works with
Citrix Application Delivery Controller.</li><li><a href="https://projectcontour.io/">Contour</a> is an <a href="https://www.envoyproxy.io/">Envoy</a> based ingress controller.</li><li><a href="https://www.getambassador.io/products/api-gateway">Emissary-Ingress</a> API Gateway is an <a href="https://www.envoyproxy.io">Envoy</a>-based ingress
controller.</li><li><a href="https://getenroute.io/">EnRoute</a> is an <a href="https://www.envoyproxy.io">Envoy</a> based API gateway that can run as an ingress controller.</li><li><a href="https://megaease.com/docs/easegress/04.cloud-native/4.1.kubernetes-ingress-controller/">Easegress IngressController</a> is an <a href="https://megaease.com/easegress/">Easegress</a> based API gateway that can run as an ingress controller.</li><li>F5 BIG-IP <a href="https://clouddocs.f5.com/containers/latest/userguide/kubernetes/">Container Ingress Services for Kubernetes</a>
lets you use an Ingress to configure F5 BIG-IP virtual servers.</li><li><a href="https://docs.fortinet.com/document/fortiadc/7.0.0/fortiadc-ingress-controller/742835/fortiadc-ingress-controller-overview">FortiADC Ingress Controller</a> support the Kubernetes Ingress resources and allows you to manage FortiADC objects from Kubernetes</li><li><a href="https://gloo.solo.io">Gloo</a> is an open-source ingress controller based on <a href="https://www.envoyproxy.io">Envoy</a>,
which offers API gateway functionality.</li><li><a href="https://haproxy-ingress.github.io/">HAProxy Ingress</a> is an ingress controller for
<a href="https://www.haproxy.org/#desc">HAProxy</a>.</li><li><a href="https://github.com/alibaba/higress">Higress</a> is an <a href="https://www.envoyproxy.io">Envoy</a> based API gateway that can run as an ingress controller.</li><li>The <a href="https://github.com/haproxytech/kubernetes-ingress#readme">HAProxy Ingress Controller for Kubernetes</a>
is also an ingress controller for <a href="https://www.haproxy.org/#desc">HAProxy</a>.</li><li><a href="https://istio.io/latest/docs/tasks/traffic-management/ingress/kubernetes-ingress/">Istio Ingress</a>
is an <a href="https://istio.io/">Istio</a> based ingress controller.</li><li>The <a href="https://github.com/Kong/kubernetes-ingress-controller#readme">Kong Ingress Controller for Kubernetes</a>
is an ingress controller driving <a href="https://konghq.com/kong/">Kong Gateway</a>.</li><li><a href="https://kusk.kubeshop.io/">Kusk Gateway</a> is an OpenAPI-driven ingress controller based on <a href="https://www.envoyproxy.io">Envoy</a>.</li><li>The <a href="https://www.nginx.com/products/nginx-ingress-controller/">NGINX Ingress Controller for Kubernetes</a>
works with the <a href="https://www.nginx.com/resources/glossary/nginx/">NGINX</a> webserver (as a proxy).</li><li>The <a href="https://github.com/ngrok/kubernetes-ingress-controller">ngrok Kubernetes Ingress Controller</a> is an open source controller for adding secure public access to your K8s services using the <a href="https://ngrok.com">ngrok platform</a>.</li><li>The <a href="https://github.com/oracle/oci-native-ingress-controller#readme">OCI Native Ingress Controller</a> is an Ingress controller for Oracle Cloud Infrastructure which allows you to manage the <a href="https://docs.oracle.com/en-us/iaas/Content/Balance/home.htm">OCI Load Balancer</a>.</li><li><a href="https://gitee.com/njet-rd/open-njet-kic">OpenNJet Ingress Controller</a> is a <a href="https://njet.org.cn/">OpenNJet</a>-based ingress controller.</li><li>The <a href="https://www.pomerium.com/docs/k8s/ingress.html">Pomerium Ingress Controller</a> is based on <a href="https://pomerium.com/">Pomerium</a>, which offers context-aware access policy.</li><li><a href="https://opensource.zalando.com/skipper/kubernetes/ingress-controller/">Skipper</a> HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress, designed as a library to build your custom proxy.</li><li>The <a href="https://doc.traefik.io/traefik/providers/kubernetes-ingress/">Traefik Kubernetes Ingress provider</a> is an
ingress controller for the <a href="https://traefik.io/traefik/">Traefik</a> proxy.</li><li><a href="https://github.com/TykTechnologies/tyk-operator">Tyk Operator</a> extends Ingress with Custom Resources to bring API Management capabilities to Ingress. Tyk Operator works with the Open Source Tyk Gateway &amp; Tyk Cloud control plane.</li><li><a href="https://voyagermesh.com">Voyager</a> is an ingress controller for
<a href="https://www.haproxy.org/#desc">HAProxy</a>.</li><li><a href="https://www.wallarm.com/solutions/waf-for-kubernetes">Wallarm Ingress Controller</a> is an Ingress Controller that provides WAAP (WAF) and API Security capabilities.</li></ul><h2 id="using-multiple-ingress-controllers">Using multiple Ingress controllers</h2><p>You may deploy any number of ingress controllers using <a href="/docs/concepts/services-networking/ingress/#ingress-class">ingress class</a>
within a cluster. Note the <code>.metadata.name</code> of your ingress class resource. When you create an ingress you would need that name to specify the <code>ingressClassName</code> field on your Ingress object (refer to <a href="/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec">IngressSpec v1 reference</a>). <code>ingressClassName</code> is a replacement of the older <a href="/docs/concepts/services-networking/ingress/#deprecated-annotation">annotation method</a>.</p><p>If you do not specify an IngressClass for an Ingress, and your cluster has exactly one IngressClass marked as default, then Kubernetes <a href="/docs/concepts/services-networking/ingress/#default-ingress-class">applies</a> the cluster's default IngressClass to the Ingress.
You mark an IngressClass as default by setting the <a href="/docs/reference/labels-annotations-taints/#ingressclass-kubernetes-io-is-default-class"><code>ingressclass.kubernetes.io/is-default-class</code> annotation</a> on that IngressClass, with the string value <code>"true"</code>.</p><p>Ideally, all ingress controllers should fulfill this specification, but the various ingress
controllers operate slightly differently.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Make sure you review your ingress controller's documentation to understand the caveats of choosing it.</div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/services-networking/ingress/">Ingress</a>.</li><li><a href="/docs/tasks/access-application-cluster/ingress-minikube/">Set up Ingress on Minikube with the NGINX Controller</a>.</li></ul></div></div><div><div class="td-content"><h1>Gateway API</h1><div class="lead">Gateway API is a family of API kinds that provide dynamic infrastructure provisioning and advanced traffic routing.</div><p>Make network services available by using an extensible, role-oriented, protocol-aware configuration
mechanism. <a href="https://gateway-api.sigs.k8s.io/">Gateway API</a> is an <a class="glossary-tooltip" title="Resources that extend the functionality of Kubernetes." href="/docs/concepts/cluster-administration/addons/" target="_blank">add-on</a>
containing API <a href="https://gateway-api.sigs.k8s.io/references/spec/">kinds</a> that provide dynamic infrastructure
provisioning and advanced traffic routing.</p><h2 id="design-principles">Design principles</h2><p>The following principles shaped the design and architecture of Gateway API:</p><ul><li><strong>Role-oriented:</strong> Gateway API kinds are modeled after organizational roles that are
responsible for managing Kubernetes service networking:<ul><li><strong>Infrastructure Provider:</strong> Manages infrastructure that allows multiple isolated clusters
to serve multiple tenants, e.g. a cloud provider.</li><li><strong>Cluster Operator:</strong> Manages clusters and is typically concerned with policies, network
access, application permissions, etc.</li><li><strong>Application Developer:</strong> Manages an application running in a cluster and is typically
concerned with application-level configuration and <a href="/docs/concepts/services-networking/service/">Service</a>
composition.</li></ul></li><li><strong>Portable:</strong> Gateway API specifications are defined as <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</a>
and are supported by many <a href="https://gateway-api.sigs.k8s.io/implementations/">implementations</a>.</li><li><strong>Expressive:</strong> Gateway API kinds support functionality for common traffic routing use cases
such as header-based matching, traffic weighting, and others that were only possible in
<a href="/docs/concepts/services-networking/ingress/">Ingress</a> by using custom annotations.</li><li><strong>Extensible:</strong> Gateway allows for custom resources to be linked at various layers of the API.
This makes granular customization possible at the appropriate places within the API structure.</li></ul><h2 id="resource-model">Resource model</h2><p>Gateway API has four stable API kinds:</p><ul><li><p><strong>GatewayClass:</strong> Defines a set of gateways with common configuration and managed by a controller
that implements the class.</p></li><li><p><strong>Gateway:</strong> Defines an instance of traffic handling infrastructure, such as cloud load balancer.</p></li><li><p><strong>HTTPRoute:</strong> Defines HTTP-specific rules for mapping traffic from a Gateway listener to a
representation of backend network endpoints. These endpoints are often represented as a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a>.</p></li><li><p><strong>GRPCRoute:</strong> Defines gRPC-specific rules for mapping traffic from a Gateway listener to a
representation of backend network endpoints. These endpoints are often represented as a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a>.</p></li></ul><p>Gateway API is organized into different API kinds that have interdependent relationships to support
the role-oriented nature of organizations. A Gateway object is associated with exactly one GatewayClass;
the GatewayClass describes the gateway controller responsible for managing Gateways of this class.
One or more route kinds such as HTTPRoute, are then associated to Gateways. A Gateway can filter the routes
that may be attached to its <code>listeners</code>, forming a bidirectional trust model with routes.</p><p>The following figure illustrates the relationships of the three stable Gateway API kinds:</p><figure class="diagram-medium"><img src="/docs/images/gateway-kind-relationships.svg" alt="A figure illustrating the relationships of the three stable Gateway API kinds"></figure><h3 id="api-kind-gateway-class">GatewayClass</h3><p>Gateways can be implemented by different controllers, often with different configurations. A Gateway
must reference a GatewayClass that contains the name of the controller that implements the
class.</p><p>A minimal GatewayClass example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>gateway.networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>GatewayClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-class<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>controllerName</span>:<span> </span>example.com/gateway-controller<span>
</span></span></span></code></pre></div><p>In this example, a controller that has implemented Gateway API is configured to manage GatewayClasses
with the controller name <code>example.com/gateway-controller</code>. Gateways of this class will be managed by
the implementation's controller.</p><p>See the <a href="https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.GatewayClass">GatewayClass</a>
reference for a full definition of this API kind.</p><h3 id="api-kind-gateway">Gateway</h3><p>A Gateway describes an instance of traffic handling infrastructure. It defines a network endpoint
that can be used for processing traffic, i.e. filtering, balancing, splitting, etc. for backends
such as a Service. For example, a Gateway may represent a cloud load balancer or an in-cluster proxy
server that is configured to accept HTTP traffic.</p><p>A minimal Gateway resource example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>gateway.networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Gateway<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-gateway<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>gatewayClassName</span>:<span> </span>example-class<span>
</span></span></span><span><span><span>  </span><span>listeners</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>http<span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>HTTP<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div><p>In this example, an instance of traffic handling infrastructure is programmed to listen for HTTP
traffic on port 80. Since the <code>addresses</code> field is unspecified, an address or hostname is assigned
to the Gateway by the implementation's controller. This address is used as a network endpoint for
processing traffic of backend network endpoints defined in routes.</p><p>See the <a href="https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.Gateway">Gateway</a>
reference for a full definition of this API kind.</p><h3 id="api-kind-httproute">HTTPRoute</h3><p>The HTTPRoute kind specifies routing behavior of HTTP requests from a Gateway listener to backend network
endpoints. For a Service backend, an implementation may represent the backend network endpoint as a Service
IP or the backing EndpointSlices of the Service. An HTTPRoute represents configuration that is applied to the
underlying Gateway implementation. For example, defining a new HTTPRoute may result in configuring additional
traffic routes in a cloud load balancer or in-cluster proxy server.</p><p>A minimal HTTPRoute example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>gateway.networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>HTTPRoute<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-httproute<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>parentRefs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example-gateway<span>
</span></span></span><span><span><span>  </span><span>hostnames</span>:<span>
</span></span></span><span><span><span>  </span>- <span>"www.example.com"</span><span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>matches</span>:<span>
</span></span></span><span><span><span>    </span>- <span>path</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>PathPrefix<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>/login<span>
</span></span></span><span><span><span>    </span><span>backendRefs</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>example-svc<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>8080</span><span>
</span></span></span></code></pre></div><p>In this example, HTTP traffic from Gateway <code>example-gateway</code> with the Host: header set to <code>www.example.com</code>
and the request path specified as <code>/login</code> will be routed to Service <code>example-svc</code> on port <code>8080</code>.</p><p>See the <a href="https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.HTTPRoute">HTTPRoute</a>
reference for a full definition of this API kind.</p><h3 id="api-kind-grpcroute">GRPCRoute</h3><p>The GRPCRoute kind specifies routing behavior of gRPC requests from a Gateway listener to backend network
endpoints. For a Service backend, an implementation may represent the backend network endpoint as a Service
IP or the backing EndpointSlices of the Service. A GRPCRoute represents configuration that is applied to the
underlying Gateway implementation. For example, defining a new GRPCRoute may result in configuring additional
traffic routes in a cloud load balancer or in-cluster proxy server.</p><p>Gateways supporting GRPCRoute are required to support HTTP/2 without an initial upgrade from HTTP/1,
so gRPC traffic is guaranteed to flow properly.</p><p>A minimal GRPCRoute example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>gateway.networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>GRPCRoute<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-grpcroute<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>parentRefs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example-gateway<span>
</span></span></span><span><span><span>  </span><span>hostnames</span>:<span>
</span></span></span><span><span><span>  </span>- <span>"svc.example.com"</span><span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>backendRefs</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>example-svc<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>50051</span><span>
</span></span></span></code></pre></div><p>In this example, gRPC traffic from Gateway <code>example-gateway</code> with the host set to <code>svc.example.com</code>
will be directed to the service <code>example-svc</code> on port <code>50051</code> from the same namespace.</p><p>GRPCRoute allows matching specific gRPC services, as per the following example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>gateway.networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>GRPCRoute<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-grpcroute<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>parentRefs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example-gateway<span>
</span></span></span><span><span><span>  </span><span>hostnames</span>:<span>
</span></span></span><span><span><span>  </span>- <span>"svc.example.com"</span><span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>matches</span>:<span>
</span></span></span><span><span><span>    </span>- <span>method</span>:<span>
</span></span></span><span><span><span>        </span><span>service</span>:<span> </span>com.example<span>
</span></span></span><span><span><span>        </span><span>method</span>:<span> </span>Login<span>
</span></span></span><span><span><span>    </span><span>backendRefs</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>foo-svc<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>50051</span><span>
</span></span></span></code></pre></div><p>In this case, the GRPCRoute will match any traffic for svc.example.com and apply its routing rules
to forward the traffic to the correct backend. Since there is only one match specified,only requests
for the com.example.User.Login method to svc.example.com will be forwarded.
RPCs of any other method` will not be matched by this Route.</p><p>See the <a href="https://gateway-api.sigs.k8s.io/reference/spec/#grpcroute">GRPCRoute</a>
reference for a full definition of this API kind.</p><h2 id="request-flow">Request flow</h2><p>Here is a simple example of HTTP traffic being routed to a Service by using a Gateway and an HTTPRoute:</p><figure class="diagram-medium"><img src="/docs/images/gateway-request-flow.svg" alt="A diagram that provides an example of HTTP traffic being routed to a Service by using a Gateway and an HTTPRoute"></figure><p>In this example, the request flow for a Gateway implemented as a reverse proxy is:</p><ol><li>The client starts to prepare an HTTP request for the URL <code>http://www.example.com</code></li><li>The client's DNS resolver queries for the destination name and learns a mapping to
one or more IP addresses associated with the Gateway.</li><li>The client sends a request to the Gateway IP address; the reverse proxy receives the HTTP
request and uses the Host: header to match a configuration that was derived from the Gateway
and attached HTTPRoute.</li><li>Optionally, the reverse proxy can perform request header and/or path matching based
on match rules of the HTTPRoute.</li><li>Optionally, the reverse proxy can modify the request; for example, to add or remove headers,
based on filter rules of the HTTPRoute.</li><li>Lastly, the reverse proxy forwards the request to one or more backends.</li></ol><h2 id="conformance">Conformance</h2><p>Gateway API covers a broad set of features and is widely implemented. This combination requires
clear conformance definitions and tests to ensure that the API provides a consistent experience
wherever it is used.</p><p>See the <a href="https://gateway-api.sigs.k8s.io/concepts/conformance/">conformance</a> documentation to
understand details such as release channels, support levels, and running conformance tests.</p><h2 id="migrating-from-ingress">Migrating from Ingress</h2><p>Gateway API is the successor to the <a href="/docs/concepts/services-networking/ingress/">Ingress</a> API.
However, it does not include the Ingress kind. As a result, a one-time conversion from your existing
Ingress resources to Gateway API resources is necessary.</p><p>Refer to the <a href="https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress/#migrating-from-ingress">ingress migration</a>
guide for details on migrating Ingress resources to Gateway API resources.</p><h2 id="what-s-next">What's next</h2><p>Instead of Gateway API resources being natively implemented by Kubernetes, the specifications
are defined as <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resources</a>
supported by a wide range of <a href="https://gateway-api.sigs.k8s.io/implementations/">implementations</a>.
<a href="https://gateway-api.sigs.k8s.io/guides/#installing-gateway-api">Install</a> the Gateway API CRDs or
follow the installation instructions of your selected implementation. After installing an
implementation, use the <a href="https://gateway-api.sigs.k8s.io/guides/">Getting Started</a> guide to help
you quickly start working with Gateway API.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Make sure to review the documentation of your selected implementation to understand any caveats.</div><p>Refer to the <a href="https://gateway-api.sigs.k8s.io/reference/spec/">API specification</a> for additional
details of all Gateway API kinds.</p></div></div><div><div class="td-content"><h1>EndpointSlices</h1><div class="lead">The EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to handle large numbers of backends, and allows the cluster to update its list of healthy backends efficiently.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div>EndpointSlices track the IP addresses of backend endpoints.
EndpointSlices are normally associated with a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a> and the backend endpoints typically represent
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>.<h2 id="endpointslice-resource">EndpointSlice API</h2><p>In Kubernetes, an EndpointSlice contains references to a set of network
endpoints. The control plane automatically creates EndpointSlices
for any Kubernetes Service that has a <a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." href="/docs/concepts/overview/working-with-objects/labels/" target="_blank">selector</a> specified. These EndpointSlices include
references to all the Pods that match the Service selector. EndpointSlices group
network endpoints together by unique combinations of IP family, protocol,
port number, and Service name.
The name of a EndpointSlice object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>As an example, here's a sample EndpointSlice object, that's owned by the <code>example</code>
Kubernetes Service.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>discovery.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EndpointSlice<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-abc<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/service-name</span>:<span> </span>example<span>
</span></span></span><span><span><span></span><span>addressType</span>:<span> </span>IPv4<span>
</span></span></span><span><span><span></span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>http<span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span></span><span>endpoints</span>:<span>
</span></span></span><span><span><span>  </span>- <span>addresses</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"10.1.2.3"</span><span>
</span></span></span><span><span><span>    </span><span>conditions</span>:<span>
</span></span></span><span><span><span>      </span><span>ready</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>hostname</span>:<span> </span>pod-1<span>
</span></span></span><span><span><span>    </span><span>nodeName</span>:<span> </span>node-1<span>
</span></span></span><span><span><span>    </span><span>zone</span>:<span> </span>us-west2-a<span>
</span></span></span></code></pre></div><p>By default, the control plane creates and manages EndpointSlices to have no
more than 100 endpoints each. You can configure this with the
<code>--max-endpoints-per-slice</code>
<a class="glossary-tooltip" title="Control Plane component that runs controller processes." href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank">kube-controller-manager</a>
flag, up to a maximum of 1000.</p><p>EndpointSlices act as the source of truth for
<a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank">kube-proxy</a> when it comes to
how to route internal traffic.</p><h3 id="address-types">Address types</h3><p>EndpointSlices support two address types:</p><ul><li>IPv4</li><li>IPv6</li></ul><p>Each <code>EndpointSlice</code> object represents a specific IP address type. If you have
a Service that is available via IPv4 and IPv6, there will be at least two
<code>EndpointSlice</code> objects (one for IPv4, and one for IPv6).</p><h3 id="conditions">Conditions</h3><p>The EndpointSlice API stores conditions about endpoints that may be useful for consumers.
The three conditions are <code>serving</code>, <code>terminating</code>, and <code>ready</code>.</p><h4 id="serving">Serving</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>The <code>serving</code> condition indicates that the endpoint is currently serving responses, and
so it should be used as a target for Service traffic. For endpoints backed by a Pod, this
maps to the Pod's <code>Ready</code> condition.</p><h4 id="terminating">Terminating</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>The <code>terminating</code> condition indicates that the endpoint is
terminating. For endpoints backed by a Pod, this condition is set when
the Pod is first deleted (that is, when it receives a deletion
timestamp, but most likely before the Pod's containers exit).</p><p>Service proxies will normally ignore endpoints that are <code>terminating</code>,
but they may route traffic to endpoints that are both <code>serving</code> and
<code>terminating</code> if all available endpoints are <code>terminating</code>. (This
helps to ensure that no Service traffic is lost during rolling updates
of the underlying Pods.)</p><h4 id="ready">Ready</h4><p>The <code>ready</code> condition is essentially a shortcut for checking
"<code>serving</code> and not <code>terminating</code>" (though it will also always be
<code>true</code> for Services with <code>spec.publishNotReadyAddresses</code> set to
<code>true</code>).</p><h3 id="topology">Topology information</h3><p>Each endpoint within an EndpointSlice can contain relevant topology information.
The topology information includes the location of the endpoint and information
about the corresponding Node and zone. These are available in the following
per endpoint fields on EndpointSlices:</p><ul><li><code>nodeName</code> - The name of the Node this endpoint is on.</li><li><code>zone</code> - The zone this endpoint is in.</li></ul><h3 id="management">Management</h3><p>Most often, the control plane (specifically, the endpoint slice
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>) creates and
manages EndpointSlice objects. There are a variety of other use cases for
EndpointSlices, such as service mesh implementations, that could result in other
entities or controllers managing additional sets of EndpointSlices.</p><p>To ensure that multiple entities can manage EndpointSlices without interfering
with each other, Kubernetes defines the
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">label</a>
<code>endpointslice.kubernetes.io/managed-by</code>, which indicates the entity managing
an EndpointSlice.
The endpoint slice controller sets <code>endpointslice-controller.k8s.io</code> as the value
for this label on all EndpointSlices it manages. Other entities managing
EndpointSlices should also set a unique value for this label.</p><h3 id="ownership">Ownership</h3><p>In most use cases, EndpointSlices are owned by the Service that the endpoint
slice object tracks endpoints for. This ownership is indicated by an owner
reference on each EndpointSlice as well as a <code>kubernetes.io/service-name</code>
label that enables simple lookups of all EndpointSlices belonging to a Service.</p><h3 id="distribution-of-endpointslices">Distribution of EndpointSlices</h3><p>Each EndpointSlice has a set of ports that applies to all endpoints within the
resource. When named ports are used for a Service, Pods may end up with
different target port numbers for the same named port, requiring different
EndpointSlices.</p><p>The control plane tries to fill EndpointSlices as full as possible, but does not
actively rebalance them. The logic is fairly straightforward:</p><ol><li>Iterate through existing EndpointSlices, remove endpoints that are no longer
desired and update matching endpoints that have changed.</li><li>Iterate through EndpointSlices that have been modified in the first step and
fill them up with any new endpoints needed.</li><li>If there's still new endpoints left to add, try to fit them into a previously
unchanged slice and/or create new ones.</li></ol><p>Importantly, the third step prioritizes limiting EndpointSlice updates over a
perfectly full distribution of EndpointSlices. As an example, if there are 10
new endpoints to add and 2 EndpointSlices with room for 5 more endpoints each,
this approach will create a new EndpointSlice instead of filling up the 2
existing EndpointSlices. In other words, a single EndpointSlice creation is
preferable to multiple EndpointSlice updates.</p><p>With kube-proxy running on each Node and watching EndpointSlices, every change
to an EndpointSlice becomes relatively expensive since it will be transmitted to
every Node in the cluster. This approach is intended to limit the number of
changes that need to be sent to every Node, even if it may result with multiple
EndpointSlices that are not full.</p><p>In practice, this less than ideal distribution should be rare. Most changes
processed by the EndpointSlice controller will be small enough to fit in an
existing EndpointSlice, and if not, a new EndpointSlice is likely going to be
necessary soon anyway. Rolling updates of Deployments also provide a natural
repacking of EndpointSlices with all Pods and their corresponding endpoints
getting replaced.</p><h3 id="duplicate-endpoints">Duplicate endpoints</h3><p>Due to the nature of EndpointSlice changes, endpoints may be represented in more
than one EndpointSlice at the same time. This naturally occurs as changes to
different EndpointSlice objects can arrive at the Kubernetes client watch / cache
at different times.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Clients of the EndpointSlice API must iterate through all the existing EndpointSlices
associated to a Service and build a complete list of unique network endpoints. It is
important to mention that endpoints may be duplicated in different EndpointSlices.</p><p>You can find a reference implementation for how to perform this endpoint aggregation
and deduplication as part of the <code>EndpointSliceCache</code> code within <code>kube-proxy</code>.</p></div><h3 id="endpointslice-mirroring">EndpointSlice mirroring</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [deprecated]</code></div><p>The EndpointSlice API is a replacement for the older Endpoints API. To
preserve compatibility with older controllers and user workloads that
expect <a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank">kube-proxy</a>
to route traffic based on Endpoints resources, the cluster's control
plane mirrors most user-created Endpoints resources to corresponding
EndpointSlices.</p><p>(However, this feature, like the rest of the Endpoints API, is
deprecated. Users who manually specify endpoints for selectorless
Services should do so by creating EndpointSlice resources directly,
rather than by creating Endpoints resources and allowing them to be
mirrored.)</p><p>The control plane mirrors Endpoints resources unless:</p><ul><li>the Endpoints resource has a <code>endpointslice.kubernetes.io/skip-mirror</code> label
set to <code>true</code>.</li><li>the Endpoints resource has a <code>control-plane.alpha.kubernetes.io/leader</code>
annotation.</li><li>the corresponding Service resource does not exist.</li><li>the corresponding Service resource has a non-nil selector.</li></ul><p>Individual Endpoints resources may translate into multiple EndpointSlices. This
will occur if an Endpoints resource has multiple subsets or includes endpoints
with multiple IP families (IPv4 and IPv6). A maximum of 1000 addresses per
subset will be mirrored to EndpointSlices.</p><h2 id="what-s-next">What's next</h2><ul><li>Follow the <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a> tutorial</li><li>Read the <a href="/docs/reference/kubernetes-api/service-resources/endpoint-slice-v1/">API reference</a> for the EndpointSlice API</li><li>Read the <a href="/docs/reference/kubernetes-api/service-resources/endpoints-v1/">API reference</a> for the Endpoints API</li></ul></div></div><div><div class="td-content"><h1>Network Policies</h1><div class="lead">If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), NetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between Pods and the outside world. Your cluster must use a network plugin that supports NetworkPolicy enforcement.</div><p>If you want to control traffic flow at the IP address or port level for TCP, UDP, and SCTP protocols,
then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster.
NetworkPolicies are an application-centric construct which allow you to specify how a
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">pod</a> is allowed to communicate with various network
"entities" (we use the word "entity" here to avoid overloading the more common terms such as
"endpoints" and "services", which have specific Kubernetes connotations) over the network.
NetworkPolicies apply to a connection with a pod on one or both ends, and are not relevant to
other connections.</p><p>The entities that a Pod can communicate with are identified through a combination of the following
three identifiers:</p><ol><li>Other pods that are allowed (exception: a pod cannot block access to itself)</li><li>Namespaces that are allowed</li><li>IP blocks (exception: traffic to and from the node where a Pod is running is always allowed,
regardless of the IP address of the Pod or the node)</li></ol><p>When defining a pod- or namespace-based NetworkPolicy, you use a
<a class="glossary-tooltip" title="Allows users to filter a list of resources based on labels." href="/docs/concepts/overview/working-with-objects/labels/" target="_blank">selector</a> to specify what traffic is allowed to
and from the Pod(s) that match the selector.</p><p>Meanwhile, when IP-based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).</p><h2 id="prerequisites">Prerequisites</h2><p>Network policies are implemented by the <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugin</a>.
To use network policies, you must be using a networking solution which supports NetworkPolicy.
Creating a NetworkPolicy resource without a controller that implements it will have no effect.</p><h2 id="the-two-sorts-of-pod-isolation">The two sorts of pod isolation</h2><p>There are two sorts of isolation for a pod: isolation for egress, and isolation for ingress.
They concern what connections may be established. "Isolation" here is not absolute, rather it
means "some restrictions apply". The alternative, "non-isolated for $direction", means that no
restrictions apply in the stated direction. The two sorts of isolation (or not) are declared
independently, and are both relevant for a connection from one pod to another.</p><p>By default, a pod is non-isolated for egress; all outbound connections are allowed.
A pod is isolated for egress if there is any NetworkPolicy that both selects the pod and has
"Egress" in its <code>policyTypes</code>; we say that such a policy applies to the pod for egress.
When a pod is isolated for egress, the only allowed connections from the pod are those allowed by
the <code>egress</code> list of some NetworkPolicy that applies to the pod for egress. Reply traffic for those
allowed connections will also be implicitly allowed.
The effects of those <code>egress</code> lists combine additively.</p><p>By default, a pod is non-isolated for ingress; all inbound connections are allowed.
A pod is isolated for ingress if there is any NetworkPolicy that both selects the pod and
has "Ingress" in its <code>policyTypes</code>; we say that such a policy applies to the pod for ingress.
When a pod is isolated for ingress, the only allowed connections into the pod are those from
the pod's node and those allowed by the <code>ingress</code> list of some NetworkPolicy that applies to
the pod for ingress. Reply traffic for those allowed connections will also be implicitly allowed.
The effects of those <code>ingress</code> lists combine additively.</p><p>Network policies do not conflict; they are additive. If any policy or policies apply to a given
pod for a given direction, the connections allowed in that direction from that pod is the union of
what the applicable policies allow. Thus, order of evaluation does not affect the policy result.</p><p>For a connection from a source pod to a destination pod to be allowed, both the egress policy on
the source pod and the ingress policy on the destination pod need to allow the connection. If
either side does not allow the connection, it will not happen.</p><h2 id="networkpolicy-resource">The NetworkPolicy resource</h2><p>See the <a href="/docs/reference/generated/kubernetes-api/v1.34/#networkpolicy-v1-networking-k8s-io">NetworkPolicy</a>
reference for a full definition of the resource.</p><p>An example NetworkPolicy might look like this:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/networkpolicy.yaml"><code>service/networking/networkpolicy.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/networkpolicy.yaml to clipboard"></div><div class="includecode" id="service-networking-networkpolicy-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>NetworkPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-network-policy<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>podSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>role</span>:<span> </span>db<span>
</span></span></span><span><span><span>  </span><span>policyTypes</span>:<span>
</span></span></span><span><span><span>  </span>- Ingress<span>
</span></span></span><span><span><span>  </span>- Egress<span>
</span></span></span><span><span><span>  </span><span>ingress</span>:<span>
</span></span></span><span><span><span>  </span>- <span>from</span>:<span>
</span></span></span><span><span><span>    </span>- <span>ipBlock</span>:<span>
</span></span></span><span><span><span>        </span><span>cidr</span>:<span> </span><span>172.17.0.0</span>/16<span>
</span></span></span><span><span><span>        </span><span>except</span>:<span>
</span></span></span><span><span><span>        </span>- <span>172.17.1.0</span>/24<span>
</span></span></span><span><span><span>    </span>- <span>namespaceSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>          </span><span>project</span>:<span> </span>myproject<span>
</span></span></span><span><span><span>    </span>- <span>podSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>          </span><span>role</span>:<span> </span>frontend<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>6379</span><span>
</span></span></span><span><span><span>  </span><span>egress</span>:<span>
</span></span></span><span><span><span>  </span>- <span>to</span>:<span>
</span></span></span><span><span><span>    </span>- <span>ipBlock</span>:<span>
</span></span></span><span><span><span>        </span><span>cidr</span>:<span> </span><span>10.0.0.0</span>/24<span>
</span></span></span><span><span><span>    </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>5978</span><span>
</span></span></span><span><span><span>
</span></span></span></code></pre></div></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>POSTing this to the API server for your cluster will have no effect unless your chosen networking
solution supports network policy.</div><p><strong>Mandatory Fields</strong>: As with all other Kubernetes config, a NetworkPolicy needs <code>apiVersion</code>,
<code>kind</code>, and <code>metadata</code> fields. For general information about working with config files, see
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">Configure a Pod to Use a ConfigMap</a>,
and <a href="/docs/concepts/overview/working-with-objects/object-management/">Object Management</a>.</p><p><strong>spec</strong>: NetworkPolicy <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status">spec</a>
has all the information needed to define a particular network policy in the given namespace.</p><p><strong>podSelector</strong>: Each NetworkPolicy includes a <code>podSelector</code> which selects the grouping of pods to
which the policy applies. The example policy selects pods with the label "role=db". An empty
<code>podSelector</code> selects all pods in the namespace.</p><p><strong>policyTypes</strong>: Each NetworkPolicy includes a <code>policyTypes</code> list which may include either
<code>Ingress</code>, <code>Egress</code>, or both. The <code>policyTypes</code> field indicates whether or not the given policy
applies to ingress traffic to selected pod, egress traffic from selected pods, or both. If no
<code>policyTypes</code> are specified on a NetworkPolicy then by default <code>Ingress</code> will always be set and
<code>Egress</code> will be set if the NetworkPolicy has any egress rules.</p><p><strong>ingress</strong>: Each NetworkPolicy may include a list of allowed <code>ingress</code> rules. Each rule allows
traffic which matches both the <code>from</code> and <code>ports</code> sections. The example policy contains a single
rule, which matches traffic on a single port, from one of three sources, the first specified via
an <code>ipBlock</code>, the second via a <code>namespaceSelector</code> and the third via a <code>podSelector</code>.</p><p><strong>egress</strong>: Each NetworkPolicy may include a list of allowed <code>egress</code> rules. Each rule allows
traffic which matches both the <code>to</code> and <code>ports</code> sections. The example policy contains a single
rule, which matches traffic on a single port to any destination in <code>10.0.0.0/24</code>.</p><p>So, the example NetworkPolicy:</p><ol><li><p>isolates <code>role=db</code> pods in the <code>default</code> namespace for both ingress and egress traffic
(if they weren't already isolated)</p></li><li><p>(Ingress rules) allows connections to all pods in the <code>default</code> namespace with the label
<code>role=db</code> on TCP port 6379 from:</p><ul><li>any pod in the <code>default</code> namespace with the label <code>role=frontend</code></li><li>any pod in a namespace with the label <code>project=myproject</code></li><li>IP addresses in the ranges <code>172.17.0.0</code>&#8211;<code>172.17.0.255</code> and <code>172.17.2.0</code>&#8211;<code>172.17.255.255</code>
(ie, all of <code>172.17.0.0/16</code> except <code>172.17.1.0/24</code>)</li></ul></li><li><p>(Egress rules) allows connections from any pod in the <code>default</code> namespace with the label
<code>role=db</code> to CIDR <code>10.0.0.0/24</code> on TCP port 5978</p></li></ol><p>See the <a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a>
walkthrough for further examples.</p><h2 id="behavior-of-to-and-from-selectors">Behavior of <code>to</code> and <code>from</code> selectors</h2><p>There are four kinds of selectors that can be specified in an <code>ingress</code> <code>from</code> section or <code>egress</code>
<code>to</code> section:</p><p><strong>podSelector</strong>: This selects particular Pods in the same namespace as the NetworkPolicy which
should be allowed as ingress sources or egress destinations.</p><p><strong>namespaceSelector</strong>: This selects particular namespaces for which all Pods should be allowed as
ingress sources or egress destinations.</p><p><strong>namespaceSelector</strong> <em>and</em> <strong>podSelector</strong>: A single <code>to</code>/<code>from</code> entry that specifies both
<code>namespaceSelector</code> and <code>podSelector</code> selects particular Pods within particular namespaces. Be
careful to use correct YAML syntax. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>ingress</span>:<span>
</span></span></span><span><span><span>  </span>- <span>from</span>:<span>
</span></span></span><span><span><span>    </span>- <span>namespaceSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>          </span><span>user</span>:<span> </span>alice<span>
</span></span></span><span><span><span>      </span><span>podSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>          </span><span>role</span>:<span> </span>client<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><p>This policy contains a single <code>from</code> element allowing connections from Pods with the label
<code>role=client</code> in namespaces with the label <code>user=alice</code>. But the following policy is different:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>ingress</span>:<span>
</span></span></span><span><span><span>  </span>- <span>from</span>:<span>
</span></span></span><span><span><span>    </span>- <span>namespaceSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>          </span><span>user</span>:<span> </span>alice<span>
</span></span></span><span><span><span>    </span>- <span>podSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>          </span><span>role</span>:<span> </span>client<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><p>It contains two elements in the <code>from</code> array, and allows connections from Pods in the local
Namespace with the label <code>role=client</code>, <em>or</em> from any Pod in any namespace with the label
<code>user=alice</code>.</p><p>When in doubt, use <code>kubectl describe</code> to see how Kubernetes has interpreted the policy.</p><p><a name="behavior-of-ipblock-selectors"></a><strong>ipBlock</strong>: This selects particular IP CIDR ranges to allow as ingress sources or egress
destinations. These should be cluster-external IPs, since Pod IPs are ephemeral and unpredictable.</p><p>Cluster ingress and egress mechanisms often require rewriting the source or destination IP
of packets. In cases where this happens, it is not defined whether this happens before or
after NetworkPolicy processing, and the behavior may be different for different
combinations of network plugin, cloud provider, <code>Service</code> implementation, etc.</p><p>In the case of ingress, this means that in some cases you may be able to filter incoming
packets based on the actual original source IP, while in other cases, the "source IP" that
the NetworkPolicy acts on may be the IP of a <code>LoadBalancer</code> or of the Pod's node, etc.</p><p>For egress, this means that connections from pods to <code>Service</code> IPs that get rewritten to
cluster-external IPs may or may not be subject to <code>ipBlock</code>-based policies.</p><h2 id="default-policies">Default policies</h2><p>By default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to
and from pods in that namespace. The following examples let you change the default behavior
in that namespace.</p><h3 id="default-deny-all-ingress-traffic">Default deny all ingress traffic</h3><p>You can create a "default" ingress isolation policy for a namespace by creating a NetworkPolicy
that selects all pods but does not allow any ingress traffic to those pods.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-default-deny-ingress.yaml"><code>service/networking/network-policy-default-deny-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/network-policy-default-deny-ingress.yaml to clipboard"></div><div class="includecode" id="service-networking-network-policy-default-deny-ingress-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>NetworkPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default-deny-ingress<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>podSelector</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span><span>policyTypes</span>:<span>
</span></span></span><span><span><span>  </span>- Ingress<span>
</span></span></span></code></pre></div></div></div><p>This ensures that even pods that aren't selected by any other NetworkPolicy will still be isolated
for ingress. This policy does not affect isolation for egress from any pod.</p><h3 id="allow-all-ingress-traffic">Allow all ingress traffic</h3><p>If you want to allow all incoming connections to all pods in a namespace, you can create a policy
that explicitly allows that.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-allow-all-ingress.yaml"><code>service/networking/network-policy-allow-all-ingress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/network-policy-allow-all-ingress.yaml to clipboard"></div><div class="includecode" id="service-networking-network-policy-allow-all-ingress-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>NetworkPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>allow-all-ingress<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>podSelector</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span><span>ingress</span>:<span>
</span></span></span><span><span><span>  </span>- {}<span>
</span></span></span><span><span><span>  </span><span>policyTypes</span>:<span>
</span></span></span><span><span><span>  </span>- Ingress<span>
</span></span></span></code></pre></div></div></div><p>With this policy in place, no additional policy or policies can cause any incoming connection to
those pods to be denied. This policy has no effect on isolation for egress from any pod.</p><h3 id="default-deny-all-egress-traffic">Default deny all egress traffic</h3><p>You can create a "default" egress isolation policy for a namespace by creating a NetworkPolicy
that selects all pods but does not allow any egress traffic from those pods.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-default-deny-egress.yaml"><code>service/networking/network-policy-default-deny-egress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/network-policy-default-deny-egress.yaml to clipboard"></div><div class="includecode" id="service-networking-network-policy-default-deny-egress-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>NetworkPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default-deny-egress<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>podSelector</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span><span>policyTypes</span>:<span>
</span></span></span><span><span><span>  </span>- Egress<span>
</span></span></span></code></pre></div></div></div><p>This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed
egress traffic. This policy does not change the ingress isolation behavior of any pod.</p><h3 id="allow-all-egress-traffic">Allow all egress traffic</h3><p>If you want to allow all connections from all pods in a namespace, you can create a policy that
explicitly allows all outgoing connections from pods in that namespace.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-allow-all-egress.yaml"><code>service/networking/network-policy-allow-all-egress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/network-policy-allow-all-egress.yaml to clipboard"></div><div class="includecode" id="service-networking-network-policy-allow-all-egress-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>NetworkPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>allow-all-egress<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>podSelector</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span><span>egress</span>:<span>
</span></span></span><span><span><span>  </span>- {}<span>
</span></span></span><span><span><span>  </span><span>policyTypes</span>:<span>
</span></span></span><span><span><span>  </span>- Egress<span>
</span></span></span></code></pre></div></div></div><p>With this policy in place, no additional policy or policies can cause any outgoing connection from
those pods to be denied. This policy has no effect on isolation for ingress to any pod.</p><h3 id="default-deny-all-ingress-and-all-egress-traffic">Default deny all ingress and all egress traffic</h3><p>You can create a "default" policy for a namespace which prevents all ingress AND egress traffic by
creating the following NetworkPolicy in that namespace.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-default-deny-all.yaml"><code>service/networking/network-policy-default-deny-all.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/network-policy-default-deny-all.yaml to clipboard"></div><div class="includecode" id="service-networking-network-policy-default-deny-all-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>NetworkPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>default-deny-all<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>podSelector</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span><span>policyTypes</span>:<span>
</span></span></span><span><span><span>  </span>- Ingress<span>
</span></span></span><span><span><span>  </span>- Egress<span>
</span></span></span></code></pre></div></div></div><p>This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed
ingress or egress traffic.</p><h2 id="network-traffic-filtering">Network traffic filtering</h2><p>NetworkPolicy is defined for <a href="https://en.wikipedia.org/wiki/OSI_model#Layer_4:_Transport_layer">layer 4</a>
connections (TCP, UDP, and optionally SCTP). For all the other protocols, the behaviour may vary
across network plugins.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You must be using a <a class="glossary-tooltip" title="Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification." href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank">CNI</a> plugin that supports SCTP
protocol NetworkPolicies.</div><p>When a <code>deny all</code> network policy is defined, it is only guaranteed to deny TCP, UDP and SCTP
connections. For other protocols, such as ARP or ICMP, the behaviour is undefined.
The same applies to allow rules: when a specific pod is allowed as ingress source or egress destination,
it is undefined what happens with (for example) ICMP packets. Protocols such as ICMP may be allowed by some
network plugins and denied by others.</p><h2 id="targeting-a-range-of-ports">Targeting a range of ports</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>When writing a NetworkPolicy, you can target a range of ports instead of a single port.</p><p>This is achievable with the usage of the <code>endPort</code> field, as the following example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/networkpolicy-multiport-egress.yaml"><code>service/networking/networkpolicy-multiport-egress.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/networkpolicy-multiport-egress.yaml to clipboard"></div><div class="includecode" id="service-networking-networkpolicy-multiport-egress-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>NetworkPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>multi-port-egress<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>podSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>role</span>:<span> </span>db<span>
</span></span></span><span><span><span>  </span><span>policyTypes</span>:<span>
</span></span></span><span><span><span>    </span>- Egress<span>
</span></span></span><span><span><span>  </span><span>egress</span>:<span>
</span></span></span><span><span><span>    </span>- <span>to</span>:<span>
</span></span></span><span><span><span>        </span>- <span>ipBlock</span>:<span>
</span></span></span><span><span><span>            </span><span>cidr</span>:<span> </span><span>10.0.0.0</span>/24<span>
</span></span></span><span><span><span>      </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>          </span><span>port</span>:<span> </span><span>32000</span><span>
</span></span></span><span><span><span>          </span><span>endPort</span>:<span> </span><span>32768</span><span>
</span></span></span><span><span><span>
</span></span></span></code></pre></div></div></div><p>The above rule allows any Pod with label <code>role=db</code> on the namespace <code>default</code> to communicate
with any IP within the range <code>10.0.0.0/24</code> over TCP, provided that the target
port is between the range 32000 and 32768.</p><p>The following restrictions apply when using this field:</p><ul><li>The <code>endPort</code> field must be equal to or greater than the <code>port</code> field.</li><li><code>endPort</code> can only be defined if <code>port</code> is also defined.</li><li>Both ports must be numeric.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Your cluster must be using a <a class="glossary-tooltip" title="Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification." href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank">CNI</a> plugin that
supports the <code>endPort</code> field in NetworkPolicy specifications.
If your <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugin</a>
does not support the <code>endPort</code> field and you specify a NetworkPolicy with that,
the policy will be applied only for the single <code>port</code> field.</div><h2 id="targeting-multiple-namespaces-by-label">Targeting multiple namespaces by label</h2><p>In this scenario, your <code>Egress</code> NetworkPolicy targets more than one namespace using their
label names. For this to work, you need to label the target namespaces. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl label namespace frontend <span>namespace</span><span>=</span>frontend
</span></span><span><span>kubectl label namespace backend <span>namespace</span><span>=</span>backend
</span></span></code></pre></div><p>Add the labels under <code>namespaceSelector</code> in your NetworkPolicy document. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>networking.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>NetworkPolicy<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>egress-namespaces<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>podSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>myapp<span>
</span></span></span><span><span><span>  </span><span>policyTypes</span>:<span>
</span></span></span><span><span><span>  </span>- Egress<span>
</span></span></span><span><span><span>  </span><span>egress</span>:<span>
</span></span></span><span><span><span>  </span>- <span>to</span>:<span>
</span></span></span><span><span><span>    </span>- <span>namespaceSelector</span>:<span>
</span></span></span><span><span><span>        </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>        </span>- <span>key</span>:<span> </span>namespace<span>
</span></span></span><span><span><span>          </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>          </span><span>values</span>:<span> </span>[<span>"frontend"</span>,<span> </span><span>"backend"</span>]<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>It is not possible to directly specify the name of the namespaces in a NetworkPolicy.
You must use a <code>namespaceSelector</code> with <code>matchLabels</code> or <code>matchExpressions</code> to select the
namespaces based on their labels.</div><h2 id="targeting-a-namespace-by-its-name">Targeting a Namespace by its name</h2><p>The Kubernetes control plane sets an immutable label <code>kubernetes.io/metadata.name</code> on all
namespaces, the value of the label is the namespace name.</p><p>While NetworkPolicy cannot target a namespace by its name with some object field, you can use the
standardized label to target a specific namespace.</p><h2 id="pod-lifecycle">Pod lifecycle</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The following applies to clusters with a conformant networking plugin and a conformant implementation of
NetworkPolicy.</div><p>When a new NetworkPolicy object is created, it may take some time for a network plugin
to handle the new object. If a pod that is affected by a NetworkPolicy
is created before the network plugin has completed NetworkPolicy handling,
that pod may be started unprotected, and isolation rules will be applied when
the NetworkPolicy handling is completed.</p><p>Once the NetworkPolicy is handled by a network plugin,</p><ol><li><p>All newly created pods affected by a given NetworkPolicy will be isolated before they are started.
Implementations of NetworkPolicy must ensure that filtering is effective throughout
the Pod lifecycle, even from the very first instant that any container in that Pod is started.
Because they are applied at Pod level, NetworkPolicies apply equally to init containers,
sidecar containers, and regular containers.</p></li><li><p>Allow rules will be applied eventually after the isolation rules (or may be applied at the same time).
In the worst case, a newly created pod may have no network connectivity at all when it is first started, if
isolation rules were already applied, but no allow rules were applied yet.</p></li></ol><p>Every created NetworkPolicy will be handled by a network plugin eventually, but there is no
way to tell from the Kubernetes API when exactly that happens.</p><p>Therefore, pods must be resilient against being started up with different network
connectivity than expected. If you need to make sure the pod can reach certain destinations
before being started, you can use an <a href="/docs/concepts/workloads/pods/init-containers/">init container</a>
to wait for those destinations to be reachable before kubelet starts the app containers.</p><p>Every NetworkPolicy will be applied to all selected pods eventually.
Because the network plugin may implement NetworkPolicy in a distributed manner,
it is possible that pods may see a slightly inconsistent view of network policies
when the pod is first created, or when pods or policies change.
For example, a newly-created pod that is supposed to be able to reach both Pod A
on Node 1 and Pod B on Node 2 may find that it can reach Pod A immediately,
but cannot reach Pod B until a few seconds later.</p><h2 id="networkpolicy-and-hostnetwork-pods">NetworkPolicy and <code>hostNetwork</code> pods</h2><p>NetworkPolicy behaviour for <code>hostNetwork</code> pods is undefined, but it should be limited to 2 possibilities:</p><ul><li>The network plugin can distinguish <code>hostNetwork</code> pod traffic from all other traffic
(including being able to distinguish traffic from different <code>hostNetwork</code> pods on
the same node), and will apply NetworkPolicy to <code>hostNetwork</code> pods just like it does
to pod-network pods.</li><li>The network plugin cannot properly distinguish <code>hostNetwork</code> pod traffic,
and so it ignores <code>hostNetwork</code> pods when matching <code>podSelector</code> and <code>namespaceSelector</code>.
Traffic to/from <code>hostNetwork</code> pods is treated the same as all other traffic to/from the node IP.
(This is the most common implementation.)</li></ul><p>This applies when</p><ol><li><p>a <code>hostNetwork</code> pod is selected by <code>spec.podSelector</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span>podSelector</span>:<span>
</span></span></span><span><span><span>      </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>        </span><span>role</span>:<span> </span>client<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div></li><li><p>a <code>hostNetwork</code> pod is selected by a <code>podSelector</code> or <code>namespaceSelector</code> in an <code>ingress</code> or <code>egress</code> rule.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span>...<span>
</span></span></span><span><span><span>  </span><span>ingress</span>:<span>
</span></span></span><span><span><span>    </span>- <span>from</span>:<span>
</span></span></span><span><span><span>      </span>- <span>podSelector</span>:<span>
</span></span></span><span><span><span>          </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>            </span><span>role</span>:<span> </span>client<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div></li></ol><p>At the same time, since <code>hostNetwork</code> pods have the same IP addresses as the nodes they reside on,
their connections will be treated as node connections. For example, you can allow traffic
from a <code>hostNetwork</code> Pod using an <code>ipBlock</code> rule.</p><h2 id="what-you-can-t-do-with-network-policies-at-least-not-yet">What you can't do with network policies (at least, not yet)</h2><p>As of Kubernetes 1.34, the following functionality does not exist in the
NetworkPolicy API, but you might be able to implement workarounds using Operating System
components (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress
controllers, Service Mesh implementations) or admission controllers. In case you are new to
network security in Kubernetes, its worth noting that the following User Stories cannot (yet) be
implemented using the NetworkPolicy API.</p><ul><li>Forcing internal cluster traffic to go through a common gateway (this might be best served with
a service mesh or other proxy).</li><li>Anything TLS related (use a service mesh or ingress controller for this).</li><li>Node specific policies (you can use CIDR notation for these, but you cannot target nodes by
their Kubernetes identities specifically).</li><li>Targeting of services by name (you can, however, target pods or namespaces by their
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">labels</a>, which is often a viable workaround).</li><li>Creation or management of "Policy requests" that are fulfilled by a third party.</li><li>Default policies which are applied to all namespaces or pods (there are some third party
Kubernetes distributions and projects which can do this).</li><li>Advanced policy querying and reachability tooling.</li><li>The ability to log network security events (for example connections that are blocked or accepted).</li><li>The ability to explicitly deny policies (currently the model for NetworkPolicies are deny by
default, with only the ability to add allow rules).</li><li>The ability to prevent loopback or incoming host traffic (Pods cannot currently block localhost
access, nor do they have the ability to block access from their resident node).</li></ul><h2 id="networkpolicy-s-impact-on-existing-connections">NetworkPolicy's impact on existing connections</h2><p>When the set of NetworkPolicies that applies to an existing connection changes - this could happen
either due to a change in NetworkPolicies or if the relevant labels of the namespaces/pods selected by the
policy (both subject and peers) are changed in the middle of an existing connection - it is
implementation defined as to whether the change will take effect for that existing connection or not.
Example: A policy is created that leads to denying a previously allowed connection, the underlying network plugin
implementation is responsible for defining if that new policy will close the existing connections or not.
It is recommended not to modify policies/pods/namespaces in ways that might affect existing connections.</p><h2 id="what-s-next">What's next</h2><ul><li>See the <a href="/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a>
walkthrough for further examples.</li><li>See more <a href="https://github.com/ahmetb/kubernetes-network-policy-recipes">recipes</a> for common
scenarios enabled by the NetworkPolicy resource.</li></ul></div></div><div><div class="td-content"><h1>DNS for Services and Pods</h1><div class="lead">Your workload can discover Services within your cluster using DNS; this page explains how that works.</div><p>Kubernetes creates DNS records for Services and Pods. You can contact
Services with consistent DNS names instead of IP addresses.</p><p>Kubernetes publishes information about Pods and Services which is used
to program DNS. kubelet configures Pods' DNS so that running containers
can look up Services by name rather than IP.</p><p>Services defined in the cluster are assigned DNS names. By default, a
client Pod's DNS search list includes the Pod's own namespace and the
cluster's default domain.</p><h3 id="namespaces-of-services">Namespaces of Services</h3><p>A DNS query may return different results based on the namespace of the Pod making
it. DNS queries that don't specify a namespace are limited to the Pod's
namespace. Access Services in other namespaces by specifying it in the DNS query.</p><p>For example, consider a Pod in a <code>test</code> namespace. A <code>data</code> Service is in
the <code>prod</code> namespace.</p><p>A query for <code>data</code> returns no results, because it uses the Pod's <code>test</code> namespace.</p><p>A query for <code>data.prod</code> returns the intended result, because it specifies the
namespace.</p><p>DNS queries may be expanded using the Pod's <code>/etc/resolv.conf</code>. kubelet
configures this file for each Pod. For example, a query for just <code>data</code> may be
expanded to <code>data.test.svc.cluster.local</code>. The values of the <code>search</code> option
are used to expand queries. To learn more about DNS queries, see
<a href="https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html">the <code>resolv.conf</code> manual page</a>.</p><pre tabindex="0"><code>nameserver 10.32.0.10
search &lt;namespace&gt;.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
</code></pre><p>In summary, a Pod in the <em>test</em> namespace can successfully resolve either
<code>data.prod</code> or <code>data.prod.svc.cluster.local</code>.</p><h3 id="dns-records">DNS Records</h3><p>What objects get DNS records?</p><ol><li>Services</li><li>Pods</li></ol><p>The following sections detail the supported DNS record types and layout that is
supported. Any other layout or names or queries that happen to work are
considered implementation details and are subject to change without warning.
For more up-to-date specification, see
<a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNS-Based Service Discovery</a>.</p><h2 id="services">Services</h2><h3 id="a-aaaa-records">A/AAAA records</h3><p>"Normal" (not headless) Services are assigned DNS A and/or AAAA records,
depending on the IP family or families of the Service, with a name of the form
<code>my-svc.my-namespace.svc.cluster-domain.example</code>. This resolves to the cluster IP
of the Service.</p><p><a href="/docs/concepts/services-networking/service/#headless-services">Headless Services</a>
(without a cluster IP) are also assigned DNS A and/or AAAA records,
with a name of the form <code>my-svc.my-namespace.svc.cluster-domain.example</code>. Unlike normal
Services, this resolves to the set of IPs of all of the Pods selected by the Service.
Clients are expected to consume the set or else use standard round-robin
selection from the set.</p><h3 id="srv-records">SRV records</h3><p>SRV Records are created for named ports that are part of normal or headless
services.</p><ul><li>For each named port, the SRV record has the form
<code>_port-name._port-protocol.my-svc.my-namespace.svc.cluster-domain.example</code>.</li><li>For a regular Service, this resolves to the port number and the domain name:
<code>my-svc.my-namespace.svc.cluster-domain.example</code>.</li><li>For a headless Service, this resolves to multiple answers, one for each Pod
that is backing the Service, and contains the port number and the domain name of the Pod
of the form <code>hostname.my-svc.my-namespace.svc.cluster-domain.example</code>.</li></ul><h2 id="pods">Pods</h2><h3 id="a-aaaa-records-1">A/AAAA records</h3><p>Kube-DNS versions, prior to the implementation of the
<a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">DNS specification</a>,
had the following DNS resolution:</p><pre tabindex="0"><code>&lt;pod-IPv4-address&gt;.&lt;namespace&gt;.pod.&lt;cluster-domain&gt;
</code></pre><p>For example, if a Pod in the <code>default</code> namespace has the IP address 172.17.0.3,
and the domain name for your cluster is <code>cluster.local</code>, then the Pod has a DNS name:</p><pre tabindex="0"><code>172-17-0-3.default.pod.cluster.local
</code></pre><p>Some cluster DNS mechanisms, like <a href="https://coredns.io/">CoreDNS</a>, also provide <code>A</code> records for:</p><pre tabindex="0"><code>&lt;pod-ipv4-address&gt;.&lt;service-name&gt;.&lt;my-namespace&gt;.svc.&lt;cluster-domain.example&gt;
</code></pre><p>For example, if a Pod in the <code>cafe</code> namespace has the IP address 172.17.0.3,
is an endpoint of a Service named <code>barista</code>, and the domain name for your cluster is
<code>cluster.local</code>, then the Pod would have this service-scoped DNS <code>A</code> record.</p><pre tabindex="0"><code>172-17-0-3.barista.cafe.svc.cluster.local
</code></pre><h3 id="pod-hostname-and-subdomain-field">Pod's hostname and subdomain fields</h3><p>Currently when a Pod is created, its hostname (as observed from within the Pod)
is the Pod's <code>metadata.name</code> value.</p><p>The Pod spec has an optional <code>hostname</code> field, which can be used to specify a
different hostname. When specified, it takes precedence over the Pod's name to be
the hostname of the Pod (again, as observed from within the Pod). For example,
given a Pod with <code>spec.hostname</code> set to <code>"my-host"</code>, the Pod will have its
hostname set to <code>"my-host"</code>.</p><p>The Pod spec also has an optional <code>subdomain</code> field which can be used to indicate
that the pod is part of sub-group of the namespace. For example, a Pod with <code>spec.hostname</code>
set to <code>"foo"</code>, and <code>spec.subdomain</code> set to <code>"bar"</code>, in namespace <code>"my-namespace"</code>, will
have its hostname set to <code>"foo"</code> and its fully qualified domain name (FQDN) set to
<code>"foo.bar.my-namespace.svc.cluster.local"</code> (once more, as observed from within
the Pod).</p><p>If there exists a headless Service in the same namespace as the Pod, with
the same name as the subdomain, the cluster's DNS Server also returns A and/or AAAA
records for the Pod's fully qualified hostname.</p><p>Example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>busybox-subdomain<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>busybox<span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span>None<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>foo<span> </span><span># name is not required for single-port Services</span><span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>1234</span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>busybox1<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>busybox<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hostname</span>:<span> </span>busybox-1<span>
</span></span></span><span><span><span>  </span><span>subdomain</span>:<span> </span>busybox-subdomain<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>      </span>- sleep<span>
</span></span></span><span><span><span>      </span>- <span>"3600"</span><span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>busybox<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>busybox2<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>busybox<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hostname</span>:<span> </span>busybox-2<span>
</span></span></span><span><span><span>  </span><span>subdomain</span>:<span> </span>busybox-subdomain<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>      </span>- sleep<span>
</span></span></span><span><span><span>      </span>- <span>"3600"</span><span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>busybox<span>
</span></span></span></code></pre></div><p>Given the above Service <code>"busybox-subdomain"</code> and the Pods which set <code>spec.subdomain</code>
to <code>"busybox-subdomain"</code>, the first Pod will see its own FQDN as
<code>"busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example"</code>. DNS serves
A and/or AAAA records at that name, pointing to the Pod's IP. Both Pods "<code>busybox1</code>" and
"<code>busybox2</code>" will have their own address records.</p><p>An <a class="glossary-tooltip" title="EndpointSlices track the IP addresses of Pods for Services." href="/docs/concepts/services-networking/endpoint-slices/" target="_blank">EndpointSlice</a> can specify
the DNS hostname for any endpoint addresses, along with its IP.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A and AAAA records are not created for Pod names since <code>hostname</code> is missing for the Pod.
A Pod with no <code>hostname</code> but with <code>subdomain</code> will only create the
A or AAAA record for the headless Service (<code>busybox-subdomain.my-namespace.svc.cluster-domain.example</code>),
pointing to the Pods' IP addresses. Also, the Pod needs to be ready in order to have a
record unless <code>publishNotReadyAddresses=True</code> is set on the Service.</div><h3 id="pod-sethostnameasfqdn-field">Pod's setHostnameAsFQDN field</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [stable]</code></div><p>When a Pod is configured to have fully qualified domain name (FQDN), its
hostname is the short hostname. For example, if you have a Pod with the fully
qualified domain name <code>busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example</code>,
then by default the <code>hostname</code> command inside that Pod returns <code>busybox-1</code> and the
<code>hostname --fqdn</code> command returns the FQDN.</p><p>When you set <code>setHostnameAsFQDN: true</code> in the Pod spec, the kubelet writes the Pod's FQDN
into the hostname for that Pod's namespace. In this case, both <code>hostname</code> and <code>hostname --fqdn</code>
return the Pod's FQDN.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>In Linux, the hostname field of the kernel (the <code>nodename</code> field of <code>struct utsname</code>) is limited to 64 characters.</p><p>If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start.
The Pod will remain in <code>Pending</code> status (<code>ContainerCreating</code> as seen by <code>kubectl</code>) generating
error events, such as Failed to construct FQDN from Pod hostname and cluster domain,
FQDN <code>long-FQDN</code> is too long (64 characters is the max, 70 characters requested).
One way of improving user experience for this scenario is to create an
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">admission webhook controller</a>
to control FQDN size when users create top level objects, for example, Deployment.</p></div><h3 id="pod-s-dns-policy">Pod's DNS Policy</h3><p>DNS policies can be set on a per-Pod basis. Currently Kubernetes supports the
following Pod-specific DNS policies. These policies are specified in the
<code>dnsPolicy</code> field of a Pod Spec.</p><ul><li><p>"<code>Default</code>": The Pod inherits the name resolution configuration from the node
that the Pods run on.
See <a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">related discussion</a>
for more details.</p></li><li><p>"<code>ClusterFirst</code>": Any DNS query that does not match the configured cluster
domain suffix, such as "<code>www.kubernetes.io</code>", is forwarded to an upstream
nameserver by the DNS server. Cluster administrators may have extra
stub-domain and upstream DNS servers configured.
See <a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">related discussion</a>
for details on how DNS queries are handled in those cases.</p></li><li><p>"<code>ClusterFirstWithHostNet</code>": For Pods running with hostNetwork, you should
explicitly set its DNS policy to "<code>ClusterFirstWithHostNet</code>". Otherwise, Pods
running with hostNetwork and <code>"ClusterFirst"</code> will fallback to the behavior
of the <code>"Default"</code> policy.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>This is not supported on Windows. See <a href="#dns-windows">below</a> for details.</div></li><li><p>"<code>None</code>": It allows a Pod to ignore DNS settings from the Kubernetes
environment. All DNS settings are supposed to be provided using the
<code>dnsConfig</code> field in the Pod Spec.
See <a href="#pod-dns-config">Pod's DNS config</a> subsection below.</p></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>"Default" is not the default DNS policy. If <code>dnsPolicy</code> is not
explicitly specified, then "ClusterFirst" is used.</div><p>The example below shows a Pod with its DNS policy set to
"<code>ClusterFirstWithHostNet</code>" because it has <code>hostNetwork</code> set to <code>true</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>busybox<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>      </span>- sleep<span>
</span></span></span><span><span><span>      </span>- <span>"3600"</span><span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>busybox<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Always<span>
</span></span></span><span><span><span>  </span><span>hostNetwork</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>dnsPolicy</span>:<span> </span>ClusterFirstWithHostNet<span>
</span></span></span></code></pre></div><h3 id="pod-dns-config">Pod's DNS Config</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.14 [stable]</code></div><p>Pod's DNS Config allows users more control on the DNS settings for a Pod.</p><p>The <code>dnsConfig</code> field is optional and it can work with any <code>dnsPolicy</code> settings.
However, when a Pod's <code>dnsPolicy</code> is set to "<code>None</code>", the <code>dnsConfig</code> field has
to be specified.</p><p>Below are the properties a user can specify in the <code>dnsConfig</code> field:</p><ul><li><code>nameservers</code>: a list of IP addresses that will be used as DNS servers for the
Pod. There can be at most 3 IP addresses specified. When the Pod's <code>dnsPolicy</code>
is set to "<code>None</code>", the list must contain at least one IP address, otherwise
this property is optional.
The servers listed will be combined to the base nameservers generated from the
specified DNS policy with duplicate addresses removed.</li><li><code>searches</code>: a list of DNS search domains for hostname lookup in the Pod.
This property is optional. When specified, the provided list will be merged
into the base search domain names generated from the chosen DNS policy.
Duplicate domain names are removed.
Kubernetes allows up to 32 search domains.</li><li><code>options</code>: an optional list of objects where each object may have a <code>name</code>
property (required) and a <code>value</code> property (optional). The contents in this
property will be merged to the options generated from the specified DNS policy.
Duplicate entries are removed.</li></ul><p>The following is an example Pod with custom DNS settings:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/custom-dns.yaml"><code>service/networking/custom-dns.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/custom-dns.yaml to clipboard"></div><div class="includecode" id="service-networking-custom-dns-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dns-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>dnsPolicy</span>:<span> </span><span>"None"</span><span>
</span></span></span><span><span><span>  </span><span>dnsConfig</span>:<span>
</span></span></span><span><span><span>    </span><span>nameservers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>192.0.2.1</span><span> </span><span># this is an example</span><span>
</span></span></span><span><span><span>    </span><span>searches</span>:<span>
</span></span></span><span><span><span>      </span>- ns1.svc.cluster-domain.example<span>
</span></span></span><span><span><span>      </span>- my.dns.search.suffix<span>
</span></span></span><span><span><span>    </span><span>options</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>ndots<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>edns0<span>
</span></span></span></code></pre></div></div></div><p>When the Pod above is created, the container <code>test</code> gets the following contents
in its <code>/etc/resolv.conf</code> file:</p><pre tabindex="0"><code>nameserver 192.0.2.1
search ns1.svc.cluster-domain.example my.dns.search.suffix
options ndots:2 edns0
</code></pre><p>For IPv6 setup, search path and name server should be set up like this:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl <span>exec</span> -it dns-example -- cat /etc/resolv.conf
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex="0"><code>nameserver 2001:db8:30::a
search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example
options ndots:5
</code></pre><h2 id="dns-search-domain-list-limits">DNS search domain list limits</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes 1.28 [stable]</code></div><p>Kubernetes itself does not limit the DNS Config until the length of the search
domain list exceeds 32 or the total length of all search domains exceeds 2048.
This limit applies to the node's resolver configuration file, the Pod's DNS
Config, and the merged DNS Config respectively.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Some container runtimes of earlier versions may have their own restrictions on
the number of DNS search domains. Depending on the container runtime
environment, the pods with a large number of DNS search domains may get stuck in
the pending state.</p><p>It is known that containerd v1.5.5 or earlier and CRI-O v1.21 or earlier have
this problem.</p></div><h2 id="dns-windows">DNS resolution on Windows nodes</h2><ul><li><code>ClusterFirstWithHostNet</code> is not supported for Pods that run on Windows nodes.
Windows treats all names with a <code>.</code> as a FQDN and skips FQDN resolution.</li><li>On Windows, there are multiple DNS resolvers that can be used. As these come with
slightly different behaviors, using the
<a href="https://docs.microsoft.com/powershell/module/dnsclient/resolve-dnsname"><code>Resolve-DNSName</code></a>
powershell cmdlet for name query resolutions is recommended.</li><li>On Linux, you have a DNS suffix list, which is used after resolution of a name as fully
qualified has failed.
On Windows, you can only have 1 DNS suffix, which is the DNS suffix associated with that
Pod's namespace (example: <code>mydns.svc.cluster.local</code>). Windows can resolve FQDNs, Services,
or network name which can be resolved with this single suffix. For example, a Pod spawned
in the <code>default</code> namespace, will have the DNS suffix <code>default.svc.cluster.local</code>.
Inside a Windows Pod, you can resolve both <code>kubernetes.default.svc.cluster.local</code>
and <code>kubernetes</code>, but not the partially qualified names (<code>kubernetes.default</code> or
<code>kubernetes.default.svc</code>).</li></ul><h2 id="what-s-next">What's next</h2><p>For guidance on administering DNS configurations, check
<a href="/docs/tasks/administer-cluster/dns-custom-nameservers/">Configure DNS Service</a>.</p></div></div><div><div class="td-content"><h1>IPv4/IPv6 dual-stack</h1><div class="lead">Kubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or dual stack networking with both network families active. This page explains how.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>IPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> and <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Services</a>.</p><p>IPv4/IPv6 dual-stack networking is enabled by default for your Kubernetes cluster starting in
1.21, allowing the simultaneous assignment of both IPv4 and IPv6 addresses.</p><h2 id="supported-features">Supported Features</h2><p>IPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:</p><ul><li>Dual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)</li><li>IPv4 and IPv6 enabled Services</li><li>Pod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces</li></ul><h2 id="prerequisites">Prerequisites</h2><p>The following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes clusters:</p><ul><li><p>Kubernetes 1.20 or later</p><p>For information about using dual-stack services with earlier
Kubernetes versions, refer to the documentation for that version
of Kubernetes.</p></li><li><p>Provider support for dual-stack networking (Cloud provider or otherwise must be able to provide
Kubernetes nodes with routable IPv4/IPv6 network interfaces)</p></li><li><p>A <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugin</a> that
supports dual-stack networking.</p></li></ul><h2 id="configure-ipv4-ipv6-dual-stack">Configure IPv4/IPv6 dual-stack</h2><p>To configure IPv4/IPv6 dual-stack, set dual-stack cluster network assignments:</p><ul><li>kube-apiserver:<ul><li><code>--service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li></ul></li><li>kube-controller-manager:<ul><li><code>--cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li><li><code>--service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li><li><code>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code> defaults to /24 for IPv4 and /64 for IPv6</li></ul></li><li>kube-proxy:<ul><li><code>--cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li></ul></li><li>kubelet:<ul><li><code>--node-ip=&lt;IPv4 IP&gt;,&lt;IPv6 IP&gt;</code><ul><li>This option is required for bare metal dual-stack nodes (nodes that do not define a
cloud provider with the <code>--cloud-provider</code> flag). If you are using a cloud provider
and choose to override the node IPs chosen by the cloud provider, set the
<code>--node-ip</code> option.</li><li>(The legacy built-in cloud providers do not support dual-stack <code>--node-ip</code>.)</li></ul></li></ul></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>An example of an IPv4 CIDR: <code>10.244.0.0/16</code> (though you would supply your own address range)</p><p>An example of an IPv6 CIDR: <code>fdXY:IJKL:MNOP:15::/64</code> (this shows the format but is not a valid
address - see <a href="https://tools.ietf.org/html/rfc4193">RFC 4193</a>)</p></div><h2 id="services">Services</h2><p>You can create <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Services</a> which can use IPv4, IPv6, or both.</p><p>The address family of a Service defaults to the address family of the first service cluster IP
range (configured via the <code>--service-cluster-ip-range</code> flag to the kube-apiserver).</p><p>When you define a Service you can optionally configure it as dual stack. To specify the behavior you want, you
set the <code>.spec.ipFamilyPolicy</code> field to one of the following values:</p><ul><li><code>SingleStack</code>: Single-stack service. The control plane allocates a cluster IP for the Service,
using the first configured service cluster IP range.</li><li><code>PreferDualStack</code>: Allocates both IPv4 and IPv6 cluster IPs for the Service when dual-stack is enabled. If dual-stack is not enabled or supported, it falls back to single-stack behavior.</li><li><code>RequireDualStack</code>: Allocates Service <code>.spec.clusterIPs</code> from both IPv4 and IPv6 address ranges when dual-stack is enabled. If dual-stack is not enabled or supported, the Service API object creation fails.<ul><li>Selects the <code>.spec.clusterIP</code> from the list of <code>.spec.clusterIPs</code> based on the address family
of the first element in the <code>.spec.ipFamilies</code> array.</li></ul></li></ul><p>If you would like to define which IP family to use for single stack or define the order of IP
families for dual-stack, you can choose the address families by setting an optional field,
<code>.spec.ipFamilies</code>, on the Service.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>.spec.ipFamilies</code> field is conditionally mutable: you can add or remove a secondary
IP address family, but you cannot change the primary IP address family of an existing Service.</div><p>You can set <code>.spec.ipFamilies</code> to any of the following array values:</p><ul><li><code>["IPv4"]</code></li><li><code>["IPv6"]</code></li><li><code>["IPv4","IPv6"]</code> (dual stack)</li><li><code>["IPv6","IPv4"]</code> (dual stack)</li></ul><p>The first family you list is used for the legacy <code>.spec.clusterIP</code> field.</p><h3 id="dual-stack-service-configuration-scenarios">Dual-stack Service configuration scenarios</h3><p>These examples demonstrate the behavior of various dual-stack Service configuration scenarios.</p><h4 id="dual-stack-options-on-new-services">Dual-stack options on new Services</h4><ol><li><p>This Service specification does not explicitly define <code>.spec.ipFamilyPolicy</code>. When you create
this Service, Kubernetes assigns a cluster IP for the Service from the first configured
<code>service-cluster-ip-range</code> and sets the <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code>. (<a href="/docs/concepts/services-networking/service/#services-without-selectors">Services
without selectors</a> and
<a href="/docs/concepts/services-networking/service/#headless-services">headless Services</a> with selectors
will behave in this same way.)</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard"></div><div class="includecode" id="service-networking-dual-stack-default-svc-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div></li><li><p>This Service specification explicitly defines <code>PreferDualStack</code> in <code>.spec.ipFamilyPolicy</code>. When
you create this Service on a dual-stack cluster, Kubernetes assigns both IPv4 and IPv6
addresses for the service. The control plane updates the <code>.spec</code> for the Service to record the IP
address assignments. The field <code>.spec.clusterIPs</code> is the primary field, and contains both assigned
IP addresses; <code>.spec.clusterIP</code> is a secondary field with its value calculated from
<code>.spec.clusterIPs</code>.</p><ul><li>For the <code>.spec.clusterIP</code> field, the control plane records the IP address that is from the
same address family as the first service cluster IP range.</li><li>On a single-stack cluster, the <code>.spec.clusterIPs</code> and <code>.spec.clusterIP</code> fields both only list
one address.</li><li>On a cluster with dual-stack enabled, specifying <code>RequireDualStack</code> in <code>.spec.ipFamilyPolicy</code>
behaves the same as <code>PreferDualStack</code>.</li></ul><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-preferred-svc.yaml"><code>service/networking/dual-stack-preferred-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/dual-stack-preferred-svc.yaml to clipboard"></div><div class="includecode" id="service-networking-dual-stack-preferred-svc-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ipFamilyPolicy</span>:<span> </span>PreferDualStack<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div></li><li><p>This Service specification explicitly defines <code>IPv6</code> and <code>IPv4</code> in <code>.spec.ipFamilies</code> as well
as defining <code>PreferDualStack</code> in <code>.spec.ipFamilyPolicy</code>. When Kubernetes assigns an IPv6 and
IPv4 address in <code>.spec.clusterIPs</code>, <code>.spec.clusterIP</code> is set to the IPv6 address because that is
the first element in the <code>.spec.clusterIPs</code> array, overriding the default.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-preferred-ipfamilies-svc.yaml"><code>service/networking/dual-stack-preferred-ipfamilies-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/dual-stack-preferred-ipfamilies-svc.yaml to clipboard"></div><div class="includecode" id="service-networking-dual-stack-preferred-ipfamilies-svc-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ipFamilyPolicy</span>:<span> </span>PreferDualStack<span>
</span></span></span><span><span><span>  </span><span>ipFamilies</span>:<span>
</span></span></span><span><span><span>  </span>- IPv6<span>
</span></span></span><span><span><span>  </span>- IPv4<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div></li></ol><h4 id="dual-stack-defaults-on-existing-services">Dual-stack defaults on existing Services</h4><p>These examples demonstrate the default behavior when dual-stack is newly enabled on a cluster
where Services already exist. (Upgrading an existing cluster to 1.21 or beyond will enable
dual-stack.)</p><ol><li><p>When dual-stack is enabled on a cluster, existing Services (whether <code>IPv4</code> or <code>IPv6</code>) are
configured by the control plane to set <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code> and set
<code>.spec.ipFamilies</code> to the address family of the existing Service. The existing Service cluster IP
will be stored in <code>.spec.clusterIPs</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard"></div><div class="includecode" id="service-networking-dual-stack-default-svc-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>You can validate this behavior by using kubectl to inspect an existing service.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get svc my-service -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span><span>10.0.197.123</span><span>
</span></span></span><span><span><span>  </span><span>clusterIPs</span>:<span>
</span></span></span><span><span><span>  </span>- <span>10.0.197.123</span><span>
</span></span></span><span><span><span>  </span><span>ipFamilies</span>:<span>
</span></span></span><span><span><span>  </span>- IPv4<span>
</span></span></span><span><span><span>  </span><span>ipFamilyPolicy</span>:<span> </span>SingleStack<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>ClusterIP<span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>loadBalancer</span>:<span> </span>{}<span>
</span></span></span></code></pre></div></li><li><p>When dual-stack is enabled on a cluster, existing
<a href="/docs/concepts/services-networking/service/#headless-services">headless Services</a> with selectors are
configured by the control plane to set <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code> and set
<code>.spec.ipFamilies</code> to the address family of the first service cluster IP range (configured via the
<code>--service-cluster-ip-range</code> flag to the kube-apiserver) even though <code>.spec.clusterIP</code> is set to
<code>None</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard"></div><div class="includecode" id="service-networking-dual-stack-default-svc-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span></code></pre></div></div></div><p>You can validate this behavior by using kubectl to inspect an existing headless service with selectors.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get svc my-service -o yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span>None<span>
</span></span></span><span><span><span>  </span><span>clusterIPs</span>:<span>
</span></span></span><span><span><span>  </span>- None<span>
</span></span></span><span><span><span>  </span><span>ipFamilies</span>:<span>
</span></span></span><span><span><span>  </span>- IPv4<span>
</span></span></span><span><span><span>  </span><span>ipFamilyPolicy</span>:<span> </span>SingleStack<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span></code></pre></div></li></ol><h4 id="switching-services-between-single-stack-and-dual-stack">Switching Services between single-stack and dual-stack</h4><p>Services can be changed from single-stack to dual-stack and from dual-stack to single-stack.</p><ol><li><p>To change a Service from single-stack to dual-stack, change <code>.spec.ipFamilyPolicy</code> from
<code>SingleStack</code> to <code>PreferDualStack</code> or <code>RequireDualStack</code> as desired. When you change this
Service from single-stack to dual-stack, Kubernetes assigns the missing address family so that the
Service now has IPv4 and IPv6 addresses.</p><p>Edit the Service specification updating the <code>.spec.ipFamilyPolicy</code> from <code>SingleStack</code> to <code>PreferDualStack</code>.</p><p>Before:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ipFamilyPolicy</span>:<span> </span>SingleStack<span>
</span></span></span></code></pre></div><p>After:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ipFamilyPolicy</span>:<span> </span>PreferDualStack<span>
</span></span></span></code></pre></div></li><li><p>To change a Service from dual-stack to single-stack, change <code>.spec.ipFamilyPolicy</code> from
<code>PreferDualStack</code> or <code>RequireDualStack</code> to <code>SingleStack</code>. When you change this Service from
dual-stack to single-stack, Kubernetes retains only the first element in the <code>.spec.clusterIPs</code>
array, and sets <code>.spec.clusterIP</code> to that IP address and sets <code>.spec.ipFamilies</code> to the address
family of <code>.spec.clusterIPs</code>.</p></li></ol><h3 id="headless-services-without-selector">Headless Services without selector</h3><p>For <a href="/docs/concepts/services-networking/service/#without-selectors">Headless Services without selectors</a>
and without <code>.spec.ipFamilyPolicy</code> explicitly set, the <code>.spec.ipFamilyPolicy</code> field defaults to
<code>RequireDualStack</code>.</p><h3 id="service-type-loadbalancer">Service type LoadBalancer</h3><p>To provision a dual-stack load balancer for your Service:</p><ul><li>Set the <code>.spec.type</code> field to <code>LoadBalancer</code></li><li>Set <code>.spec.ipFamilyPolicy</code> field to <code>PreferDualStack</code> or <code>RequireDualStack</code></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To use a dual-stack <code>LoadBalancer</code> type Service, your cloud provider must support IPv4 and IPv6
load balancers.</div><h2 id="egress-traffic">Egress traffic</h2><p>If you want to enable egress traffic in order to reach off-cluster destinations (eg. the public
Internet) from a Pod that uses non-publicly routable IPv6 addresses, you need to enable the Pod to
use a publicly routed IPv6 address via a mechanism such as transparent proxying or IP
masquerading. The <a href="https://github.com/kubernetes-sigs/ip-masq-agent">ip-masq-agent</a> project
supports IP masquerading on dual-stack clusters.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Ensure your <a class="glossary-tooltip" title="Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification." href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank">CNI</a> provider supports IPv6.</div><h2 id="windows-support">Windows support</h2><p>Kubernetes on Windows does not support single-stack "IPv6-only" networking. However,
dual-stack IPv4/IPv6 networking for pods and nodes with single-family services
is supported.</p><p>You can use IPv4/IPv6 dual-stack networking with <code>l2bridge</code> networks.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Overlay (VXLAN) networks on Windows <strong>do not</strong> support dual-stack networking.</div><p>You can read more about the different network modes for Windows within the
<a href="/docs/concepts/services-networking/windows-networking/#network-modes">Networking on Windows</a> topic.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/network/validate-dual-stack/">Validate IPv4/IPv6 dual-stack</a> networking</li><li><a href="/docs/setup/production-environment/tools/kubeadm/dual-stack-support/">Enable dual-stack networking using kubeadm</a></li></ul></div></div><div><div class="td-content"><h1>Topology Aware Routing</h1><div class="lead"><em>Topology Aware Routing</em> provides a mechanism to help keep network traffic within the zone where it originated. Preferring same-zone traffic between Pods in your cluster can help with reliability, performance (network latency and throughput), or cost.</div><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [beta]</code></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Prior to Kubernetes 1.27, this feature was known as <em>Topology Aware Hints</em>.</div><p><em>Topology Aware Routing</em> adjusts routing behavior to prefer keeping traffic in
the zone it originated from. In some cases this can help reduce costs or improve
network performance.</p><h2 id="motivation">Motivation</h2><p>Kubernetes clusters are increasingly deployed in multi-zone environments.
<em>Topology Aware Routing</em> provides a mechanism to help keep traffic within the
zone it originated from. When calculating the endpoints for a <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a>, the EndpointSlice controller considers
the topology (region and zone) of each endpoint and populates the hints field to
allocate it to a zone. Cluster components such as <a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank">kube-proxy</a> can then consume those hints, and use
them to influence how the traffic is routed (favoring topologically closer
endpoints).</p><h2 id="enabling-topology-aware-routing">Enabling Topology Aware Routing</h2><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Prior to Kubernetes 1.27, this behavior was controlled using the
<code>service.kubernetes.io/topology-aware-hints</code> annotation.</div><p>You can enable Topology Aware Routing for a Service by setting the
<code>service.kubernetes.io/topology-mode</code> annotation to <code>Auto</code>. When there are
enough endpoints available in each zone, Topology Hints will be populated on
EndpointSlices to allocate individual endpoints to specific zones, resulting in
traffic being routed closer to where it originated from.</p><h2 id="when-it-works-best">When it works best</h2><p>This feature works best when:</p><h3 id="1-incoming-traffic-is-evenly-distributed">1. Incoming traffic is evenly distributed</h3><p>If a large proportion of traffic is originating from a single zone, that traffic
could overload the subset of endpoints that have been allocated to that zone.
This feature is not recommended when incoming traffic is expected to originate
from a single zone.</p><h3 id="three-or-more-endpoints-per-zone">2. The Service has 3 or more endpoints per zone</h3><p>In a three zone cluster, this means 9 or more endpoints. If there are fewer than
3 endpoints per zone, there is a high (&#8776;50%) probability that the EndpointSlice
controller will not be able to allocate endpoints evenly and instead will fall
back to the default cluster-wide routing approach.</p><h2 id="how-it-works">How It Works</h2><p>The "Auto" heuristic attempts to proportionally allocate a number of endpoints
to each zone. Note that this heuristic works best for Services that have a
significant number of endpoints.</p><h3 id="implementation-control-plane">EndpointSlice controller</h3><p>The EndpointSlice controller is responsible for setting hints on EndpointSlices
when this heuristic is enabled. The controller allocates a proportional amount of
endpoints to each zone. This proportion is based on the
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">allocatable</a>
CPU cores for nodes running in that zone. For example, if one zone had 2 CPU
cores and another zone only had 1 CPU core, the controller would allocate twice
as many endpoints to the zone with 2 CPU cores.</p><p>The following example shows what an EndpointSlice looks like when hints have
been populated:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>discovery.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>EndpointSlice<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-hints<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/service-name</span>:<span> </span>example-svc<span>
</span></span></span><span><span><span></span><span>addressType</span>:<span> </span>IPv4<span>
</span></span></span><span><span><span></span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>http<span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span></span><span>endpoints</span>:<span>
</span></span></span><span><span><span>  </span>- <span>addresses</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"10.1.2.3"</span><span>
</span></span></span><span><span><span>    </span><span>conditions</span>:<span>
</span></span></span><span><span><span>      </span><span>ready</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>hostname</span>:<span> </span>pod-1<span>
</span></span></span><span><span><span>    </span><span>zone</span>:<span> </span>zone-a<span>
</span></span></span><span><span><span>    </span><span>hints</span>:<span>
</span></span></span><span><span><span>      </span><span>forZones</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span><span>"zone-a"</span><span>
</span></span></span></code></pre></div><h3 id="implementation-kube-proxy">kube-proxy</h3><p>The kube-proxy component filters the endpoints it routes to based on the hints set by
the EndpointSlice controller. In most cases, this means that the kube-proxy is able
to route traffic to endpoints in the same zone. Sometimes the controller allocates endpoints
from a different zone to ensure more even distribution of endpoints between zones.
This would result in some traffic being routed to other zones.</p><h2 id="safeguards">Safeguards</h2><p>The Kubernetes control plane and the kube-proxy on each node apply some
safeguard rules before using Topology Aware Hints. If these don't check out,
the kube-proxy selects endpoints from anywhere in your cluster, regardless of the
zone.</p><ol><li><p><strong>Insufficient number of endpoints:</strong> If there are less endpoints than zones
in a cluster, the controller will not assign any hints.</p></li><li><p><strong>Impossible to achieve balanced allocation:</strong> In some cases, it will be
impossible to achieve a balanced allocation of endpoints among zones. For
example, if zone-a is twice as large as zone-b, but there are only 2
endpoints, an endpoint allocated to zone-a may receive twice as much traffic
as zone-b. The controller does not assign hints if it can't get this "expected
overload" value below an acceptable threshold for each zone. Importantly this
is not based on real-time feedback. It is still possible for individual
endpoints to become overloaded.</p></li><li><p><strong>One or more Nodes has insufficient information:</strong> If any node does not have
a <code>topology.kubernetes.io/zone</code> label or is not reporting a value for
allocatable CPU, the control plane does not set any topology-aware endpoint
hints and so kube-proxy does not filter endpoints by zone.</p></li><li><p><strong>One or more endpoints does not have a zone hint:</strong> When this happens,
the kube-proxy assumes that a transition from or to Topology Aware Hints is
underway. Filtering endpoints for a Service in this state would be dangerous
so the kube-proxy falls back to using all endpoints.</p></li><li><p><strong>A zone is not represented in hints:</strong> If the kube-proxy is unable to find
at least one endpoint with a hint targeting the zone it is running in, it falls
back to using endpoints from all zones. This is most likely to happen as you add
a new zone into your existing cluster.</p></li></ol><h2 id="constraints">Constraints</h2><ul><li><p>Topology Aware Hints are not used when <code>internalTrafficPolicy</code> is set to <code>Local</code>
on a Service. It is possible to use both features in the same cluster on different
Services, just not on the same Service.</p></li><li><p>This approach will not work well for Services that have a large proportion of
traffic originating from a subset of zones. Instead this assumes that incoming
traffic will be roughly proportional to the capacity of the Nodes in each
zone.</p></li><li><p>The EndpointSlice controller ignores unready nodes as it calculates the
proportions of each zone. This could have unintended consequences if a large
portion of nodes are unready.</p></li><li><p>The EndpointSlice controller ignores nodes with the
<code>node-role.kubernetes.io/control-plane</code> or <code>node-role.kubernetes.io/master</code>
label set. This could be problematic if workloads are also running on those
nodes.</p></li><li><p>The EndpointSlice controller does not take into account <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint." href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">tolerations</a> when deploying or calculating the
proportions of each zone. If the Pods backing a Service are limited to a
subset of Nodes in the cluster, this will not be taken into account.</p></li><li><p>This may not work well with autoscaling. For example, if a lot of traffic is
originating from a single zone, only the endpoints allocated to that zone will
be handling that traffic. That could result in <a class="glossary-tooltip" title="Object that automatically scales the number of pod replicas based on targeted resource utilization or custom metric targets." href="/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank">Horizontal Pod Autoscaler</a>
either not picking up on this event, or newly added pods starting in a
different zone.</p></li></ul><h2 id="custom-heuristics">Custom heuristics</h2><p>Kubernetes is deployed in many different ways, there is no single heuristic for
allocating endpoints to zones will work for every use case. A key goal of this
feature is to enable custom heuristics to be developed if the built in heuristic
does not work for your use case. The first steps to enable custom heuristics
were included in the 1.27 release. This is a limited implementation that may not
yet cover some relevant and plausible situations.</p><h2 id="what-s-next">What's next</h2><ul><li>Follow the <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a> tutorial</li><li>Learn about the
<a href="/docs/concepts/services-networking/service/#traffic-distribution">trafficDistribution</a>
field, which is closely related to the <code>service.kubernetes.io/topology-mode</code>
annotation and provides flexible options for traffic routing within
Kubernetes.</li></ul></div></div><div><div class="td-content"><h1>Networking on Windows</h1><p>Kubernetes supports running nodes on either Linux or Windows. You can mix both kinds of node
within a single cluster.
This page provides an overview to networking specific to the Windows operating system.</p><h2 id="networking">Container networking on Windows</h2><p>Networking for Windows containers is exposed through
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">CNI plugins</a>.
Windows containers function similarly to virtual machines in regards to
networking. Each container has a virtual network adapter (vNIC) which is connected
to a Hyper-V virtual switch (vSwitch). The Host Networking Service (HNS) and the
Host Compute Service (HCS) work together to create containers and attach container
vNICs to networks. HCS is responsible for the management of containers whereas HNS
is responsible for the management of networking resources such as:</p><ul><li>Virtual networks (including creation of vSwitches)</li><li>Endpoints / vNICs</li><li>Namespaces</li><li>Policies including packet encapsulations, load-balancing rules, ACLs, and NAT rules.</li></ul><p>The Windows HNS and vSwitch implement namespacing and can
create virtual NICs as needed for a pod or container. However, many configurations such
as DNS, routes, and metrics are stored in the Windows registry database rather than as
files inside <code>/etc</code>, which is how Linux stores those configurations. The Windows registry for the container
is separate from that of the host, so concepts like mapping <code>/etc/resolv.conf</code> from
the host into a container don't have the same effect they would on Linux. These must
be configured using Windows APIs run in the context of that container. Therefore
CNI implementations need to call the HNS instead of relying on file mappings to pass
network details into the pod or container.</p><h2 id="network-modes">Network modes</h2><p>Windows supports five different networking drivers/modes: L2bridge, L2tunnel,
Overlay (Beta), Transparent, and NAT. In a heterogeneous cluster with Windows and Linux
worker nodes, you need to select a networking solution that is compatible on both
Windows and Linux. The following table lists the out-of-tree plugins are supported on Windows,
with recommendations on when to use each CNI:</p><table><thead><tr><th>Network Driver</th><th>Description</th><th>Container Packet Modifications</th><th>Network Plugins</th><th>Network Plugin Characteristics</th></tr></thead><tbody><tr><td>L2bridge</td><td>Containers are attached to an external vSwitch. Containers are attached to the underlay network, although the physical network doesn't need to learn the container MACs because they are rewritten on ingress/egress.</td><td>MAC is rewritten to host MAC, IP may be rewritten to host IP using HNS OutboundNAT policy.</td><td><a href="https://www.cni.dev/plugins/current/main/win-bridge/">win-bridge</a>, <a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md">Azure-CNI</a>, <a href="https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#host-gw">Flannel host-gateway</a> uses win-bridge</td><td>win-bridge uses L2bridge network mode, connects containers to the underlay of hosts, offering best performance. Requires user-defined routes (UDR) for inter-node connectivity.</td></tr><tr><td>L2Tunnel</td><td>This is a special case of l2bridge, but only used on Azure. All packets are sent to the virtualization host where SDN policy is applied.</td><td>MAC rewritten, IP visible on the underlay network</td><td><a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md">Azure-CNI</a></td><td>Azure-CNI allows integration of containers with Azure vNET, and allows them to leverage the set of capabilities that <a href="https://azure.microsoft.com/en-us/services/virtual-network/">Azure Virtual Network provides</a>. For example, securely connect to Azure services or use Azure NSGs. See <a href="https://docs.microsoft.com/azure/aks/concepts-network#azure-cni-advanced-networking">azure-cni for some examples</a></td></tr><tr><td>Overlay</td><td>Containers are given a vNIC connected to an external vSwitch. Each overlay network gets its own IP subnet, defined by a custom IP prefix.The overlay network driver uses VXLAN encapsulation.</td><td>Encapsulated with an outer header.</td><td><a href="https://www.cni.dev/plugins/current/main/win-overlay/">win-overlay</a>, <a href="https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#vxlan">Flannel VXLAN</a> (uses win-overlay)</td><td>win-overlay should be used when virtual container networks are desired to be isolated from underlay of hosts (e.g. for security reasons). Allows for IPs to be re-used for different overlay networks (which have different VNID tags) if you are restricted on IPs in your datacenter. This option requires <a href="https://support.microsoft.com/help/4489899">KB4489899</a> on Windows Server 2019.</td></tr><tr><td>Transparent (special use case for <a href="https://github.com/openvswitch/ovn-kubernetes">ovn-kubernetes</a>)</td><td>Requires an external vSwitch. Containers are attached to an external vSwitch which enables intra-pod communication via logical networks (logical switches and routers).</td><td>Packet is encapsulated either via <a href="https://datatracker.ietf.org/doc/draft-gross-geneve/">GENEVE</a> or <a href="https://datatracker.ietf.org/doc/draft-davie-stt/">STT</a> tunneling to reach pods which are not on the same host.<br>Packets are forwarded or dropped via the tunnel metadata information supplied by the ovn network controller.<br>NAT is done for north-south communication.</td><td><a href="https://github.com/openvswitch/ovn-kubernetes">ovn-kubernetes</a></td><td><a href="https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib">Deploy via ansible</a>. Distributed ACLs can be applied via Kubernetes policies. IPAM support. Load-balancing can be achieved without kube-proxy. NATing is done without using iptables/netsh.</td></tr><tr><td>NAT (<em>not used in Kubernetes</em>)</td><td>Containers are given a vNIC connected to an internal vSwitch. DNS/DHCP is provided using an internal component called <a href="https://techcommunity.microsoft.com/t5/virtualization/windows-nat-winnat-capabilities-and-limitations/ba-p/382303">WinNAT</a></td><td>MAC and IP is rewritten to host MAC/IP.</td><td><a href="https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat">nat</a></td><td>Included here for completeness</td></tr></tbody></table><p>As outlined above, the <a href="https://github.com/coreos/flannel">Flannel</a>
<a href="https://github.com/flannel-io/cni-plugin">CNI plugin</a>
is also <a href="https://github.com/flannel-io/cni-plugin#windows-support-experimental">supported</a> on Windows via the
<a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan">VXLAN network backend</a> (<strong>Beta support</strong> ; delegates to win-overlay)
and <a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw">host-gateway network backend</a> (stable support; delegates to win-bridge).</p><p>This plugin supports delegating to one of the reference CNI plugins (win-overlay,
win-bridge), to work in conjunction with Flannel daemon on Windows (Flanneld) for
automatic node subnet lease assignment and HNS network creation. This plugin reads
in its own configuration file (cni.conf), and aggregates it with the environment
variables from the FlannelD generated subnet.env file. It then delegates to one of
the reference CNI plugins for network plumbing, and sends the correct configuration
containing the node-assigned subnet to the IPAM plugin (for example: <code>host-local</code>).</p><p>For Node, Pod, and Service objects, the following network flows are supported for
TCP/UDP traffic:</p><ul><li>Pod &#8594; Pod (IP)</li><li>Pod &#8594; Pod (Name)</li><li>Pod &#8594; Service (Cluster IP)</li><li>Pod &#8594; Service (PQDN, but only if there are no ".")</li><li>Pod &#8594; Service (FQDN)</li><li>Pod &#8594; external (IP)</li><li>Pod &#8594; external (DNS)</li><li>Node &#8594; Pod</li><li>Pod &#8594; Node</li></ul><h2 id="ipam">IP address management (IPAM)</h2><p>The following IPAM options are supported on Windows:</p><ul><li><a href="https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local">host-local</a></li><li><a href="https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md">azure-vnet-ipam</a> (for azure-cni only)</li><li><a href="https://docs.microsoft.com/windows-server/networking/technologies/ipam/ipam-top">Windows Server IPAM</a> (fallback option if no IPAM is set)</li></ul><h2 id="dsr">Direct Server Return (DSR)</h2><div class="feature-state-notice feature-stable" title="Feature Gate: WinDSR"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>Load balancing mode where the IP address fixups and the LBNAT occurs at the container vSwitch port directly;
service traffic arrives with the source IP set as the originating pod IP.
This provides performance optimizations by allowing the return traffic routed through load balancers
to bypass the load balancer and respond directly to the client;
reducing load on the load balancer and also reducing overall latency.
For more information, read
<a href="https://techcommunity.microsoft.com/blog/networkingblog/direct-server-return-dsr-in-a-nutshell/693710">Direct Server Return (DSR) in a nutshell</a>.</p><h2 id="load-balancing-and-services">Load balancing and Services</h2><p>A Kubernetes <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a> is an abstraction
that defines a logical set of Pods and a means to access them over a network.
In a cluster that includes Windows nodes, you can use the following types of Service:</p><ul><li><code>NodePort</code></li><li><code>ClusterIP</code></li><li><code>LoadBalancer</code></li><li><code>ExternalName</code></li></ul><p>Windows container networking differs in some important ways from Linux networking.
The <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture">Microsoft documentation for Windows Container Networking</a>
provides additional details and background.</p><p>On Windows, you can use the following settings to configure Services and load
balancing behavior:</p><table><caption>Windows Service Settings</caption><thead><tr><th>Feature</th><th>Description</th><th>Minimum Supported Windows OS build</th><th>How to enable</th></tr></thead><tbody><tr><td>Session affinity</td><td>Ensures that connections from a particular client are passed to the same Pod each time.</td><td>Windows Server 2022</td><td>Set <code>service.spec.sessionAffinity</code> to "ClientIP"</td></tr><tr><td>Direct Server Return (DSR)</td><td>See <a href="#dsr">DSR</a> notes above.</td><td>Windows Server 2019</td><td>Set the following command line argument (assuming version 1.34): <code>--enable-dsr=true</code></td></tr><tr><td>Preserve-Destination</td><td>Skips DNAT of service traffic, thereby preserving the virtual IP of the target service in packets reaching the backend Pod. Also disables node-node forwarding.</td><td>Windows Server, version 1903</td><td>Set <code>"preserve-destination": "true"</code> in service annotations and enable DSR in kube-proxy.</td></tr><tr><td>IPv4/IPv6 dual-stack networking</td><td>Native IPv4-to-IPv4 in parallel with IPv6-to-IPv6 communications to, from, and within a cluster</td><td>Windows Server 2019</td><td>See <a href="/docs/concepts/services-networking/dual-stack/#windows-support">IPv4/IPv6 dual-stack</a></td></tr><tr><td>Client IP preservation</td><td>Ensures that source IP of incoming ingress traffic gets preserved. Also disables node-node forwarding.</td><td>Windows Server 2019</td><td>Set <code>service.spec.externalTrafficPolicy</code> to "Local" and enable DSR in kube-proxy</td></tr></tbody></table><h2 id="limitations">Limitations</h2><p>The following networking functionality is <em>not</em> supported on Windows nodes:</p><ul><li>Host networking mode</li><li>Local NodePort access from the node itself (works for other nodes or external clients)</li><li>More than 64 backend pods (or unique destination addresses) for a single Service</li><li>IPv6 communication between Windows pods connected to overlay networks</li><li>Local Traffic Policy in non-DSR mode</li><li>Outbound communication using the ICMP protocol via the <code>win-overlay</code>, <code>win-bridge</code>, or using the Azure-CNI plugin.
Specifically, the Windows data plane (<a href="https://www.microsoft.com/research/project/azure-virtual-filtering-platform/">VFP</a>)
doesn't support ICMP packet transpositions, and this means:<ul><li>ICMP packets directed to destinations within the same network (such as pod to pod communication via ping)
work as expected;</li><li>TCP/UDP packets work as expected;</li><li>ICMP packets directed to pass through a remote network (e.g. pod to external internet communication via ping)
cannot be transposed and thus will not be routed back to their source;</li><li>Since TCP/UDP packets can still be transposed, you can substitute <code>ping &lt;destination&gt;</code> with
<code>curl &lt;destination&gt;</code> when debugging connectivity with the outside world.</li></ul></li></ul><p>Other limitations:</p><ul><li>Windows reference network plugins win-bridge and win-overlay do not implement
<a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI spec</a> v0.4.0,
due to a missing <code>CHECK</code> implementation.</li><li>The Flannel VXLAN CNI plugin has the following limitations on Windows:<ul><li>Node-pod connectivity is only possible for local pods with Flannel v0.12.0 (or higher).</li><li>Flannel is restricted to using VNI 4096 and UDP port 4789. See the official
<a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan">Flannel VXLAN</a>
backend docs for more details on these parameters.</li></ul></li></ul></div></div><div><div class="td-content"><h1>Service ClusterIP allocation</h1><p>In Kubernetes, <a href="/docs/concepts/services-networking/service/">Services</a> are an abstract way to expose
an application running on a set of Pods. Services
can have a cluster-scoped virtual IP address (using a Service of <code>type: ClusterIP</code>).
Clients can connect using that virtual IP address, and Kubernetes then load-balances traffic to that
Service across the different backing Pods.</p><h2 id="how-service-clusterips-are-allocated">How Service ClusterIPs are allocated?</h2><p>When Kubernetes needs to assign a virtual IP address for a Service,
that assignment happens one of two ways:</p><dl><dt><em>dynamically</em></dt><dd>the cluster's control plane automatically picks a free IP address from within the configured IP range for <code>type: ClusterIP</code> Services.</dd><dt><em>statically</em></dt><dd>you specify an IP address of your choice, from within the configured IP range for Services.</dd></dl><p>Across your whole cluster, every Service <code>ClusterIP</code> must be unique.
Trying to create a Service with a specific <code>ClusterIP</code> that has already
been allocated will return an error.</p><h2 id="why-do-you-need-to-reserve-service-cluster-ips">Why do you need to reserve Service Cluster IPs?</h2><p>Sometimes you may want to have Services running in well-known IP addresses, so other components and
users in the cluster can use them.</p><p>The best example is the DNS Service for the cluster. As a soft convention, some Kubernetes installers assign the 10th IP address from
the Service IP range to the DNS service. Assuming you configured your cluster with Service IP range
10.96.0.0/16 and you want your DNS Service IP to be 10.96.0.10, you'd have to create a Service like
this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>k8s-app</span>:<span> </span>kube-dns<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/cluster-service</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/name</span>:<span> </span>CoreDNS<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>kube-dns<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>clusterIP</span>:<span> </span><span>10.96.0.10</span><span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>dns<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>53</span><span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>UDP<span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span><span>53</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>dns-tcp<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>53</span><span>
</span></span></span><span><span><span>    </span><span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>targetPort</span>:<span> </span><span>53</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>k8s-app</span>:<span> </span>kube-dns<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>ClusterIP<span>
</span></span></span></code></pre></div><p>But, as it was explained before, the IP address 10.96.0.10 has not been reserved.
If other Services are created before or in parallel with dynamic allocation, there is a chance they can allocate this IP.
Hence, you will not be able to create the DNS Service because it will fail with a conflict error.</p><h2 id="avoid-ClusterIP-conflict">How can you avoid Service ClusterIP conflicts?</h2><p>The allocation strategy implemented in Kubernetes to allocate ClusterIPs to Services reduces the
risk of collision.</p><p>The <code>ClusterIP</code> range is divided, based on the formula <code>min(max(16, cidrSize / 16), 256)</code>,
described as <em>never less than 16 or more than 256 with a graduated step between them</em>.</p><p>Dynamic IP assignment uses the upper band by default, once this has been exhausted it will
use the lower range. This will allow users to use static allocations on the lower band with a low
risk of collision.</p><h2 id="allocation-examples">Examples</h2><h3 id="allocation-example-1">Example 1</h3><p>This example uses the IP address range: 10.96.0.0/24 (CIDR notation) for the IP addresses
of Services.</p><p>Range Size: 2<sup>8</sup> - 2 = 254<br>Band Offset: <code>min(max(16, 256/16), 256)</code> = <code>min(16, 256)</code> = 16<br>Static band start: 10.96.0.1<br>Static band end: 10.96.0.16<br>Range end: 10.96.0.254</p><figure><div class="mermaid">pie showData
title 10.96.0.0/24
"Static" : 16
"Dynamic" : 238</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><h3 id="allocation-example-2">Example 2</h3><p>This example uses the IP address range: 10.96.0.0/20 (CIDR notation) for the IP addresses
of Services.</p><p>Range Size: 2<sup>12</sup> - 2 = 4094<br>Band Offset: <code>min(max(16, 4096/16), 256)</code> = <code>min(256, 256)</code> = 256<br>Static band start: 10.96.0.1<br>Static band end: 10.96.1.0<br>Range end: 10.96.15.254</p><figure><div class="mermaid">pie showData
title 10.96.0.0/20
"Static" : 256
"Dynamic" : 3838</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><h3 id="allocation-example-3">Example 3</h3><p>This example uses the IP address range: 10.96.0.0/16 (CIDR notation) for the IP addresses
of Services.</p><p>Range Size: 2<sup>16</sup> - 2 = 65534<br>Band Offset: <code>min(max(16, 65536/16), 256)</code> = <code>min(4096, 256)</code> = 256<br>Static band start: 10.96.0.1<br>Static band ends: 10.96.1.0<br>Range end: 10.96.255.254</p><figure><div class="mermaid">pie showData
title 10.96.0.0/16
"Static" : 256
"Dynamic" : 65278</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">Service External Traffic Policy</a></li><li>Read about <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a></li><li>Read about <a href="/docs/concepts/services-networking/service/">Services</a></li></ul></div></div><div><div class="td-content"><h1>Service Internal Traffic Policy</h1><div class="lead">If two Pods in your cluster want to communicate, and both Pods are actually running on the same node, use <em>Service Internal Traffic Policy</em> to keep network traffic within that node. Avoiding a round trip via the cluster network can help with reliability, performance (network latency and throughput), or cost.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p><em>Service Internal Traffic Policy</em> enables internal traffic restrictions to only route
internal traffic to endpoints within the node the traffic originated from. The
"internal" traffic here refers to traffic originated from Pods in the current
cluster. This can help to reduce costs and improve performance.</p><h2 id="using-service-internal-traffic-policy">Using Service Internal Traffic Policy</h2><p>You can enable the internal-only traffic policy for a
<a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a>, by setting its
<code>.spec.internalTrafficPolicy</code> to <code>Local</code>. This tells kube-proxy to only use node local
endpoints for cluster internal traffic.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>For pods on nodes with no endpoints for a given Service, the Service
behaves as if it has zero endpoints (for Pods on this node) even if the service
does have endpoints on other nodes.</div><p>The following example shows what a Service looks like when you set
<code>.spec.internalTrafficPolicy</code> to <code>Local</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-service<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app.kubernetes.io/name</span>:<span> </span>MyApp<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>      </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>9376</span><span>
</span></span></span><span><span><span>  </span><span>internalTrafficPolicy</span>:<span> </span>Local<span>
</span></span></span></code></pre></div><h2 id="how-it-works">How it works</h2><p>The kube-proxy filters the endpoints it routes to based on the
<code>spec.internalTrafficPolicy</code> setting. When it's set to <code>Local</code>, only node local
endpoints are considered. When it's <code>Cluster</code> (the default), or is not set,
Kubernetes considers all endpoints.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/services-networking/topology-aware-routing/">Topology Aware Routing</a></li><li>Read about <a href="/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">Service External Traffic Policy</a></li><li>Follow the <a href="/docs/tutorials/services/connect-applications-service/">Connecting Applications with Services</a> tutorial</li></ul></div></div><div><div class="td-content"><h1>Storage</h1><div class="lead">Ways to provide both long-term and temporary storage to Pods in your cluster.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/concepts/storage/volumes/">Volumes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/projected-volumes/">Projected Volumes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/ephemeral-volumes/">Ephemeral Volumes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/storage-classes/">Storage Classes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/volume-attributes-classes/">Volume Attributes Classes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/dynamic-provisioning/">Dynamic Volume Provisioning</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/volume-snapshots/">Volume Snapshots</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/volume-snapshot-classes/">Volume Snapshot Classes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/volume-pvc-datasource/">CSI Volume Cloning</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/storage-capacity/">Storage Capacity</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/storage-limits/">Node-specific Volume Limits</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/volume-health-monitoring/">Volume Health Monitoring</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/storage/windows-storage/">Windows Storage</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Volumes</h1><p>Kubernetes <em>volumes</em> provide a way for containers in a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">pod</a>
to access and share data via the filesystem. There are different kinds of volume that you can use for different purposes,
such as:</p><ul><li>populating a configuration file based on a <a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." href="/docs/concepts/configuration/configmap/" target="_blank">ConfigMap</a>
or a <a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a></li><li>providing some temporary scratch space for a pod</li><li>sharing a filesystem between two different containers in the same pod</li><li>sharing a filesystem between two different pods (even if those Pods run on different nodes)</li><li>durably storing data so that it stays available even if the Pod restarts or is replaced</li><li>passing configuration information to an app running in a container, based on details of the Pod
the container is in
(for example: telling a <a class="glossary-tooltip" title="An auxilliary container that stays running throughout the lifecycle of a Pod." href="/docs/concepts/workloads/pods/sidecar-containers/" target="_blank">sidecar container</a>
what namespace the Pod is running in)</li><li>providing read-only access to data in a different container image</li></ul><p>Data sharing can be between different local processes within a container, or between different containers,
or between Pods.</p><h2 id="why-volumes-are-important">Why volumes are important</h2><ul><li><p><strong>Data persistence:</strong> On-disk files in a container are ephemeral, which presents some problems for
non-trivial applications when running in containers. One problem occurs when
a container crashes or is stopped, the container state is not saved so all of the
files that were created or modified during the lifetime of the container are lost.
After a crash, kubelet restarts the container with a clean state.</p></li><li><p><strong>Shared storage:</strong> Another problem occurs when multiple containers are running in a <code>Pod</code> and
need to share files. It can be challenging to set up
and access a shared filesystem across all of the containers.</p></li></ul><p>The Kubernetes <a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">volume</a> abstraction
can help you to solve both of these problems.</p><p>Before you learn about volumes, PersistentVolumes and PersistentVolumeClaims, you should read up
about <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> and make sure that you understand how
Kubernetes uses Pods to run containers.</p><h2 id="how-volumes-work">How volumes work</h2><p>Kubernetes supports many types of volumes. A <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>
can use any number of volume types simultaneously.
<a href="/docs/concepts/storage/ephemeral-volumes/">Ephemeral volume</a> types have a lifetime linked to a specific Pod,
but <a href="/docs/concepts/storage/persistent-volumes/">persistent volumes</a> exist beyond
the lifetime of any individual pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes;
however, Kubernetes does not destroy persistent volumes.
For any kind of volume in a given pod, data is preserved across container restarts.</p><p>At its core, a volume is a directory, possibly with some data in it, which
is accessible to the containers in a pod. How that directory comes to be, the
medium that backs it, and the contents of it are determined by the particular
volume type used.</p><p>To use a volume, specify the volumes to provide for the Pod in <code>.spec.volumes</code>
and declare where to mount those volumes into containers in <code>.spec.containers[*].volumeMounts</code>.</p><p>When a pod is launched, a process in the container sees a filesystem view composed from the initial contents of
the <a class="glossary-tooltip" title="Stored instance of a container that holds a set of software needed to run an application." href="/docs/reference/glossary/?all=true#term-image" target="_blank">container image</a>, plus volumes
(if defined) mounted inside the container.
The process sees a root filesystem that initially matches the contents of the container image.
Any writes to within that filesystem hierarchy, if allowed, affect what that process views
when it performs a subsequent filesystem access.
Volumes are mounted at <a href="#using-subpath">specified paths</a> within the container filesystem.
For each container defined within a Pod, you must independently specify where
to mount each volume that the container uses.</p><p>Volumes cannot mount within other volumes (but see <a href="#using-subpath">Using subPath</a>
for a related mechanism). Also, a volume cannot contain a hard link to anything in
a different volume.</p><h2 id="volume-types">Types of volumes</h2><p>Kubernetes supports several types of volumes.</p><h3 id="awselasticblockstore">awsElasticBlockStore (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>awsElasticBlockStore</code> type
are redirected to the <code>ebs.csi.aws.com</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> driver.</p><p>The AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release
and then removed entirely in the v1.27 release.</p><p>The Kubernetes project suggests that you use the <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">AWS EBS</a>
third party storage driver instead.</p><h3 id="azuredisk">azureDisk (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>azureDisk</code> type
are redirected to the <code>disk.csi.azure.com</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> driver.</p><p>The AzureDisk in-tree storage driver was deprecated in the Kubernetes v1.19 release
and then removed entirely in the v1.27 release.</p><p>The Kubernetes project suggests that you use the <a href="https://github.com/kubernetes-sigs/azuredisk-csi-driver">Azure Disk</a>
third party storage driver instead.</p><h3 id="azurefile">azureFile (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>azureFile</code> type
are redirected to the <code>file.csi.azure.com</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> driver.</p><p>The AzureFile in-tree storage driver was deprecated in the Kubernetes v1.21 release
and then removed entirely in the v1.30 release.</p><p>The Kubernetes project suggests that you use the <a href="https://github.com/kubernetes-sigs/azurefile-csi-driver">Azure File</a>
third party storage driver instead.</p><h3 id="cephfs">cephfs (removed)</h3><p>Kubernetes 1.34 does not include a <code>cephfs</code> volume type.</p><p>The <code>cephfs</code> in-tree storage driver was deprecated in the Kubernetes v1.28
release and then removed entirely in the v1.31 release.</p><h3 id="cinder">cinder (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>cinder</code> type
are redirected to the <code>cinder.csi.openstack.org</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> driver.</p><p>The OpenStack Cinder in-tree storage driver was deprecated in the Kubernetes v1.11 release
and then removed entirely in the v1.26 release.</p><p>The Kubernetes project suggests that you use the
<a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md">OpenStack Cinder</a>
third party storage driver instead.</p><h3 id="configmap">configMap</h3><p>A <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>
provides a way to inject configuration data into pods.
The data stored in a ConfigMap can be referenced in a volume of type
<code>configMap</code> and then consumed by containerized applications running in a pod.</p><p>When referencing a ConfigMap, you provide the name of the ConfigMap in the
volume. You can customize the path to use for a specific
entry in the ConfigMap. The following configuration shows how to mount
the <code>log-config</code> ConfigMap onto a Pod called <code>configmap-pod</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>configmap-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>test<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span>'sh'</span>,<span> </span><span>'-c'</span>,<span> </span><span>'echo "The app is running!" &amp;&amp; tail -f /dev/null'</span>]<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>config-vol<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/etc/config<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>config-vol<span>
</span></span></span><span><span><span>      </span><span>configMap</span>:<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>log-config<span>
</span></span></span><span><span><span>        </span><span>items</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>log_level<span>
</span></span></span><span><span><span>            </span><span>path</span>:<span> </span>log_level.conf<span>
</span></span></span></code></pre></div><p>The <code>log-config</code> ConfigMap is mounted as a volume, and all contents stored in
its <code>log_level</code> entry are mounted into the Pod at path <code>/etc/config/log_level.conf</code>.
Note that this path is derived from the volume's <code>mountPath</code> and the <code>path</code>
keyed with <code>log_level</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li><p>You must <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/#create-a-configmap">create a ConfigMap</a>
before you can use it.</p></li><li><p>A ConfigMap is always mounted as <code>readOnly</code>.</p></li><li><p>A container using a ConfigMap as a <a href="#using-subpath"><code>subPath</code></a> volume mount will not
receive updates when the ConfigMap changes.</p></li><li><p>Text data is exposed as files using the UTF-8 character encoding.
For other character encodings, use <code>binaryData</code>.</p></li></ul></div><h3 id="downwardapi">downwardAPI</h3><p>A <code>downwardAPI</code> volume makes <a class="glossary-tooltip" title="A mechanism to expose Pod and container field values to code running in a container." href="/docs/concepts/workloads/pods/downward-api/" target="_blank">downward API</a>
data available to applications. Within the volume, you can find the exposed
data as read-only files in plain text format.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A container using the downward API as a <a href="#using-subpath"><code>subPath</code></a> volume mount does not
receive updates when field values change.</div><p>See <a href="/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">Expose Pod Information to Containers Through Files</a>
to learn more.</p><h3 id="emptydir">emptyDir</h3><p>For a Pod that defines an <code>emptyDir</code> volume, the volume is created when the Pod is assigned to a node.
As the name says, the <code>emptyDir</code> volume is initially empty. All containers in the Pod can read and write the same
files in the <code>emptyDir</code> volume, though that volume can be mounted at the same
or different paths in each container. When a Pod is removed from a node for
any reason, the data in the <code>emptyDir</code> is deleted permanently.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A container crashing does <em>not</em> remove a Pod from a node. The data in an <code>emptyDir</code> volume
is safe across container crashes.</div><p>Some uses for an <code>emptyDir</code> are:</p><ul><li>scratch space, such as for a disk-based merge sort</li><li>checkpointing a long computation for recovery from crashes</li><li>holding files that a content-manager container fetches while a webserver
container serves the data</li></ul><p>The <code>emptyDir.medium</code> field controls where <code>emptyDir</code> volumes are stored. By
default <code>emptyDir</code> volumes are stored on whatever medium that backs the node
such as disk, SSD, or network storage, depending on your environment. If you set
the <code>emptyDir.medium</code> field to <code>"Memory"</code>, Kubernetes mounts a tmpfs (RAM-backed
filesystem) for you instead. While tmpfs is very fast, be aware that, unlike
disks, files you write count against the memory limit of the container that wrote them.</p><p>A size limit can be specified for the default medium, which limits the capacity
of the <code>emptyDir</code> volume. The storage is allocated from
<a href="/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage">node ephemeral storage</a>.
If that is filled up from another source (for example, log files or image overlays),
the <code>emptyDir</code> may run out of capacity before this limit.
If no size is specified, memory-backed volumes are sized to node allocatable memory.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Please check <a href="/docs/concepts/configuration/manage-resources-containers/#memory-backed-emptydir">here</a>
for points to note in terms of resource management when using memory-backed <code>emptyDir</code>.</div><h4 id="emptydir-configuration-example">emptyDir configuration example</h4><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-pd<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>registry.k8s.io/test-webserver<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/cache<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>cache-volume<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cache-volume<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span>
</span></span></span><span><span><span>      </span><span>sizeLimit</span>:<span> </span>500Mi<span>
</span></span></span></code></pre></div><h4 id="emptydir-memory-configuration-example">emptyDir memory configuration example</h4><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-pd<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>registry.k8s.io/test-webserver<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/cache<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>cache-volume<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cache-volume<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span>
</span></span></span><span><span><span>      </span><span>sizeLimit</span>:<span> </span>500Mi<span>
</span></span></span><span><span><span>      </span><span>medium</span>:<span> </span>Memory<span>
</span></span></span></code></pre></div><h3 id="fc">fc (fibre channel)</h3><p>An <code>fc</code> volume type allows an existing fibre channel block storage volume
to be mounted in a Pod. You can specify single or multiple target world wide names (WWNs)
using the parameter <code>targetWWNs</code> in your Volume configuration. If multiple WWNs are specified,
targetWWNs expect that those WWNs are from multi-path connections.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the target WWNs
beforehand so that Kubernetes hosts can access them.</div><h3 id="gcepersistentdisk">gcePersistentDisk (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>gcePersistentDisk</code> type
are redirected to the <code>pd.csi.storage.gke.io</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> driver.</p><p>The <code>gcePersistentDisk</code> in-tree storage driver was deprecated in the Kubernetes v1.17 release
and then removed entirely in the v1.28 release.</p><p>The Kubernetes project suggests that you use the
<a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">Google Compute Engine Persistent Disk CSI</a>
third party storage driver instead.</p><h3 id="gitrepo">gitRepo (deprecated)</h3><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4><p>The <code>gitRepo</code> volume plugin is deprecated and is disabled by default.</p><p>To provision a Pod that has a Git repository mounted, you can mount an
<a href="#emptydir"><code>emptyDir</code></a> volume into an <a href="/docs/concepts/workloads/pods/init-containers/">init container</a>
that clones the repo using Git, then mount the <a href="#emptydir">EmptyDir</a> into the Pod's container.</p><hr><p>You can restrict the use of <code>gitRepo</code> volumes in your cluster using
<a href="/docs/concepts/policy/">policies</a>, such as
<a href="/docs/reference/access-authn-authz/validating-admission-policy/">ValidatingAdmissionPolicy</a>.
You can use the following Common Expression Language (CEL) expression as
part of a policy to reject use of <code>gitRepo</code> volumes:</p><pre tabindex="0"><code class="language-cel">!has(object.spec.volumes) || !object.spec.volumes.exists(v, has(v.gitRepo))
</code></pre></div><p>You can use this deprecated storage plugin in your cluster if you explicitly
enable the <code>GitRepoVolumeDriver</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p><p>A <code>gitRepo</code> volume is an example of a volume plugin. This plugin
mounts an empty directory and clones a git repository into this directory
for your Pod to use.</p><p>Here is an example of a <code>gitRepo</code> volume:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>server<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/mypath<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>git-volume<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>git-volume<span>
</span></span></span><span><span><span>    </span><span>gitRepo</span>:<span>
</span></span></span><span><span><span>      </span><span>repository</span>:<span> </span><span>"git@somewhere:me/my-git-repository.git"</span><span>
</span></span></span><span><span><span>      </span><span>revision</span>:<span> </span><span>"22f1d8406d464b0c0874075539c1f2e96c253775"</span><span>
</span></span></span></code></pre></div><h3 id="glusterfs">glusterfs (removed)</h3><p>Kubernetes 1.34 does not include a <code>glusterfs</code> volume type.</p><p>The GlusterFS in-tree storage driver was deprecated in the Kubernetes v1.25 release
and then removed entirely in the v1.26 release.</p><h3 id="hostpath">hostPath</h3><p>A <code>hostPath</code> volume mounts a file or directory from the host node's filesystem
into your Pod. This is not something that most Pods will need, but it offers a
powerful escape hatch for some applications.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4><p>Using the <code>hostPath</code> volume type presents many security risks.
If you can avoid using a <code>hostPath</code> volume, you should. For example,
define a <a href="#local"><code>local</code> PersistentVolume</a>, and use that instead.</p><p>If you are restricting access to specific directories on the node using
admission-time validation, that restriction is only effective when you
additionally require that any mounts of that <code>hostPath</code> volume are
<strong>read only</strong>. If you allow a read-write mount of any host path by an
untrusted Pod, the containers in that Pod may be able to subvert the
read-write host mount.</p><hr><p>Take care when using <code>hostPath</code> volumes, whether these are mounted as read-only
or as read-write, because:</p><ul><li>Access to the host filesystem can expose privileged system credentials (such as for the kubelet) or privileged APIs
(such as the container runtime socket) that can be used for container escape or to attack other
parts of the cluster.</li><li>Pods with identical configuration (such as created from a PodTemplate) may
behave differently on different nodes due to different files on the nodes.</li><li><code>hostPath</code> volume usage is not treated as ephemeral storage usage.
You need to monitor the disk usage by yourself because excessive <code>hostPath</code> disk
usage will lead to disk pressure on the node.</li></ul></div><p>Some uses for a <code>hostPath</code> are:</p><ul><li>running a container that needs access to node-level system components
(such as a container that transfers system logs to a central location,
accessing those logs using a read-only mount of <code>/var/log</code>)</li><li>making a configuration file stored on the host system available read-only
to a <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static pod</a>;
unlike normal Pods, static Pods cannot access ConfigMaps</li></ul><h4 id="hostpath-volume-types"><code>hostPath</code> volume types</h4><p>In addition to the required <code>path</code> property, you can optionally specify a
<code>type</code> for a <code>hostPath</code> volume.</p><p>The available values for <code>type</code> are:</p><table><thead><tr><th>Value</th><th>Behavior</th></tr></thead><tbody><tr><td><code>&#8204;""</code></td><td>Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the <code>hostPath</code> volume.</td></tr><tr><td><code>DirectoryOrCreate</code></td><td>If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet.</td></tr><tr><td><code>Directory</code></td><td>A directory must exist at the given path</td></tr><tr><td><code>FileOrCreate</code></td><td>If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet.</td></tr><tr><td><code>File</code></td><td>A file must exist at the given path</td></tr><tr><td><code>Socket</code></td><td>A UNIX socket must exist at the given path</td></tr><tr><td><code>CharDevice</code></td><td><em>(Linux nodes only)</em> A character device must exist at the given path</td></tr><tr><td><code>BlockDevice</code></td><td><em>(Linux nodes only)</em> A block device must exist at the given path</td></tr></tbody></table><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>The <code>FileOrCreate</code> mode does <strong>not</strong> create the parent directory of the file. If the parent directory
of the mounted file does not exist, the pod fails to start. To ensure that this mode works,
you can try to mount directories and files separately, as shown in the
<a href="#hostpath-fileorcreate-example"><code>FileOrCreate</code> example</a> for <code>hostPath</code>.</div><p>Some files or directories created on the underlying hosts might only be
accessible by root. You then either need to run your process as root in a
<a href="/docs/tasks/configure-pod-container/security-context/">privileged container</a>
or modify the file permissions on the host to read from or write to a <code>hostPath</code> volume.</p><h4 id="hostpath-configuration-example">hostPath configuration example</h4><ul class="nav nav-tabs" id="hostpath-examples"><li class="nav-item"><a class="nav-link active" href="#hostpath-examples-0">Linux node</a></li><li class="nav-item"><a class="nav-link" href="#hostpath-examples-1">Windows node</a></li></ul><div class="tab-content" id="hostpath-examples"><div id="hostpath-examples-0" class="tab-pane show active"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span># This manifest mounts /data/foo on the host as /foo inside the</span><span>
</span></span></span><span><span><span></span><span># single container that runs within the hostpath-example-linux Pod.</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span># The mount into the container is read-only.</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hostpath-example-linux<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>os</span>:<span> </span>{<span> </span><span>name</span>:<span> </span>linux }<span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/os</span>:<span> </span>linux<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/test-webserver<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/foo<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>example-volume<span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example-volume<span>
</span></span></span><span><span><span>    </span><span># mount /data/foo, but only if that directory already exists</span><span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/data/foo<span> </span><span># directory location on host</span><span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>Directory<span> </span><span># this field is optional</span><span>
</span></span></span></code></pre></div></p></div><div id="hostpath-examples-1" class="tab-pane"><p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span># This manifest mounts C:\Data\foo on the host as C:\foo, inside the</span><span>
</span></span></span><span><span><span></span><span># single container that runs within the hostpath-example-windows Pod.</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span># The mount into the container is read-only.</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hostpath-example-windows<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>os</span>:<span> </span>{<span> </span><span>name</span>:<span> </span>windows }<span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/os</span>:<span> </span>windows<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>microsoft/windowsservercore:1709<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>example-volume<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"C:\\foo"</span><span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span><span># mount C:\Data\foo from the host, but only if that directory already exists</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example-volume<span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span><span>"C:\\Data\\foo"</span><span> </span><span># directory location on host</span><span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>Directory      <span> </span><span># this field is optional</span><span>
</span></span></span></code></pre></div></p></div></div><h4 id="hostpath-fileorcreate-example">hostPath FileOrCreate configuration example</h4><p>The following manifest defines a Pod that mounts <code>/var/local/aaa</code>
inside the single container in the Pod. If the node does not
already have a path <code>/var/local/aaa</code>, the kubelet creates
it as a directory and then mounts it into the Pod.</p><p>If <code>/var/local/aaa</code> already exists but is not a directory,
the Pod fails. Additionally, the kubelet attempts to make
a file named <code>/var/local/aaa/1.txt</code> inside that directory
(as seen from the host); if something already exists at
that path and isn't a regular file, the Pod fails.</p><p>Here's the example manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-webserver<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>os</span>:<span> </span>{<span> </span><span>name</span>:<span> </span>linux }<span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/os</span>:<span> </span>linux<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>test-webserver<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/test-webserver:latest<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/var/local/aaa<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>mydir<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/var/local/aaa/1.txt<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>myfile<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>mydir<span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>      </span><span># Ensure the file directory is created.</span><span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/var/local/aaa<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>DirectoryOrCreate<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>myfile<span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/var/local/aaa/1.txt<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>FileOrCreate<span>
</span></span></span></code></pre></div><h3 id="image">image</h3><div class="feature-state-notice feature-beta" title="Feature Gate: ImageVolume"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: false)</div><p>An <code>image</code> volume source represents an OCI object (a container image or
artifact) which is available on the kubelet's host machine.</p><p>An example of using the <code>image</code> volume source is:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/image-volumes.yaml"><code>pods/image-volumes.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/image-volumes.yaml to clipboard"></div><div class="includecode" id="pods-image-volumes-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>image-volume<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>shell<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"infinity"</span>]<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>debian<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>volume<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/volume<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>volume<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span>
</span></span></span><span><span><span>      </span><span>reference</span>:<span> </span>quay.io/crio/artifact:v2<span>
</span></span></span><span><span><span>      </span><span>pullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span></code></pre></div></div></div><p>The volume is resolved at pod startup depending on which <code>pullPolicy</code> value is
provided:</p><dl><dt><code>Always</code></dt><dd>the kubelet always attempts to pull the reference. If the pull fails,
the kubelet sets the Pod to <code>Failed</code>.</dd><dt><code>Never</code></dt><dd>the kubelet never pulls the reference and only uses a local image or artifact.
The Pod becomes <code>Failed</code> if any layers of the image aren't already present locally,
or if the manifest for that image isn't already cached.</dd><dt><code>IfNotPresent</code></dt><dd>the kubelet pulls if the reference isn't already present on disk. The Pod becomes
<code>Failed</code> if the reference isn't present and the pull fails.</dd></dl><p>The volume gets re-resolved if the pod gets deleted and recreated, which means
that new remote content will become available on pod recreation. A failure to
resolve or pull the image during pod startup will block containers from starting
and may add significant latency. Failures will be retried using normal volume
backoff and will be reported on the pod reason and message.</p><p>The types of objects that may be mounted by this volume are defined by the
container runtime implementation on a host machine. At a minimum, they must include
all valid types supported by the container image field. The OCI object gets
mounted in a single directory (<code>spec.containers[*].volumeMounts.mountPath</code>)
and will be mounted read-only. On Linux, the container runtime typically also mounts the
volume with file execution blocked (<code>noexec</code>).</p><p>Besides that:</p><ul><li><a href="/docs/concepts/storage/volumes/#using-subpath"><code>subPath</code></a> or
<a href="/docs/concepts/storage/volumes/#using-subpath-expanded-environment"><code>subPathExpr</code></a>
mounts for containers (<code>spec.containers[*].volumeMounts.[subPath,subPathExpr]</code>)
are only supported from Kubernetes v1.33.</li><li>The field <code>spec.securityContext.fsGroupChangePolicy</code> has no effect on this
volume type.</li><li>The <a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages"><code>AlwaysPullImages</code> Admission Controller</a>
does also work for this volume source like for container images.</li></ul><p>The following fields are available for the <code>image</code> type:</p><dl><dt><code>reference</code></dt><dd>Artifact reference to be used. For example, you could specify
<code>registry.k8s.io/conformance:v1.34.0</code> to load the
files from the Kubernetes conformance test image. Behaves in the same way as
<code>pod.spec.containers[*].image</code>. Pull secrets will be assembled in the same way
as for the container image by looking up node credentials, service account image
pull secrets, and pod spec image pull secrets. This field is optional to allow
higher level config management to default or override container images in
workload controllers like Deployments and StatefulSets.
<a href="/docs/concepts/containers/images/">More info about container images</a></dd><dt><code>pullPolicy</code></dt><dd>Policy for pulling OCI objects. Possible values are: <code>Always</code>, <code>Never</code> or
<code>IfNotPresent</code>. Defaults to <code>Always</code> if <code>:latest</code> tag is specified, or
<code>IfNotPresent</code> otherwise.</dd></dl><p>See the <a href="/docs/tasks/configure-pod-container/image-volumes/"><em>Use an Image Volume With a Pod</em></a>
example for more details on how to use the volume source.</p><h3 id="iscsi">iscsi</h3><p>An <code>iscsi</code> volume allows an existing iSCSI (SCSI over IP) volume to be mounted
into your Pod. Unlike <code>emptyDir</code>, which is erased when a Pod is removed, the
contents of an <code>iscsi</code> volume are preserved and the volume is merely
unmounted. This means that an iscsi volume can be pre-populated with data, and
that data can be shared between pods.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You must have your own iSCSI server running with the volume created before you can use it.</div><p>A feature of iSCSI is that it can be mounted as read-only by multiple consumers
simultaneously. This means that you can pre-populate a volume with your dataset
and then serve it in parallel from as many Pods as you need. Unfortunately,
iSCSI volumes can only be mounted by a single consumer in read-write mode.
Simultaneous writers are not allowed.</p><h3 id="local">local</h3><p>A <code>local</code> volume represents a mounted local storage device such as a disk,
partition or directory.</p><p>Local volumes can only be used as a statically created PersistentVolume. Dynamic
provisioning is not supported.</p><p>Compared to <code>hostPath</code> volumes, <code>local</code> volumes are used in a durable and
portable manner without manually scheduling pods to nodes. The system is aware
of the volume's node constraints by looking at the node affinity on the PersistentVolume.</p><p>However, <code>local</code> volumes are subject to the availability of the underlying
node and are not suitable for all applications. If a node becomes unhealthy,
then the <code>local</code> volume becomes inaccessible to the pod. The pod using this volume
is unable to run. Applications using <code>local</code> volumes must be able to tolerate this
reduced availability, as well as potential data loss, depending on the
durability characteristics of the underlying disk.</p><p>The following example shows a PersistentVolume using a <code>local</code> volume and
<code>nodeAffinity</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolume<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-pv<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>capacity</span>:<span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span>100Gi<span>
</span></span></span><span><span><span>  </span><span>volumeMode</span>:<span> </span>Filesystem<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>  </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>persistentVolumeReclaimPolicy</span>:<span> </span>Delete<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>local-storage<span>
</span></span></span><span><span><span>  </span><span>local</span>:<span>
</span></span></span><span><span><span>    </span><span>path</span>:<span> </span>/mnt/disks/ssd1<span>
</span></span></span><span><span><span>  </span><span>nodeAffinity</span>:<span>
</span></span></span><span><span><span>    </span><span>required</span>:<span>
</span></span></span><span><span><span>      </span><span>nodeSelectorTerms</span>:<span>
</span></span></span><span><span><span>      </span>- <span>matchExpressions</span>:<span>
</span></span></span><span><span><span>        </span>- <span>key</span>:<span> </span>kubernetes.io/hostname<span>
</span></span></span><span><span><span>          </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>          </span><span>values</span>:<span>
</span></span></span><span><span><span>          </span>- example-node<span>
</span></span></span></code></pre></div><p>You must set a PersistentVolume <code>nodeAffinity</code> when using <code>local</code> volumes.
The Kubernetes scheduler uses the PersistentVolume <code>nodeAffinity</code> to schedule
these Pods to the correct node.</p><p>PersistentVolume <code>volumeMode</code> can be set to "Block" (instead of the default
value "Filesystem") to expose the local volume as a raw block device.</p><p>When using local volumes, it is recommended to create a StorageClass with
<code>volumeBindingMode</code> set to <code>WaitForFirstConsumer</code>. For more details, see the
local <a href="/docs/concepts/storage/storage-classes/#local">StorageClass</a> example.
Delaying volume binding ensures that the PersistentVolumeClaim binding decision
will also be evaluated with any other node constraints the Pod may have,
such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.</p><p>An external static provisioner can be run separately for improved management of
the local volume lifecycle. Note that this provisioner does not support dynamic
provisioning yet. For an example on how to run an external local provisioner, see the
<a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">local volume provisioner user guide</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The local PersistentVolume requires manual cleanup and deletion by the
user if the external static provisioner is not used to manage the volume
lifecycle.</div><h3 id="nfs">nfs</h3><p>An <code>nfs</code> volume allows an existing NFS (Network File System) share to be
mounted into a Pod. Unlike <code>emptyDir</code>, which is erased when a Pod is
removed, the contents of an <code>nfs</code> volume are preserved and the volume is merely
unmounted. This means that an NFS volume can be pre-populated with data, and
that data can be shared between pods. NFS can be mounted by multiple
writers simultaneously.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-pd<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>registry.k8s.io/test-webserver<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/my-nfs-data<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>test-volume<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>test-volume<span>
</span></span></span><span><span><span>    </span><span>nfs</span>:<span>
</span></span></span><span><span><span>      </span><span>server</span>:<span> </span>my-nfs-server.example.com<span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/my-nfs-volume<span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>You must have your own NFS server running with the share exported before you can use it.</p><p>Also note that you can't specify NFS mount options in a Pod spec. You can either set mount options server-side or
use <a href="https://man7.org/linux/man-pages/man5/nfsmount.conf.5.html">/etc/nfsmount.conf</a>.
You can also mount NFS volumes via PersistentVolumes which do allow you to set mount options.</p></div><h3 id="persistentvolumeclaim">persistentVolumeClaim</h3><p>A <code>persistentVolumeClaim</code> volume is used to mount a
<a href="/docs/concepts/storage/persistent-volumes/">PersistentVolume</a> into a Pod. PersistentVolumeClaims
are a way for users to "claim" durable storage (such as an iSCSI volume)
without knowing the details of the particular cloud environment.</p><p>See the information about <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a> for more
details.</p><h3 id="portworxvolume">portworxVolume (deprecated)</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [deprecated]</code></div><p>A <code>portworxVolume</code> is an elastic block storage layer that runs hyperconverged with
Kubernetes. <a href="https://portworx.com/use-case/kubernetes-storage/">Portworx</a> fingerprints storage
in a server, tiers based on capabilities, and aggregates capacity across multiple servers.
Portworx runs in-guest in virtual machines or on bare metal Linux nodes.</p><p>A <code>portworxVolume</code> can be dynamically created through Kubernetes or it can also
be pre-provisioned and referenced inside a Pod.
Here is an example Pod referencing a pre-provisioned Portworx volume:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-portworx-volume-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>registry.k8s.io/test-webserver<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>test-container<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>mountPath</span>:<span> </span>/mnt<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>pxvol<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pxvol<span>
</span></span></span><span><span><span>    </span><span># This Portworx volume must already exist.</span><span>
</span></span></span><span><span><span>    </span><span>portworxVolume</span>:<span>
</span></span></span><span><span><span>      </span><span>volumeID</span>:<span> </span><span>"pxvol"</span><span>
</span></span></span><span><span><span>      </span><span>fsType</span>:<span> </span><span>"&lt;fs-type&gt;"</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Make sure you have an existing PortworxVolume with name <code>pxvol</code>
before using it in the Pod.</div><h4 id="portworx-csi-migration">Portworx CSI migration</h4><div class="feature-state-notice feature-stable" title="Feature Gate: CSIMigrationPortworx"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>In Kubernetes 1.34, all operations for the in-tree
Portworx volumes are redirected to the <code>pxd.portworx.com</code>
Container Storage Interface (CSI) Driver by default.<br><a href="https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi">Portworx CSI Driver</a>
must be installed on the cluster.</p><h3 id="projected">projected</h3><p>A projected volume maps several existing volume sources into the same
directory. For more details, see <a href="/docs/concepts/storage/projected-volumes/">projected volumes</a>.</p><h3 id="rbd">rbd (removed)</h3><p>Kubernetes 1.34 does not include a <code>rbd</code> volume type.</p><p>The <a href="https://docs.ceph.com/en/latest/rbd/">Rados Block Device</a> (RBD) in-tree storage driver
and its csi migration support were deprecated in the Kubernetes v1.28 release
and then removed entirely in the v1.31 release.</p><h3 id="secret">secret</h3><p>A <code>secret</code> volume is used to pass sensitive information, such as passwords, to
Pods. You can store secrets in the Kubernetes API and mount them as files for
use by pods without coupling to Kubernetes directly. <code>secret</code> volumes are
backed by tmpfs (a RAM-backed filesystem) so they are never written to
non-volatile storage.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li><p>You must create a Secret in the Kubernetes API before you can use it.</p></li><li><p>A Secret is always mounted as <code>readOnly</code>.</p></li><li><p>A container using a Secret as a <a href="#using-subpath"><code>subPath</code></a> volume mount will not
receive Secret updates.</p></li></ul></div><p>For more details, see <a href="/docs/concepts/configuration/secret/">Configuring Secrets</a>.</p><h3 id="vspherevolume">vsphereVolume (deprecated)</h3><p>In Kubernetes 1.34, all operations for the in-tree <code>vsphereVolume</code> type
are redirected to the <code>csi.vsphere.vmware.com</code> <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> driver.</p><p>The <code>vsphereVolume</code> in-tree storage driver was deprecated in the Kubernetes v1.19 release
and then removed entirely in the v1.30 release.</p><p>The Kubernetes project suggests that you use the
<a href="https://github.com/kubernetes-sigs/vsphere-csi-driver">vSphere CSI</a>
third party storage driver instead.</p><h2 id="using-subpath">Using subPath</h2><p>Sometimes, it is useful to share one volume for multiple uses in a single pod.
The <code>volumeMounts[*].subPath</code> property specifies a sub-path inside the referenced volume
instead of its root.</p><p>The following example shows how to configure a Pod with a LAMP stack (Linux Apache MySQL PHP)
using a single, shared volume. This sample <code>subPath</code> configuration is not recommended
for production use.</p><p>The PHP application's code and assets map to the volume's <code>html</code> folder and
the MySQL database is stored in the volume's <code>mysql</code> folder. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-lamp-site<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>      </span><span>env</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>MYSQL_ROOT_PASSWORD<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span><span>"rootpasswd"</span><span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>      </span>- <span>mountPath</span>:<span> </span>/var/lib/mysql<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>site-data<span>
</span></span></span><span><span><span>        </span><span>subPath</span>:<span> </span>mysql<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>php<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>php:7.0-apache<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>      </span>- <span>mountPath</span>:<span> </span>/var/www/html<span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>site-data<span>
</span></span></span><span><span><span>        </span><span>subPath</span>:<span> </span>html<span>
</span></span></span><span><span><span>    </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>site-data<span>
</span></span></span><span><span><span>      </span><span>persistentVolumeClaim</span>:<span>
</span></span></span><span><span><span>        </span><span>claimName</span>:<span> </span>my-lamp-site-data<span>
</span></span></span></code></pre></div><h3 id="using-subpath-expanded-environment">Using subPath with expanded environment variables</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.17 [stable]</code></div><p>Use the <code>subPathExpr</code> field to construct <code>subPath</code> directory names from
downward API environment variables.
The <code>subPath</code> and <code>subPathExpr</code> properties are mutually exclusive.</p><p>In this example, a <code>Pod</code> uses <code>subPathExpr</code> to create a directory <code>pod1</code> within
the <code>hostPath</code> volume <code>/var/log/pods</code>.
The <code>hostPath</code> volume takes the <code>Pod</code> name from the <code>downwardAPI</code>.
The host directory <code>/var/log/pods/pod1</code> is mounted at <code>/logs</code> in the container.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod1<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>container1<span>
</span></span></span><span><span><span>    </span><span>env</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>POD_NAME<span>
</span></span></span><span><span><span>      </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>        </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>          </span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span>          </span><span>fieldPath</span>:<span> </span>metadata.name<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span> </span><span>"sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt"</span><span> </span>]<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>workdir1<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/logs<span>
</span></span></span><span><span><span>      </span><span># The variable expansion uses round brackets (not curly brackets).</span><span>
</span></span></span><span><span><span>      </span><span>subPathExpr</span>:<span> </span>$(POD_NAME)<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>workdir1<span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/var/log/pods<span>
</span></span></span></code></pre></div><h2 id="resources">Resources</h2><p>The storage medium (such as Disk or SSD) of an <code>emptyDir</code> volume is determined by the
medium of the filesystem holding the kubelet root dir (typically
<code>/var/lib/kubelet</code>). There is no limit on how much space an <code>emptyDir</code> or
<code>hostPath</code> volume can consume, and no isolation between containers or
pods.</p><p>To learn about requesting space using a resource specification, see
<a href="/docs/concepts/configuration/manage-resources-containers/">how to manage resources</a>.</p><h2 id="out-of-tree-volume-plugins">Out-of-tree volume plugins</h2><p>The out-of-tree volume plugins include
<a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">Container Storage Interface</a> (CSI), and also
FlexVolume (which is deprecated). These plugins enable storage vendors to create custom storage plugins
without adding their plugin source code to the Kubernetes repository.</p><p>Previously, all volume plugins were "in-tree". The "in-tree" plugins were built, linked, compiled,
and shipped with the core Kubernetes binaries. This meant that adding a new storage system to
Kubernetes (a volume plugin) required checking code into the core Kubernetes code repository.</p><p>Both CSI and FlexVolume allow volume plugins to be developed independently of
the Kubernetes code base, and deployed (installed) on Kubernetes clusters as
extensions.</p><p>For storage vendors looking to create an out-of-tree volume plugin, please refer
to the <a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md">volume plugin FAQ</a>.</p><h3 id="csi">csi</h3><p><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">Container Storage Interface</a>
(CSI) defines a standard interface for container orchestration systems (like
Kubernetes) to expose arbitrary storage systems to their container workloads.</p><p>Please read the <a href="https://git.k8s.io/design-proposals-archive/storage/container-storage-interface.md">CSI design proposal</a>
for more information.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Support for CSI spec versions 0.2 and 0.3 is deprecated in Kubernetes
v1.13 and will be removed in a future release.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>CSI drivers may not be compatible across all Kubernetes releases.
Please check the specific CSI driver's documentation for supported
deployments steps for each Kubernetes release and a compatibility matrix.</div><p>Once a CSI-compatible volume driver is deployed on a Kubernetes cluster, users
may use the <code>csi</code> volume type to attach or mount the volumes exposed by the
CSI driver.</p><p>A <code>csi</code> volume can be used in a Pod in three different ways:</p><ul><li>through a reference to a <a href="#persistentvolumeclaim">PersistentVolumeClaim</a></li><li>with a <a href="/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">generic ephemeral volume</a></li><li>with a <a href="/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">CSI ephemeral volume</a>
if the driver supports that</li></ul><p>The following fields are available to storage administrators to configure a CSI
persistent volume:</p><ul><li><code>driver</code>: A string value that specifies the name of the volume driver to use.
This value must correspond to the value returned in the <code>GetPluginInfoResponse</code>
by the CSI driver as defined in the
<a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo">CSI spec</a>.
It is used by Kubernetes to identify which CSI driver to call out to, and by
CSI driver components to identify which PV objects belong to the CSI driver.</li><li><code>volumeHandle</code>: A string value that uniquely identifies the volume. This value
must correspond to the value returned in the <code>volume.id</code> field of the
<code>CreateVolumeResponse</code> by the CSI driver as defined in the
<a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume">CSI spec</a>.
The value is passed as <code>volume_id</code> in all calls to the CSI volume driver when
referencing the volume.</li><li><code>readOnly</code>: An optional boolean value indicating whether the volume is to be
"ControllerPublished" (attached) as read only. Default is false. This value is passed
to the CSI driver via the <code>readonly</code> field in the <code>ControllerPublishVolumeRequest</code>.</li><li><code>fsType</code>: If the PV's <code>VolumeMode</code> is <code>Filesystem</code>, then this field may be used
to specify the filesystem that should be used to mount the volume. If the
volume has not been formatted and formatting is supported, this value will be
used to format the volume.
This value is passed to the CSI driver via the <code>VolumeCapability</code> field of
<code>ControllerPublishVolumeRequest</code>, <code>NodeStageVolumeRequest</code>, and
<code>NodePublishVolumeRequest</code>.</li><li><code>volumeAttributes</code>: A map of string to string that specifies static properties
of a volume. This map must correspond to the map returned in the
<code>volume.attributes</code> field of the <code>CreateVolumeResponse</code> by the CSI driver as
defined in the <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume">CSI spec</a>.
The map is passed to the CSI driver via the <code>volume_context</code> field in the
<code>ControllerPublishVolumeRequest</code>, <code>NodeStageVolumeRequest</code>, and
<code>NodePublishVolumeRequest</code>.</li><li><code>controllerPublishSecretRef</code>: A reference to the secret object containing
sensitive information to pass to the CSI driver to complete the CSI
<code>ControllerPublishVolume</code> and <code>ControllerUnpublishVolume</code> calls. This field is
optional, and may be empty if no secret is required. If the Secret
contains more than one secret, all secrets are passed.</li><li><code>nodeExpandSecretRef</code>: A reference to the secret containing sensitive
information to pass to the CSI driver to complete the CSI
<code>NodeExpandVolume</code> call. This field is optional and may be empty if no
secret is required. If the object contains more than one secret, all
secrets are passed. When you have configured secret data for node-initiated
volume expansion, the kubelet passes that data via the <code>NodeExpandVolume()</code>
call to the CSI driver. All supported versions of Kubernetes offer the
<code>nodeExpandSecretRef</code> field, and have it available by default. Kubernetes releases
prior to v1.25 did not include this support.</li><li>Enable the <a href="/docs/reference/command-line-tools-reference/feature-gates-removed/">feature gate</a>
named <code>CSINodeExpandSecret</code> for each kube-apiserver and for the kubelet on every
node. Since Kubernetes version 1.27, this feature has been enabled by default
and no explicit enablement of the feature gate is required.
You must also be using a CSI driver that supports or requires secret data during
node-initiated storage resize operations.</li><li><code>nodePublishSecretRef</code>: A reference to the secret object containing
sensitive information to pass to the CSI driver to complete the CSI
<code>NodePublishVolume</code> call. This field is optional and may be empty if no
secret is required. If the secret object contains more than one secret, all
secrets are passed.</li><li><code>nodeStageSecretRef</code>: A reference to the secret object containing
sensitive information to pass to the CSI driver to complete the CSI
<code>NodeStageVolume</code> call. This field is optional and may be empty if no secret
is required. If the Secret contains more than one secret, all secrets
are passed.</li></ul><h4 id="csi-raw-block-volume-support">CSI raw block volume support</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>Vendors with external CSI drivers can implement raw block volume support
in Kubernetes workloads.</p><p>You can set up your
<a href="/docs/concepts/storage/persistent-volumes/#raw-block-volume-support">PersistentVolume/PersistentVolumeClaim with raw block volume support</a>
as usual, without any CSI-specific changes.</p><h4 id="csi-ephemeral-volumes">CSI ephemeral volumes</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>You can directly configure CSI volumes within the Pod
specification. Volumes specified in this way are ephemeral and do not
persist across pod restarts. See
<a href="/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">Ephemeral Volumes</a>
for more information.</p><p>For more information on how to develop a CSI driver, refer to the
<a href="https://kubernetes-csi.github.io/docs/">kubernetes-csi documentation</a></p><h4 id="windows-csi-proxy">Windows CSI proxy</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.22 [stable]</code></div><p>CSI node plugins need to perform various privileged
operations like scanning of disk devices and mounting of file systems. These operations
differ for each host operating system. For Linux worker nodes, containerized CSI node
plugins are typically deployed as privileged containers. For Windows worker nodes,
privileged operations for containerized CSI node plugins is supported using
<a href="https://github.com/kubernetes-csi/csi-proxy">csi-proxy</a>, a community-managed,
stand-alone binary that needs to be pre-installed on each Windows node.</p><p>For more details, refer to the deployment guide of the CSI plugin you wish to deploy.</p><h4 id="migrating-to-csi-drivers-from-in-tree-plugins">Migrating to CSI drivers from in-tree plugins</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>The <code>CSIMigration</code> feature directs operations against existing in-tree
plugins to corresponding CSI plugins (which are expected to be installed and configured).
As a result, operators do not have to make any
configuration changes to existing Storage Classes, PersistentVolumes or PersistentVolumeClaims
(referring to in-tree plugins) when transitioning to a CSI driver that supersedes an in-tree plugin.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Existing PVs created by an in-tree volume plugin can still be used in the future without any configuration
changes, even after the migration to CSI is completed for that volume type, and even after you upgrade to a
version of Kubernetes that doesn't have compiled-in support for that kind of storage.</p><p>As part of that migration, you - or another cluster administrator - <strong>must</strong> have installed and configured
the appropriate CSI driver for that storage. The core of Kubernetes does not install that software for you.</p><hr><p>After that migration, you can also define new PVCs and PVs that refer to the legacy, built-in
storage integrations.
Provided you have the appropriate CSI driver installed and configured, the PV creation continues
to work, even for brand new volumes. The actual storage management now happens through
the CSI driver.</p></div><p>The operations and features that are supported include:
provisioning/delete, attach/detach, mount/unmount and resizing of volumes.</p><p>In-tree plugins that support <code>CSIMigration</code> and have a corresponding CSI driver implemented
are listed in <a href="#volume-types">Types of Volumes</a>.</p><h3 id="flexvolume">flexVolume (deprecated)</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [deprecated]</code></div><p>FlexVolume is an out-of-tree plugin interface that uses an exec-based model to interface
with storage drivers. The FlexVolume driver binaries must be installed in a pre-defined
volume plugin path on each node and in some cases the control plane nodes as well.</p><p>Pods interact with FlexVolume drivers through the <code>flexVolume</code> in-tree volume plugin.</p><p>The following FlexVolume <a href="https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows">plugins</a>,
deployed as PowerShell scripts on the host, support Windows nodes:</p><ul><li><a href="https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~smb.cmd">SMB</a></li><li><a href="https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~iscsi.cmd">iSCSI</a></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>FlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate external storage with Kubernetes.</p><p>Maintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of FlexVolume drivers to CSI.
Users of FlexVolume should move their workloads to use the equivalent CSI Driver.</p></div><h2 id="mount-propagation">Mount propagation</h2><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Mount propagation is a low-level feature that does not work consistently on all
volume types. The Kubernetes project recommends only using mount propagation with <code>hostPath</code>
or memory-backed <code>emptyDir</code> volumes. See
<a href="https://github.com/kubernetes/kubernetes/issues/95049">Kubernetes issue #95049</a>
for more context.</div><p>Mount propagation allows for sharing volumes mounted by a container to
other containers in the same pod, or even to other pods on the same node.</p><p>Mount propagation of a volume is controlled by the <code>mountPropagation</code> field
in <code>containers[*].volumeMounts</code>. Its values are:</p><ul><li><p><code>None</code> - This volume mount will not receive any subsequent mounts
that are mounted to this volume or any of its subdirectories by the host.
In similar fashion, no mounts created by the container will be visible on
the host. This is the default mode.</p><p>This mode is equal to <code>rprivate</code> mount propagation as described in
<a href="https://man7.org/linux/man-pages/man8/mount.8.html"><code>mount(8)</code></a></p><p>However, the CRI runtime may choose <code>rslave</code> mount propagation (i.e.,
<code>HostToContainer</code>) instead, when <code>rprivate</code> propagation is not applicable.
cri-dockerd (Docker) is known to choose <code>rslave</code> mount propagation when the
mount source contains the Docker daemon's root directory (<code>/var/lib/docker</code>).</p></li><li><p><code>HostToContainer</code> - This volume mount will receive all subsequent mounts
that are mounted to this volume or any of its subdirectories.</p><p>In other words, if the host mounts anything inside the volume mount, the
container will see it mounted there.</p><p>Similarly, if any Pod with <code>Bidirectional</code> mount propagation to the same
volume mounts anything there, the container with <code>HostToContainer</code> mount
propagation will see it.</p><p>This mode is equal to <code>rslave</code> mount propagation as described in the
<a href="https://man7.org/linux/man-pages/man8/mount.8.html"><code>mount(8)</code></a></p></li><li><p><code>Bidirectional</code> - This volume mount behaves the same the <code>HostToContainer</code> mount.
In addition, all volume mounts created by the container will be propagated
back to the host and to all containers of all pods that use the same volume.</p><p>A typical use case for this mode is a Pod with a FlexVolume or CSI driver or
a Pod that needs to mount something on the host using a <code>hostPath</code> volume.</p><p>This mode is equal to <code>rshared</code> mount propagation as described in the
<a href="https://man7.org/linux/man-pages/man8/mount.8.html"><code>mount(8)</code></a></p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4><code>Bidirectional</code> mount propagation can be dangerous. It can damage
the host operating system and therefore it is allowed only in privileged
containers. Familiarity with Linux kernel behavior is strongly recommended.
In addition, any volume mounts created by containers in pods must be destroyed
(unmounted) by the containers on termination.</div></li></ul><h2 id="read-only-mounts">Read-only mounts</h2><p>A mount can be made read-only by setting the <code>.spec.containers[].volumeMounts[].readOnly</code>
field to <code>true</code>.
This does not make the volume itself read-only, but that specific container will
not be able to write to it.
Other containers in the Pod may mount the same volume as read-write.</p><p>On Linux, read-only mounts are not recursively read-only by default.
For example, consider a Pod which mounts the hosts <code>/mnt</code> as a <code>hostPath</code> volume. If
there is another filesystem mounted read-write on <code>/mnt/&lt;SUBMOUNT&gt;</code> (such as tmpfs,
NFS, or USB storage), the volume mounted into the container(s) will also have a writeable
<code>/mnt/&lt;SUBMOUNT&gt;</code>, even if the mount itself was specified as read-only.</p><h3 id="recursive-read-only-mounts">Recursive read-only mounts</h3><div class="feature-state-notice feature-stable" title="Feature Gate: RecursiveReadOnlyMounts"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>Recursive read-only mounts can be enabled by setting the
<code>RecursiveReadOnlyMounts</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
for kubelet and kube-apiserver, and setting the <code>.spec.containers[].volumeMounts[].recursiveReadOnly</code>
field for a pod.</p><p>The allowed values are:</p><ul><li><p><code>Disabled</code> (default): no effect.</p></li><li><p><code>Enabled</code>: makes the mount recursively read-only.
Needs all the following requirements to be satisfied:</p><ul><li><code>readOnly</code> is set to <code>true</code></li><li><code>mountPropagation</code> is unset, or, set to <code>None</code></li><li>The host is running with Linux kernel v5.12 or later</li><li>The <a href="/docs/concepts/architecture/cri">CRI-level</a> container runtime supports recursive read-only mounts</li><li>The OCI-level container runtime supports recursive read-only mounts.</li></ul><p>It will fail if any of these is not true.</p></li><li><p><code>IfPossible</code>: attempts to apply <code>Enabled</code>, and falls back to <code>Disabled</code>
if the feature is not supported by the kernel or the runtime class.</p></li></ul><p>Example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/rro.yaml"><code>storage/rro.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/rro.yaml to clipboard"></div><div class="includecode" id="storage-rro-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>rro<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>mnt<span>
</span></span></span><span><span><span>      </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>        </span><span># tmpfs is mounted on /mnt/tmpfs</span><span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span>/mnt<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>busybox<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>busybox<span>
</span></span></span><span><span><span>      </span><span>args</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"infinity"</span>]<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span><span># /mnt-rro/tmpfs is not writable</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>mnt<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/mnt-rro<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>          </span><span>mountPropagation</span>:<span> </span>None<span>
</span></span></span><span><span><span>          </span><span>recursiveReadOnly</span>:<span> </span>Enabled<span>
</span></span></span><span><span><span>        </span><span># /mnt-ro/tmpfs is writable</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>mnt<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/mnt-ro<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>        </span><span># /mnt-rw/tmpfs is writable</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>mnt<span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span>/mnt-rw<span>
</span></span></span></code></pre></div></div></div><p>When this property is recognized by kubelet and kube-apiserver,
the <code>.status.containerStatuses[].volumeMounts[].recursiveReadOnly</code> field is set to either
<code>Enabled</code> or <code>Disabled</code>.</p><h4 id="implementations-rro">Implementations</h4><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>The following container runtimes are known to support recursive read-only mounts.</p><p>CRI-level:</p><ul><li><a href="https://containerd.io/">containerd</a>, since v2.0</li><li><a href="https://cri-o.io/">CRI-O</a>, since v1.30</li></ul><p>OCI-level:</p><ul><li><a href="https://runc.io/">runc</a>, since v1.1</li><li><a href="https://github.com/containers/crun">crun</a>, since v1.8.6</li></ul><h2 id="what-s-next">What's next</h2><p>Follow an example of <a href="/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/">deploying WordPress and MySQL with Persistent Volumes</a>.</p></div></div><div><div class="td-content"><h1>Persistent Volumes</h1><p>This document describes <em>persistent volumes</em> in Kubernetes. Familiarity with
<a href="/docs/concepts/storage/volumes/">volumes</a>, <a href="/docs/concepts/storage/storage-classes/">StorageClasses</a>
and <a href="/docs/concepts/storage/volume-attributes-classes/">VolumeAttributesClasses</a> is suggested.</p><h2 id="introduction">Introduction</h2><p>Managing storage is a distinct problem from managing compute instances.
The PersistentVolume subsystem provides an API for users and administrators
that abstracts details of how storage is provided from how it is consumed.
To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.</p><p>A <em>PersistentVolume</em> (PV) is a piece of storage in the cluster that has been
provisioned by an administrator or dynamically provisioned using
<a href="/docs/concepts/storage/storage-classes/">Storage Classes</a>. It is a resource in
the cluster just like a node is a cluster resource. PVs are volume plugins like
Volumes, but have a lifecycle independent of any individual Pod that uses the PV.
This API object captures the details of the implementation of the storage, be that
NFS, iSCSI, or a cloud-provider-specific storage system.</p><p>A <em>PersistentVolumeClaim</em> (PVC) is a request for storage by a user. It is similar
to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can
request specific levels of resources (CPU and Memory). Claims can request specific
size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany,
ReadWriteMany, or ReadWriteOncePod, see <a href="#access-modes">AccessModes</a>).</p><p>While PersistentVolumeClaims allow a user to consume abstract storage resources,
it is common that users need PersistentVolumes with varying properties, such as
performance, for different problems. Cluster administrators need to be able to
offer a variety of PersistentVolumes that differ in more ways than size and access
modes, without exposing users to the details of how those volumes are implemented.
For these needs, there is the <em>StorageClass</em> resource.</p><p>See the <a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">detailed walkthrough with working examples</a>.</p><h2 id="lifecycle-of-a-volume-and-claim">Lifecycle of a volume and claim</h2><p>PVs are resources in the cluster. PVCs are requests for those resources and also act
as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:</p><h3 id="provisioning">Provisioning</h3><p>There are two ways PVs may be provisioned: statically or dynamically.</p><h4 id="static">Static</h4><p>A cluster administrator creates a number of PVs. They carry the details of the
real storage, which is available for use by cluster users. They exist in the
Kubernetes API and are available for consumption.</p><h4 id="dynamic">Dynamic</h4><p>When none of the static PVs the administrator created match a user's PersistentVolumeClaim,
the cluster may try to dynamically provision a volume specially for the PVC.
This provisioning is based on StorageClasses: the PVC must request a
<a href="/docs/concepts/storage/storage-classes/">storage class</a> and
the administrator must have created and configured that class for dynamic
provisioning to occur. Claims that request the class <code>""</code> effectively disable
dynamic provisioning for themselves.</p><p>To enable dynamic storage provisioning based on storage class, the cluster administrator
needs to enable the <code>DefaultStorageClass</code>
<a href="/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass">admission controller</a>
on the API server. This can be done, for example, by ensuring that <code>DefaultStorageClass</code> is
among the comma-delimited, ordered list of values for the <code>--enable-admission-plugins</code> flag of
the API server component. For more information on API server command-line flags,
check <a href="/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a> documentation.</p><h3 id="binding">Binding</h3><p>A user creates, or in the case of dynamic provisioning, has already created,
a PersistentVolumeClaim with a specific amount of storage requested and with
certain access modes. A control loop in the control plane watches for new PVCs, finds
a matching PV (if possible), and binds them together. If a PV was dynamically
provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise,
the user will always get at least what they asked for, but the volume may be in
excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive,
regardless of how they were bound. A PVC to PV binding is a one-to-one mapping,
using a ClaimRef which is a bi-directional binding between the PersistentVolume
and the PersistentVolumeClaim.</p><p>Claims will remain unbound indefinitely if a matching volume does not exist.
Claims will be bound as matching volumes become available. For example, a
cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi.
The PVC can be bound when a 100Gi PV is added to the cluster.</p><h3 id="using">Using</h3><p>Pods use claims as volumes. The cluster inspects the claim to find the bound
volume and mounts that volume for a Pod. For volumes that support multiple
access modes, the user specifies which mode is desired when using their claim
as a volume in a Pod.</p><p>Once a user has a claim and that claim is bound, the bound PV belongs to the
user for as long as they need it. Users schedule Pods and access their claimed
PVs by including a <code>persistentVolumeClaim</code> section in a Pod's <code>volumes</code> block.
See <a href="#claims-as-volumes">Claims As Volumes</a> for more details on this.</p><h3 id="storage-object-in-use-protection">Storage Object in Use Protection</h3><p>The purpose of the Storage Object in Use Protection feature is to ensure that
PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs)
that are bound to PVCs are not removed from the system, as this may result in data loss.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>PVC is in active use by a Pod when a Pod object exists that is using the PVC.</div><p>If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately.
PVC removal is postponed until the PVC is no longer actively used by any Pods. Also,
if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately.
PV removal is postponed until the PV is no longer bound to a PVC.</p><p>You can see that a PVC is protected when the PVC's status is <code>Terminating</code> and the
<code>Finalizers</code> list includes <code>kubernetes.io/pvc-protection</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pvc hostpath
</span></span><span><span>Name:          hostpath
</span></span><span><span>Namespace:     default
</span></span><span><span>StorageClass:  example-hostpath
</span></span><span><span>Status:        Terminating
</span></span><span><span>Volume:
</span></span><span><span>Labels:        &lt;none&gt;
</span></span><span><span>Annotations:   volume.beta.kubernetes.io/storage-class<span>=</span>example-hostpath
</span></span><span><span>               volume.beta.kubernetes.io/storage-provisioner<span>=</span>example.com/hostpath
</span></span><span><span>Finalizers:    <span>[</span>kubernetes.io/pvc-protection<span>]</span>
</span></span><span><span>...
</span></span></code></pre></div><p>You can see that a PV is protected when the PV's status is <code>Terminating</code> and
the <code>Finalizers</code> list includes <code>kubernetes.io/pv-protection</code> too:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pv task-pv-volume
</span></span><span><span>Name:            task-pv-volume
</span></span><span><span>Labels:          <span>type</span><span>=</span><span>local</span>
</span></span><span><span>Annotations:     &lt;none&gt;
</span></span><span><span>Finalizers:      <span>[</span>kubernetes.io/pv-protection<span>]</span>
</span></span><span><span>StorageClass:    standard
</span></span><span><span>Status:          Terminating
</span></span><span><span>Claim:
</span></span><span><span>Reclaim Policy:  Delete
</span></span><span><span>Access Modes:    RWO
</span></span><span><span>Capacity:        1Gi
</span></span><span><span>Message:
</span></span><span><span>Source:
</span></span><span><span>    Type:          HostPath <span>(</span>bare host directory volume<span>)</span>
</span></span><span><span>    Path:          /tmp/data
</span></span><span><span>    HostPathType:
</span></span><span><span>Events:            &lt;none&gt;
</span></span></code></pre></div><h3 id="reclaiming">Reclaiming</h3><p>When a user is done with their volume, they can delete the PVC objects from the
API that allows reclamation of the resource. The reclaim policy for a PersistentVolume
tells the cluster what to do with the volume after it has been released of its claim.
Currently, volumes can either be Retained, Recycled, or Deleted.</p><h4 id="retain">Retain</h4><p>The <code>Retain</code> reclaim policy allows for manual reclamation of the resource.
When the PersistentVolumeClaim is deleted, the PersistentVolume still exists
and the volume is considered "released". But it is not yet available for
another claim because the previous claimant's data remains on the volume.
An administrator can manually reclaim the volume with the following steps.</p><ol><li>Delete the PersistentVolume. The associated storage asset in external infrastructure
still exists after the PV is deleted.</li><li>Manually clean up the data on the associated storage asset accordingly.</li><li>Manually delete the associated storage asset.</li></ol><p>If you want to reuse the same storage asset, create a new PersistentVolume with
the same storage asset definition.</p><h4 id="delete">Delete</h4><p>For volume plugins that support the <code>Delete</code> reclaim policy, deletion removes
both the PersistentVolume object from Kubernetes, as well as the associated
storage asset in the external infrastructure. Volumes that were dynamically provisioned
inherit the <a href="#reclaim-policy">reclaim policy of their StorageClass</a>, which
defaults to <code>Delete</code>. The administrator should configure the StorageClass
according to users' expectations; otherwise, the PV must be edited or
patched after it is created. See
<a href="/docs/tasks/administer-cluster/change-pv-reclaim-policy/">Change the Reclaim Policy of a PersistentVolume</a>.</p><h4 id="recycle">Recycle</h4><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>The <code>Recycle</code> reclaim policy is deprecated. Instead, the recommended approach
is to use dynamic provisioning.</div><p>If supported by the underlying volume plugin, the <code>Recycle</code> reclaim policy performs
a basic scrub (<code>rm -rf /thevolume/*</code>) on the volume and makes it available again for a new claim.</p><p>However, an administrator can configure a custom recycler Pod template using
the Kubernetes controller manager command line arguments as described in the
<a href="/docs/reference/command-line-tools-reference/kube-controller-manager/">reference</a>.
The custom recycler Pod template must contain a <code>volumes</code> specification, as
shown in the example below:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pv-recycler<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>restartPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>vol<span>
</span></span></span><span><span><span>    </span><span>hostPath</span>:<span>
</span></span></span><span><span><span>      </span><span>path</span>:<span> </span>/any/path/it/will/be/replaced<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pv-recycler<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span><span>"registry.k8s.io/busybox"</span><span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"/bin/sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \"$(ls -A /scrub)\" || exit 1"</span>]<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>vol<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/scrub<span>
</span></span></span></code></pre></div><p>However, the particular path specified in the custom recycler Pod template in the
<code>volumes</code> part is replaced with the particular path of the volume that is being recycled.</p><h3 id="persistentvolume-deletion-protection-finalizer">PersistentVolume deletion protection finalizer</h3><div class="feature-state-notice feature-stable" title="Feature Gate: HonorPVReclaimPolicy"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><p>Finalizers can be added on a PersistentVolume to ensure that PersistentVolumes
having <code>Delete</code> reclaim policy are deleted only after the backing storage are deleted.</p><p>The finalizer <code>external-provisioner.volume.kubernetes.io/finalizer</code>(introduced
in v1.31) is added to both dynamically provisioned and statically provisioned
CSI volumes.</p><p>The finalizer <code>kubernetes.io/pv-controller</code>(introduced in v1.31) is added to
dynamically provisioned in-tree plugin volumes and skipped for statically
provisioned in-tree plugin volumes.</p><p>The following is an example of dynamically provisioned in-tree plugin volume:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78
</span></span><span><span>Name:            pvc-74a498d6-3929-47e8-8c02-078c1ece4d78
</span></span><span><span>Labels:          &lt;none&gt;
</span></span><span><span>Annotations:     kubernetes.io/createdby: vsphere-volume-dynamic-provisioner
</span></span><span><span>                 pv.kubernetes.io/bound-by-controller: yes
</span></span><span><span>                 pv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume
</span></span><span><span>Finalizers:      <span>[</span>kubernetes.io/pv-protection kubernetes.io/pv-controller<span>]</span>
</span></span><span><span>StorageClass:    vcp-sc
</span></span><span><span>Status:          Bound
</span></span><span><span>Claim:           default/vcp-pvc-1
</span></span><span><span>Reclaim Policy:  Delete
</span></span><span><span>Access Modes:    RWO
</span></span><span><span>VolumeMode:      Filesystem
</span></span><span><span>Capacity:        1Gi
</span></span><span><span>Node Affinity:   &lt;none&gt;
</span></span><span><span>Message:
</span></span><span><span>Source:
</span></span><span><span>    Type:               vSphereVolume <span>(</span>a Persistent Disk resource in vSphere<span>)</span>
</span></span><span><span>    VolumePath:         <span>[</span>vsanDatastore<span>]</span> d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-dynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk
</span></span><span><span>    FSType:             ext4
</span></span><span><span>    StoragePolicyName:  vSAN Default Storage Policy
</span></span><span><span>Events:                 &lt;none&gt;
</span></span></code></pre></div><p>The finalizer <code>external-provisioner.volume.kubernetes.io/finalizer</code> is added for CSI volumes.
The following is an example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>Name:            pvc-2f0bab97-85a8-4552-8044-eb8be45cf48d
</span></span><span><span>Labels:          &lt;none&gt;
</span></span><span><span>Annotations:     pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com
</span></span><span><span>Finalizers:      <span>[</span>kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer<span>]</span>
</span></span><span><span>StorageClass:    fast
</span></span><span><span>Status:          Bound
</span></span><span><span>Claim:           demo-app/nginx-logs
</span></span><span><span>Reclaim Policy:  Delete
</span></span><span><span>Access Modes:    RWO
</span></span><span><span>VolumeMode:      Filesystem
</span></span><span><span>Capacity:        200Mi
</span></span><span><span>Node Affinity:   &lt;none&gt;
</span></span><span><span>Message:
</span></span><span><span>Source:
</span></span><span><span>    Type:              CSI <span>(</span>a Container Storage Interface <span>(</span>CSI<span>)</span> volume <span>source</span><span>)</span>
</span></span><span><span>    Driver:            csi.vsphere.vmware.com
</span></span><span><span>    FSType:            ext4
</span></span><span><span>    VolumeHandle:      44830fa8-79b4-406b-8b58-621ba25353fd
</span></span><span><span>    ReadOnly:          <span>false</span>
</span></span><span><span>    VolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity<span>=</span>1648442357185-8081-csi.vsphere.vmware.com
</span></span><span><span>                           <span>type</span><span>=</span>vSphere CNS Block Volume
</span></span><span><span>Events:                &lt;none&gt;
</span></span></code></pre></div><p>When the <code>CSIMigration{provider}</code> feature flag is enabled for a specific in-tree volume plugin,
the <code>kubernetes.io/pv-controller</code> finalizer is replaced by the
<code>external-provisioner.volume.kubernetes.io/finalizer</code> finalizer.</p><p>The finalizers ensure that the PV object is removed only after the volume is deleted
from the storage backend provided the reclaim policy of the PV is <code>Delete</code>. This
also ensures that the volume is deleted from storage backend irrespective of the
order of deletion of PV and PVC.</p><h3 id="reserving-a-persistentvolume">Reserving a PersistentVolume</h3><p>The control plane can <a href="#binding">bind PersistentVolumeClaims to matching PersistentVolumes</a>
in the cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.</p><p>By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding
between that specific PV and PVC. If the PersistentVolume exists and has not reserved
PersistentVolumeClaims through its <code>claimRef</code> field, then the PersistentVolume and
PersistentVolumeClaim will be bound.</p><p>The binding happens regardless of some volume matching criteria, including node affinity.
The control plane still checks that <a href="/docs/concepts/storage/storage-classes/">storage class</a>,
access modes, and requested storage size are valid.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>foo-pvc<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>foo<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span><span>""</span><span> </span><span># Empty string must be explicitly set otherwise default StorageClass will be set</span><span>
</span></span></span><span><span><span>  </span><span>volumeName</span>:<span> </span>foo-pv<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><p>This method does not guarantee any binding privileges to the PersistentVolume.
If other PersistentVolumeClaims could use the PV that you specify, you first
need to reserve that storage volume. Specify the relevant PersistentVolumeClaim
in the <code>claimRef</code> field of the PV so that other PVCs can not bind to it.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolume<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>foo-pv<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>  </span><span>claimRef</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>foo-pvc<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>foo<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><p>This is useful if you want to consume PersistentVolumes that have their <code>persistentVolumeReclaimPolicy</code> set
to <code>Retain</code>, including cases where you are reusing an existing PV.</p><h3 id="expanding-persistent-volumes-claims">Expanding Persistent Volumes Claims</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>Support for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand
the following types of volumes:</p><ul><li><a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">csi</a> (including some CSI migrated
volme types)</li><li>flexVolume (deprecated)</li><li>portworxVolume (deprecated)</li></ul><p>You can only expand a PVC if its storage class's <code>allowVolumeExpansion</code> field is set to true.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-vol-default<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>vendor-name.example/magicstorage<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>resturl</span>:<span> </span><span>"http://192.168.10.100:8080"</span><span>
</span></span></span><span><span><span>  </span><span>restuser</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>  </span><span>secretNamespace</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>  </span><span>secretName</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span></span><span>allowVolumeExpansion</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><p>To request a larger volume for a PVC, edit the PVC object and specify a larger
size. This triggers expansion of the volume that backs the underlying PersistentVolume. A
new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Directly editing the size of a PersistentVolume can prevent an automatic resize of that volume.
If you edit the capacity of a PersistentVolume, and then edit the <code>.spec</code> of a matching
PersistentVolumeClaim to make the size of the PersistentVolumeClaim match the PersistentVolume,
then no storage resize happens.
The Kubernetes control plane will see that the desired state of both resources matches,
conclude that the backing volume size has been manually
increased and that no resize is necessary.</div><h4 id="csi-volume-expansion">CSI Volume expansion</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>Support for expanding CSI volumes is enabled by default but it also requires a
specific CSI driver to support volume expansion. Refer to documentation of the
specific CSI driver for more information.</p><h4 id="resizing-a-volume-containing-a-file-system">Resizing a volume containing a file system</h4><p>You can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.</p><p>When a volume contains a file system, the file system is only resized when a new Pod is using
the PersistentVolumeClaim in <code>ReadWrite</code> mode. File system expansion is either done when a Pod is starting up
or when a Pod is running and the underlying file system supports online expansion.</p><p>FlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with the
<code>RequiresFSResize</code> capability to <code>true</code>. The FlexVolume can be resized on Pod restart.</p><h4 id="resizing-an-in-use-persistentvolumeclaim">Resizing an in-use PersistentVolumeClaim</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>In this case, you don't need to delete and recreate a Pod or deployment that is using an existing PVC.
Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.
This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that
uses the PVC before the expansion can complete.</p><p>Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>FlexVolume resize is possible only when the underlying driver supports resize.</div><h4 id="recovering-from-failure-when-expanding-volumes">Recovering from Failure when Expanding Volumes</h4><p>If a user specifies a new size that is too big to be satisfied by underlying
storage system, expansion of PVC will be continuously retried until user or
cluster administrator takes some action. This can be undesirable and hence
Kubernetes provides following methods of recovering from such failures.</p><ul class="nav nav-tabs" id="recovery-methods"><li class="nav-item"><a class="nav-link active" href="#recovery-methods-0">Manually with Cluster Administrator access</a></li><li class="nav-item"><a class="nav-link" href="#recovery-methods-1">By requesting expansion to smaller size</a></li></ul><div class="tab-content" id="recovery-methods"><div id="recovery-methods-0" class="tab-pane show active"><p><p>If expanding underlying storage fails, the cluster administrator can manually
recover the Persistent Volume Claim (PVC) state and cancel the resize requests.
Otherwise, the resize requests are continuously retried by the controller without
administrator intervention.</p><ol><li>Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC)
with <code>Retain</code> reclaim policy.</li><li>Delete the PVC. Since PV has <code>Retain</code> reclaim policy - we will not lose any data
when we recreate the PVC.</li><li>Delete the <code>claimRef</code> entry from PV specs, so as new PVC can bind to it.
This should make the PV <code>Available</code>.</li><li>Re-create the PVC with smaller size than PV and set <code>volumeName</code> field of the
PVC to the name of the PV. This should bind new PVC to existing PV.</li><li>Don't forget to restore the reclaim policy of the PV.</li></ol></p></div><div id="recovery-methods-1" class="tab-pane"><p><p>If expansion has failed for a PVC, you can retry expansion with a
smaller size than the previously requested value. To request a new expansion attempt with a
smaller proposed size, edit <code>.spec.resources</code> for that PVC and choose a value that is less than the
value you previously tried.
This is useful if expansion to a higher value did not succeed because of capacity constraint.
If that has happened, or you suspect that it might have, you can retry expansion by specifying a
size that is within the capacity limits of underlying storage provider. You can monitor status of
resize operation by watching <code>.status.allocatedResourceStatuses</code> and events on the PVC.</p><p>Note that,
although you can specify a lower amount of storage than what was requested previously,
the new value must still be higher than <code>.status.capacity</code>.
Kubernetes does not support shrinking a PVC to less than its current size.</p></p></div></div><h2 id="types-of-persistent-volumes">Types of Persistent Volumes</h2><p>PersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:</p><ul><li><a href="/docs/concepts/storage/volumes/#csi"><code>csi</code></a> - Container Storage Interface (CSI)</li><li><a href="/docs/concepts/storage/volumes/#fc"><code>fc</code></a> - Fibre Channel (FC) storage</li><li><a href="/docs/concepts/storage/volumes/#hostpath"><code>hostPath</code></a> - HostPath volume
(for single node testing only; WILL NOT WORK in a multi-node cluster;
consider using <code>local</code> volume instead)</li><li><a href="/docs/concepts/storage/volumes/#iscsi"><code>iscsi</code></a> - iSCSI (SCSI over IP) storage</li><li><a href="/docs/concepts/storage/volumes/#local"><code>local</code></a> - local storage devices
mounted on nodes.</li><li><a href="/docs/concepts/storage/volumes/#nfs"><code>nfs</code></a> - Network File System (NFS) storage</li></ul><p>The following types of PersistentVolume are deprecated but still available.
If you are using these volume types except for <code>flexVolume</code>, <code>cephfs</code> and <code>rbd</code>,
please install corresponding CSI drivers.</p><ul><li><a href="/docs/concepts/storage/volumes/#awselasticblockstore"><code>awsElasticBlockStore</code></a> - AWS Elastic Block Store (EBS)
(<strong>migration on by default</strong> starting v1.23)</li><li><a href="/docs/concepts/storage/volumes/#azuredisk"><code>azureDisk</code></a> - Azure Disk
(<strong>migration on by default</strong> starting v1.23)</li><li><a href="/docs/concepts/storage/volumes/#azurefile"><code>azureFile</code></a> - Azure File
(<strong>migration on by default</strong> starting v1.24)</li><li><a href="/docs/concepts/storage/volumes/#cinder"><code>cinder</code></a> - Cinder (OpenStack block storage)
(<strong>migration on by default</strong> starting v1.21)</li><li><a href="/docs/concepts/storage/volumes/#flexvolume"><code>flexVolume</code></a> - FlexVolume
(<strong>deprecated</strong> starting v1.23, no migration plan and no plan to remove support)</li><li><a href="/docs/concepts/storage/volumes/#gcePersistentDisk"><code>gcePersistentDisk</code></a> - GCE Persistent Disk
(<strong>migration on by default</strong> starting v1.23)</li><li><a href="/docs/concepts/storage/volumes/#portworxvolume"><code>portworxVolume</code></a> - Portworx volume
(<strong>migration on by default</strong> starting v1.31)</li><li><a href="/docs/concepts/storage/volumes/#vspherevolume"><code>vsphereVolume</code></a> - vSphere VMDK volume
(<strong>migration on by default</strong> starting v1.25)</li></ul><p>Older versions of Kubernetes also supported the following in-tree PersistentVolume types:</p><ul><li><a href="/docs/concepts/storage/volumes/#cephfs"><code>cephfs</code></a>
(<strong>not available</strong> starting v1.31)</li><li><code>flocker</code> - Flocker storage.
(<strong>not available</strong> starting v1.25)</li><li><code>glusterfs</code> - GlusterFS storage.
(<strong>not available</strong> starting v1.26)</li><li><code>photonPersistentDisk</code> - Photon controller persistent disk.
(<strong>not available</strong> starting v1.15)</li><li><code>quobyte</code> - Quobyte volume.
(<strong>not available</strong> starting v1.25)</li><li><a href="/docs/concepts/storage/volumes/#rbd"><code>rbd</code></a> - Rados Block Device (RBD) volume
(<strong>not available</strong> starting v1.31)</li><li><code>scaleIO</code> - ScaleIO volume.
(<strong>not available</strong> starting v1.21)</li><li><code>storageos</code> - StorageOS volume.
(<strong>not available</strong> starting v1.25)</li></ul><h2 id="persistent-volumes">Persistent Volumes</h2><p>Each PV contains a spec and status, which is the specification and status of the volume.
The name of a PersistentVolume object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolume<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pv0003<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>capacity</span>:<span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span>5Gi<span>
</span></span></span><span><span><span>  </span><span>volumeMode</span>:<span> </span>Filesystem<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>persistentVolumeReclaimPolicy</span>:<span> </span>Recycle<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>slow<span>
</span></span></span><span><span><span>  </span><span>mountOptions</span>:<span>
</span></span></span><span><span><span>    </span>- hard<span>
</span></span></span><span><span><span>    </span>- nfsvers=4.1<span>
</span></span></span><span><span><span>  </span><span>nfs</span>:<span>
</span></span></span><span><span><span>    </span><span>path</span>:<span> </span>/tmp<span>
</span></span></span><span><span><span>    </span><span>server</span>:<span> </span><span>172.17.0.2</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Helper programs relating to the volume type may be required for consumption of
a PersistentVolume within a cluster. In this example, the PersistentVolume is
of type NFS and the helper program /sbin/mount.nfs is required to support the
mounting of NFS filesystems.</div><h3 id="capacity">Capacity</h3><p>Generally, a PV will have a specific storage capacity. This is set using the PV's
<code>capacity</code> attribute which is a <a class="glossary-tooltip" title="A whole-number representation of small or large numbers using SI suffixes." href="/docs/reference/glossary/?all=true#term-quantity" target="_blank">Quantity</a> value.</p><p>Currently, storage size is the only resource that can be set or requested.
Future attributes may include IOPS, throughput, etc.</p><h3 id="volume-mode">Volume Mode</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>Kubernetes supports two <code>volumeModes</code> of PersistentVolumes: <code>Filesystem</code> and <code>Block</code>.</p><p><code>volumeMode</code> is an optional API parameter.
<code>Filesystem</code> is the default mode used when <code>volumeMode</code> parameter is omitted.</p><p>A volume with <code>volumeMode: Filesystem</code> is <em>mounted</em> into Pods into a directory. If the volume
is backed by a block device and the device is empty, Kubernetes creates a filesystem
on the device before mounting it for the first time.</p><p>You can set the value of <code>volumeMode</code> to <code>Block</code> to use a volume as a raw block device.
Such volume is presented into a Pod as a block device, without any filesystem on it.
This mode is useful to provide a Pod the fastest possible way to access a volume, without
any filesystem layer between the Pod and the volume. On the other hand, the application
running in the Pod must know how to handle a raw block device.
See <a href="#raw-block-volume-support">Raw Block Volume Support</a>
for an example on how to use a volume with <code>volumeMode: Block</code> in a Pod.</p><h3 id="access-modes">Access Modes</h3><p>A PersistentVolume can be mounted on a host in any way supported by the resource
provider. As shown in the table below, providers will have different capabilities
and each PV's access modes are set to the specific modes supported by that particular
volume. For example, NFS can support multiple read/write clients, but a specific
NFS PV might be exported on the server as read-only. Each PV gets its own set of
access modes describing that specific PV's capabilities.</p><p>The access modes are:</p><dl><dt><code>ReadWriteOnce</code></dt><dd>the volume can be mounted as read-write by a single node. ReadWriteOnce access
mode still can allow multiple pods to access (read from or write to) that volume when the pods are
running on the same node. For single pod access, please see ReadWriteOncePod.</dd><dt><code>ReadOnlyMany</code></dt><dd>the volume can be mounted as read-only by many nodes.</dd><dt><code>ReadWriteMany</code></dt><dd>the volume can be mounted as read-write by many nodes.</dd><dt><code>ReadWriteOncePod</code></dt><dd><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [stable]</code></div>the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod
access mode if you want to ensure that only one pod across the whole cluster can
read that PVC or write to it.</dd></dl><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The <code>ReadWriteOncePod</code> access mode is only supported for
<a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> volumes and Kubernetes version
1.22+. To use this feature you will need to update the following
<a href="https://kubernetes-csi.github.io/docs/sidecar-containers.html">CSI sidecars</a>
to these versions or greater:</p><ul><li><a href="https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0">csi-provisioner:v3.0.0+</a></li><li><a href="https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0">csi-attacher:v3.3.0+</a></li><li><a href="https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0">csi-resizer:v1.3.0+</a></li></ul></div><p>In the CLI, the access modes are abbreviated to:</p><ul><li>RWO - ReadWriteOnce</li><li>ROX - ReadOnlyMany</li><li>RWX - ReadWriteMany</li><li>RWOP - ReadWriteOncePod</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes.
In some cases, the volume access modes also constrain where the PersistentVolume can be mounted.
Volume access modes do <strong>not</strong> enforce write protection once the storage has been mounted.
Even if the access modes are specified as ReadWriteOnce, ReadOnlyMany, or ReadWriteMany,
they don't set any constraints on the volume. For example, even if a PersistentVolume is
created as ReadOnlyMany, it is no guarantee that it will be read-only. If the access modes
are specified as ReadWriteOncePod, the volume is constrained and can be mounted on only a single Pod.</div><blockquote><p><strong>Important!</strong> A volume can only be mounted using one access mode at a time,
even if it supports many.</p></blockquote><table><thead><tr><th>Volume Plugin</th><th>ReadWriteOnce</th><th>ReadOnlyMany</th><th>ReadWriteMany</th><th>ReadWriteOncePod</th></tr></thead><tbody><tr><td>AzureFile</td><td>&#10003;</td><td>&#10003;</td><td>&#10003;</td><td>-</td></tr><tr><td>CephFS</td><td>&#10003;</td><td>&#10003;</td><td>&#10003;</td><td>-</td></tr><tr><td>CSI</td><td>depends on the driver</td><td>depends on the driver</td><td>depends on the driver</td><td>depends on the driver</td></tr><tr><td>FC</td><td>&#10003;</td><td>&#10003;</td><td>-</td><td>-</td></tr><tr><td>FlexVolume</td><td>&#10003;</td><td>&#10003;</td><td>depends on the driver</td><td>-</td></tr><tr><td>HostPath</td><td>&#10003;</td><td>-</td><td>-</td><td>-</td></tr><tr><td>iSCSI</td><td>&#10003;</td><td>&#10003;</td><td>-</td><td>-</td></tr><tr><td>NFS</td><td>&#10003;</td><td>&#10003;</td><td>&#10003;</td><td>-</td></tr><tr><td>RBD</td><td>&#10003;</td><td>&#10003;</td><td>-</td><td>-</td></tr><tr><td>VsphereVolume</td><td>&#10003;</td><td>-</td><td>- (works when Pods are collocated)</td><td>-</td></tr><tr><td>PortworxVolume</td><td>&#10003;</td><td>-</td><td>&#10003;</td><td>-</td></tr></tbody></table><h3 id="class">Class</h3><p>A PV can have a class, which is specified by setting the
<code>storageClassName</code> attribute to the name of a
<a href="/docs/concepts/storage/storage-classes/">StorageClass</a>.
A PV of a particular class can only be bound to PVCs requesting
that class. A PV with no <code>storageClassName</code> has no class and can only be bound
to PVCs that request no particular class.</p><p>In the past, the annotation <code>volume.beta.kubernetes.io/storage-class</code> was used instead
of the <code>storageClassName</code> attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.</p><h3 id="reclaim-policy">Reclaim Policy</h3><p>Current reclaim policies are:</p><ul><li>Retain -- manual reclamation</li><li>Recycle -- basic scrub (<code>rm -rf /thevolume/*</code>)</li><li>Delete -- delete the volume</li></ul><p>For Kubernetes 1.34, only <code>nfs</code> and <code>hostPath</code> volume types support recycling.</p><h3 id="mount-options">Mount Options</h3><p>A Kubernetes administrator can specify additional mount options for when a
Persistent Volume is mounted on a node.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Not all Persistent Volume types support mount options.</div><p>The following volume types support mount options:</p><ul><li><code>csi</code> (including CSI migrated volume types)</li><li><code>iscsi</code></li><li><code>nfs</code></li></ul><p>Mount options are not validated. If a mount option is invalid, the mount fails.</p><p>In the past, the annotation <code>volume.beta.kubernetes.io/mount-options</code> was used instead
of the <code>mountOptions</code> attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.</p><h3 id="node-affinity">Node Affinity</h3><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>For most volume types, you do not need to set this field.
You need to explicitly set this for <a href="/docs/concepts/storage/volumes/#local">local</a> volumes.</div><p>A PV can specify node affinity to define constraints that limit what nodes this
volume can be accessed from. Pods that use a PV will only be scheduled to nodes
that are selected by the node affinity. To specify node affinity, set
<code>nodeAffinity</code> in the <code>.spec</code> of a PV. The
<a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/#PersistentVolumeSpec">PersistentVolume</a>
API reference has more details on this field.</p><h3 id="phase">Phase</h3><p>A PersistentVolume will be in one of the following phases:</p><dl><dt><code>Available</code></dt><dd>a free resource that is not yet bound to a claim</dd><dt><code>Bound</code></dt><dd>the volume is bound to a claim</dd><dt><code>Released</code></dt><dd>the claim has been deleted, but the associated storage resource is not yet reclaimed by the cluster</dd><dt><code>Failed</code></dt><dd>the volume has failed its (automated) reclamation</dd></dl><p>You can see the name of the PVC bound to the PV using <code>kubectl describe persistentvolume &lt;name&gt;</code>.</p><h4 id="phase-transition-timestamp">Phase transition timestamp</h4><div class="feature-state-notice feature-stable" title="Feature Gate: PersistentVolumeLastPhaseTransitionTime"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [stable]</code> (enabled by default: true)</div><p>The <code>.status</code> field for a PersistentVolume can include an alpha <code>lastPhaseTransitionTime</code> field. This field records
the timestamp of when the volume last transitioned its phase. For newly created
volumes the phase is set to <code>Pending</code> and <code>lastPhaseTransitionTime</code> is set to
the current time.</p><h2 id="persistentvolumeclaims">PersistentVolumeClaims</h2><p>Each PVC contains a spec and status, which is the specification and status of the claim.
The name of a PersistentVolumeClaim object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>myclaim<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>volumeMode</span>:<span> </span>Filesystem<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>8Gi<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>slow<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>release</span>:<span> </span><span>"stable"</span><span>
</span></span></span><span><span><span>    </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>      </span>- {<span>key: environment, operator: In, values</span>:<span> </span>[dev]}<span>
</span></span></span></code></pre></div><h3 id="access-modes-1">Access Modes</h3><p>Claims use <a href="#access-modes">the same conventions as volumes</a> when requesting
storage with specific access modes.</p><h3 id="volume-modes">Volume Modes</h3><p>Claims use <a href="#volume-mode">the same convention as volumes</a> to indicate the
consumption of the volume as either a filesystem or block device.</p><h3 id="volume-name">Volume Name</h3><p>Claims can use the <code>volumeName</code> field to explicitly bind to a specific PersistentVolume. You can also leave
<code>volumeName</code> unset, indicating that you'd like Kubernetes to set up a new PersistentVolume
that matches the claim.
If the specified PV is already bound to another PVC, the binding will be stuck
in a pending state.</p><h3 id="resources">Resources</h3><p>Claims, like Pods, can request specific quantities of a resource. In this case,
the request is for storage. The same
<a href="https://git.k8s.io/design-proposals-archive/scheduling/resources.md">resource model</a>
applies to both volumes and claims.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>For <code>Filesystem</code> volumes, the storage request refers to the "outer" volume size
(i.e. the allocated size from the storage backend).
This means that the writeable size may be slightly lower for providers that
build a filesystem on top of a block device, due to filesystem overhead.
This is especially visible with XFS, where many metadata features are enabled by default.</div><h3 id="selector">Selector</h3><p>Claims can specify a
<a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selector</a>
to further filter the set of volumes.
Only the volumes whose labels match the selector can be bound to the claim.
The selector can consist of two fields:</p><ul><li><code>matchLabels</code> - the volume must have a label with this value</li><li><code>matchExpressions</code> - a list of requirements made by specifying key, list of values,
and operator that relates the key and values.
Valid operators include <code>In</code>, <code>NotIn</code>, <code>Exists</code>, and <code>DoesNotExist</code>.</li></ul><p>All of the requirements, from both <code>matchLabels</code> and <code>matchExpressions</code>, are
ANDed together &#8211; they must all be satisfied in order to match.</p><h3 id="class-1">Class</h3><p>A claim can request a particular class by specifying the name of a
<a href="/docs/concepts/storage/storage-classes/">StorageClass</a>
using the attribute <code>storageClassName</code>.
Only PVs of the requested class, ones with the same <code>storageClassName</code> as the PVC,
can be bound to the PVC.</p><p>PVCs don't necessarily have to request a class. A PVC with its <code>storageClassName</code> set
equal to <code>""</code> is always interpreted to be requesting a PV with no class, so it
can only be bound to PVs with no class (no annotation or one set equal to <code>""</code>).
A PVC with no <code>storageClassName</code> is not quite the same and is treated differently
by the cluster, depending on whether the
<a href="/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass"><code>DefaultStorageClass</code> admission plugin</a>
is turned on.</p><ul><li>If the admission plugin is turned on, the administrator may specify a default StorageClass.
All PVCs that have no <code>storageClassName</code> can be bound only to PVs of that default.
Specifying a default StorageClass is done by setting the annotation
<code>storageclass.kubernetes.io/is-default-class</code> equal to <code>true</code> in a StorageClass object.
If the administrator does not specify a default, the cluster responds to PVC creation
as if the admission plugin were turned off.
If more than one default StorageClass is specified, the newest default is used when
the PVC is dynamically provisioned.</li><li>If the admission plugin is turned off, there is no notion of a default StorageClass.
All PVCs that have <code>storageClassName</code> set to <code>""</code> can be bound only to PVs
that have <code>storageClassName</code> also set to <code>""</code>.
However, PVCs with missing <code>storageClassName</code> can be updated later once default StorageClass becomes available.
If the PVC gets updated it will no longer bind to PVs that have <code>storageClassName</code> also set to <code>""</code>.</li></ul><p>See <a href="#retroactive-default-storageclass-assignment">retroactive default StorageClass assignment</a> for more details.</p><p>Depending on installation method, a default StorageClass may be deployed
to a Kubernetes cluster by addon manager during installation.</p><p>When a PVC specifies a <code>selector</code> in addition to requesting a StorageClass,
the requirements are ANDed together: only a PV of the requested class and with
the requested labels may be bound to the PVC.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Currently, a PVC with a non-empty <code>selector</code> can't have a PV dynamically provisioned for it.</div><p>In the past, the annotation <code>volume.beta.kubernetes.io/storage-class</code> was used instead
of <code>storageClassName</code> attribute. This annotation is still working; however,
it won't be supported in a future Kubernetes release.</p><h4 id="retroactive-default-storageclass-assignment">Retroactive default StorageClass assignment</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [stable]</code></div><p>You can create a PersistentVolumeClaim without specifying a <code>storageClassName</code>
for the new PVC, and you can do so even when no default StorageClass exists
in your cluster. In this case, the new PVC creates as you defined it, and the
<code>storageClassName</code> of that PVC remains unset until default becomes available.</p><p>When a default StorageClass becomes available, the control plane identifies any
existing PVCs without <code>storageClassName</code>. For the PVCs that either have an empty
value for <code>storageClassName</code> or do not have this key, the control plane then
updates those PVCs to set <code>storageClassName</code> to match the new default StorageClass.
If you have an existing PVC where the <code>storageClassName</code> is <code>""</code>, and you configure
a default StorageClass, then this PVC will not get updated.</p><p>In order to keep binding to PVs with <code>storageClassName</code> set to <code>""</code>
(while a default StorageClass is present), you need to set the <code>storageClassName</code>
of the associated PVC to <code>""</code>.</p><p>This behavior helps administrators change default StorageClass by removing the
old one first and then creating or setting another one. This brief window while
there is no default causes PVCs without <code>storageClassName</code> created at that time
to not have any default, but due to the retroactive default StorageClass
assignment this way of changing defaults is safe.</p><h2 id="claims-as-volumes">Claims As Volumes</h2><p>Pods access storage by using the claim as a volume. Claims must exist in the
same namespace as the Pod using the claim. The cluster finds the claim in the
Pod's namespace and uses it to get the PersistentVolume backing the claim.
The volume is then mounted to the host and into the Pod.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>myfrontend<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>      </span>- <span>mountPath</span>:<span> </span><span>"/var/www/html"</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>mypd<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>mypd<span>
</span></span></span><span><span><span>      </span><span>persistentVolumeClaim</span>:<span>
</span></span></span><span><span><span>        </span><span>claimName</span>:<span> </span>myclaim<span>
</span></span></span></code></pre></div><h3 id="a-note-on-namespaces">A Note on Namespaces</h3><p>PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are
namespaced objects, mounting claims with "Many" modes (<code>ROX</code>, <code>RWX</code>) is only
possible within one namespace.</p><h3 id="persistentvolumes-typed-hostpath">PersistentVolumes typed <code>hostPath</code></h3><p>A <code>hostPath</code> PersistentVolume uses a file or directory on the Node to emulate
network-attached storage. See
<a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume">an example of <code>hostPath</code> typed volume</a>.</p><h2 id="raw-block-volume-support">Raw Block Volume Support</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>The following volume plugins support raw block volumes, including dynamic provisioning where
applicable:</p><ul><li>CSI (including some CSI migrated volume types)</li><li>FC (Fibre Channel)</li><li>iSCSI</li><li>Local volume</li></ul><h3 id="persistent-volume-using-a-raw-block-volume">PersistentVolume using a Raw Block Volume</h3><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolume<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>block-pv<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>capacity</span>:<span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span>10Gi<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>volumeMode</span>:<span> </span>Block<span>
</span></span></span><span><span><span>  </span><span>persistentVolumeReclaimPolicy</span>:<span> </span>Retain<span>
</span></span></span><span><span><span>  </span><span>fc</span>:<span>
</span></span></span><span><span><span>    </span><span>targetWWNs</span>:<span> </span>[<span>"50060e801049cfd1"</span>]<span>
</span></span></span><span><span><span>    </span><span>lun</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>    </span><span>readOnly</span>:<span> </span><span>false</span><span>
</span></span></span></code></pre></div><h3 id="persistent-volume-claim-requesting-a-raw-block-volume">PersistentVolumeClaim requesting a Raw Block Volume</h3><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>block-pvc<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>volumeMode</span>:<span> </span>Block<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>10Gi<span>
</span></span></span></code></pre></div><h3 id="pod-specification-adding-raw-block-device-path-in-container">Pod specification adding Raw Block Device path in container</h3><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod-with-block-volume<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>fc-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>fedora:26<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span>"/bin/sh"</span>,<span> </span><span>"-c"</span>]<span>
</span></span></span><span><span><span>      </span><span>args</span>:<span> </span>[<span> </span><span>"tail -f /dev/null"</span><span> </span>]<span>
</span></span></span><span><span><span>      </span><span>volumeDevices</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>          </span><span>devicePath</span>:<span> </span>/dev/xvda<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>data<span>
</span></span></span><span><span><span>      </span><span>persistentVolumeClaim</span>:<span>
</span></span></span><span><span><span>        </span><span>claimName</span>:<span> </span>block-pvc<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When adding a raw block device for a Pod, you specify the device path in the
container instead of a mount path.</div><h3 id="binding-block-volumes">Binding Block Volumes</h3><p>If a user requests a raw block volume by indicating this using the <code>volumeMode</code>
field in the PersistentVolumeClaim spec, the binding rules differ slightly from
previous releases that didn't consider this mode as part of the spec.
Listed is a table of possible combinations the user and admin might specify for
requesting a raw block device. The table indicates if the volume will be bound or
not given the combinations: Volume binding matrix for statically provisioned volumes:</p><table><thead><tr><th>PV volumeMode</th><th>PVC volumeMode</th><th>Result</th></tr></thead><tbody><tr><td>unspecified</td><td>unspecified</td><td>BIND</td></tr><tr><td>unspecified</td><td>Block</td><td>NO BIND</td></tr><tr><td>unspecified</td><td>Filesystem</td><td>BIND</td></tr><tr><td>Block</td><td>unspecified</td><td>NO BIND</td></tr><tr><td>Block</td><td>Block</td><td>BIND</td></tr><tr><td>Block</td><td>Filesystem</td><td>NO BIND</td></tr><tr><td>Filesystem</td><td>Filesystem</td><td>BIND</td></tr><tr><td>Filesystem</td><td>Block</td><td>NO BIND</td></tr><tr><td>Filesystem</td><td>unspecified</td><td>BIND</td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Only statically provisioned volumes are supported for alpha release. Administrators
should take care to consider these values when working with raw block devices.</div><h2 id="volume-snapshot-and-restore-volume-from-snapshot-support">Volume Snapshot and Restore Volume from Snapshot Support</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><p>Volume snapshots only support the out-of-tree CSI volume plugins.
For details, see <a href="/docs/concepts/storage/volume-snapshots/">Volume Snapshots</a>.
In-tree volume plugins are deprecated. You can read about the deprecated volume
plugins in the
<a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md">Volume Plugin FAQ</a>.</p><h3 id="create-persistent-volume-claim-from-volume-snapshot">Create a PersistentVolumeClaim from a Volume Snapshot</h3><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>restore-pvc<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>csi-hostpath-sc<span>
</span></span></span><span><span><span>  </span><span>dataSource</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>new-snapshot-test<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>VolumeSnapshot<span>
</span></span></span><span><span><span>    </span><span>apiGroup</span>:<span> </span>snapshot.storage.k8s.io<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>10Gi<span>
</span></span></span></code></pre></div><h2 id="volume-cloning">Volume Cloning</h2><p><a href="/docs/concepts/storage/volume-pvc-datasource/">Volume Cloning</a>
only available for CSI volume plugins.</p><h3 id="create-persistent-volume-claim-from-an-existing-pvc">Create PersistentVolumeClaim from an existing PVC</h3><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cloned-pvc<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>my-csi-plugin<span>
</span></span></span><span><span><span>  </span><span>dataSource</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>existing-src-pvc-name<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>10Gi<span>
</span></span></span></code></pre></div><h2 id="volume-populators-and-data-sources">Volume populators and data sources</h2><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [beta]</code></div><p>Kubernetes supports custom volume populators.
To use custom volume populators, you must enable the <code>AnyVolumeDataSource</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> for
the kube-apiserver and kube-controller-manager.</p><p>Volume populators take advantage of a PVC spec field called <code>dataSourceRef</code>. Unlike the
<code>dataSource</code> field, which can only contain either a reference to another PersistentVolumeClaim
or to a VolumeSnapshot, the <code>dataSourceRef</code> field can contain a reference to any object in the
same namespace, except for core objects other than PVCs. For clusters that have the feature
gate enabled, use of the <code>dataSourceRef</code> is preferred over <code>dataSource</code>.</p><h2 id="cross-namespace-data-sources">Cross namespace data sources</h2><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [alpha]</code></div><p>Kubernetes supports cross namespace volume data sources.
To use cross namespace volume data sources, you must enable the <code>AnyVolumeDataSource</code>
and <code>CrossNamespaceVolumeDataSource</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gates</a> for
the kube-apiserver and kube-controller-manager.
Also, you must enable the <code>CrossNamespaceVolumeDataSource</code> feature gate for the csi-provisioner.</p><p>Enabling the <code>CrossNamespaceVolumeDataSource</code> feature gate allows you to specify
a namespace in the dataSourceRef field.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>When you specify a namespace for a volume data source, Kubernetes checks for a
ReferenceGrant in the other namespace before accepting the reference.
ReferenceGrant is part of the <code>gateway.networking.k8s.io</code> extension APIs.
See <a href="https://gateway-api.sigs.k8s.io/api-types/referencegrant/">ReferenceGrant</a>
in the Gateway API documentation for details.
This means that you must extend your Kubernetes cluster with at least ReferenceGrant from the
Gateway API before you can use this mechanism.</div><h2 id="data-source-references">Data source references</h2><p>The <code>dataSourceRef</code> field behaves almost the same as the <code>dataSource</code> field. If one is
specified while the other is not, the API server will give both fields the same value. Neither
field can be changed after creation, and attempting to specify different values for the two
fields will result in a validation error. Therefore the two fields will always have the same
contents.</p><p>There are two differences between the <code>dataSourceRef</code> field and the <code>dataSource</code> field that
users should be aware of:</p><ul><li>The <code>dataSource</code> field ignores invalid values (as if the field was blank) while the
<code>dataSourceRef</code> field never ignores values and will cause an error if an invalid value is
used. Invalid values are any core object (objects with no apiGroup) except for PVCs.</li><li>The <code>dataSourceRef</code> field may contain different types of objects, while the <code>dataSource</code> field
only allows PVCs and VolumeSnapshots.</li></ul><p>When the <code>CrossNamespaceVolumeDataSource</code> feature is enabled, there are additional differences:</p><ul><li>The <code>dataSource</code> field only allows local objects, while the <code>dataSourceRef</code> field allows
objects in any namespaces.</li><li>When namespace is specified, <code>dataSource</code> and <code>dataSourceRef</code> are not synced.</li></ul><p>Users should always use <code>dataSourceRef</code> on clusters that have the feature gate enabled, and
fall back to <code>dataSource</code> on clusters that do not. It is not necessary to look at both fields
under any circumstance. The duplicated values with slightly different semantics exist only for
backwards compatibility. In particular, a mixture of older and newer controllers are able to
interoperate because the fields are the same.</p><h3 id="using-volume-populators">Using volume populators</h3><p>Volume populators are <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a> that can
create non-empty volumes, where the contents of the volume are determined by a Custom Resource.
Users create a populated volume by referring to a Custom Resource using the <code>dataSourceRef</code> field:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>populated-pvc<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>dataSourceRef</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>example-name<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>ExampleDataSource<span>
</span></span></span><span><span><span>    </span><span>apiGroup</span>:<span> </span>example.storage.k8s.io<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>10Gi<span>
</span></span></span></code></pre></div><p>Because volume populators are external components, attempts to create a PVC that uses one
can fail if not all the correct components are installed. External controllers should generate
events on the PVC to provide feedback on the status of the creation, including warnings if
the PVC cannot be created due to some missing component.</p><p>You can install the alpha <a href="https://github.com/kubernetes-csi/volume-data-source-validator">volume data source validator</a>
controller into your cluster. That controller generates warning Events on a PVC in the case that no populator
is registered to handle that kind of data source. When a suitable populator is installed for a PVC, it's the
responsibility of that populator controller to report Events that relate to volume creation and issues during
the process.</p><h3 id="using-a-cross-namespace-volume-data-source">Using a cross-namespace volume data source</h3><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [alpha]</code></div><p>Create a ReferenceGrant to allow the namespace owner to accept the reference.
You define a populated volume by specifying a cross namespace volume data source
using the <code>dataSourceRef</code> field. You must already have a valid ReferenceGrant
in the source namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>gateway.networking.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ReferenceGrant<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>allow-ns1-pvc<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>from</span>:<span>
</span></span></span><span><span><span>  </span>- <span>group</span>:<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>ns1<span>
</span></span></span><span><span><span>  </span><span>to</span>:<span>
</span></span></span><span><span><span>  </span>- <span>group</span>:<span> </span>snapshot.storage.k8s.io<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>VolumeSnapshot<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>new-snapshot-demo<span>
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>foo-pvc<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>ns1<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>example<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>  </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>1Gi<span>
</span></span></span><span><span><span>  </span><span>dataSourceRef</span>:<span>
</span></span></span><span><span><span>    </span><span>apiGroup</span>:<span> </span>snapshot.storage.k8s.io<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>VolumeSnapshot<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>new-snapshot-demo<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>volumeMode</span>:<span> </span>Filesystem<span>
</span></span></span></code></pre></div><h2 id="writing-portable-configuration">Writing Portable Configuration</h2><p>If you're writing configuration templates or examples that run on a wide range of clusters
and need persistent storage, it is recommended that you use the following pattern:</p><ul><li>Include PersistentVolumeClaim objects in your bundle of config (alongside
Deployments, ConfigMaps, etc).</li><li>Do not include PersistentVolume objects in the config, since the user instantiating
the config may not have permission to create PersistentVolumes.</li><li>Give the user the option of providing a storage class name when instantiating
the template.<ul><li>If the user provides a storage class name, put that value into the
<code>persistentVolumeClaim.storageClassName</code> field.
This will cause the PVC to match the right storage
class if the cluster has StorageClasses enabled by the admin.</li><li>If the user does not provide a storage class name, leave the
<code>persistentVolumeClaim.storageClassName</code> field as nil. This will cause a
PV to be automatically provisioned for the user with the default StorageClass
in the cluster. Many cluster environments have a default StorageClass installed,
or administrators can create their own default StorageClass.</li></ul></li><li>In your tooling, watch for PVCs that are not getting bound after some time
and surface this to the user, as this may indicate that the cluster has no
dynamic storage support (in which case the user should create a matching PV)
or the cluster has no storage system (in which case the user cannot deploy
config requiring PVCs).</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume">Creating a PersistentVolume</a>.</li><li>Learn more about <a href="/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim">Creating a PersistentVolumeClaim</a>.</li><li>Read the <a href="https://git.k8s.io/design-proposals-archive/storage/persistent-storage.md">Persistent Storage design document</a>.</li></ul><h3 id="reference">API references</h3><p>Read about the APIs described in this page:</p><ul><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/"><code>PersistentVolume</code></a></li><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/"><code>PersistentVolumeClaim</code></a></li></ul></div></div><div><div class="td-content"><h1>Projected Volumes</h1><p>This document describes <em>projected volumes</em> in Kubernetes. Familiarity with <a href="/docs/concepts/storage/volumes/">volumes</a> is suggested.</p><h2 id="introduction">Introduction</h2><p>A <code>projected</code> volume maps several existing volume sources into the same directory.</p><p>Currently, the following types of volume sources can be projected:</p><ul><li><a href="/docs/concepts/storage/volumes/#secret"><code>secret</code></a></li><li><a href="/docs/concepts/storage/volumes/#downwardapi"><code>downwardAPI</code></a></li><li><a href="/docs/concepts/storage/volumes/#configmap"><code>configMap</code></a></li><li><a href="#serviceaccounttoken"><code>serviceAccountToken</code></a></li><li><a href="#clustertrustbundle"><code>clusterTrustBundle</code></a></li><li><a href="#podcertificate"><code>podCertificate</code></a></li></ul><p>All sources are required to be in the same namespace as the Pod. For more details,
see the <a href="https://git.k8s.io/design-proposals-archive/node/all-in-one-volume.md">all-in-one volume</a> design document.</p><h3 id="example-configuration-secret-downwardapi-configmap">Example configuration with a secret, a downwardAPI, and a configMap</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-secret-downwardapi-configmap.yaml"><code>pods/storage/projected-secret-downwardapi-configmap.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/projected-secret-downwardapi-configmap.yaml to clipboard"></div><div class="includecode" id="pods-storage-projected-secret-downwardapi-configmap-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>volume-test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>container-test<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"3600"</span>]<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>all-in-one<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/projected-volume"</span><span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>all-in-one<span>
</span></span></span><span><span><span>    </span><span>projected</span>:<span>
</span></span></span><span><span><span>      </span><span>sources</span>:<span>
</span></span></span><span><span><span>      </span>- <span>secret</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span>          </span><span>items</span>:<span>
</span></span></span><span><span><span>            </span>- <span>key</span>:<span> </span>username<span>
</span></span></span><span><span><span>              </span><span>path</span>:<span> </span>my-group/my-username<span>
</span></span></span><span><span><span>      </span>- <span>downwardAPI</span>:<span>
</span></span></span><span><span><span>          </span><span>items</span>:<span>
</span></span></span><span><span><span>            </span>- <span>path</span>:<span> </span><span>"labels"</span><span>
</span></span></span><span><span><span>              </span><span>fieldRef</span>:<span>
</span></span></span><span><span><span>                </span><span>fieldPath</span>:<span> </span>metadata.labels<span>
</span></span></span><span><span><span>            </span>- <span>path</span>:<span> </span><span>"cpu_limit"</span><span>
</span></span></span><span><span><span>              </span><span>resourceFieldRef</span>:<span>
</span></span></span><span><span><span>                </span><span>containerName</span>:<span> </span>container-test<span>
</span></span></span><span><span><span>                </span><span>resource</span>:<span> </span>limits.cpu<span>
</span></span></span><span><span><span>      </span>- <span>configMap</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>myconfigmap<span>
</span></span></span><span><span><span>          </span><span>items</span>:<span>
</span></span></span><span><span><span>            </span>- <span>key</span>:<span> </span>config<span>
</span></span></span><span><span><span>              </span><span>path</span>:<span> </span>my-group/my-config<span>
</span></span></span></code></pre></div></div></div><h3 id="example-configuration-secrets-nondefault-permission-mode">Example configuration: secrets with a non-default permission mode set</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-secrets-nondefault-permission-mode.yaml"><code>pods/storage/projected-secrets-nondefault-permission-mode.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/projected-secrets-nondefault-permission-mode.yaml to clipboard"></div><div class="includecode" id="pods-storage-projected-secrets-nondefault-permission-mode-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>volume-test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>container-test<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"3600"</span>]<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>all-in-one<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/projected-volume"</span><span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>all-in-one<span>
</span></span></span><span><span><span>    </span><span>projected</span>:<span>
</span></span></span><span><span><span>      </span><span>sources</span>:<span>
</span></span></span><span><span><span>      </span>- <span>secret</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span>          </span><span>items</span>:<span>
</span></span></span><span><span><span>            </span>- <span>key</span>:<span> </span>username<span>
</span></span></span><span><span><span>              </span><span>path</span>:<span> </span>my-group/my-username<span>
</span></span></span><span><span><span>      </span>- <span>secret</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>mysecret2<span>
</span></span></span><span><span><span>          </span><span>items</span>:<span>
</span></span></span><span><span><span>            </span>- <span>key</span>:<span> </span>password<span>
</span></span></span><span><span><span>              </span><span>path</span>:<span> </span>my-group/my-password<span>
</span></span></span><span><span><span>              </span><span>mode</span>:<span> </span><span>511</span><span>
</span></span></span></code></pre></div></div></div><p>Each projected volume source is listed in the spec under <code>sources</code>. The
parameters are nearly the same with two exceptions:</p><ul><li>For secrets, the <code>secretName</code> field has been changed to <code>name</code> to be consistent
with ConfigMap naming.</li><li>The <code>defaultMode</code> can only be specified at the projected level and not for each
volume source. However, as illustrated above, you can explicitly set the <code>mode</code>
for each individual projection.</li></ul><h2 id="serviceaccounttoken">serviceAccountToken projected volumes</h2><p>You can inject the token for the current <a href="/docs/reference/access-authn-authz/authentication/#service-account-tokens">service account</a>
into a Pod at a specified path. For example:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-service-account-token.yaml"><code>pods/storage/projected-service-account-token.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/projected-service-account-token.yaml to clipboard"></div><div class="includecode" id="pods-storage-projected-service-account-token-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>sa-token-test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>container-test<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"3600"</span>]<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>token-vol<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/service-account"</span><span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>serviceAccountName</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>token-vol<span>
</span></span></span><span><span><span>    </span><span>projected</span>:<span>
</span></span></span><span><span><span>      </span><span>sources</span>:<span>
</span></span></span><span><span><span>      </span>- <span>serviceAccountToken</span>:<span>
</span></span></span><span><span><span>          </span><span>audience</span>:<span> </span>api<span>
</span></span></span><span><span><span>          </span><span>expirationSeconds</span>:<span> </span><span>3600</span><span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>token<span>
</span></span></span></code></pre></div></div></div><p>The example Pod has a projected volume containing the injected service account
token. Containers in this Pod can use that token to access the Kubernetes API
server, authenticating with the identity of <a href="/docs/tasks/configure-pod-container/configure-service-account/">the pod's ServiceAccount</a>.
The <code>audience</code> field contains the intended audience of the
token. A recipient of the token must identify itself with an identifier specified
in the audience of the token, and otherwise should reject the token. This field
is optional and it defaults to the identifier of the API server.</p><p>The <code>expirationSeconds</code> is the expected duration of validity of the service account
token. It defaults to 1 hour and must be at least 10 minutes (600 seconds). An administrator
can also limit its maximum value by specifying the <code>--service-account-max-token-expiration</code>
option for the API server. The <code>path</code> field specifies a relative path to the mount point
of the projected volume.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A container using a projected volume source as a <a href="/docs/concepts/storage/volumes/#using-subpath"><code>subPath</code></a>
volume mount will not receive updates for those volume sources.</div><h2 id="clustertrustbundle">clusterTrustBundle projected volumes</h2><div class="feature-state-notice feature-beta" title="Feature Gate: ClusterTrustBundleProjection"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: false)</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>To use this feature in Kubernetes 1.34, you must enable support for ClusterTrustBundle objects with the <code>ClusterTrustBundle</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> and <code>--runtime-config=certificates.k8s.io/v1beta1/clustertrustbundles=true</code> kube-apiserver flag, then enable the <code>ClusterTrustBundleProjection</code> feature gate.</div><p>The <code>clusterTrustBundle</code> projected volume source injects the contents of one or more <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#cluster-trust-bundles">ClusterTrustBundle</a> objects as an automatically-updating file in the container filesystem.</p><p>ClusterTrustBundles can be selected either by <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#ctb-signer-unlinked">name</a> or by <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#ctb-signer-linked">signer name</a>.</p><p>To select by name, use the <code>name</code> field to designate a single ClusterTrustBundle object.</p><p>To select by signer name, use the <code>signerName</code> field (and optionally the
<code>labelSelector</code> field) to designate a set of ClusterTrustBundle objects that use
the given signer name. If <code>labelSelector</code> is not present, then all
ClusterTrustBundles for that signer are selected.</p><p>The kubelet deduplicates the certificates in the selected ClusterTrustBundle objects, normalizes the PEM representations (discarding comments and headers), reorders the certificates, and writes them into the file named by <code>path</code>. As the set of selected ClusterTrustBundles or their content changes, kubelet keeps the file up-to-date.</p><p>By default, the kubelet will prevent the pod from starting if the named ClusterTrustBundle is not found, or if <code>signerName</code> / <code>labelSelector</code> do not match any ClusterTrustBundles. If this behavior is not what you want, then set the <code>optional</code> field to <code>true</code>, and the pod will start up with an empty file at <code>path</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-clustertrustbundle.yaml"><code>pods/storage/projected-clustertrustbundle.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/projected-clustertrustbundle.yaml to clipboard"></div><div class="includecode" id="pods-storage-projected-clustertrustbundle-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>sa-ctb-name-test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>container-test<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"3600"</span>]<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>token-vol<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/root-certificates"</span><span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>serviceAccountName</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>token-vol<span>
</span></span></span><span><span><span>    </span><span>projected</span>:<span>
</span></span></span><span><span><span>      </span><span>sources</span>:<span>
</span></span></span><span><span><span>      </span>- <span>clusterTrustBundle</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>example<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>example-roots.pem<span>
</span></span></span><span><span><span>      </span>- <span>clusterTrustBundle</span>:<span>
</span></span></span><span><span><span>          </span><span>signerName</span>:<span> </span><span>"example.com/mysigner"</span><span>
</span></span></span><span><span><span>          </span><span>labelSelector</span>:<span>
</span></span></span><span><span><span>            </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>              </span><span>version</span>:<span> </span>live<span>
</span></span></span><span><span><span>          </span><span>path</span>:<span> </span>mysigner-roots.pem<span>
</span></span></span><span><span><span>          </span><span>optional</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div></div></div><h2 id="podcertificate">podCertificate projected volumes</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: PodCertificateRequest"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In Kubernetes 1.34, you must enable support for Pod
Certificates using the <code>PodCertificateRequest</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature
gate</a> and the
<code>--runtime-config=certificates.k8s.io/v1alpha1/podcertificaterequests=true</code>
kube-apiserver flag.</div><p>The <code>podCertificate</code> projected volumes source securely provisions a private key
and X.509 certificate chain for pod to use as client or server credentials.
Kubelet will then handle refreshing the private key and certificate chain when
they get close to expiration. The application just has to make sure that it
reloads the file promptly when it changes, with a mechanism like <code>inotify</code> or
polling.</p><p>Each <code>podCertificate</code> projection supports the following configuration fields:</p><ul><li><code>signerName</code>: The
<a href="/docs/reference/access-authn-authz/certificate-signing-requests/#signers">signer</a>
you want to issue the certificate. Note that signers may have their own
access requirements, and may refuse to issue certificates to your pod.</li><li><code>keyType</code>: The type of private key that should be generated. Valid values are
<code>ED25519</code>, <code>ECDSAP256</code>, <code>ECDSAP384</code>, <code>ECDSAP521</code>, <code>RSA3072</code>, and <code>RSA4096</code>.</li><li><code>maxExpirationSeconds</code>: The maximum lifetime you will accept for the
certificate issued to the pod. If not set, will be defaulted to <code>86400</code> (24
hours). Must be at least <code>3600</code> (1 hour), and at most <code>7862400</code> (91 days).
Kubernetes built-in signers are restricted to a max lifetime of <code>86400</code> (1
day). The signer is allowed to issue a certificate with a lifetime shorter
than what you've specified.</li><li><code>credentialBundlePath</code>: Relative path within the projection where the
credential bundle should be written. The credential bundle is a PEM-formatted
file, where the first block is a "PRIVATE KEY" block that contains a
PKCS#8-serialized private key, and the remaining blocks are "CERTIFICATE"
blocks that comprise the certificate chain (leaf certificate and any
intermediates).</li><li><code>keyPath</code> and <code>certificateChainPath</code>: Separate paths where Kubelet should
write <em>just</em> the private key or certificate chain.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Most applications should prefer using <code>credentialBundlePath</code> unless they need
the key and certificates in separate files for compatibility reasons. Kubelet
uses an atomic writing strategy based on symlinks to make sure that when you
open the files it projects, you read either the old content or the new content.
However, if you read the key and certificate chain from separate files, Kubelet
may rotate the credentials after your first read and before your second read,
resulting in your application loading a mismatched key and certificate.</div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-podcertificate.yaml"><code>pods/storage/projected-podcertificate.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/storage/projected-podcertificate.yaml to clipboard"></div><div class="includecode" id="pods-storage-projected-podcertificate-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># Sample Pod spec that uses a podCertificate projection to request an ED25519</span><span>
</span></span></span><span><span><span></span><span># private key, a certificate from the `coolcert.example.com/foo` signer, and</span><span>
</span></span></span><span><span><span></span><span># write the results to `/var/run/my-x509-credentials/credentialbundle.pem`.</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>podcertificate-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>serviceAccountName</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>image</span>:<span> </span>debian<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>main<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"infinity"</span>]<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>my-x509-credentials<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/var/run/my-x509-credentials<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>my-x509-credentials<span>
</span></span></span><span><span><span>    </span><span>projected</span>:<span>
</span></span></span><span><span><span>      </span><span>defaultMode</span>:<span> </span><span>420</span><span>
</span></span></span><span><span><span>      </span><span>sources</span>:<span>
</span></span></span><span><span><span>      </span>- <span>podCertificate</span>:<span>
</span></span></span><span><span><span>          </span><span>keyType</span>:<span> </span>ED25519<span>
</span></span></span><span><span><span>          </span><span>signerName</span>:<span> </span>coolcert.example.com/foo<span>
</span></span></span><span><span><span>          </span><span>credentialBundlePath</span>:<span> </span>credentialbundle.pem<span>
</span></span></span></code></pre></div></div></div><h2 id="securitycontext-interactions">SecurityContext interactions</h2><p>The <a href="https://git.k8s.io/enhancements/keps/sig-storage/2451-service-account-token-volumes#proposal">proposal</a> for file permission handling in projected service account volume enhancement introduced the projected files having the correct owner permissions set.</p><h3 id="linux">Linux</h3><p>In Linux pods that have a projected volume and <code>RunAsUser</code> set in the Pod
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context"><code>SecurityContext</code></a>,
the projected files have the correct ownership set including container user
ownership.</p><p>When all containers in a pod have the same <code>runAsUser</code> set in their
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context"><code>PodSecurityContext</code></a>
or container
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1"><code>SecurityContext</code></a>,
then the kubelet ensures that the contents of the <code>serviceAccountToken</code> volume are owned by that user,
and the token file has its permission mode set to <code>0600</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p><a class="glossary-tooltip" title="A type of container type that you can temporarily run inside a Pod" href="/docs/concepts/workloads/pods/ephemeral-containers/" target="_blank">Ephemeral containers</a>
added to a Pod after it is created do <em>not</em> change volume permissions that were
set when the pod was created.</p><p>If a Pod's <code>serviceAccountToken</code> volume permissions were set to <code>0600</code> because
all other containers in the Pod have the same <code>runAsUser</code>, ephemeral
containers must use the same <code>runAsUser</code> to be able to read the token.</p></div><h3 id="windows">Windows</h3><p>In Windows pods that have a projected volume and <code>RunAsUsername</code> set in the
Pod <code>SecurityContext</code>, the ownership is not enforced due to the way user
accounts are managed in Windows. Windows stores and manages local user and group
accounts in a database file called Security Account Manager (SAM). Each
container maintains its own instance of the SAM database, to which the host has
no visibility into while the container is running. Windows containers are
designed to run the user mode portion of the OS in isolation from the host,
hence the maintenance of a virtual SAM database. As a result, the kubelet running
on the host does not have the ability to dynamically configure host file
ownership for virtualized container accounts. It is recommended that if files on
the host machine are to be shared with the container then they should be placed
into their own volume mount outside of <code>C:\</code>.</p><p>By default, the projected files will have the following ownership as shown for
an example projected volume file:</p><div class="highlight"><pre tabindex="0"><code class="language-powershell"><span><span><span>PS </span>C:\&gt; <span>Get-Acl</span> C:\var\run\secrets\kubernetes.io\serviceaccount\..2021_08_31_22_22_18.<span>318230061</span>\ca.crt | <span>Format-List</span>
</span></span><span><span>
</span></span><span><span>Path   <span>:</span> Microsoft.PowerShell.Core\FileSystem::C:\var\run\secrets\kubernetes.io\serviceaccount\..2021_08_31_22_22_18.<span>318230061</span>\ca.crt
</span></span><span><span>Owner  <span>:</span> BUILTIN\Administrators
</span></span><span><span><span>Group </span> <span>:</span> NT AUTHORITY\SYSTEM
</span></span><span><span>Access <span>:</span> NT AUTHORITY\SYSTEM Allow  FullControl
</span></span><span><span>         BUILTIN\Administrators Allow  FullControl
</span></span><span><span>         BUILTIN\Users Allow  ReadAndExecute, Synchronize
</span></span><span><span>Audit  <span>:</span>
</span></span><span><span>Sddl   <span>:</span> O:BAG<span>:</span>SYD<span>:</span>AI(A;ID;FA;;;SY)(A;ID;FA;;;BA)(A;ID;0x1200a9;;;BU)
</span></span></code></pre></div><p>This implies all administrator users like <code>ContainerAdministrator</code> will have
read, write and execute access while, non-administrator users will have read and
execute access.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>In general, granting the container access to the host is discouraged as it can
open the door for potential security exploits.</p><p>Creating a Windows Pod with <code>RunAsUser</code> in it's <code>SecurityContext</code> will result in
the Pod being stuck at <code>ContainerCreating</code> forever. So it is advised to not use
the Linux only <code>RunAsUser</code> option with Windows Pods.</p></div></div></div><div><div class="td-content"><h1>Ephemeral Volumes</h1><p>This document describes <em>ephemeral volumes</em> in Kubernetes. Familiarity
with <a href="/docs/concepts/storage/volumes/">volumes</a> is suggested, in
particular PersistentVolumeClaim and PersistentVolume.</p><p>Some applications need additional storage but don't care whether that
data is stored persistently across restarts. For example, caching
services are often limited by memory size and can move infrequently
used data into storage that is slower than memory with little impact
on overall performance.</p><p>Other applications expect some read-only input data to be present in
files, like configuration data or secret keys.</p><p><em>Ephemeral volumes</em> are designed for these use cases. Because volumes
follow the Pod's lifetime and get created and deleted along with the
Pod, Pods can be stopped and restarted without being limited to where
some persistent volume is available.</p><p>Ephemeral volumes are specified <em>inline</em> in the Pod spec, which
simplifies application deployment and management.</p><h3 id="types-of-ephemeral-volumes">Types of ephemeral volumes</h3><p>Kubernetes supports several different kinds of ephemeral volumes for
different purposes:</p><ul><li><a href="/docs/concepts/storage/volumes/#emptydir">emptyDir</a>: empty at Pod startup,
with storage coming locally from the kubelet base directory (usually
the root disk) or RAM</li><li><a href="/docs/concepts/storage/volumes/#configmap">configMap</a>,
<a href="/docs/concepts/storage/volumes/#downwardapi">downwardAPI</a>,
<a href="/docs/concepts/storage/volumes/#secret">secret</a>: inject different
kinds of Kubernetes data into a Pod</li><li><a href="/docs/concepts/storage/volumes/#image">image</a>: allows mounting container image files or artifacts,
directly to a Pod.</li><li><a href="#csi-ephemeral-volumes">CSI ephemeral volumes</a>:
similar to the previous volume kinds, but provided by special <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> drivers
which specifically <a href="https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html">support this feature</a></li><li><a href="#generic-ephemeral-volumes">generic ephemeral volumes</a>, which
can be provided by all storage drivers that also support persistent volumes</li></ul><p><code>emptyDir</code>, <code>configMap</code>, <code>downwardAPI</code>, <code>secret</code> are provided as
<a href="/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">local ephemeral
storage</a>.
They are managed by kubelet on each node.</p><p>CSI ephemeral volumes <em>must</em> be provided by third-party CSI storage
drivers.</p><p>Generic ephemeral volumes <em>can</em> be provided by third-party CSI storage
drivers, but also by any other storage driver that supports dynamic
provisioning. Some CSI drivers are written specifically for CSI
ephemeral volumes and do not support dynamic provisioning: those then
cannot be used for generic ephemeral volumes.</p><p>The advantage of using third-party drivers is that they can offer
functionality that Kubernetes itself does not support, for example
storage with different performance characteristics than the disk that
is managed by kubelet, or injecting different data.</p><h3 id="csi-ephemeral-volumes">CSI ephemeral volumes</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>CSI ephemeral volumes are only supported by a subset of CSI drivers.
The Kubernetes CSI <a href="https://kubernetes-csi.github.io/docs/drivers.html">Drivers list</a>
shows which drivers support ephemeral volumes.</div><p>Conceptually, CSI ephemeral volumes are similar to <code>configMap</code>,
<code>downwardAPI</code> and <code>secret</code> volume types: the storage is managed locally on each
node and is created together with other local resources after a Pod has been
scheduled onto a node. Kubernetes has no concept of rescheduling Pods
anymore at this stage. Volume creation has to be unlikely to fail,
otherwise Pod startup gets stuck. In particular, <a href="/docs/concepts/storage/storage-capacity/">storage capacity
aware Pod scheduling</a> is <em>not</em>
supported for these volumes. They are currently also not covered by
the storage resource usage limits of a Pod, because that is something
that kubelet can only enforce for storage that it manages itself.</p><p>Here's an example manifest for a Pod that uses CSI ephemeral storage:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-csi-app<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>my-frontend<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>      </span>- <span>mountPath</span>:<span> </span><span>"/data"</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>my-csi-inline-vol<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"sleep"</span>,<span> </span><span>"1000000"</span><span> </span>]<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>my-csi-inline-vol<span>
</span></span></span><span><span><span>      </span><span>csi</span>:<span>
</span></span></span><span><span><span>        </span><span>driver</span>:<span> </span>inline.storage.kubernetes.io<span>
</span></span></span><span><span><span>        </span><span>volumeAttributes</span>:<span>
</span></span></span><span><span><span>          </span><span>foo</span>:<span> </span>bar<span>
</span></span></span></code></pre></div><p>The <code>volumeAttributes</code> determine what volume is prepared by the
driver. These attributes are specific to each driver and not
standardized. See the documentation of each CSI driver for further
instructions.</p><h3 id="csi-driver-restrictions">CSI driver restrictions</h3><p>CSI ephemeral volumes allow users to provide <code>volumeAttributes</code>
directly to the CSI driver as part of the Pod spec. A CSI driver
allowing <code>volumeAttributes</code> that are typically restricted to
administrators is NOT suitable for use in an inline ephemeral volume.
For example, parameters that are normally defined in the StorageClass
should not be exposed to users through the use of inline ephemeral volumes.</p><p>Cluster administrators who need to restrict the CSI drivers that are
allowed to be used as inline volumes within a Pod spec may do so by:</p><ul><li>Removing <code>Ephemeral</code> from <code>volumeLifecycleModes</code> in the CSIDriver spec, which prevents the
driver from being used as an inline ephemeral volume.</li><li>Using an <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhook</a>
to restrict how this driver is used.</li></ul><h3 id="generic-ephemeral-volumes">Generic ephemeral volumes</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [stable]</code></div><p>Generic ephemeral volumes are similar to <code>emptyDir</code> volumes in the
sense that they provide a per-pod directory for scratch data that is
usually empty after provisioning. But they may also have additional
features:</p><ul><li>Storage can be local or network-attached.</li><li>Volumes can have a fixed size that Pods are not able to exceed.</li><li>Volumes may have some initial data, depending on the driver and
parameters.</li><li>Typical operations on volumes are supported assuming that the driver
supports them, including
<a href="/docs/concepts/storage/volume-snapshots/">snapshotting</a>,
<a href="/docs/concepts/storage/volume-pvc-datasource/">cloning</a>,
<a href="/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims">resizing</a>,
and <a href="/docs/concepts/storage/storage-capacity/">storage capacity tracking</a>.</li></ul><p>Example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-app<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>my-frontend<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>      </span>- <span>mountPath</span>:<span> </span><span>"/scratch"</span><span>
</span></span></span><span><span><span>        </span><span>name</span>:<span> </span>scratch-volume<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span> </span><span>"sleep"</span>,<span> </span><span>"1000000"</span><span> </span>]<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>scratch-volume<span>
</span></span></span><span><span><span>      </span><span>ephemeral</span>:<span>
</span></span></span><span><span><span>        </span><span>volumeClaimTemplate</span>:<span>
</span></span></span><span><span><span>          </span><span>metadata</span>:<span>
</span></span></span><span><span><span>            </span><span>labels</span>:<span>
</span></span></span><span><span><span>              </span><span>type</span>:<span> </span>my-frontend-volume<span>
</span></span></span><span><span><span>          </span><span>spec</span>:<span>
</span></span></span><span><span><span>            </span><span>accessModes</span>:<span> </span>[<span> </span><span>"ReadWriteOnce"</span><span> </span>]<span>
</span></span></span><span><span><span>            </span><span>storageClassName</span>:<span> </span><span>"scratch-storage-class"</span><span>
</span></span></span><span><span><span>            </span><span>resources</span>:<span>
</span></span></span><span><span><span>              </span><span>requests</span>:<span>
</span></span></span><span><span><span>                </span><span>storage</span>:<span> </span>1Gi<span>
</span></span></span></code></pre></div><h3 id="lifecycle-and-persistentvolumeclaim">Lifecycle and PersistentVolumeClaim</h3><p>The key design idea is that the
<a href="/docs/reference/generated/kubernetes-api/v1.34/#ephemeralvolumesource-v1-core">parameters for a volume claim</a>
are allowed inside a volume source of the Pod. Labels, annotations and
the whole set of fields for a PersistentVolumeClaim are supported. When such a Pod gets
created, the ephemeral volume controller then creates an actual PersistentVolumeClaim
object in the same namespace as the Pod and ensures that the PersistentVolumeClaim
gets deleted when the Pod gets deleted.</p><p>That triggers volume binding and/or provisioning, either immediately if
the <a class="glossary-tooltip" title="A StorageClass provides a way for administrators to describe different available storage types." href="/docs/concepts/storage/storage-classes" target="_blank">StorageClass</a> uses immediate volume binding or when the Pod is
tentatively scheduled onto a node (<code>WaitForFirstConsumer</code> volume
binding mode). The latter is recommended for generic ephemeral volumes
because then the scheduler is free to choose a suitable node for
the Pod. With immediate binding, the scheduler is forced to select a node that has
access to the volume once it is available.</p><p>In terms of <a href="/docs/concepts/architecture/garbage-collection/#owners-dependents">resource ownership</a>,
a Pod that has generic ephemeral storage is the owner of the PersistentVolumeClaim(s)
that provide that ephemeral storage. When the Pod is deleted,
the Kubernetes garbage collector deletes the PVC, which then usually
triggers deletion of the volume because the default reclaim policy of
storage classes is to delete volumes. You can create quasi-ephemeral local storage
using a StorageClass with a reclaim policy of <code>retain</code>: the storage outlives the Pod,
and in this case you need to ensure that volume clean up happens separately.</p><p>While these PVCs exist, they can be used like any other PVC. In
particular, they can be referenced as data source in volume cloning or
snapshotting. The PVC object also holds the current status of the
volume.</p><h3 id="persistentvolumeclaim-naming">PersistentVolumeClaim naming</h3><p>Naming of the automatically created PVCs is deterministic: the name is
a combination of the Pod name and volume name, with a hyphen (<code>-</code>) in the
middle. In the example above, the PVC name will be
<code>my-app-scratch-volume</code>. This deterministic naming makes it easier to
interact with the PVC because one does not have to search for it once
the Pod name and volume name are known.</p><p>The deterministic naming also introduces a potential conflict between different
Pods (a Pod "pod-a" with volume "scratch" and another Pod with name
"pod" and volume "a-scratch" both end up with the same PVC name
"pod-a-scratch") and between Pods and manually created PVCs.</p><p>Such conflicts are detected: a PVC is only used for an ephemeral
volume if it was created for the Pod. This check is based on the
ownership relationship. An existing PVC is not overwritten or
modified. But this does not resolve the conflict because without the
right PVC, the Pod cannot start.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Take care when naming Pods and volumes inside the
same namespace, so that these conflicts can't occur.</div><h3 id="security">Security</h3><p>Using generic ephemeral volumes allows users to create PVCs indirectly
if they can create Pods, even if they do not have permission to create PVCs directly.
Cluster administrators must be aware of this. If this does not fit their security model,
they should use an <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhook</a>
that rejects objects like Pods that have a generic ephemeral volume.</p><p>The normal <a href="/docs/concepts/policy/resource-quotas/#storage-resource-quota">namespace quota for PVCs</a>
still applies, so even if users are allowed to use this new mechanism, they cannot use
it to circumvent other policies.</p><h2 id="what-s-next">What's next</h2><h3 id="ephemeral-volumes-managed-by-kubelet">Ephemeral volumes managed by kubelet</h3><p>See <a href="/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">local ephemeral storage</a>.</p><h3 id="csi-ephemeral-volumes-1">CSI ephemeral volumes</h3><ul><li>For more information on the design, see the
<a href="https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md">Ephemeral Inline CSI volumes KEP</a>.</li><li>For more information on further development of this feature, see the
<a href="https://github.com/kubernetes/enhancements/issues/596">enhancement tracking issue #596</a>.</li></ul><h3 id="generic-ephemeral-volumes-1">Generic ephemeral volumes</h3><ul><li>For more information on the design, see the
<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md">Generic ephemeral inline volumes KEP</a>.</li></ul></div></div><div><div class="td-content"><h1>Storage Classes</h1><p>This document describes the concept of a StorageClass in Kubernetes. Familiarity
with <a href="/docs/concepts/storage/volumes/">volumes</a> and
<a href="/docs/concepts/storage/persistent-volumes/">persistent volumes</a> is suggested.</p><p>A StorageClass provides a way for administrators to describe the <em>classes</em> of
storage they offer. Different classes might map to quality-of-service levels,
or to backup policies, or to arbitrary policies determined by the cluster
administrators. Kubernetes itself is unopinionated about what classes
represent.</p><p>The Kubernetes concept of a storage class is similar to &#8220;profiles&#8221; in some other
storage system designs.</p><h2 id="storageclass-objects">StorageClass objects</h2><p>Each StorageClass contains the fields <code>provisioner</code>, <code>parameters</code>, and
<code>reclaimPolicy</code>, which are used when a PersistentVolume belonging to the
class needs to be dynamically provisioned to satisfy a PersistentVolumeClaim (PVC).</p><p>The name of a StorageClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating StorageClass objects.</p><p>As an administrator, you can specify a default StorageClass that applies to any PVCs that
don't request a specific class. For more details, see the
<a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim concept</a>.</p><p>Here's an example of a StorageClass:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass-low-latency.yaml"><code>storage/storageclass-low-latency.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/storageclass-low-latency.yaml to clipboard"></div><div class="includecode" id="storage-storageclass-low-latency-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>low-latency<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>storageclass.kubernetes.io/is-default-class</span>:<span> </span><span>"false"</span><span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>csi-driver.example-vendor.example<span>
</span></span></span><span><span><span></span><span>reclaimPolicy</span>:<span> </span>Retain<span> </span><span># default value is Delete</span><span>
</span></span></span><span><span><span></span><span>allowVolumeExpansion</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span></span><span>mountOptions</span>:<span>
</span></span></span><span><span><span>  </span>- discard<span> </span><span># this might enable UNMAP / TRIM at the block storage layer</span><span>
</span></span></span><span><span><span></span><span>volumeBindingMode</span>:<span> </span>WaitForFirstConsumer<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>guaranteedReadWriteLatency</span>:<span> </span><span>"true"</span><span> </span><span># provider-specific</span><span>
</span></span></span></code></pre></div></div></div><h2 id="default-storageclass">Default StorageClass</h2><p>You can mark a StorageClass as the default for your cluster.
For instructions on setting the default StorageClass, see
<a href="/docs/tasks/administer-cluster/change-default-storage-class/">Change the default StorageClass</a>.</p><p>When a PVC does not specify a <code>storageClassName</code>, the default StorageClass is
used.</p><p>If you set the
<a href="/docs/reference/labels-annotations-taints/#storageclass-kubernetes-io-is-default-class"><code>storageclass.kubernetes.io/is-default-class</code></a>
annotation to true on more than one StorageClass in your cluster, and you then
create a PersistentVolumeClaim with no <code>storageClassName</code> set, Kubernetes
uses the most recently created default StorageClass.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You should try to only have one StorageClass in your cluster that is
marked as the default. The reason that Kubernetes allows you to have
multiple default StorageClasses is to allow for seamless migration.</div><p>You can create a PersistentVolumeClaim without specifying a <code>storageClassName</code>
for the new PVC, and you can do so even when no default StorageClass exists
in your cluster. In this case, the new PVC creates as you defined it, and the
<code>storageClassName</code> of that PVC remains unset until a default becomes available.</p><p>You can have a cluster without any default StorageClass. If you don't mark any
StorageClass as default (and one hasn't been set for you by, for example, a cloud provider),
then Kubernetes cannot apply that defaulting for PersistentVolumeClaims that need
it.</p><p>If or when a default StorageClass becomes available, the control plane identifies any
existing PVCs without <code>storageClassName</code>. For the PVCs that either have an empty
value for <code>storageClassName</code> or do not have this key, the control plane then
updates those PVCs to set <code>storageClassName</code> to match the new default StorageClass.
If you have an existing PVC where the <code>storageClassName</code> is <code>""</code>, and you configure
a default StorageClass, then this PVC will not get updated.</p><p>In order to keep binding to PVs with <code>storageClassName</code> set to <code>""</code>
(while a default StorageClass is present), you need to set the <code>storageClassName</code>
of the associated PVC to <code>""</code>.</p><h2 id="provisioner">Provisioner</h2><p>Each StorageClass has a provisioner that determines what volume plugin is used
for provisioning PVs. This field must be specified.</p><table><thead><tr><th>Volume Plugin</th><th>Internal Provisioner</th><th>Config Example</th></tr></thead><tbody><tr><td>AzureFile</td><td>&#10003;</td><td><a href="#azure-file">Azure File</a></td></tr><tr><td>CephFS</td><td>-</td><td>-</td></tr><tr><td>FC</td><td>-</td><td>-</td></tr><tr><td>FlexVolume</td><td>-</td><td>-</td></tr><tr><td>iSCSI</td><td>-</td><td>-</td></tr><tr><td>Local</td><td>-</td><td><a href="#local">Local</a></td></tr><tr><td>NFS</td><td>-</td><td><a href="#nfs">NFS</a></td></tr><tr><td>PortworxVolume</td><td>&#10003;</td><td><a href="#portworx-volume">Portworx Volume</a></td></tr><tr><td>RBD</td><td>-</td><td><a href="#ceph-rbd">Ceph RBD</a></td></tr><tr><td>VsphereVolume</td><td>&#10003;</td><td><a href="#vsphere">vSphere</a></td></tr></tbody></table><p>You are not restricted to specifying the "internal" provisioners
listed here (whose names are prefixed with "kubernetes.io" and shipped
alongside Kubernetes). You can also run and specify external provisioners,
which are independent programs that follow a <a href="https://git.k8s.io/design-proposals-archive/storage/volume-provisioning.md">specification</a>
defined by Kubernetes. Authors of external provisioners have full discretion
over where their code lives, how the provisioner is shipped, how it needs to be
run, what volume plugin it uses (including Flex), etc. The repository
<a href="https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner">kubernetes-sigs/sig-storage-lib-external-provisioner</a>
houses a library for writing external provisioners that implements the bulk of
the specification. Some external provisioners are listed under the repository
<a href="https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner">kubernetes-sigs/sig-storage-lib-external-provisioner</a>.</p><p>For example, NFS doesn't provide an internal provisioner, but an external
provisioner can be used. There are also cases when 3rd party storage
vendors provide their own external provisioner.</p><h2 id="reclaim-policy">Reclaim policy</h2><p>PersistentVolumes that are dynamically created by a StorageClass will have the
<a href="/docs/concepts/storage/persistent-volumes/#reclaiming">reclaim policy</a>
specified in the <code>reclaimPolicy</code> field of the class, which can be
either <code>Delete</code> or <code>Retain</code>. If no <code>reclaimPolicy</code> is specified when a
StorageClass object is created, it will default to <code>Delete</code>.</p><p>PersistentVolumes that are created manually and managed via a StorageClass will have
whatever reclaim policy they were assigned at creation.</p><h2 id="allow-volume-expansion">Volume expansion</h2><p>PersistentVolumes can be configured to be expandable. This allows you to resize the
volume by editing the corresponding PVC object, requesting a new larger amount of
storage.</p><p>The following types of volumes support volume expansion, when the underlying
StorageClass has the field <code>allowVolumeExpansion</code> set to true.</p><table><caption>Table of Volume types and the version of Kubernetes they require</caption><thead><tr><th>Volume type</th><th>Required Kubernetes version for volume expansion</th></tr></thead><tbody><tr><td>Azure File</td><td>1.11</td></tr><tr><td>CSI</td><td>1.24</td></tr><tr><td>FlexVolume</td><td>1.13</td></tr><tr><td>Portworx</td><td>1.11</td></tr><tr><td>rbd</td><td>1.11</td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You can only use the volume expansion feature to grow a Volume, not to shrink it.</div><h2 id="mount-options">Mount options</h2><p>PersistentVolumes that are dynamically created by a StorageClass will have the
mount options specified in the <code>mountOptions</code> field of the class.</p><p>If the volume plugin does not support mount options but mount options are
specified, provisioning will fail. Mount options are <strong>not</strong> validated on either
the class or PV. If a mount option is invalid, the PV mount fails.</p><h2 id="volume-binding-mode">Volume binding mode</h2><p>The <code>volumeBindingMode</code> field controls when
<a href="/docs/concepts/storage/persistent-volumes/#provisioning">volume binding and dynamic provisioning</a>
should occur. When unset, <code>Immediate</code> mode is used by default.</p><p>The <code>Immediate</code> mode indicates that volume binding and dynamic
provisioning occurs once the PersistentVolumeClaim is created. For storage
backends that are topology-constrained and not globally accessible from all Nodes
in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling
requirements. This may result in unschedulable Pods.</p><p>A cluster administrator can address this issue by specifying the <code>WaitForFirstConsumer</code> mode which
will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.
PersistentVolumes will be selected or provisioned conforming to the topology that is
specified by the Pod's scheduling constraints. These include, but are not limited to, <a href="/docs/concepts/configuration/manage-resources-containers/">resource
requirements</a>,
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector">node selectors</a>,
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">pod affinity and
anti-affinity</a>,
and <a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">taints and tolerations</a>.</p><p>The following plugins support <code>WaitForFirstConsumer</code> with dynamic provisioning:</p><ul><li>CSI volumes, provided that the specific CSI driver supports this</li></ul><p>The following plugins support <code>WaitForFirstConsumer</code> with pre-created PersistentVolume binding:</p><ul><li>CSI volumes, provided that the specific CSI driver supports this</li><li><a href="#local"><code>local</code></a></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If you choose to use <code>WaitForFirstConsumer</code>, do not use <code>nodeName</code> in the Pod spec
to specify node affinity.
If <code>nodeName</code> is used in this case, the scheduler will be bypassed and PVC will remain in <code>pending</code> state.</p><p>Instead, you can use node selector for <code>kubernetes.io/hostname</code>:</p></div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/pod-volume-binding.yaml"><code>storage/storageclass/pod-volume-binding.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/storageclass/pod-volume-binding.yaml to clipboard"></div><div class="includecode" id="storage-storageclass-pod-volume-binding-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>task-pv-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/hostname</span>:<span> </span>kube-01<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>task-pv-storage<span>
</span></span></span><span><span><span>      </span><span>persistentVolumeClaim</span>:<span>
</span></span></span><span><span><span>        </span><span>claimName</span>:<span> </span>task-pv-claim<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>task-pv-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>      </span><span>ports</span>:<span>
</span></span></span><span><span><span>        </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span><span>"http-server"</span><span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>mountPath</span>:<span> </span><span>"/usr/share/nginx/html"</span><span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>task-pv-storage<span>
</span></span></span></code></pre></div></div></div><h2 id="allowed-topologies">Allowed topologies</h2><p>When a cluster operator specifies the <code>WaitForFirstConsumer</code> volume binding mode, it is no longer necessary
to restrict provisioning to specific topologies in most situations. However,
if still required, <code>allowedTopologies</code> can be specified.</p><p>This example demonstrates how to restrict the topology of provisioned volumes to specific
zones and should be used as a replacement for the <code>zone</code> and <code>zones</code> parameters for the
supported plugins.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-topology.yaml"><code>storage/storageclass/storageclass-topology.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/storageclass/storageclass-topology.yaml to clipboard"></div><div class="includecode" id="storage-storageclass-storageclass-topology-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>standard<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span>  </span>example.com/example<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>pd-standard<span>
</span></span></span><span><span><span></span><span>volumeBindingMode</span>:<span> </span>WaitForFirstConsumer<span>
</span></span></span><span><span><span></span><span>allowedTopologies</span>:<span>
</span></span></span><span><span><span></span>- <span>matchLabelExpressions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>key</span>:<span> </span>topology.kubernetes.io/zone<span>
</span></span></span><span><span><span>    </span><span>values</span>:<span>
</span></span></span><span><span><span>    </span>- us-central-1a<span>
</span></span></span><span><span><span>    </span>- us-central-1b<span>
</span></span></span></code></pre></div></div></div><h2 id="parameters">Parameters</h2><p>StorageClasses have parameters that describe volumes belonging to the storage
class. Different parameters may be accepted depending on the <code>provisioner</code>.
When a parameter is omitted, some default is used.</p><p>There can be at most 512 parameters defined for a StorageClass.
The total length of the parameters object including its keys and values cannot
exceed 256 KiB.</p><h3 id="aws-ebs">AWS EBS</h3><p>Kubernetes 1.34 does not include a <code>awsElasticBlockStore</code> volume type.</p><p>The AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release
and then removed entirely in the v1.27 release.</p><p>The Kubernetes project suggests that you use the <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">AWS EBS</a>
out-of-tree storage driver instead.</p><p>Here is an example StorageClass for the AWS EBS CSI driver:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-aws-ebs.yaml"><code>storage/storageclass/storageclass-aws-ebs.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/storageclass/storageclass-aws-ebs.yaml to clipboard"></div><div class="includecode" id="storage-storageclass-storageclass-aws-ebs-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>ebs-sc<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>ebs.csi.aws.com<span>
</span></span></span><span><span><span></span><span>volumeBindingMode</span>:<span> </span>WaitForFirstConsumer<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>csi.storage.k8s.io/fstype</span>:<span> </span>xfs<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>io1<span>
</span></span></span><span><span><span>  </span><span>iopsPerGB</span>:<span> </span><span>"50"</span><span>
</span></span></span><span><span><span>  </span><span>encrypted</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>  </span><span>tagSpecification_1</span>:<span> </span><span>"key1=value1"</span><span>
</span></span></span><span><span><span>  </span><span>tagSpecification_2</span>:<span> </span><span>"key2=value2"</span><span>
</span></span></span><span><span><span></span><span>allowedTopologies</span>:<span>
</span></span></span><span><span><span></span>- <span>matchLabelExpressions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>key</span>:<span> </span>topology.ebs.csi.aws.com/zone<span>
</span></span></span><span><span><span>    </span><span>values</span>:<span>
</span></span></span><span><span><span>    </span>- us-east-2c<span>
</span></span></span></code></pre></div></div></div><p><code>tagSpecification</code>: Tags with this prefix are applied to dynamically provisioned EBS volumes.</p><h3 id="aws-efs">AWS EFS</h3><p>To configure AWS EFS storage, you can use the out-of-tree <a href="https://github.com/kubernetes-sigs/aws-efs-csi-driver">AWS_EFS_CSI_DRIVER</a>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-aws-efs.yaml"><code>storage/storageclass/storageclass-aws-efs.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/storageclass/storageclass-aws-efs.yaml to clipboard"></div><div class="includecode" id="storage-storageclass-storageclass-aws-efs-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>efs-sc<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>efs.csi.aws.com<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>provisioningMode</span>:<span> </span>efs-ap<span>
</span></span></span><span><span><span>  </span><span>fileSystemId</span>:<span> </span>fs-92107410<span>
</span></span></span><span><span><span>  </span><span>directoryPerms</span>:<span> </span><span>"700"</span><span>
</span></span></span></code></pre></div></div></div><ul><li><code>provisioningMode</code>: The type of volume to be provisioned by Amazon EFS. Currently, only access point based provisioning is supported (<code>efs-ap</code>).</li><li><code>fileSystemId</code>: The file system under which the access point is created.</li><li><code>directoryPerms</code>: The directory permissions of the root directory created by the access point.</li></ul><p>For more details, refer to the <a href="https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/dynamic_provisioning/README.md">AWS_EFS_CSI_Driver Dynamic Provisioning</a> documentation.</p><h3 id="nfs">NFS</h3><p>To configure NFS storage, you can use the in-tree driver or the
<a href="https://github.com/kubernetes-csi/csi-driver-nfs#readme">NFS CSI driver for Kubernetes</a>
(recommended).</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-nfs.yaml"><code>storage/storageclass/storageclass-nfs.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/storageclass/storageclass-nfs.yaml to clipboard"></div><div class="includecode" id="storage-storageclass-storageclass-nfs-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-nfs<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>example.com/external-nfs<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>server</span>:<span> </span>nfs-server.example.com<span>
</span></span></span><span><span><span>  </span><span>path</span>:<span> </span>/share<span>
</span></span></span><span><span><span>  </span><span>readOnly</span>:<span> </span><span>"false"</span><span>
</span></span></span></code></pre></div></div></div><ul><li><code>server</code>: Server is the hostname or IP address of the NFS server.</li><li><code>path</code>: Path that is exported by the NFS server.</li><li><code>readOnly</code>: A flag indicating whether the storage will be mounted as read only (default false).</li></ul><p>Kubernetes doesn't include an internal NFS provisioner.
You need to use an external provisioner to create a StorageClass for NFS.
Here are some examples:</p><ul><li><a href="https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner">NFS Ganesha server and external provisioner</a></li><li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">NFS subdir external provisioner</a></li></ul><h3 id="vsphere">vSphere</h3><p>There are two types of provisioners for vSphere storage classes:</p><ul><li><a href="#vsphere-provisioner-csi">CSI provisioner</a>: <code>csi.vsphere.vmware.com</code></li><li><a href="#vcp-provisioner">vCP provisioner</a>: <code>kubernetes.io/vsphere-volume</code></li></ul><p>In-tree provisioners are <a href="/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi">deprecated</a>.
For more information on the CSI provisioner, see
<a href="https://vsphere-csi-driver.sigs.k8s.io/">Kubernetes vSphere CSI Driver</a> and
<a href="/docs/concepts/storage/volumes/#vsphere-csi-migration">vSphereVolume CSI migration</a>.</p><h4 id="vsphere-provisioner-csi">CSI Provisioner</h4><p>The vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters.
For an example, refer to the <a href="https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/master/example/vanilla-k8s-RWM-filesystem-volumes/example-sc.yaml">vSphere CSI repository</a>.</p><h4 id="vcp-provisioner">vCP Provisioner</h4><p>The following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.</p><ol><li><p>Create a StorageClass with a user specified disk format.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>fast<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>kubernetes.io/vsphere-volume<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>diskformat</span>:<span> </span>zeroedthick<span>
</span></span></span></code></pre></div><p><code>diskformat</code>: <code>thin</code>, <code>zeroedthick</code> and <code>eagerzeroedthick</code>. Default: <code>"thin"</code>.</p></li><li><p>Create a StorageClass with a disk format on a user specified datastore.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>fast<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>kubernetes.io/vsphere-volume<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>diskformat</span>:<span> </span>zeroedthick<span>
</span></span></span><span><span><span>  </span><span>datastore</span>:<span> </span>VSANDatastore<span>
</span></span></span></code></pre></div><p><code>datastore</code>: The user can also specify the datastore in the StorageClass.
The volume will be created on the datastore specified in the StorageClass,
which in this case is <code>VSANDatastore</code>. This field is optional. If the
datastore is not specified, then the volume will be created on the datastore
specified in the vSphere config file used to initialize the vSphere Cloud
Provider.</p></li><li><p>Storage Policy Management inside kubernetes</p><ul><li><p>Using existing vCenter SPBM policy</p><p>One of the most important features of vSphere for Storage Management is
policy based Management. Storage Policy Based Management (SPBM) is a
storage policy framework that provides a single unified control plane
across a broad range of data services and storage solutions. SPBM enables
vSphere administrators to overcome upfront storage provisioning challenges,
such as capacity planning, differentiated service levels and managing
capacity headroom.</p><p>The SPBM policies can be specified in the StorageClass using the
<code>storagePolicyName</code> parameter.</p></li><li><p>Virtual SAN policy support inside Kubernetes</p><p>Vsphere Infrastructure (VI) Admins will have the ability to specify custom
Virtual SAN Storage Capabilities during dynamic volume provisioning. You
can now define storage requirements, such as performance and availability,
in the form of storage capabilities during dynamic volume provisioning.
The storage capability requirements are converted into a Virtual SAN
policy which are then pushed down to the Virtual SAN layer when a
persistent volume (virtual disk) is being created. The virtual disk is
distributed across the Virtual SAN datastore to meet the requirements.</p><p>You can see <a href="https://github.com/vmware-archive/vsphere-storage-for-kubernetes/blob/fa4c8b8ad46a85b6555d715dd9d27ff69839df53/documentation/policy-based-mgmt.md">Storage Policy Based Management for dynamic provisioning of volumes</a>
for more details on how to use storage policies for persistent volumes
management.</p></li></ul></li></ol><p>There are few
<a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere">vSphere examples</a>
which you try out for persistent volume management inside Kubernetes for vSphere.</p><h3 id="ceph-rbd">Ceph RBD (deprecated)</h3><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [deprecated]</code></div><p>This internal provisioner of Ceph RBD is deprecated. Please use
<a href="https://github.com/ceph/ceph-csi">CephFS RBD CSI driver</a>.</p></div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-ceph-rbd.yaml"><code>storage/storageclass/storageclass-ceph-rbd.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/storageclass/storageclass-ceph-rbd.yaml to clipboard"></div><div class="includecode" id="storage-storageclass-storageclass-ceph-rbd-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>fast<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>kubernetes.io/rbd<span> </span><span># This provisioner is deprecated</span><span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>monitors</span>:<span> </span><span>198.19.254.105</span>:<span>6789</span><span>
</span></span></span><span><span><span>  </span><span>adminId</span>:<span> </span>kube<span>
</span></span></span><span><span><span>  </span><span>adminSecretName</span>:<span> </span>ceph-secret<span>
</span></span></span><span><span><span>  </span><span>adminSecretNamespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span>  </span><span>pool</span>:<span> </span>kube<span>
</span></span></span><span><span><span>  </span><span>userId</span>:<span> </span>kube<span>
</span></span></span><span><span><span>  </span><span>userSecretName</span>:<span> </span>ceph-secret-user<span>
</span></span></span><span><span><span>  </span><span>userSecretNamespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>  </span><span>fsType</span>:<span> </span>ext4<span>
</span></span></span><span><span><span>  </span><span>imageFormat</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>  </span><span>imageFeatures</span>:<span> </span><span>"layering"</span><span>
</span></span></span></code></pre></div></div></div><ul><li><p><code>monitors</code>: Ceph monitors, comma delimited. This parameter is required.</p></li><li><p><code>adminId</code>: Ceph client ID that is capable of creating images in the pool.
Default is "admin".</p></li><li><p><code>adminSecretName</code>: Secret Name for <code>adminId</code>. This parameter is required.
The provided secret must have type "kubernetes.io/rbd".</p></li><li><p><code>adminSecretNamespace</code>: The namespace for <code>adminSecretName</code>. Default is "default".</p></li><li><p><code>pool</code>: Ceph RBD pool. Default is "rbd".</p></li><li><p><code>userId</code>: Ceph client ID that is used to map the RBD image. Default is the
same as <code>adminId</code>.</p></li><li><p><code>userSecretName</code>: The name of Ceph Secret for <code>userId</code> to map RBD image. It
must exist in the same namespace as PVCs. This parameter is required.
The provided secret must have type "kubernetes.io/rbd", for example created in this
way:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic ceph-secret --type<span>=</span><span>"kubernetes.io/rbd"</span> <span>\
</span></span></span><span><span><span></span>  --from-literal<span>=</span><span>key</span><span>=</span><span>'QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ=='</span> <span>\
</span></span></span><span><span><span></span>  --namespace<span>=</span>kube-system
</span></span></code></pre></div></li><li><p><code>userSecretNamespace</code>: The namespace for <code>userSecretName</code>.</p></li><li><p><code>fsType</code>: fsType that is supported by kubernetes. Default: <code>"ext4"</code>.</p></li><li><p><code>imageFormat</code>: Ceph RBD image format, "1" or "2". Default is "2".</p></li><li><p><code>imageFeatures</code>: This parameter is optional and should only be used if you
set <code>imageFormat</code> to "2". Currently supported features are <code>layering</code> only.
Default is "", and no features are turned on.</p></li></ul><h3 id="azure-disk">Azure Disk</h3><p>Kubernetes 1.34 does not include a <code>azureDisk</code> volume type.</p><p>The <code>azureDisk</code> in-tree storage driver was deprecated in the Kubernetes v1.19 release
and then removed entirely in the v1.27 release.</p><p>The Kubernetes project suggests that you use the <a href="https://github.com/kubernetes-sigs/azuredisk-csi-driver">Azure Disk</a> third party
storage driver instead.</p><h3 id="azure-file">Azure File (deprecated)</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-azure-file.yaml"><code>storage/storageclass/storageclass-azure-file.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/storageclass/storageclass-azure-file.yaml to clipboard"></div><div class="includecode" id="storage-storageclass-storageclass-azure-file-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>azurefile<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>kubernetes.io/azure-file<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>skuName</span>:<span> </span>Standard_LRS<span>
</span></span></span><span><span><span>  </span><span>location</span>:<span> </span>eastus<span>
</span></span></span><span><span><span>  </span><span>storageAccount</span>:<span> </span>azure_storage_account_name<span> </span><span># example value</span><span>
</span></span></span></code></pre></div></div></div><ul><li><code>skuName</code>: Azure storage account SKU tier. Default is empty.</li><li><code>location</code>: Azure storage account location. Default is empty.</li><li><code>storageAccount</code>: Azure storage account name. Default is empty. If a storage
account is not provided, all storage accounts associated with the resource
group are searched to find one that matches <code>skuName</code> and <code>location</code>. If a
storage account is provided, it must reside in the same resource group as the
cluster, and <code>skuName</code> and <code>location</code> are ignored.</li><li><code>secretNamespace</code>: the namespace of the secret that contains the Azure Storage
Account Name and Key. Default is the same as the Pod.</li><li><code>secretName</code>: the name of the secret that contains the Azure Storage Account Name and
Key. Default is <code>azure-storage-account-&lt;accountName&gt;-secret</code></li><li><code>readOnly</code>: a flag indicating whether the storage will be mounted as read only.
Defaults to false which means a read/write mount. This setting will impact the
<code>ReadOnly</code> setting in VolumeMounts as well.</li></ul><p>During storage provisioning, a secret named by <code>secretName</code> is created for the
mounting credentials. If the cluster has enabled both
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a> and
<a href="/docs/reference/access-authn-authz/rbac/#controller-roles">Controller Roles</a>,
add the <code>create</code> permission of resource <code>secret</code> for clusterrole
<code>system:controller:persistent-volume-binder</code>.</p><p>In a multi-tenancy context, it is strongly recommended to set the value for
<code>secretNamespace</code> explicitly, otherwise the storage account credentials may
be read by other users.</p><h3 id="portworx-volume">Portworx volume (deprecated)</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-portworx-volume.yaml"><code>storage/storageclass/storageclass-portworx-volume.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/storageclass/storageclass-portworx-volume.yaml to clipboard"></div><div class="includecode" id="storage-storageclass-storageclass-portworx-volume-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>portworx-io-priority-high<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>kubernetes.io/portworx-volume<span> </span><span># This provisioner is deprecated</span><span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>repl</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>  </span><span>snap_interval</span>:<span> </span><span>"70"</span><span>
</span></span></span><span><span><span>  </span><span>priority_io</span>:<span> </span><span>"high"</span><span>
</span></span></span></code></pre></div></div></div><ul><li><code>fs</code>: filesystem to be laid out: <code>none/xfs/ext4</code> (default: <code>ext4</code>).</li><li><code>block_size</code>: block size in Kbytes (default: <code>32</code>).</li><li><code>repl</code>: number of synchronous replicas to be provided in the form of
replication factor <code>1..3</code> (default: <code>1</code>) A string is expected here i.e.
<code>"1"</code> and not <code>1</code>.</li><li><code>priority_io</code>: determines whether the volume will be created from higher
performance or a lower priority storage <code>high/medium/low</code> (default: <code>low</code>).</li><li><code>snap_interval</code>: clock/time interval in minutes for when to trigger snapshots.
Snapshots are incremental based on difference with the prior snapshot, 0
disables snaps (default: <code>0</code>). A string is expected here i.e.
<code>"70"</code> and not <code>70</code>.</li><li><code>aggregation_level</code>: specifies the number of chunks the volume would be
distributed into, 0 indicates a non-aggregated volume (default: <code>0</code>). A string
is expected here i.e. <code>"0"</code> and not <code>0</code></li><li><code>ephemeral</code>: specifies whether the volume should be cleaned-up after unmount
or should be persistent. <code>emptyDir</code> use case can set this value to true and
<code>persistent volumes</code> use case such as for databases like Cassandra should set
to false, <code>true/false</code> (default <code>false</code>). A string is expected here i.e.
<code>"true"</code> and not <code>true</code>.</li></ul><h3 id="local">Local</h3><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/storage/storageclass/storageclass-local.yaml"><code>storage/storageclass/storageclass-local.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy storage/storageclass/storageclass-local.yaml to clipboard"></div><div class="includecode" id="storage-storageclass-storageclass-local-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>local-storage<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>kubernetes.io/no-provisioner<span> </span><span># indicates that this StorageClass does not support automatic provisioning</span><span>
</span></span></span><span><span><span></span><span>volumeBindingMode</span>:<span> </span>WaitForFirstConsumer<span>
</span></span></span></code></pre></div></div></div><p>Local volumes do not support dynamic provisioning in Kubernetes 1.34;
however a StorageClass should still be created to delay volume binding until a Pod is actually
scheduled to the appropriate node. This is specified by the <code>WaitForFirstConsumer</code> volume
binding mode.</p><p>Delaying volume binding allows the scheduler to consider all of a Pod's
scheduling constraints when choosing an appropriate PersistentVolume for a
PersistentVolumeClaim.</p></div></div><div><div class="td-content"><h1>Volume Attributes Classes</h1><div class="feature-state-notice feature-stable" title="Feature Gate: VolumeAttributesClass"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>This page assumes that you are familiar with <a href="/docs/concepts/storage/storage-classes/">StorageClasses</a>,
<a href="/docs/concepts/storage/volumes/">volumes</a> and <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>
in Kubernetes.</p><p>A VolumeAttributesClass provides a way for administrators to describe the mutable
"classes" of storage they offer. Different classes might map to different quality-of-service levels.
Kubernetes itself is un-opinionated about what these classes represent.</p><p>This feature is generally available (GA) as of version 1.34, and users have the option to disable it.</p><p>You can also only use VolumeAttributesClasses with storage backed by
<a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">Container Storage Interface</a>, and only where the
relevant CSI driver implements the <code>ModifyVolume</code> API.</p><h2 id="the-volumeattributesclass-api">The VolumeAttributesClass API</h2><p>Each VolumeAttributesClass contains the <code>driverName</code> and <code>parameters</code>, which are
used when a PersistentVolume (PV) belonging to the class needs to be dynamically provisioned
or modified.</p><p>The name of a VolumeAttributesClass object is significant and is how users can request a particular class.
Administrators set the name and other parameters of a class when first creating VolumeAttributesClass objects.
While the name of a VolumeAttributesClass object in a <code>PersistentVolumeClaim</code> is mutable, the parameters in an existing class are immutable.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>VolumeAttributesClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>silver<span>
</span></span></span><span><span><span></span><span>driverName</span>:<span> </span>pd.csi.storage.gke.io<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>provisioned-iops</span>:<span> </span><span>"3000"</span><span>
</span></span></span><span><span><span>  </span><span>provisioned-throughput</span>:<span> </span><span>"50"</span><span> 
</span></span></span></code></pre></div><h3 id="provisioner">Provisioner</h3><p>Each VolumeAttributesClass has a provisioner that determines what volume plugin is used for
provisioning PVs. The field <code>driverName</code> must be specified.</p><p>The feature support for VolumeAttributesClass is implemented in
<a href="https://github.com/kubernetes-csi/external-provisioner">kubernetes-csi/external-provisioner</a>.</p><p>You are not restricted to specifying the <a href="https://github.com/kubernetes-csi/external-provisioner">kubernetes-csi/external-provisioner</a>.
You can also run and specify external provisioners,
which are independent programs that follow a specification defined by Kubernetes.
Authors of external provisioners have full discretion over where their code lives, how
the provisioner is shipped, how it needs to be run, what volume plugin it uses, etc.</p><p>To understand how the provisioner works with VolumeAttributesClass, refer to
the <a href="https://kubernetes-csi.github.io/docs/external-provisioner.html">CSI external-provisioner documentation</a>.</p><h3 id="resizer">Resizer</h3><p>Each VolumeAttributesClass has a resizer that determines what volume plugin is used
for modifying PVs. The field <code>driverName</code> must be specified.</p><p>The modifying volume feature support for VolumeAttributesClass is implemented in
<a href="https://github.com/kubernetes-csi/external-resizer">kubernetes-csi/external-resizer</a>.</p><p>For example, an existing PersistentVolumeClaim is using a VolumeAttributesClass named silver:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-pv-claim<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>&#8230;<span>
</span></span></span><span><span><span>  </span><span>volumeAttributesClassName</span>:<span> </span>silver<span>
</span></span></span><span><span><span>  </span>&#8230;<span>
</span></span></span></code></pre></div><p>A new VolumeAttributesClass gold is available in the cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>VolumeAttributesClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>gold<span>
</span></span></span><span><span><span></span><span>driverName</span>:<span> </span>pd.csi.storage.gke.io<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>iops</span>:<span> </span><span>"4000"</span><span>
</span></span></span><span><span><span>  </span><span>throughput</span>:<span> </span><span>"60"</span><span>
</span></span></span></code></pre></div><p>The end user can update the PVC with the new VolumeAttributesClass gold and apply:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-pv-claim<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span>&#8230;<span>
</span></span></span><span><span><span>  </span><span>volumeAttributesClassName</span>:<span> </span>gold<span>
</span></span></span><span><span><span>  </span>&#8230;<span>
</span></span></span></code></pre></div><p>To understand how the resizer works with VolumeAttributesClass, refer to
the <a href="https://kubernetes-csi.github.io/docs/external-resizer.html">CSI external-resizer documentation</a>.</p><h2 id="parameters">Parameters</h2><p>VolumeAttributeClasses have parameters that describe volumes belonging to them. Different parameters may be accepted
depending on the provisioner or the resizer. For example, the value <code>4000</code>, for the parameter <code>iops</code>,
and the parameter <code>throughput</code> are specific to GCE PD.
When a parameter is omitted, the default is used at volume provisioning.
If a user applies the PVC with a different VolumeAttributesClass with omitted parameters, the default value of
the parameters may be used depending on the CSI driver implementation.
Please refer to the related CSI driver documentation for more details.</p><p>There can be at most 512 parameters defined for a VolumeAttributesClass.
The total length of the parameters object including its keys and values cannot exceed 256 KiB.</p></div></div><div><div class="td-content"><h1>Dynamic Volume Provisioning</h1><p>Dynamic volume provisioning allows storage volumes to be created on-demand.
Without dynamic provisioning, cluster administrators have to manually make
calls to their cloud or storage provider to create new storage volumes, and
then create <a href="/docs/concepts/storage/persistent-volumes/"><code>PersistentVolume</code> objects</a>
to represent them in Kubernetes. The dynamic provisioning feature eliminates
the need for cluster administrators to pre-provision storage. Instead, it
automatically provisions storage when users create
<a href="/docs/concepts/storage/persistent-volumes/"><code>PersistentVolumeClaim</code> objects</a>.</p><h2 id="background">Background</h2><p>The implementation of dynamic volume provisioning is based on the API object <code>StorageClass</code>
from the API group <code>storage.k8s.io</code>. A cluster administrator can define as many
<code>StorageClass</code> objects as needed, each specifying a <em>volume plugin</em> (aka
<em>provisioner</em>) that provisions a volume and the set of parameters to pass to
that provisioner when provisioning.
A cluster administrator can define and expose multiple flavors of storage (from
the same or different storage systems) within a cluster, each with a custom set
of parameters. This design also ensures that end users don't have to worry
about the complexity and nuances of how storage is provisioned, but still
have the ability to select from multiple storage options.</p><p>For more details, see the <a href="/docs/concepts/storage/storage-classes/">Storage Classes</a> concept.</p><h2 id="enabling-dynamic-provisioning">Enabling Dynamic Provisioning</h2><p>To enable dynamic provisioning, a cluster administrator needs to pre-create
one or more StorageClass objects for users.
StorageClass objects define which provisioner should be used and what parameters
should be passed to that provisioner when dynamic provisioning is invoked.
The name of a StorageClass object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>The following manifest creates a storage class "slow" which provisions standard
disk-like persistent disks.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>slow<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>kubernetes.io/gce-pd<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>pd-standard<span>
</span></span></span></code></pre></div><p>The following manifest creates a storage class "fast" which provisions
SSD-like persistent disks.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>StorageClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>fast<span>
</span></span></span><span><span><span></span><span>provisioner</span>:<span> </span>kubernetes.io/gce-pd<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>pd-ssd<span>
</span></span></span></code></pre></div><h2 id="using-dynamic-provisioning">Using Dynamic Provisioning</h2><p>Users request dynamically provisioned storage by including a storage class in
their <code>PersistentVolumeClaim</code>. Before Kubernetes v1.6, this was done via the
<code>volume.beta.kubernetes.io/storage-class</code> annotation. However, this annotation
is deprecated since v1.9. Users now can and should instead use the
<code>storageClassName</code> field of the <code>PersistentVolumeClaim</code> object. The value of
this field must match the name of a <code>StorageClass</code> configured by the
administrator (see <a href="#enabling-dynamic-provisioning">Enabling Dynamic Provisioning</a>).</p><p>To select the "fast" storage class, for example, a user would create the
following PersistentVolumeClaim:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>claim1<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>    </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>fast<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>30Gi<span>
</span></span></span></code></pre></div><p>This claim results in an SSD-like Persistent Disk being automatically
provisioned. When the claim is deleted, the volume is destroyed.</p><h2 id="defaulting-behavior">Defaulting Behavior</h2><p>Dynamic provisioning can be enabled on a cluster such that all claims are
dynamically provisioned if no storage class is specified. A cluster administrator
can enable this behavior by:</p><ul><li>Marking one <code>StorageClass</code> object as <em>default</em>.</li><li>Making sure that the <a href="/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass"><code>DefaultStorageClass</code> admission controller</a>
is enabled on the API server.</li></ul><p>An administrator can mark a specific <code>StorageClass</code> as default by adding the
<a href="/docs/reference/labels-annotations-taints/#storageclass-kubernetes-io-is-default-class"><code>storageclass.kubernetes.io/is-default-class</code> annotation</a> to it.
When a default <code>StorageClass</code> exists in a cluster and a user creates a
<code>PersistentVolumeClaim</code> with <code>storageClassName</code> unspecified, the
<code>DefaultStorageClass</code> admission controller automatically adds the
<code>storageClassName</code> field pointing to the default storage class.</p><p>Note that if you set the <code>storageclass.kubernetes.io/is-default-class</code>
annotation to true on more than one StorageClass in your cluster, and you then
create a <code>PersistentVolumeClaim</code> with no <code>storageClassName</code> set, Kubernetes
uses the most recently created default StorageClass.</p><h2 id="topology-awareness">Topology Awareness</h2><p>In <a href="/docs/setup/best-practices/multiple-zones/">Multi-Zone</a> clusters, Pods can be spread across
Zones in a Region. Single-Zone storage backends should be provisioned in the Zones where
Pods are scheduled. This can be accomplished by setting the
<a href="/docs/concepts/storage/storage-classes/#volume-binding-mode">Volume Binding Mode</a>.</p></div></div><div><div class="td-content"><h1>Volume Snapshots</h1><p>In Kubernetes, a <em>VolumeSnapshot</em> represents a snapshot of a volume on a storage
system. This document assumes that you are already familiar with Kubernetes
<a href="/docs/concepts/storage/persistent-volumes/">persistent volumes</a>.</p><h2 id="introduction">Introduction</h2><p>Similar to how API resources <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> are
used to provision volumes for users and administrators, <code>VolumeSnapshotContent</code>
and <code>VolumeSnapshot</code> API resources are provided to create volume snapshots for
users and administrators.</p><p>A <code>VolumeSnapshotContent</code> is a snapshot taken from a volume in the cluster that
has been provisioned by an administrator. It is a resource in the cluster just
like a PersistentVolume is a cluster resource.</p><p>A <code>VolumeSnapshot</code> is a request for snapshot of a volume by a user. It is similar
to a PersistentVolumeClaim.</p><p><code>VolumeSnapshotClass</code> allows you to specify different attributes belonging to a
<code>VolumeSnapshot</code>. These attributes may differ among snapshots taken from the same
volume on the storage system and therefore cannot be expressed by using the same
<code>StorageClass</code> of a <code>PersistentVolumeClaim</code>.</p><p>Volume snapshots provide Kubernetes users with a standardized way to copy a volume's
contents at a particular point in time without creating an entirely new volume. This
functionality enables, for example, database administrators to backup databases before
performing edit or delete modifications.</p><p>Users need to be aware of the following when using this feature:</p><ul><li>API Objects <code>VolumeSnapshot</code>, <code>VolumeSnapshotContent</code>, and <code>VolumeSnapshotClass</code>
are <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CRDs</a>, not
part of the core API.</li><li><code>VolumeSnapshot</code> support is only available for CSI drivers.</li><li>As part of the deployment process of <code>VolumeSnapshot</code>, the Kubernetes team provides
a snapshot controller to be deployed into the control plane, and a sidecar helper
container called csi-snapshotter to be deployed together with the CSI driver.
The snapshot controller watches <code>VolumeSnapshot</code> and <code>VolumeSnapshotContent</code> objects
and is responsible for the creation and deletion of <code>VolumeSnapshotContent</code> object.
The sidecar csi-snapshotter watches <code>VolumeSnapshotContent</code> objects and triggers
<code>CreateSnapshot</code> and <code>DeleteSnapshot</code> operations against a CSI endpoint.</li><li>There is also a validating webhook server which provides tightened validation on
snapshot objects. This should be installed by the Kubernetes distros along with
the snapshot controller and CRDs, not CSI drivers. It should be installed in all
Kubernetes clusters that has the snapshot feature enabled.</li><li>CSI drivers may or may not have implemented the volume snapshot functionality.
The CSI drivers that have provided support for volume snapshot will likely use
the csi-snapshotter. See <a href="https://kubernetes-csi.github.io/docs/">CSI Driver documentation</a> for details.</li><li>The CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.</li></ul><p>For advanced use cases, such as creating group snapshots of multiple volumes, see the external
<a href="https://kubernetes-csi.github.io/docs/group-snapshot-restore-feature.html">CSI Volume Group Snapshot documentation</a>.</p><h2 id="lifecycle-of-a-volume-snapshot-and-volume-snapshot-content">Lifecycle of a volume snapshot and volume snapshot content</h2><p><code>VolumeSnapshotContents</code> are resources in the cluster. <code>VolumeSnapshots</code> are requests
for those resources. The interaction between <code>VolumeSnapshotContents</code> and <code>VolumeSnapshots</code>
follow this lifecycle:</p><h3 id="provisioning-volume-snapshot">Provisioning Volume Snapshot</h3><p>There are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.</p><h4 id="static">Pre-provisioned</h4><p>A cluster administrator creates a number of <code>VolumeSnapshotContents</code>. They carry the details
of the real volume snapshot on the storage system which is available for use by cluster users.
They exist in the Kubernetes API and are available for consumption.</p><h4 id="dynamic">Dynamic</h4><p>Instead of using a pre-existing snapshot, you can request that a snapshot to be dynamically
taken from a PersistentVolumeClaim. The <a href="/docs/concepts/storage/volume-snapshot-classes/">VolumeSnapshotClass</a>
specifies storage provider-specific parameters to use when taking a snapshot.</p><h3 id="binding">Binding</h3><p>The snapshot controller handles the binding of a <code>VolumeSnapshot</code> object with an appropriate
<code>VolumeSnapshotContent</code> object, in both pre-provisioned and dynamically provisioned scenarios.
The binding is a one-to-one mapping.</p><p>In the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the
requested VolumeSnapshotContent object is created.</p><h3 id="persistent-volume-claim-as-snapshot-source-protection">Persistent Volume Claim as Snapshot Source Protection</h3><p>The purpose of this protection is to ensure that in-use
<a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank">PersistentVolumeClaim</a>
API objects are not removed from the system while a snapshot is being taken from it
(as this may result in data loss).</p><p>While a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim
is in-use. If you delete a PersistentVolumeClaim API object in active use as a snapshot
source, the PersistentVolumeClaim object is not removed immediately. Instead, removal of
the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.</p><h3 id="delete">Delete</h3><p>Deletion is triggered by deleting the <code>VolumeSnapshot</code> object, and the <code>DeletionPolicy</code>
will be followed. If the <code>DeletionPolicy</code> is <code>Delete</code>, then the underlying storage snapshot
will be deleted along with the <code>VolumeSnapshotContent</code> object. If the <code>DeletionPolicy</code> is
<code>Retain</code>, then both the underlying snapshot and <code>VolumeSnapshotContent</code> remain.</p><h2 id="volumesnapshots">VolumeSnapshots</h2><p>Each VolumeSnapshot contains a spec and a status.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>snapshot.storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>VolumeSnapshot<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>new-snapshot-test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>volumeSnapshotClassName</span>:<span> </span>csi-hostpath-snapclass<span>
</span></span></span><span><span><span>  </span><span>source</span>:<span>
</span></span></span><span><span><span>    </span><span>persistentVolumeClaimName</span>:<span> </span>pvc-test<span>
</span></span></span></code></pre></div><p><code>persistentVolumeClaimName</code> is the name of the PersistentVolumeClaim data source
for the snapshot. This field is required for dynamically provisioning a snapshot.</p><p>A volume snapshot can request a particular class by specifying the name of a
<a href="/docs/concepts/storage/volume-snapshot-classes/">VolumeSnapshotClass</a>
using the attribute <code>volumeSnapshotClassName</code>. If nothing is set, then the
default class is used if available.</p><p>For pre-provisioned snapshots, you need to specify a <code>volumeSnapshotContentName</code>
as the source for the snapshot as shown in the following example. The
<code>volumeSnapshotContentName</code> source field is required for pre-provisioned snapshots.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>snapshot.storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>VolumeSnapshot<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-snapshot<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>source</span>:<span>
</span></span></span><span><span><span>    </span><span>volumeSnapshotContentName</span>:<span> </span>test-content<span>
</span></span></span></code></pre></div><h2 id="volume-snapshot-contents">Volume Snapshot Contents</h2><p>Each VolumeSnapshotContent contains a spec and status. In dynamic provisioning,
the snapshot common controller creates <code>VolumeSnapshotContent</code> objects. Here is an example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>snapshot.storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>VolumeSnapshotContent<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>snapcontent-72d9a349-aacd-42d2-a240-d775650d2455<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>deletionPolicy</span>:<span> </span>Delete<span>
</span></span></span><span><span><span>  </span><span>driver</span>:<span> </span>hostpath.csi.k8s.io<span>
</span></span></span><span><span><span>  </span><span>source</span>:<span>
</span></span></span><span><span><span>    </span><span>volumeHandle</span>:<span> </span>ee0cfb94-f8d4-11e9-b2d8-0242ac110002<span>
</span></span></span><span><span><span>  </span><span>sourceVolumeMode</span>:<span> </span>Filesystem<span>
</span></span></span><span><span><span>  </span><span>volumeSnapshotClassName</span>:<span> </span>csi-hostpath-snapclass<span>
</span></span></span><span><span><span>  </span><span>volumeSnapshotRef</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>new-snapshot-test<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>default<span>
</span></span></span><span><span><span>    </span><span>uid</span>:<span> </span>72d9a349-aacd-42d2-a240-d775650d2455<span>
</span></span></span></code></pre></div><p><code>volumeHandle</code> is the unique identifier of the volume created on the storage
backend and returned by the CSI driver during the volume creation. This field
is required for dynamically provisioning a snapshot.
It specifies the volume source of the snapshot.</p><p>For pre-provisioned snapshots, you (as cluster administrator) are responsible
for creating the <code>VolumeSnapshotContent</code> object as follows.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>snapshot.storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>VolumeSnapshotContent<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>new-snapshot-content-test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>deletionPolicy</span>:<span> </span>Delete<span>
</span></span></span><span><span><span>  </span><span>driver</span>:<span> </span>hostpath.csi.k8s.io<span>
</span></span></span><span><span><span>  </span><span>source</span>:<span>
</span></span></span><span><span><span>    </span><span>snapshotHandle</span>:<span> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span>
</span></span></span><span><span><span>  </span><span>sourceVolumeMode</span>:<span> </span>Filesystem<span>
</span></span></span><span><span><span>  </span><span>volumeSnapshotRef</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>new-snapshot-test<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>default<span>
</span></span></span></code></pre></div><p><code>snapshotHandle</code> is the unique identifier of the volume snapshot created on
the storage backend. This field is required for the pre-provisioned snapshots.
It specifies the CSI snapshot id on the storage system that this
<code>VolumeSnapshotContent</code> represents.</p><p><code>sourceVolumeMode</code> is the mode of the volume whose snapshot is taken. The value
of the <code>sourceVolumeMode</code> field can be either <code>Filesystem</code> or <code>Block</code>. If the
source volume mode is not specified, Kubernetes treats the snapshot as if the
source volume's mode is unknown.</p><p><code>volumeSnapshotRef</code> is the reference of the corresponding <code>VolumeSnapshot</code>. Note that
when the <code>VolumeSnapshotContent</code> is being created as a pre-provisioned snapshot, the
<code>VolumeSnapshot</code> referenced in <code>volumeSnapshotRef</code> might not exist yet.</p><h2 id="convert-volume-mode">Converting the volume mode of a Snapshot</h2><p>If the <code>VolumeSnapshots</code> API installed on your cluster supports the <code>sourceVolumeMode</code>
field, then the API has the capability to prevent unauthorized users from converting
the mode of a volume.</p><p>To check if your cluster has capability for this feature, run the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span>$ kubectl get crd volumesnapshotcontent -o yaml<span>
</span></span></span></code></pre></div><p>If you want to allow users to create a <code>PersistentVolumeClaim</code> from an existing
<code>VolumeSnapshot</code>, but with a different volume mode than the source, the annotation
<code>snapshot.storage.kubernetes.io/allow-volume-mode-change: "true"</code>needs to be added to
the <code>VolumeSnapshotContent</code> that corresponds to the <code>VolumeSnapshot</code>.</p><p>For pre-provisioned snapshots, <code>spec.sourceVolumeMode</code> needs to be populated
by the cluster administrator.</p><p>An example <code>VolumeSnapshotContent</code> resource with this feature enabled would look like:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>snapshot.storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>VolumeSnapshotContent<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>new-snapshot-content-test<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span>- <span>snapshot.storage.kubernetes.io/allow-volume-mode-change</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>deletionPolicy</span>:<span> </span>Delete<span>
</span></span></span><span><span><span>  </span><span>driver</span>:<span> </span>hostpath.csi.k8s.io<span>
</span></span></span><span><span><span>  </span><span>source</span>:<span>
</span></span></span><span><span><span>    </span><span>snapshotHandle</span>:<span> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span>
</span></span></span><span><span><span>  </span><span>sourceVolumeMode</span>:<span> </span>Filesystem<span>
</span></span></span><span><span><span>  </span><span>volumeSnapshotRef</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>new-snapshot-test<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>default<span>
</span></span></span></code></pre></div><h2 id="provisioning-volumes-from-snapshots">Provisioning Volumes from Snapshots</h2><p>You can provision a new volume, pre-populated with data from a snapshot, by using
the <em>dataSource</em> field in the <code>PersistentVolumeClaim</code> object.</p><p>For more details, see
<a href="/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support">Volume Snapshot and Restore Volume from Snapshot</a>.</p></div></div><div><div class="td-content"><h1>Volume Snapshot Classes</h1><p>This document describes the concept of VolumeSnapshotClass in Kubernetes. Familiarity
with <a href="/docs/concepts/storage/volume-snapshots/">volume snapshots</a> and
<a href="/docs/concepts/storage/storage-classes/">storage classes</a> is suggested.</p><h2 id="introduction">Introduction</h2><p>Just like StorageClass provides a way for administrators to describe the "classes"
of storage they offer when provisioning a volume, VolumeSnapshotClass provides a
way to describe the "classes" of storage when provisioning a volume snapshot.</p><h2 id="the-volumesnapshotclass-resource">The VolumeSnapshotClass Resource</h2><p>Each VolumeSnapshotClass contains the fields <code>driver</code>, <code>deletionPolicy</code>, and <code>parameters</code>,
which are used when a VolumeSnapshot belonging to the class needs to be
dynamically provisioned.</p><p>The name of a VolumeSnapshotClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating VolumeSnapshotClass objects, and the objects cannot
be updated once they are created.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Installation of the CRDs is the responsibility of the Kubernetes distribution.
Without the required CRDs present, the creation of a VolumeSnapshotClass fails.</div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>snapshot.storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>VolumeSnapshotClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>csi-hostpath-snapclass<span>
</span></span></span><span><span><span></span><span>driver</span>:<span> </span>hostpath.csi.k8s.io<span>
</span></span></span><span><span><span></span><span>deletionPolicy</span>:<span> </span>Delete<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span></code></pre></div><p>Administrators can specify a default VolumeSnapshotClass for VolumeSnapshots
that don't request any particular class to bind to by adding the
<code>snapshot.storage.kubernetes.io/is-default-class: "true"</code> annotation:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>snapshot.storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>VolumeSnapshotClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>csi-hostpath-snapclass<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>snapshot.storage.kubernetes.io/is-default-class</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span></span><span>driver</span>:<span> </span>hostpath.csi.k8s.io<span>
</span></span></span><span><span><span></span><span>deletionPolicy</span>:<span> </span>Delete<span>
</span></span></span><span><span><span></span><span>parameters</span>:<span>
</span></span></span></code></pre></div><p>If multiple CSI drivers exist, a default VolumeSnapshotClass can be specified
for each of them.</p><h3 id="volumesnapshotclass-dependencies">VolumeSnapshotClass dependencies</h3><p>When you create a VolumeSnapshot without specifying a VolumeSnapshotClass, Kubernetes
automatically selects a default VolumeSnapshotClass that has a CSI driver matching
the CSI driver of the PVC&#8217;s StorageClass.</p><p>This behavior allows multiple default VolumeSnapshotClass objects to coexist in a cluster, as long as
each one is associated with a unique CSI driver.</p><p>Always ensure that there is only one default VolumeSnapshotClass for each CSI driver. If
multiple default VolumeSnapshotClass objects are created using the same CSI driver,
a VolumeSnapshot creation will fail because Kubernetes cannot determine which one to use.</p><h3 id="driver">Driver</h3><p>Volume snapshot classes have a driver that determines what CSI volume plugin is
used for provisioning VolumeSnapshots. This field must be specified.</p><h3 id="deletionpolicy">DeletionPolicy</h3><p>Volume snapshot classes have a <a href="/docs/concepts/storage/volume-snapshots/#delete">deletionPolicy</a>.
It enables you to configure what happens to a VolumeSnapshotContent when the VolumeSnapshot
object it is bound to is to be deleted. The deletionPolicy of a volume snapshot class can
either be <code>Retain</code> or <code>Delete</code>. This field must be specified.</p><p>If the deletionPolicy is <code>Delete</code>, then the underlying storage snapshot will be
deleted along with the VolumeSnapshotContent object. If the deletionPolicy is <code>Retain</code>,
then both the underlying snapshot and VolumeSnapshotContent remain.</p><h2 id="parameters">Parameters</h2><p>Volume snapshot classes have parameters that describe volume snapshots belonging to
the volume snapshot class. Different parameters may be accepted depending on the
<code>driver</code>.</p></div></div><div><div class="td-content"><h1>CSI Volume Cloning</h1><p>This document describes the concept of cloning existing CSI Volumes in Kubernetes.
Familiarity with <a href="/docs/concepts/storage/volumes/">Volumes</a> is suggested.</p><h2 id="introduction">Introduction</h2><p>The <a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> Volume Cloning feature adds
support for specifying existing <a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank">PVC</a>s
in the <code>dataSource</code> field to indicate a user would like to clone a <a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">Volume</a>.</p><p>A Clone is defined as a duplicate of an existing Kubernetes Volume that can be
consumed as any standard Volume would be. The only difference is that upon
provisioning, rather than creating a "new" empty Volume, the back end device
creates an exact duplicate of the specified Volume.</p><p>The implementation of cloning, from the perspective of the Kubernetes API, adds
the ability to specify an existing PVC as a dataSource during new PVC creation.
The source PVC must be bound and available (not in use).</p><p>Users need to be aware of the following when using this feature:</p><ul><li>Cloning support (<code>VolumePVCDataSource</code>) is only available for CSI drivers.</li><li>Cloning support is only available for dynamic provisioners.</li><li>CSI drivers may or may not have implemented the volume cloning functionality.</li><li>You can only clone a PVC when it exists in the same namespace as the destination PVC
(source and destination must be in the same namespace).</li><li>Cloning is supported with a different Storage Class.<ul><li>Destination volume can be the same or a different storage class as the source.</li><li>Default storage class can be used and storageClassName omitted in the spec.</li></ul></li><li>Cloning can only be performed between two volumes that use the same VolumeMode setting
(if you request a block mode volume, the source MUST also be block mode)</li></ul><h2 id="provisioning">Provisioning</h2><p>Clones are provisioned like any other PVC with the exception of adding a dataSource
that references an existing PVC in the same namespace.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>clone-of-pvc-1<span>
</span></span></span><span><span><span>    </span><span>namespace</span>:<span> </span>myns<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>accessModes</span>:<span>
</span></span></span><span><span><span>  </span>- ReadWriteOnce<span>
</span></span></span><span><span><span>  </span><span>storageClassName</span>:<span> </span>cloning<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>storage</span>:<span> </span>5Gi<span>
</span></span></span><span><span><span>  </span><span>dataSource</span>:<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>PersistentVolumeClaim<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>pvc-1<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You must specify a capacity value for <code>spec.resources.requests.storage</code>, and the
value you specify must be the same or larger than the capacity of the source volume.</div><p>The result is a new PVC with the name <code>clone-of-pvc-1</code> that has the exact same
content as the specified source <code>pvc-1</code>.</p><h2 id="usage">Usage</h2><p>Upon availability of the new PVC, the cloned PVC is consumed the same as other PVC.
It's also expected at this point that the newly created PVC is an independent object.
It can be consumed, cloned, snapshotted, or deleted independently and without
consideration for it's original dataSource PVC. This also implies that the source
is not linked in any way to the newly created clone, it may also be modified or
deleted without affecting the newly created clone.</p></div></div><div><div class="td-content"><h1>Storage Capacity</h1><p>Storage capacity is limited and may vary depending on the node on
which a pod runs: network-attached storage might not be accessible by
all nodes, or storage is local to a node to begin with.</p><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>This page describes how Kubernetes keeps track of storage capacity and
how the scheduler uses that information to <a href="/docs/concepts/scheduling-eviction/">schedule Pods</a> onto nodes
that have access to enough storage capacity for the remaining missing
volumes. Without storage capacity tracking, the scheduler may choose a
node that doesn't have enough capacity to provision a volume and
multiple scheduling retries will be needed.</p><h2 id="before-you-begin">Before you begin</h2><p>Kubernetes v1.34 includes cluster-level API support for
storage capacity tracking. To use this you must also be using a CSI driver that
supports capacity tracking. Consult the documentation for the CSI drivers that
you use to find out whether this support is available and, if so, how to use
it. If you are not running Kubernetes v1.34, check the
documentation for that version of Kubernetes.</p><h2 id="api">API</h2><p>There are two API extensions for this feature:</p><ul><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/csi-storage-capacity-v1/">CSIStorageCapacity</a> objects:
these get produced by a CSI driver in the namespace
where the driver is installed. Each object contains capacity
information for one storage class and defines which nodes have
access to that storage.</li><li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/csi-driver-v1/#CSIDriverSpec">The <code>CSIDriverSpec.StorageCapacity</code> field</a>:
when set to <code>true</code>, the Kubernetes scheduler will consider storage
capacity for volumes that use the CSI driver.</li></ul><h2 id="scheduling">Scheduling</h2><p>Storage capacity information is used by the Kubernetes scheduler if:</p><ul><li>a Pod uses a volume that has not been created yet,</li><li>that volume uses a <a class="glossary-tooltip" title="A StorageClass provides a way for administrators to describe different available storage types." href="/docs/concepts/storage/storage-classes" target="_blank">StorageClass</a> which references a CSI driver and
uses <code>WaitForFirstConsumer</code> <a href="/docs/concepts/storage/storage-classes/#volume-binding-mode">volume binding
mode</a>,
and</li><li>the <code>CSIDriver</code> object for the driver has <code>StorageCapacity</code> set to
true.</li></ul><p>In that case, the scheduler only considers nodes for the Pod which
have enough storage available to them. This check is very
simplistic and only compares the size of the volume against the
capacity listed in <code>CSIStorageCapacity</code> objects with a topology that
includes the node.</p><p>For volumes with <code>Immediate</code> volume binding mode, the storage driver
decides where to create the volume, independently of Pods that will
use the volume. The scheduler then schedules Pods onto nodes where the
volume is available after the volume has been created.</p><p>For <a href="/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">CSI ephemeral volumes</a>,
scheduling always happens without considering storage capacity. This
is based on the assumption that this volume type is only used by
special CSI drivers which are local to a node and do not need
significant resources there.</p><h2 id="rescheduling">Rescheduling</h2><p>When a node has been selected for a Pod with <code>WaitForFirstConsumer</code>
volumes, that decision is still tentative. The next step is that the
CSI storage driver gets asked to create the volume with a hint that the
volume is supposed to be available on the selected node.</p><p>Because Kubernetes might have chosen a node based on out-dated
capacity information, it is possible that the volume cannot really be
created. The node selection is then reset and the Kubernetes scheduler
tries again to find a node for the Pod.</p><h2 id="limitations">Limitations</h2><p>Storage capacity tracking increases the chance that scheduling works
on the first try, but cannot guarantee this because the scheduler has
to decide based on potentially out-dated information. Usually, the
same retry mechanism as for scheduling without any storage capacity
information handles scheduling failures.</p><p>One situation where scheduling can fail permanently is when a Pod uses
multiple volumes: one volume might have been created already in a
topology segment which then does not have enough capacity left for
another volume. Manual intervention is necessary to recover from this,
for example by increasing capacity or deleting the volume that was
already created.</p><h2 id="what-s-next">What's next</h2><ul><li>For more information on the design, see the
<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md">Storage Capacity Constraints for Pod Scheduling KEP</a>.</li></ul></div></div><div><div class="td-content"><h1>Node-specific Volume Limits</h1><p>This page describes the maximum number of volumes that can be attached
to a Node for various cloud providers.</p><p>Cloud providers like Google, Amazon, and Microsoft typically have a limit on
how many volumes can be attached to a Node. It is important for Kubernetes to
respect those limits. Otherwise, Pods scheduled on a Node could get stuck
waiting for volumes to attach.</p><h2 id="kubernetes-default-limits">Kubernetes default limits</h2><p>The Kubernetes scheduler has default limits on the number of volumes
that can be attached to a Node:</p><table><tr><th>Cloud service</th><th>Maximum volumes per Node</th></tr><tr><td><a href="https://aws.amazon.com/ebs/">Amazon Elastic Block Store (EBS)</a></td><td>39</td></tr><tr><td><a href="https://cloud.google.com/persistent-disk/">Google Persistent Disk</a></td><td>16</td></tr><tr><td><a href="https://azure.microsoft.com/en-us/services/storage/main-disks/">Microsoft Azure Disk Storage</a></td><td>16</td></tr></table><h2 id="dynamic-volume-limits">Dynamic volume limits</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.17 [stable]</code></div><p>Dynamic volume limits are supported for following volume types.</p><ul><li>Amazon EBS</li><li>Google Persistent Disk</li><li>Azure Disk</li><li>CSI</li></ul><p>For volumes managed by in-tree volume plugins, Kubernetes automatically determines the Node
type and enforces the appropriate maximum number of volumes for the node. For example:</p><ul><li><p>On
<a href="https://cloud.google.com/compute/">Google Compute Engine</a>,
up to 127 volumes can be attached to a node, <a href="https://cloud.google.com/compute/docs/disks/#pdnumberlimits">depending on the node
type</a>.</p></li><li><p>For Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only 25
volumes to be attached to a Node. For other instance types on
<a href="https://aws.amazon.com/ec2/">Amazon Elastic Compute Cloud (EC2)</a>,
Kubernetes allows 39 volumes to be attached to a Node.</p></li><li><p>On Azure, up to 64 disks can be attached to a node, depending on the node type. For more details, refer to <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes">Sizes for virtual machines in Azure</a>.</p></li><li><p>If a CSI storage driver advertises a maximum number of volumes for a Node (using <code>NodeGetInfo</code>), the <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">kube-scheduler</a> honors that limit.
Refer to the <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo">CSI specifications</a> for details.</p></li><li><p>For volumes managed by in-tree plugins that have been migrated to a CSI driver, the maximum number of volumes will be the one reported by the CSI driver.</p></li></ul><h3 id="mutable-csi-node-allocatable-count">Mutable CSI Node Allocatable Count</h3><div class="feature-state-notice feature-beta" title="Feature Gate: MutableCSINodeAllocatableCount"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: false)</div><p>CSI drivers can dynamically adjust the maximum number of volumes that can be attached to a Node at runtime. This enhances scheduling accuracy and reduces pod scheduling failures due to changes in resource availability.</p><p>To use this feature, you must enable the <code>MutableCSINodeAllocatableCount</code> feature gate on the following components:</p><ul><li><code>kube-apiserver</code></li><li><code>kubelet</code></li></ul><h4 id="periodic-updates">Periodic Updates</h4><p>When enabled, CSI drivers can request periodic updates to their volume limits by setting the <code>nodeAllocatableUpdatePeriodSeconds</code> field in the <code>CSIDriver</code> specification. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>storage.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CSIDriver<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>hostpath.csi.k8s.io<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>nodeAllocatableUpdatePeriodSeconds</span>:<span> </span><span>60</span><span>
</span></span></span></code></pre></div><p>Kubelet will periodically call the corresponding CSI driver&#8217;s <code>NodeGetInfo</code> endpoint to refresh the maximum number of attachable volumes, using the interval specified in <code>nodeAllocatableUpdatePeriodSeconds</code>. The minimum allowed value for this field is 10 seconds.</p><p>If a volume attachment operation fails with a <code>ResourceExhausted</code> error (gRPC code 8), Kubernetes triggers an immediate update to the allocatable volume count for that Node. Additionally, kubelet marks affected pods as Failed, allowing their controllers to handle recreation. This prevents pods from getting stuck indefinitely in the <code>ContainerCreating</code> state.</p></div></div><div><div class="td-content"><h1>Volume Health Monitoring</h1><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [alpha]</code></div><p><a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">CSI</a> volume health monitoring allows
CSI Drivers to detect abnormal volume conditions from the underlying storage systems
and report them as events on <a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank">PVCs</a>
or <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>.</p><h2 id="volume-health-monitoring">Volume health monitoring</h2><p>Kubernetes <em>volume health monitoring</em> is part of how Kubernetes implements the
Container Storage Interface (CSI). Volume health monitoring feature is implemented
in two components: an External Health Monitor controller, and the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a>.</p><p>If a CSI Driver supports Volume Health Monitoring feature from the controller side,
an event will be reported on the related
<a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank">PersistentVolumeClaim</a> (PVC)
when an abnormal volume condition is detected on a CSI volume.</p><p>The External Health Monitor <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>
also watches for node failure events. You can enable node failure monitoring by setting
the <code>enable-node-watcher</code> flag to true. When the external health monitor detects a node
failure event, the controller reports an Event will be reported on the PVC to indicate
that pods using this PVC are on a failed node.</p><p>If a CSI Driver supports Volume Health Monitoring feature from the node side,
an Event will be reported on every Pod using the PVC when an abnormal volume
condition is detected on a CSI volume. In addition, Volume Health information
is exposed as Kubelet VolumeStats metrics. A new metric kubelet_volume_stats_health_status_abnormal
is added. This metric includes two labels: <code>namespace</code> and <code>persistentvolumeclaim</code>.
The count is either 1 or 0. 1 indicates the volume is unhealthy, 0 indicates volume
is healthy. For more information, please check
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor#kubelet-metrics-changes">KEP</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You need to enable the <code>CSIVolumeHealth</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
to use this feature from the node side.</div><h2 id="what-s-next">What's next</h2><p>See the <a href="https://kubernetes-csi.github.io/docs/drivers.html">CSI driver documentation</a>
to find out which CSI drivers have implemented this feature.</p></div></div><div><div class="td-content"><h1>Windows Storage</h1><p>This page provides an storage overview specific to the Windows operating system.</p><h2 id="storage">Persistent storage</h2><p>Windows has a layered filesystem driver to mount container layers and create a copy
filesystem based on NTFS. All file paths in the container are resolved only within
the context of that container.</p><ul><li>With Docker, volume mounts can only target a directory in the container, and not
an individual file. This limitation does not apply to containerd.</li><li>Volume mounts cannot project files or directories back to the host filesystem.</li><li>Read-only filesystems are not supported because write access is always required
for the Windows registry and SAM database. However, read-only volumes are supported.</li><li>Volume user-masks and permissions are not available. Because the SAM is not shared
between the host &amp; container, there's no mapping between them. All permissions are
resolved within the context of the container.</li></ul><p>As a result, the following storage functionality is not supported on Windows nodes:</p><ul><li>Volume subpath mounts: only the entire volume can be mounted in a Windows container</li><li>Subpath volume mounting for Secrets</li><li>Host mount projection</li><li>Read-only root filesystem (mapped volumes still support <code>readOnly</code>)</li><li>Block device mapping</li><li>Memory as the storage medium (for example, <code>emptyDir.medium</code> set to <code>Memory</code>)</li><li>File system features like uid/gid; per-user Linux filesystem permissions</li><li>Setting <a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#set-posix-permissions-for-secret-keys">secret permissions with DefaultMode</a> (due to UID/GID dependency)</li><li>NFS based storage/volume support</li><li>Expanding the mounted volume (resizefs)</li></ul><p>Kubernetes <a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">volumes</a> enable complex
applications, with data persistence and Pod volume sharing requirements, to be deployed
on Kubernetes. Management of persistent volumes associated with a specific storage
back-end or protocol includes actions such as provisioning/de-provisioning/resizing
of volumes, attaching/detaching a volume to/from a Kubernetes node and
mounting/dismounting a volume to/from individual containers in a pod that needs to
persist data.</p><p>Volume management components are shipped as Kubernetes volume
<a href="/docs/concepts/storage/volumes/#volume-types">plugin</a>.
The following broad classes of Kubernetes volume plugins are supported on Windows:</p><ul><li><a href="/docs/concepts/storage/volumes/#flexvolume"><code>FlexVolume plugins</code></a><ul><li>Please note that FlexVolumes have been deprecated as of 1.23</li></ul></li><li><a href="/docs/concepts/storage/volumes/#csi"><code>CSI Plugins</code></a></li></ul><h5 id="in-tree-volume-plugins">In-tree volume plugins</h5><p>The following in-tree plugins support persistent storage on Windows nodes:</p><ul><li><a href="/docs/concepts/storage/volumes/#azurefile"><code>azureFile</code></a></li><li><a href="/docs/concepts/storage/volumes/#vspherevolume"><code>vsphereVolume</code></a></li></ul></div></div><div><div class="td-content"><h1>Configuration</h1><div class="lead">Resources that Kubernetes provides for configuring Pods.</div><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/concepts/configuration/overview/">Configuration Best Practices</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/configuration/configmap/">ConfigMaps</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/configuration/secret/">Secrets</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/configuration/liveness-readiness-startup-probes/">Liveness, Readiness, and Startup Probes</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/configuration/manage-resources-containers/">Resource Management for Pods and Containers</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/configuration/organize-cluster-access-kubeconfig/">Organizing Cluster Access Using kubeconfig Files</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/configuration/windows-resource-management/">Resource Management for Windows nodes</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Configuration Best Practices</h1><p>This document highlights and consolidates configuration best practices that are introduced
throughout the user guide, Getting Started documentation, and examples.</p><p>This is a living document. If you think of something that is not on this list but might be useful
to others, please don't hesitate to file an issue or submit a PR.</p><h2 id="general-configuration-tips">General Configuration Tips</h2><ul><li><p>When defining configurations, specify the latest stable API version.</p></li><li><p>Configuration files should be stored in version control before being pushed to the cluster. This
allows you to quickly roll back a configuration change if necessary. It also aids cluster
re-creation and restoration.</p></li><li><p>Write your configuration files using YAML rather than JSON. Though these formats can be used
interchangeably in almost all scenarios, YAML tends to be more user-friendly.</p></li><li><p>Group related objects into a single file whenever it makes sense. One file is often easier to
manage than several. See the
<a href="https://github.com/kubernetes/examples/tree/master/web/guestbook/all-in-one/guestbook-all-in-one.yaml">guestbook-all-in-one.yaml</a>
file as an example of this syntax.</p></li><li><p>Note also that many <code>kubectl</code> commands can be called on a directory. For example, you can call
<code>kubectl apply</code> on a directory of config files.</p></li><li><p>Don't specify default values unnecessarily: simple, minimal configuration will make errors less likely.</p></li><li><p>Put object descriptions in annotations, to allow better introspection.</p></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>There is a breaking change introduced in the <a href="https://yaml.org/spec/1.2.0/#id2602744">YAML 1.2</a>
boolean values specification with respect to <a href="https://yaml.org/spec/1.1/#id864510">YAML 1.1</a>.
This is a known <a href="https://github.com/kubernetes/kubernetes/issues/34146">issue</a> in Kubernetes.
YAML 1.2 only recognizes <strong>true</strong> and <strong>false</strong> as valid booleans, while YAML 1.1 also accepts
<strong>yes</strong>, <strong>no</strong>, <strong>on</strong>, and <strong>off</strong> as booleans. However, Kubernetes uses YAML
<a href="https://github.com/kubernetes/kubernetes/issues/34146#issuecomment-252692024">parsers</a> that are
mostly compatible with YAML 1.1, which means that using <strong>yes</strong> or <strong>no</strong> instead of <strong>true</strong> or
<strong>false</strong> in a YAML manifest may cause unexpected errors or behaviors. To avoid this issue, it is
recommended to always use <strong>true</strong> or <strong>false</strong> for boolean values in YAML manifests, and to quote
any strings that may be confused with booleans, such as <strong>"yes"</strong> or <strong>"no"</strong>.</p><p>Besides booleans, there are additional specifications changes between YAML versions. Please refer
to the <a href="https://spec.yaml.io/main/spec/1.2.2/ext/changes">YAML Specification Changes</a> documentation
for a comprehensive list.</p></div><h2 id="naked-pods-vs-replicasets-deployments-and-jobs">"Naked" Pods versus ReplicaSets, Deployments, and Jobs</h2><ul><li><p>Don't use naked Pods (that is, Pods not bound to a <a href="/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> or
<a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a>) if you can avoid it. Naked Pods
will not be rescheduled in the event of a node failure.</p><p>A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is
always available, and specifies a strategy to replace Pods (such as
<a href="/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment">RollingUpdate</a>), is
almost always preferable to creating Pods directly, except for some explicit
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>restartPolicy: Never</code></a> scenarios.
A <a href="/docs/concepts/workloads/controllers/job/">Job</a> may also be appropriate.</p></li></ul><h2 id="services">Services</h2><ul><li><p>Create a <a href="/docs/concepts/services-networking/service/">Service</a> before its corresponding backend
workloads (Deployments or ReplicaSets), and before any workloads that need to access it.
When Kubernetes starts a container, it provides environment variables pointing to all the Services
which were running when the container was started. For example, if a Service named <code>foo</code> exists,
all containers will get the following variables in their initial environment:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span>FOO_SERVICE_HOST</span><span>=</span>&lt;the host the Service is running on&gt;
</span></span><span><span><span>FOO_SERVICE_PORT</span><span>=</span>&lt;the port the Service is running on&gt;
</span></span></code></pre></div><p><em>This does imply an ordering requirement</em> - any <code>Service</code> that a <code>Pod</code> wants to access must be
created before the <code>Pod</code> itself, or else the environment variables will not be populated.
DNS does not have this restriction.</p></li><li><p>An optional (though strongly recommended) <a href="/docs/concepts/cluster-administration/addons/">cluster add-on</a>
is a DNS server. The DNS server watches the Kubernetes API for new <code>Services</code> and creates a set
of DNS records for each. If DNS has been enabled throughout the cluster then all <code>Pods</code> should be
able to do name resolution of <code>Services</code> automatically.</p></li><li><p>Don't specify a <code>hostPort</code> for a Pod unless it is absolutely necessary. When you bind a Pod to a
<code>hostPort</code>, it limits the number of places the Pod can be scheduled, because each &lt;<code>hostIP</code>,
<code>hostPort</code>, <code>protocol</code>&gt; combination must be unique. If you don't specify the <code>hostIP</code> and
<code>protocol</code> explicitly, Kubernetes will use <code>0.0.0.0</code> as the default <code>hostIP</code> and <code>TCP</code> as the
default <code>protocol</code>.</p><p>If you only need access to the port for debugging purposes, you can use the
<a href="/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls">apiserver proxy</a>
or <a href="/docs/tasks/access-application-cluster/port-forward-access-application-cluster/"><code>kubectl port-forward</code></a>.</p><p>If you explicitly need to expose a Pod's port on the node, consider using a
<a href="/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> Service before resorting to
<code>hostPort</code>.</p></li><li><p>Avoid using <code>hostNetwork</code>, for the same reasons as <code>hostPort</code>.</p></li><li><p>Use <a href="/docs/concepts/services-networking/service/#headless-services">headless Services</a>
(which have a <code>ClusterIP</code> of <code>None</code>) for service discovery when you don't need <code>kube-proxy</code>
load balancing.</p></li></ul><h2 id="using-labels">Using Labels</h2><ul><li><p>Define and use <a href="/docs/concepts/overview/working-with-objects/labels/">labels</a> that identify
<strong>semantic attributes</strong> of your application or Deployment, such as <code>{ app.kubernetes.io/name: MyApp, tier: frontend, phase: test, deployment: v3 }</code>. You can use these labels to select the
appropriate Pods for other resources; for example, a Service that selects all <code>tier: frontend</code>
Pods, or all <code>phase: test</code> components of <code>app.kubernetes.io/name: MyApp</code>.
See the <a href="https://github.com/kubernetes/examples/tree/master/web/guestbook/">guestbook</a> app
for examples of this approach.</p><p>A Service can be made to span multiple Deployments by omitting release-specific labels from its
selector. When you need to update a running service without downtime, use a
<a href="/docs/concepts/workloads/controllers/deployment/">Deployment</a>.</p><p>A desired state of an object is described by a Deployment, and if changes to that spec are
<em>applied</em>, the deployment controller changes the actual state to the desired state at a controlled
rate.</p></li><li><p>Use the <a href="/docs/concepts/overview/working-with-objects/common-labels/">Kubernetes common labels</a>
for common use cases. These standardized labels enrich the metadata in a way that allows tools,
including <code>kubectl</code> and <a href="/docs/tasks/access-application-cluster/web-ui-dashboard/">dashboard</a>, to
work in an interoperable way.</p></li><li><p>You can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) and
Services match to Pods using selector labels, removing the relevant labels from a Pod will stop
it from being considered by a controller or from being served traffic by a Service. If you remove
the labels of an existing Pod, its controller will create a new Pod to take its place. This is a
useful way to debug a previously "live" Pod in a "quarantine" environment. To interactively remove
or add labels, use <a href="/docs/reference/generated/kubectl/kubectl-commands#label"><code>kubectl label</code></a>.</p></li></ul><h2 id="using-kubectl">Using kubectl</h2><ul><li><p>Use <code>kubectl apply -f &lt;directory&gt;</code>. This looks for Kubernetes configuration in all <code>.yaml</code>,
<code>.yml</code>, and <code>.json</code> files in <code>&lt;directory&gt;</code> and passes it to <code>apply</code>.</p></li><li><p>Use label selectors for <code>get</code> and <code>delete</code> operations instead of specific object names. See the
sections on <a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selectors</a>
and <a href="/docs/concepts/overview/working-with-objects/labels/#using-labels-effectively">using labels effectively</a>.</p></li><li><p>Use <code>kubectl create deployment</code> and <code>kubectl expose</code> to quickly create single-container
Deployments and Services.
See <a href="/docs/tasks/access-application-cluster/service-access-application-cluster/">Use a Service to Access an Application in a Cluster</a>
for an example.</p></li></ul></div></div><div><div class="td-content"><h1>ConfigMaps</h1><p><p>A ConfigMap is an API object used to store non-confidential data in key-value pairs.
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> can consume ConfigMaps as
environment variables, command-line arguments, or as configuration files in a
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">volume</a>.</p></p><p>A ConfigMap allows you to decouple environment-specific configuration from your <a class="glossary-tooltip" title="Stored instance of a container that holds a set of software needed to run an application." href="/docs/reference/glossary/?all=true#term-image" target="_blank">container images</a>, so that your applications are easily portable.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>ConfigMap does not provide secrecy or encryption.
If the data you want to store are confidential, use a
<a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a> rather than a ConfigMap,
or use additional (third party) tools to keep your data private.</div><h2 id="motivation">Motivation</h2><p>Use a ConfigMap for setting configuration data separately from application code.</p><p>For example, imagine that you are developing an application that you can run on your
own computer (for development) and in the cloud (to handle real traffic).
You write the code to look in an environment variable named <code>DATABASE_HOST</code>.
Locally, you set that variable to <code>localhost</code>. In the cloud, you set it to
refer to a Kubernetes <a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Service</a>
that exposes the database component to your cluster.
This lets you fetch a container image running in the cloud and
debug the exact same code locally if needed.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A ConfigMap is not designed to hold large chunks of data. The data stored in a
ConfigMap cannot exceed 1 MiB. If you need to store settings that are
larger than this limit, you may want to consider mounting a volume or use a
separate database or file service.</div><h2 id="configmap-object">ConfigMap object</h2><p>A ConfigMap is an <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">API object</a>
that lets you store configuration for other objects to use. Unlike most
Kubernetes objects that have a <code>spec</code>, a ConfigMap has <code>data</code> and <code>binaryData</code>
fields. These fields accept key-value pairs as their values. Both the <code>data</code>
field and the <code>binaryData</code> are optional. The <code>data</code> field is designed to
contain UTF-8 strings while the <code>binaryData</code> field is designed to
contain binary data as base64-encoded strings.</p><p>The name of a ConfigMap must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>Each key under the <code>data</code> or the <code>binaryData</code> field must consist of
alphanumeric characters, <code>-</code>, <code>_</code> or <code>.</code>. The keys stored in <code>data</code> must not
overlap with the keys in the <code>binaryData</code> field.</p><p>Starting from v1.19, you can add an <code>immutable</code> field to a ConfigMap
definition to create an <a href="#configmap-immutable">immutable ConfigMap</a>.</p><h2 id="configmaps-and-pods">ConfigMaps and Pods</h2><p>You can write a Pod <code>spec</code> that refers to a ConfigMap and configures the container(s)
in that Pod based on the data in the ConfigMap. The Pod and the ConfigMap must be in
the same <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>spec</code> of a <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static Pod</a> cannot refer to a ConfigMap
or any other API objects.</div><p>Here's an example ConfigMap that has some keys with single values,
and other keys where the value looks like a fragment of a configuration
format.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>game-demo<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span># property-like keys; each key maps to a simple value</span><span>
</span></span></span><span><span><span>  </span><span>player_initial_lives</span>:<span> </span><span>"3"</span><span>
</span></span></span><span><span><span>  </span><span>ui_properties_file_name</span>:<span> </span><span>"user-interface.properties"</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span># file-like keys</span><span>
</span></span></span><span><span><span>  </span><span>game.properties</span>:<span> </span>|<span>
</span></span></span><span><span><span>    enemy.types=aliens,monsters
</span></span></span><span><span><span>    player.maximum-lives=5</span><span>    
</span></span></span><span><span><span>  </span><span>user-interface.properties</span>:<span> </span>|<span>
</span></span></span><span><span><span>    color.good=purple
</span></span></span><span><span><span>    color.bad=yellow
</span></span></span><span><span><span>    allow.textmode=true</span><span>    
</span></span></span></code></pre></div><p>There are four different ways that you can use a ConfigMap to configure
a container inside a Pod:</p><ol><li>Inside a container command and args</li><li>Environment variables for a container</li><li>Add a file in read-only volume, for the application to read</li><li>Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap</li></ol><p>These different methods lend themselves to different ways of modeling
the data being consumed.
For the first three methods, the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> uses the data from
the ConfigMap when it launches container(s) for a Pod.</p><p>The fourth method means you have to write code to read the ConfigMap and its data.
However, because you're using the Kubernetes API directly, your application can
subscribe to get updates whenever the ConfigMap changes, and react
when that happens. By accessing the Kubernetes API directly, this
technique also lets you access a ConfigMap in a different namespace.</p><p>Here's an example Pod that uses values from <code>game-demo</code> to configure a Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/configmap/configure-pod.yaml"><code>configmap/configure-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy configmap/configure-pod.yaml to clipboard"></div><div class="includecode" id="configmap-configure-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>configmap-demo-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>demo<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>alpine<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span>"sleep"</span>,<span> </span><span>"3600"</span>]<span>
</span></span></span><span><span><span>      </span><span>env</span>:<span>
</span></span></span><span><span><span>        </span><span># Define the environment variable</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>PLAYER_INITIAL_LIVES<span> </span><span># Notice that the case is different here</span><span>
</span></span></span><span><span><span>                                     </span><span># from the key name in the ConfigMap.</span><span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>configMapKeyRef</span>:<span>
</span></span></span><span><span><span>              </span><span>name</span>:<span> </span>game-demo          <span> </span><span># The ConfigMap this value comes from.</span><span>
</span></span></span><span><span><span>              </span><span>key</span>:<span> </span>player_initial_lives<span> </span><span># The key to fetch.</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>UI_PROPERTIES_FILE_NAME<span>
</span></span></span><span><span><span>          </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>            </span><span>configMapKeyRef</span>:<span>
</span></span></span><span><span><span>              </span><span>name</span>:<span> </span>game-demo<span>
</span></span></span><span><span><span>              </span><span>key</span>:<span> </span>ui_properties_file_name<span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>config<span>
</span></span></span><span><span><span>        </span><span>mountPath</span>:<span> </span><span>"/config"</span><span>
</span></span></span><span><span><span>        </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span><span># You set volumes at the Pod level, then mount them into containers inside that Pod</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>config<span>
</span></span></span><span><span><span>    </span><span>configMap</span>:<span>
</span></span></span><span><span><span>      </span><span># Provide the name of the ConfigMap you want to mount.</span><span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>game-demo<span>
</span></span></span><span><span><span>      </span><span># An array of keys from the ConfigMap to create as files</span><span>
</span></span></span><span><span><span>      </span><span>items</span>:<span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span><span>"game.properties"</span><span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span><span>"game.properties"</span><span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span><span>"user-interface.properties"</span><span>
</span></span></span><span><span><span>        </span><span>path</span>:<span> </span><span>"user-interface.properties"</span><span>
</span></span></span><span><span><span>        </span></span></span></code></pre></div></div></div><p>A ConfigMap doesn't differentiate between single line property values and
multi-line file-like values.
What matters is how Pods and other objects consume those values.</p><p>For this example, defining a volume and mounting it inside the <code>demo</code>
container as <code>/config</code> creates two files,
<code>/config/game.properties</code> and <code>/config/user-interface.properties</code>,
even though there are four keys in the ConfigMap. This is because the Pod
definition specifies an <code>items</code> array in the <code>volumes</code> section.
If you omit the <code>items</code> array entirely, every key in the ConfigMap becomes
a file with the same name as the key, and you get 4 files.</p><h2 id="using-configmaps">Using ConfigMaps</h2><p>ConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other
parts of the system, without being directly exposed to the Pod. For example,
ConfigMaps can hold data that other parts of the system should use for configuration.</p><p>The most common way to use ConfigMaps is to configure settings for
containers running in a Pod in the same namespace. You can also use a
ConfigMap separately.</p><p>For example, you
might encounter <a class="glossary-tooltip" title="Resources that extend the functionality of Kubernetes." href="/docs/concepts/cluster-administration/addons/" target="_blank">addons</a>
or <a class="glossary-tooltip" title="A specialized controller used to manage a custom resource" href="/docs/concepts/extend-kubernetes/operator/" target="_blank">operators</a> that
adjust their behavior based on a ConfigMap.</p><h3 id="using-configmaps-as-files-from-a-pod">Using ConfigMaps as files from a Pod</h3><p>To consume a ConfigMap in a volume in a Pod:</p><ol><li>Create a ConfigMap or use an existing one. Multiple Pods can reference the
same ConfigMap.</li><li>Modify your Pod definition to add a volume under <code>.spec.volumes[]</code>. Name
the volume anything, and have a <code>.spec.volumes[].configMap.name</code> field set
to reference your ConfigMap object.</li><li>Add a <code>.spec.containers[].volumeMounts[]</code> to each container that needs the
ConfigMap. Specify <code>.spec.containers[].volumeMounts[].readOnly = true</code> and
<code>.spec.containers[].volumeMounts[].mountPath</code> to an unused directory name
where you would like the ConfigMap to appear.</li><li>Modify your image or command line so that the program looks for files in
that directory. Each key in the ConfigMap <code>data</code> map becomes the filename
under <code>mountPath</code>.</li></ol><p>This is an example of a Pod that mounts a ConfigMap in a volume:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>redis<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>foo<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/etc/foo"</span><span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>foo<span>
</span></span></span><span><span><span>    </span><span>configMap</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>myconfigmap<span>
</span></span></span></code></pre></div><p>Each ConfigMap you want to use needs to be referred to in <code>.spec.volumes</code>.</p><p>If there are multiple containers in the Pod, then each container needs its
own <code>volumeMounts</code> block, but only one <code>.spec.volumes</code> is needed per ConfigMap.</p><h4 id="mounted-configmaps-are-updated-automatically">Mounted ConfigMaps are updated automatically</h4><p>When a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well.
The kubelet checks whether the mounted ConfigMap is fresh on every periodic sync.
However, the kubelet uses its local cache for getting the current value of the ConfigMap.
The type of the cache is configurable using the <code>configMapAndSecretChangeDetectionStrategy</code> field in
the <a href="/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration struct</a>.
A ConfigMap can be either propagated by watch (default), ttl-based, or by redirecting
all requests directly to the API server.
As a result, the total delay from the moment when the ConfigMap is updated to the moment
when new keys are projected to the Pod can be as long as the kubelet sync period + cache
propagation delay, where the cache propagation delay depends on the chosen cache type
(it equals to watch propagation delay, ttl of cache, or zero correspondingly).</p><p>ConfigMaps consumed as environment variables are not updated automatically and require a pod restart.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A container using a ConfigMap as a <a href="/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume mount will not receive ConfigMap updates.</div><h3 id="using-configmaps-as-environment-variables">Using Configmaps as environment variables</h3><p>To use a Configmap in an <a class="glossary-tooltip" title="Container environment variables are name=value pairs that provide useful information into containers running in a Pod." href="/docs/concepts/containers/container-environment/" target="_blank">environment variable</a>
in a Pod:</p><ol><li>For each container in your Pod specification, add an environment variable
for each Configmap key that you want to use to the
<code>env[].valueFrom.configMapKeyRef</code> field.</li><li>Modify your image and/or command line so that the program looks for values
in the specified environment variables.</li></ol><p>This is an example of defining a ConfigMap as a pod environment variable:</p><p>The following ConfigMap (myconfigmap.yaml) stores two properties: username and access_level:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>myconfigmap<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>username</span>:<span> </span>k8s-admin<span>
</span></span></span><span><span><span>  </span><span>access_level</span>:<span> </span><span>"1"</span><span>
</span></span></span></code></pre></div><p>The following command will create the ConfigMap object:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f myconfigmap.yaml
</span></span></code></pre></div><p>The following Pod consumes the content of the ConfigMap as environment variables:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/configmap/env-configmap.yaml"><code>configmap/env-configmap.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy configmap/env-configmap.yaml to clipboard"></div><div class="includecode" id="configmap-env-configmap-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>env-configmap<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>app<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span> </span>[<span>"/bin/sh"</span>,<span> </span><span>"-c"</span>,<span> </span><span>"printenv"</span>]<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>busybox:latest<span>
</span></span></span><span><span><span>      </span><span>envFrom</span>:<span>
</span></span></span><span><span><span>        </span>- <span>configMapRef</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>myconfigmap<span>
</span></span></span></code></pre></div></div></div><p>The <code>envFrom</code> field instructs Kubernetes to create environment variables from the sources nested within it.
The inner <code>configMapRef</code> refers to a ConfigMap by its name and selects all its key-value pairs.
Add the Pod to your cluster, then retrieve its logs to see the output from the printenv command.
This should confirm that the two key-value pairs from the ConfigMap have been set as environment variables:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f env-configmap.yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs pod/ env-configmap
</span></span></code></pre></div><p>The output is similar to this:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>...
</span></span></span><span><span><span>username: "k8s-admin"
</span></span></span><span><span><span>access_level: "1"
</span></span></span><span><span><span>...
</span></span></span></code></pre></div><p>Sometimes a Pod won't require access to all the values in a ConfigMap.
For example, you could have another Pod which only uses the username value from the ConfigMap.
For this use case, you can use the <code>env.valueFrom</code> syntax instead, which lets you select individual keys in
a ConfigMap. The name of the environment variable can also be different from the key within the ConfigMap.
For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>env-configmap<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>envars-test-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>env</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>CONFIGMAP_USERNAME<span>
</span></span></span><span><span><span>      </span><span>valueFrom</span>:<span>
</span></span></span><span><span><span>        </span><span>configMapKeyRef</span>:<span>
</span></span></span><span><span><span>          </span><span>name</span>:<span> </span>myconfigmap<span>
</span></span></span><span><span><span>          </span><span>key</span>:<span> </span>username<span>
</span></span></span></code></pre></div><p>In the Pod created from this manifest, you will see that the environment variable
<code>CONFIGMAP_USERNAME</code> is set to the value of the <code>username</code> value from the ConfigMap.
Other keys from the ConfigMap data are not copied into the environment.</p><p>It's important to note that the range of characters allowed for environment
variable names in pods is <a href="/docs/tasks/inject-data-application/define-environment-variable-container/#using-environment-variables-inside-of-your-config">restricted</a>.
If any keys do not meet the rules, those keys are not made available to your container, though
the Pod is allowed to start.</p><h2 id="configmap-immutable">Immutable ConfigMaps</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>The Kubernetes feature <em>Immutable Secrets and ConfigMaps</em> provides an option to set
individual Secrets and ConfigMaps as immutable. For clusters that extensively use ConfigMaps
(at least tens of thousands of unique ConfigMap to Pod mounts), preventing changes to their
data has the following advantages:</p><ul><li>protects you from accidental (or unwanted) updates that could cause applications outages</li><li>improves performance of your cluster by significantly reducing load on kube-apiserver, by
closing watches for ConfigMaps marked as immutable.</li></ul><p>You can create an immutable ConfigMap by setting the <code>immutable</code> field to <code>true</code>.
For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span><span><span><span></span><span>immutable</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><p>Once a ConfigMap is marked as immutable, it is <em>not</em> possible to revert this change
nor to mutate the contents of the <code>data</code> or the <code>binaryData</code> field. You can
only delete and recreate the ConfigMap. Because existing Pods maintain a mount point
to the deleted ConfigMap, it is recommended to recreate these pods.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/configuration/secret/">Secrets</a>.</li><li>Read <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">Configure a Pod to Use a ConfigMap</a>.</li><li>Read about <a href="/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">changing a ConfigMap (or any other Kubernetes object)</a></li><li>Read <a href="https://12factor.net/">The Twelve-Factor App</a> to understand the motivation for
separating code from configuration.</li></ul></div></div><div><div class="td-content"><h1>Secrets</h1><p>A Secret is an object that contains a small amount of sensitive data such as
a password, a token, or a key. Such information might otherwise be put in a
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a> specification or in a
<a class="glossary-tooltip" title="Stored instance of a container that holds a set of software needed to run an application." href="/docs/reference/glossary/?all=true#term-image" target="_blank">container image</a>. Using a
Secret means that you don't need to include confidential data in your
application code.</p><p>Because Secrets can be created independently of the Pods that use them, there
is less risk of the Secret (and its data) being exposed during the workflow of
creating, viewing, and editing Pods. Kubernetes, and applications that run in
your cluster, can also take additional precautions with Secrets, such as avoiding
writing sensitive data to nonvolatile storage.</p><p>Secrets are similar to <a class="glossary-tooltip" title="An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume." href="/docs/concepts/configuration/configmap/" target="_blank">ConfigMaps</a>
but are specifically intended to hold confidential data.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>Kubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store
(etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd.
Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read
any Secret in that namespace; this includes indirect access such as the ability to create a
Deployment.</p><p>In order to safely use Secrets, take at least the following steps:</p><ol><li><a href="/docs/tasks/administer-cluster/encrypt-data/">Enable Encryption at Rest</a> for Secrets.</li><li><a href="/docs/reference/access-authn-authz/authorization/">Enable or configure RBAC rules</a> with
least-privilege access to Secrets.</li><li>Restrict Secret access to specific containers.</li><li><a href="https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver">Consider using external Secret store providers</a>.</li></ol><p>For more guidelines to manage and improve the security of your Secrets, refer to
<a href="/docs/concepts/security/secrets-good-practices/">Good practices for Kubernetes Secrets</a>.</p></div><p>See <a href="#information-security-for-secrets">Information security for Secrets</a> for more details.</p><h2 id="uses-for-secrets">Uses for Secrets</h2><p>You can use Secrets for purposes such as the following:</p><ul><li><a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data">Set environment variables for a container</a>.</li><li><a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#provide-prod-test-creds">Provide credentials such as SSH keys or passwords to Pods</a>.</li><li><a href="/docs/tasks/configure-pod-container/pull-image-private-registry/">Allow the kubelet to pull container images from private registries</a>.</li></ul><p>The Kubernetes control plane also uses Secrets; for example,
<a href="#bootstrap-token-secrets">bootstrap token Secrets</a> are a mechanism to
help automate node registration.</p><h3 id="use-case-dotfiles-in-a-secret-volume">Use case: dotfiles in a secret volume</h3><p>You can make your data "hidden" by defining a key that begins with a dot.
This key represents a dotfile or "hidden" file. For example, when the following Secret
is mounted into a volume, <code>secret-volume</code>, the volume will contain a single file,
called <code>.secret-file</code>, and the <code>dotfile-test-container</code> will have this file
present at the path <code>/etc/secret-volume/.secret-file</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Files beginning with dot characters are hidden from the output of <code>ls -l</code>;
you must use <code>ls -la</code> to see them when listing directory contents.</div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/dotfile-secret.yaml"><code>secret/dotfile-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy secret/dotfile-secret.yaml to clipboard"></div><div class="includecode" id="secret-dotfile-secret-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>dotfile-secret<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>.secret-file</span>:<span> </span>dmFsdWUtMg0KDQo=<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>secret-dotfiles-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>secret-volume<span>
</span></span></span><span><span><span>      </span><span>secret</span>:<span>
</span></span></span><span><span><span>        </span><span>secretName</span>:<span> </span>dotfile-secret<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>dotfile-test-container<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/busybox<span>
</span></span></span><span><span><span>      </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- ls<span>
</span></span></span><span><span><span>        </span>- <span>"-l"</span><span>
</span></span></span><span><span><span>        </span>- <span>"/etc/secret-volume"</span><span>
</span></span></span><span><span><span>      </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>secret-volume<span>
</span></span></span><span><span><span>          </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>          </span><span>mountPath</span>:<span> </span><span>"/etc/secret-volume"</span></span></span></code></pre></div></div></div><h3 id="use-case-secret-visible-to-one-container-in-a-pod">Use case: Secret visible to one container in a Pod</h3><p>Consider a program that needs to handle HTTP requests, do some complex business
logic, and then sign some messages with an HMAC. Because it has complex
application logic, there might be an unnoticed remote file reading exploit in
the server, which could expose the private key to an attacker.</p><p>This could be divided into two processes in two containers: a frontend container
which handles user interaction and business logic, but which cannot see the
private key; and a signer container that can see the private key, and responds
to simple signing requests from the frontend (for example, over localhost networking).</p><p>With this partitioned approach, an attacker now has to trick the application
server into doing something rather arbitrary, which may be harder than getting
it to read a file.</p><h3 id="alternatives-to-secrets">Alternatives to Secrets</h3><p>Rather than using a Secret to protect confidential data, you can pick from alternatives.</p><p>Here are some of your options:</p><ul><li>If your cloud-native component needs to authenticate to another application that you
know is running within the same Kubernetes cluster, you can use a
<a href="/docs/reference/access-authn-authz/authentication/#service-account-tokens">ServiceAccount</a>
and its tokens to identify your client.</li><li>There are third-party tools that you can run, either within or outside your cluster,
that manage sensitive data. For example, a service that Pods access over HTTPS,
that reveals a Secret if the client correctly authenticates (for example, with a ServiceAccount
token).</li><li>For authentication, you can implement a custom signer for X.509 certificates, and use
<a href="/docs/reference/access-authn-authz/certificate-signing-requests/">CertificateSigningRequests</a>
to let that custom signer issue certificates to Pods that need them.</li><li>You can use a <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">device plugin</a>
to expose node-local encryption hardware to a specific Pod. For example, you can schedule
trusted Pods onto nodes that provide a Trusted Platform Module, configured out-of-band.</li></ul><p>You can also combine two or more of those options, including the option to use Secret objects themselves.</p><p>For example: implement (or deploy) an <a class="glossary-tooltip" title="A specialized controller used to manage a custom resource" href="/docs/concepts/extend-kubernetes/operator/" target="_blank">operator</a>
that fetches short-lived session tokens from an external service, and then creates Secrets based
on those short-lived session tokens. Pods running in your cluster can make use of the session tokens,
and operator ensures they are valid. This separation means that you can run Pods that are unaware of
the exact mechanisms for issuing and refreshing those session tokens.</p><h2 id="secret-types">Types of Secret</h2><p>When creating a Secret, you can specify its type using the <code>type</code> field of
the <a href="/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/">Secret</a>
resource, or certain equivalent <code>kubectl</code> command line flags (if available).
The Secret type is used to facilitate programmatic handling of the Secret data.</p><p>Kubernetes provides several built-in types for some common usage scenarios.
These types vary in terms of the validations performed and the constraints
Kubernetes imposes on them.</p><table><thead><tr><th>Built-in Type</th><th>Usage</th></tr></thead><tbody><tr><td><code>Opaque</code></td><td>arbitrary user-defined data</td></tr><tr><td><code>kubernetes.io/service-account-token</code></td><td>ServiceAccount token</td></tr><tr><td><code>kubernetes.io/dockercfg</code></td><td>serialized <code>~/.dockercfg</code> file</td></tr><tr><td><code>kubernetes.io/dockerconfigjson</code></td><td>serialized <code>~/.docker/config.json</code> file</td></tr><tr><td><code>kubernetes.io/basic-auth</code></td><td>credentials for basic authentication</td></tr><tr><td><code>kubernetes.io/ssh-auth</code></td><td>credentials for SSH authentication</td></tr><tr><td><code>kubernetes.io/tls</code></td><td>data for a TLS client or server</td></tr><tr><td><code>bootstrap.kubernetes.io/token</code></td><td>bootstrap token data</td></tr></tbody></table><p>You can define and use your own Secret type by assigning a non-empty string as the
<code>type</code> value for a Secret object (an empty string is treated as an <code>Opaque</code> type).</p><p>Kubernetes doesn't impose any constraints on the type name. However, if you
are using one of the built-in types, you must meet all the requirements defined
for that type.</p><p>If you are defining a type of Secret that's for public use, follow the convention
and structure the Secret type to have your domain name before the name, separated
by a <code>/</code>. For example: <code>cloud-hosting.example.net/cloud-api-credentials</code>.</p><h3 id="opaque-secrets">Opaque Secrets</h3><p><code>Opaque</code> is the default Secret type if you don't explicitly specify a type in
a Secret manifest. When you create a Secret using <code>kubectl</code>, you must use the
<code>generic</code> subcommand to indicate an <code>Opaque</code> Secret type. For example, the
following command creates an empty Secret of type <code>Opaque</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret generic empty-secret
</span></span><span><span>kubectl get secret empty-secret
</span></span></code></pre></div><p>The output looks like:</p><pre tabindex="0"><code>NAME           TYPE     DATA   AGE
empty-secret   Opaque   0      2m6s
</code></pre><p>The <code>DATA</code> column shows the number of data items stored in the Secret.
In this case, <code>0</code> means you have created an empty Secret.</p><h3 id="serviceaccount-token-secrets">ServiceAccount token Secrets</h3><p>A <code>kubernetes.io/service-account-token</code> type of Secret is used to store a
token credential that identifies a
<a class="glossary-tooltip" title="Provides an identity for processes that run in a Pod." href="/docs/tasks/configure-pod-container/configure-service-account/" target="_blank">ServiceAccount</a>. This
is a legacy mechanism that provides long-lived ServiceAccount credentials to
Pods.</p><p>In Kubernetes v1.22 and later, the recommended approach is to obtain a
short-lived, automatically rotating ServiceAccount token by using the
<a href="/docs/reference/kubernetes-api/authentication-resources/token-request-v1/"><code>TokenRequest</code></a>
API instead. You can get these short-lived tokens using the following methods:</p><ul><li>Call the <code>TokenRequest</code> API either directly or by using an API client like
<code>kubectl</code>. For example, you can use the
<a href="/docs/reference/generated/kubectl/kubectl-commands#-em-token-em-"><code>kubectl create token</code></a>
command.</li><li>Request a mounted token in a
<a href="/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume">projected volume</a>
in your Pod manifest. Kubernetes creates the token and mounts it in the Pod.
The token is automatically invalidated when the Pod that it's mounted in is
deleted. For details, see
<a href="/docs/tasks/configure-pod-container/configure-service-account/#launch-a-pod-using-service-account-token-projection">Launch a Pod using service account token projection</a>.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You should only create a ServiceAccount token Secret
if you can't use the <code>TokenRequest</code> API to obtain a token,
and the security exposure of persisting a non-expiring token credential
in a readable API object is acceptable to you. For instructions, see
<a href="/docs/tasks/configure-pod-container/configure-service-account/#manually-create-an-api-token-for-a-serviceaccount">Manually create a long-lived API token for a ServiceAccount</a>.</div><p>When using this Secret type, you need to ensure that the
<code>kubernetes.io/service-account.name</code> annotation is set to an existing
ServiceAccount name. If you are creating both the ServiceAccount and
the Secret objects, you should create the ServiceAccount object first.</p><p>After the Secret is created, a Kubernetes <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>
fills in some other fields such as the <code>kubernetes.io/service-account.uid</code> annotation, and the
<code>token</code> key in the <code>data</code> field, which is populated with an authentication token.</p><p>The following example configuration declares a ServiceAccount token Secret:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/serviceaccount-token-secret.yaml"><code>secret/serviceaccount-token-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy secret/serviceaccount-token-secret.yaml to clipboard"></div><div class="includecode" id="secret-serviceaccount-token-secret-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>secret-sa-sample<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/service-account.name</span>:<span> </span><span>"sa-name"</span><span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>kubernetes.io/service-account-token<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>extra</span>:<span> </span>YmFyCg==</span></span></code></pre></div></div></div><p>After creating the Secret, wait for Kubernetes to populate the <code>token</code> key in the <code>data</code> field.</p><p>See the <a href="/docs/concepts/security/service-accounts/">ServiceAccount</a>
documentation for more information on how ServiceAccounts work.
You can also check the <code>automountServiceAccountToken</code> field and the
<code>serviceAccountName</code> field of the
<a href="/docs/reference/generated/kubernetes-api/v1.34/#pod-v1-core"><code>Pod</code></a>
for information on referencing ServiceAccount credentials from within Pods.</p><h3 id="docker-config-secrets">Docker config Secrets</h3><p>If you are creating a Secret to store credentials for accessing a container image registry,
you must use one of the following <code>type</code> values for that Secret:</p><ul><li><code>kubernetes.io/dockercfg</code>: store a serialized <code>~/.dockercfg</code> which is the
legacy format for configuring Docker command line. The Secret
<code>data</code> field contains a <code>.dockercfg</code> key whose value is the content of a
base64 encoded <code>~/.dockercfg</code> file.</li><li><code>kubernetes.io/dockerconfigjson</code>: store a serialized JSON that follows the
same format rules as the <code>~/.docker/config.json</code> file, which is a new format
for <code>~/.dockercfg</code>. The Secret <code>data</code> field must contain a
<code>.dockerconfigjson</code> key for which the value is the content of a base64
encoded <code>~/.docker/config.json</code> file.</li></ul><p>Below is an example for a <code>kubernetes.io/dockercfg</code> type of Secret:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/dockercfg-secret.yaml"><code>secret/dockercfg-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy secret/dockercfg-secret.yaml to clipboard"></div><div class="includecode" id="secret-dockercfg-secret-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>secret-dockercfg<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>kubernetes.io/dockercfg<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>.dockercfg</span>:<span> </span>|<span>
</span></span></span><span><span><span>    eyJhdXRocyI6eyJodHRwczovL2V4YW1wbGUvdjEvIjp7ImF1dGgiOiJvcGVuc2VzYW1lIn19fQo=</span><span>    </span></span></span></code></pre></div></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you do not want to perform the base64 encoding, you can choose to use the
<code>stringData</code> field instead.</div><p>When you create Docker config Secrets using a manifest, the API
server checks whether the expected key exists in the <code>data</code> field, and
it verifies if the value provided can be parsed as a valid JSON. The API
server doesn't validate if the JSON actually is a Docker config file.</p><p>You can also use <code>kubectl</code> to create a Secret for accessing a container
registry, such as when you don't have a Docker configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret docker-registry secret-tiger-docker <span>\
</span></span></span><span><span><span></span>  --docker-email<span>=</span>tiger@acme.example <span>\
</span></span></span><span><span><span></span>  --docker-username<span>=</span>tiger <span>\
</span></span></span><span><span><span></span>  --docker-password<span>=</span>pass1234 <span>\
</span></span></span><span><span><span></span>  --docker-server<span>=</span>my-registry.example:5000
</span></span></code></pre></div><p>This command creates a Secret of type <code>kubernetes.io/dockerconfigjson</code>.</p><p>Retrieve the <code>.data.dockerconfigjson</code> field from that new Secret and decode the
data:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get secret secret-tiger-docker -o <span>jsonpath</span><span>=</span><span>'{.data.*}'</span> | base64 -d
</span></span></code></pre></div><p>The output is equivalent to the following JSON document (which is also a valid
Docker configuration file):</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"auths"</span>: {
</span></span><span><span>    <span>"my-registry.example:5000"</span>: {
</span></span><span><span>      <span>"username"</span>: <span>"tiger"</span>,
</span></span><span><span>      <span>"password"</span>: <span>"pass1234"</span>,
</span></span><span><span>      <span>"email"</span>: <span>"tiger@acme.example"</span>,
</span></span><span><span>      <span>"auth"</span>: <span>"dGlnZXI6cGFzczEyMzQ="</span>
</span></span><span><span>    }
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>The <code>auth</code> value there is base64 encoded; it is obscured but not secret.
Anyone who can read that Secret can learn the registry access bearer token.</p><p>It is suggested to use <a href="/docs/tasks/administer-cluster/kubelet-credential-provider/">credential providers</a> to dynamically and securely provide pull secrets on-demand.</p></div><h3 id="basic-authentication-secret">Basic authentication Secret</h3><p>The <code>kubernetes.io/basic-auth</code> type is provided for storing credentials needed
for basic authentication. When using this Secret type, the <code>data</code> field of the
Secret must contain one of the following two keys:</p><ul><li><code>username</code>: the user name for authentication</li><li><code>password</code>: the password or token for authentication</li></ul><p>Both values for the above two keys are base64 encoded strings. You can
alternatively provide the clear text content using the <code>stringData</code> field in the
Secret manifest.</p><p>The following manifest is an example of a basic authentication Secret:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/basicauth-secret.yaml"><code>secret/basicauth-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy secret/basicauth-secret.yaml to clipboard"></div><div class="includecode" id="secret-basicauth-secret-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>secret-basic-auth<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>kubernetes.io/basic-auth<span>
</span></span></span><span><span><span></span><span>stringData</span>:<span>
</span></span></span><span><span><span>  </span><span>username</span>:<span> </span>admin<span> </span><span># required field for kubernetes.io/basic-auth</span><span>
</span></span></span><span><span><span>  </span><span>password</span>:<span> </span>t0p-Secret<span> </span><span># required field for kubernetes.io/basic-auth</span></span></span></code></pre></div></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>stringData</code> field for a Secret does not work well with server-side apply.</div><p>The basic authentication Secret type is provided only for convenience.
You can create an <code>Opaque</code> type for credentials used for basic authentication.
However, using the defined and public Secret type (<code>kubernetes.io/basic-auth</code>) helps other
people to understand the purpose of your Secret, and sets a convention for what key names
to expect.</p><h3 id="ssh-authentication-secrets">SSH authentication Secrets</h3><p>The builtin type <code>kubernetes.io/ssh-auth</code> is provided for storing data used in
SSH authentication. When using this Secret type, you will have to specify a
<code>ssh-privatekey</code> key-value pair in the <code>data</code> (or <code>stringData</code>) field
as the SSH credential to use.</p><p>The following manifest is an example of a Secret used for SSH public/private
key authentication:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/ssh-auth-secret.yaml"><code>secret/ssh-auth-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy secret/ssh-auth-secret.yaml to clipboard"></div><div class="includecode" id="secret-ssh-auth-secret-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>secret-ssh-auth<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>kubernetes.io/ssh-auth<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span># the data is abbreviated in this example</span><span>
</span></span></span><span><span><span>  </span><span>ssh-privatekey</span>:<span> </span>|<span>
</span></span></span><span><span><span>    UG91cmluZzYlRW1vdGljb24lU2N1YmE=</span><span>    </span></span></span></code></pre></div></div></div><p>The SSH authentication Secret type is provided only for convenience.
You can create an <code>Opaque</code> type for credentials used for SSH authentication.
However, using the defined and public Secret type (<code>kubernetes.io/ssh-auth</code>) helps other
people to understand the purpose of your Secret, and sets a convention for what key names
to expect.
The Kubernetes API verifies that the required keys are set for a Secret of this type.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>SSH private keys do not establish trusted communication between an SSH client and
host server on their own. A secondary means of establishing trust is needed to
mitigate "man in the middle" attacks, such as a <code>known_hosts</code> file added to a ConfigMap.</div><h3 id="tls-secrets">TLS Secrets</h3><p>The <code>kubernetes.io/tls</code> Secret type is for storing
a certificate and its associated key that are typically used for TLS.</p><p>One common use for TLS Secrets is to configure encryption in transit for
an <a href="/docs/concepts/services-networking/ingress/">Ingress</a>, but you can also use it
with other resources or directly in your workload.
When using this type of Secret, the <code>tls.key</code> and the <code>tls.crt</code> key must be provided
in the <code>data</code> (or <code>stringData</code>) field of the Secret configuration, although the API
server doesn't actually validate the values for each key.</p><p>As an alternative to using <code>stringData</code>, you can use the <code>data</code> field to provide
the base64 encoded certificate and private key. For details, see
<a href="#restriction-names-data">Constraints on Secret names and data</a>.</p><p>The following YAML contains an example config for a TLS Secret:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/tls-auth-secret.yaml"><code>secret/tls-auth-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy secret/tls-auth-secret.yaml to clipboard"></div><div class="includecode" id="secret-tls-auth-secret-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>secret-tls<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>kubernetes.io/tls<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span># values are base64 encoded, which obscures them but does NOT provide</span><span>
</span></span></span><span><span><span>  </span><span># any useful level of confidentiality</span><span>
</span></span></span><span><span><span>  </span><span># Replace the following values with your own base64-encoded certificate and key.</span><span>
</span></span></span><span><span><span>  </span><span>tls.crt</span>:<span> </span><span>"REPLACE_WITH_BASE64_CERT"</span><span> 
</span></span></span><span><span><span>  </span><span>tls.key</span>:<span> </span><span>"REPLACE_WITH_BASE64_KEY"</span></span></span></code></pre></div></div></div><p>The TLS Secret type is provided only for convenience.
You can create an <code>Opaque</code> type for credentials used for TLS authentication.
However, using the defined and public Secret type (<code>kubernetes.io/tls</code>)
helps ensure the consistency of Secret format in your project. The API server
verifies if the required keys are set for a Secret of this type.</p><p>To create a TLS Secret using <code>kubectl</code>, use the <code>tls</code> subcommand:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create secret tls my-tls-secret <span>\
</span></span></span><span><span><span></span>  --cert<span>=</span>path/to/cert/file <span>\
</span></span></span><span><span><span></span>  --key<span>=</span>path/to/key/file
</span></span></code></pre></div><p>The public/private key pair must exist before hand. The public key certificate for <code>--cert</code> must be .PEM encoded
and must match the given private key for <code>--key</code>.</p><h3 id="bootstrap-token-secrets">Bootstrap token Secrets</h3><p>The <code>bootstrap.kubernetes.io/token</code> Secret type is for
tokens used during the node bootstrap process. It stores tokens used to sign
well-known ConfigMaps.</p><p>A bootstrap token Secret is usually created in the <code>kube-system</code> namespace and
named in the form <code>bootstrap-token-&lt;token-id&gt;</code> where <code>&lt;token-id&gt;</code> is a 6 character
string of the token ID.</p><p>As a Kubernetes manifest, a bootstrap token Secret might look like the
following:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/bootstrap-token-secret-base64.yaml"><code>secret/bootstrap-token-secret-base64.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy secret/bootstrap-token-secret-base64.yaml to clipboard"></div><div class="includecode" id="secret-bootstrap-token-secret-base64-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>bootstrap-token-5emitj<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>bootstrap.kubernetes.io/token<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>auth-extra-groups</span>:<span> </span>c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=<span>
</span></span></span><span><span><span>  </span><span>expiration</span>:<span> </span>MjAyMC0wOS0xM1QwNDozOToxMFo=<span>
</span></span></span><span><span><span>  </span><span>token-id</span>:<span> </span>NWVtaXRq<span>
</span></span></span><span><span><span>  </span><span>token-secret</span>:<span> </span>a3E0Z2lodnN6emduMXAwcg==<span>
</span></span></span><span><span><span>  </span><span>usage-bootstrap-authentication</span>:<span> </span>dHJ1ZQ==<span>
</span></span></span><span><span><span>  </span><span>usage-bootstrap-signing</span>:<span> </span>dHJ1ZQ==</span></span></code></pre></div></div></div><p>A bootstrap token Secret has the following keys specified under <code>data</code>:</p><ul><li><code>token-id</code>: A random 6 character string as the token identifier. Required.</li><li><code>token-secret</code>: A random 16 character string as the actual token Secret. Required.</li><li><code>description</code>: A human-readable string that describes what the token is
used for. Optional.</li><li><code>expiration</code>: An absolute UTC time using <a href="https://datatracker.ietf.org/doc/html/rfc3339">RFC3339</a> specifying when the token
should be expired. Optional.</li><li><code>usage-bootstrap-&lt;usage&gt;</code>: A boolean flag indicating additional usage for
the bootstrap token.</li><li><code>auth-extra-groups</code>: A comma-separated list of group names that will be
authenticated as in addition to the <code>system:bootstrappers</code> group.</li></ul><p>You can alternatively provide the values in the <code>stringData</code> field of the Secret
without base64 encoding them:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/bootstrap-token-secret-literal.yaml"><code>secret/bootstrap-token-secret-literal.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy secret/bootstrap-token-secret-literal.yaml to clipboard"></div><div class="includecode" id="secret-bootstrap-token-secret-literal-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span># Note how the Secret is named</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>bootstrap-token-5emitj<span>
</span></span></span><span><span><span>  </span><span># A bootstrap token Secret usually resides in the kube-system namespace</span><span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>kube-system<span>
</span></span></span><span><span><span></span><span>type</span>:<span> </span>bootstrap.kubernetes.io/token<span>
</span></span></span><span><span><span></span><span>stringData</span>:<span>
</span></span></span><span><span><span>  </span><span>auth-extra-groups</span>:<span> </span><span>"system:bootstrappers:kubeadm:default-node-token"</span><span>
</span></span></span><span><span><span>  </span><span>expiration</span>:<span> </span><span>"2020-09-13T04:39:10Z"</span><span>
</span></span></span><span><span><span>  </span><span># This token ID is used in the name</span><span>
</span></span></span><span><span><span>  </span><span>token-id</span>:<span> </span><span>"5emitj"</span><span>
</span></span></span><span><span><span>  </span><span>token-secret</span>:<span> </span><span>"kq4gihvszzgn1p0r"</span><span>
</span></span></span><span><span><span>  </span><span># This token can be used for authentication</span><span>
</span></span></span><span><span><span>  </span><span>usage-bootstrap-authentication</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>  </span><span># and it can be used for signing</span><span>
</span></span></span><span><span><span>  </span><span>usage-bootstrap-signing</span>:<span> </span><span>"true"</span></span></span></code></pre></div></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>stringData</code> field for a Secret does not work well with server-side apply.</div><h2 id="working-with-secrets">Working with Secrets</h2><h3 id="creating-a-secret">Creating a Secret</h3><p>There are several options to create a Secret:</p><ul><li><a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/">Use <code>kubectl</code></a></li><li><a href="/docs/tasks/configmap-secret/managing-secret-using-config-file/">Use a configuration file</a></li><li><a href="/docs/tasks/configmap-secret/managing-secret-using-kustomize/">Use the Kustomize tool</a></li></ul><h4 id="restriction-names-data">Constraints on Secret names and data</h4><p>The name of a Secret object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>You can specify the <code>data</code> and/or the <code>stringData</code> field when creating a
configuration file for a Secret. The <code>data</code> and the <code>stringData</code> fields are optional.
The values for all keys in the <code>data</code> field have to be base64-encoded strings.
If the conversion to base64 string is not desirable, you can choose to specify
the <code>stringData</code> field instead, which accepts arbitrary strings as values.</p><p>The keys of <code>data</code> and <code>stringData</code> must consist of alphanumeric characters,
<code>-</code>, <code>_</code> or <code>.</code>. All key-value pairs in the <code>stringData</code> field are internally
merged into the <code>data</code> field. If a key appears in both the <code>data</code> and the
<code>stringData</code> field, the value specified in the <code>stringData</code> field takes
precedence.</p><h4 id="restriction-data-size">Size limit</h4><p>Individual Secrets are limited to 1MiB in size. This is to discourage creation
of very large Secrets that could exhaust the API server and kubelet memory.
However, creation of many smaller Secrets could also exhaust memory. You can
use a <a href="/docs/concepts/policy/resource-quotas/">resource quota</a> to limit the
number of Secrets (or other resources) in a namespace.</p><h3 id="editing-a-secret">Editing a Secret</h3><p>You can edit an existing Secret unless it is <a href="#secret-immutable">immutable</a>. To
edit a Secret, use one of the following methods:</p><ul><li><a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/#edit-secret">Use <code>kubectl</code></a></li><li><a href="/docs/tasks/configmap-secret/managing-secret-using-config-file/#edit-secret">Use a configuration file</a></li></ul><p>You can also edit the data in a Secret using the <a href="/docs/tasks/configmap-secret/managing-secret-using-kustomize/#edit-secret">Kustomize tool</a>. However, this
method creates a new <code>Secret</code> object with the edited data.</p><p>Depending on how you created the Secret, as well as how the Secret is used in
your Pods, updates to existing <code>Secret</code> objects are propagated automatically to
Pods that use the data. For more information, refer to <a href="#using-secrets-as-files-from-a-pod">Using Secrets as files from a Pod</a> section.</p><h3 id="using-a-secret">Using a Secret</h3><p>Secrets can be mounted as data volumes or exposed as
<a class="glossary-tooltip" title="Container environment variables are name=value pairs that provide useful information into containers running in a Pod." href="/docs/concepts/containers/container-environment/" target="_blank">environment variables</a>
to be used by a container in a Pod. Secrets can also be used by other parts of the
system, without being directly exposed to the Pod. For example, Secrets can hold
credentials that other parts of the system should use to interact with external
systems on your behalf.</p><p>Secret volume sources are validated to ensure that the specified object
reference actually points to an object of type Secret. Therefore, a Secret
needs to be created before any Pods that depend on it.</p><p>If the Secret cannot be fetched (perhaps because it does not exist, or
due to a temporary lack of connection to the API server) the kubelet
periodically retries running that Pod. The kubelet also reports an Event
for that Pod, including details of the problem fetching the Secret.</p><h4 id="restriction-secret-must-exist">Optional Secrets</h4><p>When you reference a Secret in a Pod, you can mark the Secret as <em>optional</em>,
such as in the following example. If an optional Secret doesn't exist,
Kubernetes ignores it.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/secret/optional-secret.yaml"><code>secret/optional-secret.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy secret/optional-secret.yaml to clipboard"></div><div class="includecode" id="secret-optional-secret-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>redis<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>foo<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/etc/foo"</span><span>
</span></span></span><span><span><span>      </span><span>readOnly</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>foo<span>
</span></span></span><span><span><span>    </span><span>secret</span>:<span>
</span></span></span><span><span><span>      </span><span>secretName</span>:<span> </span>mysecret<span>
</span></span></span><span><span><span>      </span><span>optional</span>:<span> </span><span>true</span></span></span></code></pre></div></div></div><p>By default, Secrets are required. None of a Pod's containers will start until
all non-optional Secrets are available.</p><p>If a Pod references a specific key in a non-optional Secret and that Secret
does exist, but is missing the named key, the Pod fails during startup.</p><h3 id="using-secrets-as-files-from-a-pod">Using Secrets as files from a Pod</h3><p>If you want to access data from a Secret in a Pod, one way to do that is to
have Kubernetes make the value of that Secret be available as a file inside
the filesystem of one or more of the Pod's containers.</p><p>For instructions, refer to
<a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#create-a-pod-that-has-access-to-the-secret-data-through-a-volume">Create a Pod that has access to the secret data through a Volume</a>.</p><p>When a volume contains data from a Secret, and that Secret is updated, Kubernetes tracks
this and updates the data in the volume, using an eventually-consistent approach.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A container using a Secret as a
<a href="/docs/concepts/storage/volumes/#using-subpath">subPath</a> volume mount does not receive
automated Secret updates.</div><p>The kubelet keeps a cache of the current keys and values for the Secrets that are used in
volumes for pods on that node.
You can configure the way that the kubelet detects changes from the cached values. The
<code>configMapAndSecretChangeDetectionStrategy</code> field in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration</a> controls
which strategy the kubelet uses. The default strategy is <code>Watch</code>.</p><p>Updates to Secrets can be either propagated by an API watch mechanism (the default), based on
a cache with a defined time-to-live, or polled from the cluster API server on each kubelet
synchronisation loop.</p><p>As a result, the total delay from the moment when the Secret is updated to the moment
when new keys are projected to the Pod can be as long as the kubelet sync period + cache
propagation delay, where the cache propagation delay depends on the chosen cache type
(following the same order listed in the previous paragraph, these are:
watch propagation delay, the configured cache TTL, or zero for direct polling).</p><h3 id="using-secrets-as-environment-variables">Using Secrets as environment variables</h3><p>To use a Secret in an <a class="glossary-tooltip" title="Container environment variables are name=value pairs that provide useful information into containers running in a Pod." href="/docs/concepts/containers/container-environment/" target="_blank">environment variable</a>
in a Pod:</p><ol><li>For each container in your Pod specification, add an environment variable
for each Secret key that you want to use to the
<code>env[].valueFrom.secretKeyRef</code> field.</li><li>Modify your image and/or command line so that the program looks for values
in the specified environment variables.</li></ol><p>For instructions, refer to
<a href="/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data">Define container environment variables using Secret data</a>.</p><p>It's important to note that the range of characters allowed for environment variable
names in pods is <a href="/docs/tasks/inject-data-application/define-environment-variable-container/#using-environment-variables-inside-of-your-config">restricted</a>.
If any keys do not meet the rules, those keys are not made available to your container, though
the Pod is allowed to start.</p><h3 id="using-imagepullsecrets">Container image pull Secrets</h3><p>If you want to fetch container images from a private repository, you need a way for
the kubelet on each node to authenticate to that repository. You can configure
<em>image pull Secrets</em> to make this possible. These Secrets are configured at the Pod
level.</p><h4 id="using-imagepullsecrets-1">Using imagePullSecrets</h4><p>The <code>imagePullSecrets</code> field is a list of references to Secrets in the same namespace.
You can use an <code>imagePullSecrets</code> to pass a Secret that contains a Docker (or other) image registry
password to the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.
See the <a href="/docs/reference/generated/kubernetes-api/v1.34/#podspec-v1-core">PodSpec API</a>
for more information about the <code>imagePullSecrets</code> field.</p><h5 id="manually-specifying-an-imagepullsecret">Manually specifying an imagePullSecret</h5><p>You can learn how to specify <code>imagePullSecrets</code> from the
<a href="/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">container images</a>
documentation.</p><h5 id="arranging-for-imagepullsecrets-to-be-automatically-attached">Arranging for imagePullSecrets to be automatically attached</h5><p>You can manually create <code>imagePullSecrets</code>, and reference these from a ServiceAccount. Any Pods
created with that ServiceAccount or created with that ServiceAccount by default, will get their
<code>imagePullSecrets</code> field set to that of the service account.
See <a href="/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">Add ImagePullSecrets to a service account</a>
for a detailed explanation of that process.</p><h3 id="restriction-static-pod">Using Secrets with static Pods</h3><p>You cannot use ConfigMaps or Secrets with <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static Pods</a>.</p><h2 id="secret-immutable">Immutable Secrets</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>Kubernetes lets you mark specific Secrets (and ConfigMaps) as <em>immutable</em>.
Preventing changes to the data of an existing Secret has the following benefits:</p><ul><li>protects you from accidental (or unwanted) updates that could cause applications outages</li><li>(for clusters that extensively use Secrets - at least tens of thousands of unique Secret
to Pod mounts), switching to immutable Secrets improves the performance of your cluster
by significantly reducing load on kube-apiserver. The kubelet does not need to maintain
a [watch] on any Secrets that are marked as immutable.</li></ul><h3 id="secret-immutable-create">Marking a Secret as immutable</h3><p>You can create an immutable Secret by setting the <code>immutable</code> field to <code>true</code>. For example,</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Secret<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span> </span>...<span>
</span></span></span><span><span><span></span><span>data</span>:<span> </span>...<span>
</span></span></span><span><span><span></span><span>immutable</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><p>You can also update any existing mutable Secret to make it immutable.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Once a Secret or ConfigMap is marked as immutable, it is <em>not</em> possible to revert this change
nor to mutate the contents of the <code>data</code> field. You can only delete and recreate the Secret.
Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreate
these pods.</div><h2 id="information-security-for-secrets">Information security for Secrets</h2><p>Although ConfigMap and Secret work similarly, Kubernetes applies some additional
protection for Secret objects.</p><p>Secrets often hold values that span a spectrum of importance, many of which can
cause escalations within Kubernetes (e.g. service account tokens) and to
external systems. Even if an individual app can reason about the power of the
Secrets it expects to interact with, other apps within the same namespace can
render those assumptions invalid.</p><p>A Secret is only sent to a node if a Pod on that node requires it.
For mounting Secrets into Pods, the kubelet stores a copy of the data into a <code>tmpfs</code>
so that the confidential data is not written to durable storage.
Once the Pod that depends on the Secret is deleted, the kubelet deletes its local copy
of the confidential data from the Secret.</p><p>There may be several containers in a Pod. By default, containers you define
only have access to the default ServiceAccount and its related Secret.
You must explicitly define environment variables or map a volume into a
container in order to provide access to any other Secret.</p><p>There may be Secrets for several Pods on the same node. However, only the
Secrets that a Pod requests are potentially visible within its containers.
Therefore, one Pod does not have access to the Secrets of another Pod.</p><h3 id="configure-least-privilege-access-to-secrets">Configure least-privilege access to Secrets</h3><p>To enhance the security measures around Secrets, use separate namespaces to isolate access to mounted secrets.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Any containers that run with <code>privileged: true</code> on a node can access all
Secrets used on that node.</div><h2 id="what-s-next">What's next</h2><ul><li>For guidelines to manage and improve the security of your Secrets, refer to
<a href="/docs/concepts/security/secrets-good-practices/">Good practices for Kubernetes Secrets</a>.</li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-kubectl/">manage Secrets using <code>kubectl</code></a></li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-config-file/">manage Secrets using config file</a></li><li>Learn how to <a href="/docs/tasks/configmap-secret/managing-secret-using-kustomize/">manage Secrets using kustomize</a></li><li>Read the <a href="/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/">API reference</a> for <code>Secret</code></li></ul></div></div><div><div class="td-content"><h1>Liveness, Readiness, and Startup Probes</h1><p>Kubernetes has various types of probes:</p><ul><li><a href="#liveness-probe">Liveness probe</a></li><li><a href="#readiness-probe">Readiness probe</a></li><li><a href="#startup-probe">Startup probe</a></li></ul><h2 id="liveness-probe">Liveness probe</h2><p>Liveness probes determine when to restart a container. For example, liveness probes could catch a deadlock when an application is running but unable to make progress.</p><p>If a container fails its liveness probe repeatedly, the kubelet restarts the container.</p><p>Liveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe, you can either define <code>initialDelaySeconds</code> or use a
<a href="#startup-probe">startup probe</a>.</p><h2 id="readiness-probe">Readiness probe</h2><p>Readiness probes determine when a container is ready to accept traffic. This is useful when waiting for an application to perform time-consuming initial tasks that depend on its backing services; for example: establishing network connections, loading files, and warming caches. Readiness probes can also be useful later in the container&#8217;s lifecycle, for example, when recovering from temporary faults or overloads.</p><p>If the readiness probe returns a failed state, Kubernetes removes the pod from all matching service endpoints.</p><p>Readiness probes run on the container during its whole lifecycle.</p><h2 id="startup-probe">Startup probe</h2><p>A startup probe verifies whether the application within a container is started. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet before they are up and running.</p><p>If such a probe is configured, it disables liveness and readiness checks until it succeeds.</p><p>This type of probe is only executed at startup, unlike liveness and readiness probes, which are run periodically.</p><ul><li>Read more about the <a href="/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Configure Liveness, Readiness and Startup Probes</a>.</li></ul></div></div><div><div class="td-content"><h1>Resource Management for Pods and Containers</h1><p>When you specify a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a>, you can optionally specify how much of each resource a
<a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." href="/docs/concepts/containers/" target="_blank">container</a> needs. The most common resources to specify are CPU and memory
(RAM); there are others.</p><p>When you specify the resource <em>request</em> for containers in a Pod, the
<a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">kube-scheduler</a> uses this information to decide which node to place the Pod on.
When you specify a resource <em>limit</em> for a container, the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> enforces those
limits so that the running container is not allowed to use more of that resource
than the limit you set. The kubelet also reserves at least the <em>request</em> amount of
that system resource specifically for that container to use.</p><h2 id="requests-and-limits">Requests and limits</h2><p>If the node where a Pod is running has enough of a resource available, it's possible (and
allowed) for a container to use more resource than its <code>request</code> for that resource specifies.</p><p>For example, if you set a <code>memory</code> request of 256 MiB for a container, and that container is in
a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use
more RAM.</p><p>Limits are a different story. Both <code>cpu</code> and <code>memory</code> limits are applied by the kubelet (and
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>),
and are ultimately enforced by the kernel. On Linux nodes, the Linux kernel
enforces limits with
<a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." href="/docs/reference/glossary/?all=true#term-cgroup" target="_blank">cgroups</a>.
The behavior of <code>cpu</code> and <code>memory</code> limit enforcement is slightly different.</p><p><code>cpu</code> limits are enforced by CPU throttling. When a container approaches
its <code>cpu</code> limit, the kernel will restrict access to the CPU corresponding to the
container's limit. Thus, a <code>cpu</code> limit is a hard limit the kernel enforces.
Containers may not use more CPU than is specified in their <code>cpu</code> limit.</p><p><code>memory</code> limits are enforced by the kernel with out of memory (OOM) kills. When
a container uses more than its <code>memory</code> limit, the kernel may terminate it. However,
terminations only happen when the kernel detects memory pressure. Thus, a
container that over allocates memory may not be immediately killed. This means
<code>memory</code> limits are enforced reactively. A container may use more memory than
its <code>memory</code> limit, but if it does, it may get killed.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>There is an alpha feature <code>MemoryQoS</code> which attempts to add more preemptive
limit enforcement for memory (as opposed to reactive enforcement by the OOM
killer). However, this effort is
<a href="https://github.com/kubernetes/enhancements/tree/a47155b340/keps/sig-node/2570-memory-qos#latest-update-stalled">stalled</a>
due to a potential livelock situation a memory hungry can cause.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you specify a limit for a resource, but do not specify any request, and no admission-time
mechanism has applied a default request for that resource, then Kubernetes copies the limit
you specified and uses it as the requested value for the resource.</div><h2 id="resource-types">Resource types</h2><p><em>CPU</em> and <em>memory</em> are each a <em>resource type</em>. A resource type has a base unit.
CPU represents compute processing and is specified in units of <a href="#meaning-of-cpu">Kubernetes CPUs</a>.
Memory is specified in units of bytes.
For Linux workloads, you can specify <em>huge page</em> resources.
Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory
that are much larger than the default page size.</p><p>For example, on a system where the default page size is 4KiB, you could specify a limit,
<code>hugepages-2Mi: 80Mi</code>. If the container tries allocating over 40 2MiB huge pages (a
total of 80 MiB), that allocation fails.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You cannot overcommit <code>hugepages-*</code> resources.
This is different from the <code>memory</code> and <code>cpu</code> resources.</div><p>CPU and memory are collectively referred to as <em>compute resources</em>, or <em>resources</em>. Compute
resources are measurable quantities that can be requested, allocated, and
consumed. They are distinct from
<a href="/docs/concepts/overview/kubernetes-api/">API resources</a>. API resources, such as Pods and
<a href="/docs/concepts/services-networking/service/">Services</a> are objects that can be read and modified
through the Kubernetes API server.</p><h2 id="resource-requests-and-limits-of-pod-and-container">Resource requests and limits of Pod and container</h2><p>For each container, you can specify resource limits and requests,
including the following:</p><ul><li><code>spec.containers[].resources.limits.cpu</code></li><li><code>spec.containers[].resources.limits.memory</code></li><li><code>spec.containers[].resources.limits.hugepages-&lt;size&gt;</code></li><li><code>spec.containers[].resources.requests.cpu</code></li><li><code>spec.containers[].resources.requests.memory</code></li><li><code>spec.containers[].resources.requests.hugepages-&lt;size&gt;</code></li></ul><p>Although you can only specify requests and limits for individual containers,
it is also useful to think about the overall resource requests and limits for
a Pod.
For a particular resource, a <em>Pod resource request/limit</em> is the sum of the
resource requests/limits of that type for each container in the Pod.</p><h2 id="pod-level-resource-specification">Pod-level resource specification</h2><div class="feature-state-notice feature-beta" title="Feature Gate: PodLevelResources"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>Provided your cluster has the <code>PodLevelResources</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> enabled,
you can specify resource requests and limits at
the Pod level. At the Pod level, Kubernetes 1.34
only supports resource requests or limits for specific resource types: <code>cpu</code> and /
or <code>memory</code> and / or <code>hugepages</code>. With this feature, Kubernetes allows you to declare an overall resource
budget for the Pod, which is especially helpful when dealing with a large number of
containers where it can be difficult to accurately gauge individual resource needs.
Additionally, it enables containers within a Pod to share idle resources with each
other, improving resource utilization.</p><p>For a Pod, you can specify resource limits and requests for CPU and memory by including the following:</p><ul><li><code>spec.resources.limits.cpu</code></li><li><code>spec.resources.limits.memory</code></li><li><code>spec.resources.limits.hugepages-&lt;size&gt;</code></li><li><code>spec.resources.requests.cpu</code></li><li><code>spec.resources.requests.memory</code></li><li><code>spec.resources.requests.hugepages-&lt;size&gt;</code></li></ul><h2 id="resource-units-in-kubernetes">Resource units in Kubernetes</h2><h3 id="meaning-of-cpu">CPU resource units</h3><p>Limits and requests for CPU resources are measured in <em>cpu</em> units.
In Kubernetes, 1 CPU unit is equivalent to <strong>1 physical CPU core</strong>,
or <strong>1 virtual core</strong>, depending on whether the node is a physical host
or a virtual machine running inside a physical machine.</p><p>Fractional requests are allowed. When you define a container with
<code>spec.containers[].resources.requests.cpu</code> set to <code>0.5</code>, you are requesting half
as much CPU time compared to if you asked for <code>1.0</code> CPU.
For CPU resource units, the <a href="/docs/reference/kubernetes-api/common-definitions/quantity/">quantity</a> expression <code>0.1</code> is equivalent to the
expression <code>100m</code>, which can be read as "one hundred millicpu". Some people say
"one hundred millicores", and this is understood to mean the same thing.</p><p>CPU resource is always specified as an absolute amount of resource, never as a relative amount. For example,
<code>500m</code> CPU represents the roughly same amount of computing power whether that container
runs on a single-core, dual-core, or 48-core machine.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Kubernetes doesn't allow you to specify CPU resources with a precision finer than
<code>1m</code> or <code>0.001</code> CPU. To avoid accidentally using an invalid CPU quantity, it's useful to specify CPU units using the milliCPU form
instead of the decimal form when using less than 1 CPU unit.</p><p>For example, you have a Pod that uses <code>5m</code> or <code>0.005</code> CPU and would like to decrease
its CPU resources. By using the decimal form, it's harder to spot that <code>0.0005</code> CPU
is an invalid value, while by using the milliCPU form, it's easier to spot that
<code>0.5m</code> is an invalid value.</p></div><h3 id="meaning-of-memory">Memory resource units</h3><p>Limits and requests for <code>memory</code> are measured in bytes. You can express memory as
a plain integer or as a fixed-point number using one of these
<a href="/docs/reference/kubernetes-api/common-definitions/quantity/">quantity</a> suffixes:
E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>128974848, 129e6, 129M,  128974848000m, 123Mi
</span></span></code></pre></div><p>Pay attention to the case of the suffixes. If you request <code>400m</code> of memory, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (<code>400Mi</code>)
or 400 megabytes (<code>400M</code>).</p><h2 id="example-1">Container resources example</h2><p>The following Pod has two containers. Both containers are defined with a request for
0.25 CPU
and 64MiB (2<sup>26</sup> bytes) of memory. Each container has a limit of 0.5
CPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128
MiB of memory, and a limit of 1 CPU and 256MiB of memory.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>frontend<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>app<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>images.my-company.example/app:v4<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"64Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"250m"</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"128Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"500m"</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>log-aggregator<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>images.my-company.example/log-aggregator:v6<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"64Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"250m"</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"128Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"500m"</span><span>
</span></span></span></code></pre></div><h2 id="example-2">Pod resources example</h2><div class="feature-state-notice feature-beta" title="Feature Gate: PodLevelResources"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>This feature can be enabled by setting the <code>PodLevelResources</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.
The following Pod has an explicit request of 1 CPU and 100 MiB of memory, and an
explicit limit of 1 CPU and 200 MiB of memory. The <code>pod-resources-demo-ctr-1</code>
container has explicit requests and limits set. However, the
<code>pod-resources-demo-ctr-2</code> container will simply share the resources available
within the Pod resource boundaries, as it does not have explicit requests and limits
set.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/resource/pod-level-resources.yaml"><code>pods/resource/pod-level-resources.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/resource/pod-level-resources.yaml to clipboard"></div><div class="includecode" id="pods-resource-pod-level-resources-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod-resources-demo<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>pod-resources-example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>resources</span>:<span>
</span></span></span><span><span><span>    </span><span>limits</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>    </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pod-resources-demo-ctr-1<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"0.5"</span><span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"0.5"</span><span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"50Mi"</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pod-resources-demo-ctr-2<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>fedora<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span>
</span></span></span><span><span><span>    </span>- sleep<span>
</span></span></span><span><span><span>    </span>- inf <span>
</span></span></span></code></pre></div></div></div><h2 id="how-pods-with-resource-requests-are-scheduled">How Pods with resource requests are scheduled</h2><p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum capacity for each of the resource types: the
amount of CPU and memory it can provide for Pods. The scheduler ensures that,
for each resource type, the sum of the resource requests of the scheduled
containers is less than the capacity of the node.
Note that although actual memory
or CPU resource usage on nodes is very low, the scheduler still refuses to place
a Pod on a node if the capacity check fails. This protects against a resource
shortage on a node when resource usage later increases, for example, during a
daily peak in request rate.</p><h2 id="how-pods-with-resource-limits-are-run">How Kubernetes applies resource requests and limits</h2><p>When the kubelet starts a container as part of a Pod, the kubelet passes that container's
requests and limits for memory and CPU to the container runtime.</p><p>On Linux, the container runtime typically configures
kernel <a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." href="/docs/reference/glossary/?all=true#term-cgroup" target="_blank">cgroups</a> that apply and enforce the
limits you defined.</p><ul><li>The CPU limit defines a hard ceiling on how much CPU time the container can use.
During each scheduling interval (time slice), the Linux kernel checks to see if this
limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.</li><li>The CPU request typically defines a weighting. If several different containers (cgroups)
want to run on a contended system, workloads with larger CPU requests are allocated more
CPU time than workloads with small requests.</li><li>The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses
cgroups v2, the container runtime might use the memory request as a hint to set
<code>memory.min</code> and <code>memory.low</code>.</li><li>The memory limit defines a memory limit for that cgroup. If the container tries to
allocate more memory than this limit, the Linux kernel out-of-memory subsystem activates
and, typically, intervenes by stopping one of the processes in the container that tried
to allocate memory. If that process is the container's PID 1, and the container is marked
as restartable, Kubernetes restarts the container.</li><li>The memory limit for the Pod or container can also apply to pages in memory backed
volumes, such as an <code>emptyDir</code>. The kubelet tracks <code>tmpfs</code> emptyDir volumes as container
memory use, rather than as local ephemeral storage.&#12288;When using memory backed <code>emptyDir</code>,
be sure to check the notes <a href="#memory-backed-emptydir">below</a>.</li></ul><p>If a container exceeds its memory request and the node that it runs on becomes short of
memory overall, it is likely that the Pod the container belongs to will be
<a class="glossary-tooltip" title="Process of terminating one or more Pods on Nodes" href="/docs/concepts/scheduling-eviction/" target="_blank">evicted</a>.</p><p>A container might or might not be allowed to exceed its CPU limit for extended periods of time.
However, container runtimes don't terminate Pods or containers for excessive CPU usage.</p><p>To determine whether a container cannot be scheduled or is being killed due to resource limits,
see the <a href="#troubleshooting">Troubleshooting</a> section.</p><h3 id="monitoring-compute-memory-resource-usage">Monitoring compute &amp; memory resource usage</h3><p>The kubelet reports the resource usage of a Pod as part of the Pod
<a href="/docs/concepts/overview/working-with-objects/#object-spec-and-status"><code>status</code></a>.</p><p>If optional <a href="/docs/tasks/debug/debug-cluster/resource-usage-monitoring/">tools for monitoring</a>
are available in your cluster, then Pod resource usage can be retrieved either
from the <a href="/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-api">Metrics API</a>
directly or from your monitoring tools.</p><h3 id="memory-backed-emptydir">Considerations for memory backed <code>emptyDir</code> volumes</h3><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>If you do not specify a <code>sizeLimit</code> for an <code>emptyDir</code> volume, that volume may
consume up to that pod's memory limit (<code>Pod.spec.containers[].resources.limits.memory</code>).
If you do not set a memory limit, the pod has no upper bound on memory consumption,
and can consume all available memory on the node. Kubernetes schedules pods based
on resource requests (<code>Pod.spec.containers[].resources.requests</code>) and will not
consider memory usage above the request when deciding if another pod can fit on
a given node. This can result in a denial of service and cause the OS to do
out-of-memory (OOM) handling. It is possible to create any number of <code>emptyDir</code>s
that could potentially consume all available memory on the node, making OOM
more likely.</div><p>From the perspective of memory management, there are some similarities between
when a process uses memory as a work area and when using memory-backed
<code>emptyDir</code>. But when using memory as a volume, like memory-backed <code>emptyDir</code>,
there are additional points below that you should be careful of:</p><ul><li>Files stored on a memory-backed volume are almost entirely managed by the
user application. Unlike when used as a work area for a process, you can not
rely on things like language-level garbage collection.</li><li>The purpose of writing files to a volume is to save data or pass it between
applications. Neither Kubernetes nor the OS may automatically delete files
from a volume, so memory used by those files can not be reclaimed when the
system or the pod are under memory pressure.</li><li>A memory-backed <code>emptyDir</code> is useful because of its performance, but memory
is generally much smaller in size and much higher in cost than other storage
media, such as disks or SSDs. Using large amounts of memory for <code>emptyDir</code>
volumes may affect the normal operation of your pod or of the whole node,
so should be used carefully.</li></ul><p>If you are administering a cluster or namespace, you can also set
<a href="/docs/concepts/policy/resource-quotas/">ResourceQuota</a> that limits memory use;
you may also want to define a <a href="/docs/concepts/policy/limit-range/">LimitRange</a>
for additional enforcement.
If you specify a <code>spec.containers[].resources.limits.memory</code> for each Pod,
then the maximum size of an <code>emptyDir</code> volume will be the pod's memory limit.</p><p>As an alternative, a cluster administrator can enforce size limits for
<code>emptyDir</code> volumes in new Pods using a policy mechanism such as
<a href="/docs/reference/access-authn-authz/validating-admission-policy/">ValidationAdmissionPolicy</a>.</p><h2 id="local-ephemeral-storage">Local ephemeral storage</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>Nodes have local ephemeral storage, backed by
locally-attached writeable devices or, sometimes, by RAM.
"Ephemeral" means that there is no long-term guarantee about durability.</p><p>Pods use ephemeral local storage for scratch space, caching, and for logs.
The kubelet can provide scratch space to Pods using local ephemeral storage to
mount <a href="/docs/concepts/storage/volumes/#emptydir"><code>emptyDir</code></a>
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">volumes</a> into containers.</p><p>The kubelet also uses this kind of storage to hold
<a href="/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a>,
container images, and the writable layers of running containers.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>If a node fails, the data in its ephemeral storage can be lost.
Your applications cannot expect any performance SLAs (disk IOPS for example)
from local ephemeral storage.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>To make the resource quota work on ephemeral-storage, two things need to be done:</p><ul><li>An admin sets the resource quota for ephemeral-storage in a namespace.</li><li>A user needs to specify limits for the ephemeral-storage resource in the Pod spec.</li></ul><p>If the user doesn't specify the ephemeral-storage resource limit in the Pod spec,
the resource quota is not enforced on ephemeral-storage.</p></div><p>Kubernetes lets you track, reserve and limit the amount
of ephemeral local storage a Pod can consume.</p><h3 id="configurations-for-local-ephemeral-storage">Configurations for local ephemeral storage</h3><p>Kubernetes supports two ways to configure local ephemeral storage on a node:<ul class="nav nav-tabs" id="local-storage-configurations"><li class="nav-item"><a class="nav-link active" href="#local-storage-configurations-0">Single filesystem</a></li><li class="nav-item"><a class="nav-link" href="#local-storage-configurations-1">Two filesystems</a></li></ul><div class="tab-content" id="local-storage-configurations"><div id="local-storage-configurations-0" class="tab-pane show active"><p><p>In this configuration, you place all different kinds of ephemeral local data
(<code>emptyDir</code> volumes, writeable layers, container images, logs) into one filesystem.
The most effective way to configure the kubelet means dedicating this filesystem
to Kubernetes (kubelet) data.</p><p>The kubelet also writes
<a href="/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a>
and treats these similarly to ephemeral local storage.</p><p>The kubelet writes logs to files inside its configured log directory (<code>/var/log</code>
by default); and has a base directory for other locally stored data
(<code>/var/lib/kubelet</code> by default).</p><p>Typically, both <code>/var/lib/kubelet</code> and <code>/var/log</code> are on the system root filesystem,
and the kubelet is designed with that layout in mind.</p><p>Your node can have as many other filesystems, not used for Kubernetes,
as you like.</p></p></div><div id="local-storage-configurations-1" class="tab-pane"><p><p>You have a filesystem on the node that you're using for ephemeral data that
comes from running Pods: logs, and <code>emptyDir</code> volumes. You can use this filesystem
for other data (for example: system logs not related to Kubernetes); it can even
be the root filesystem.</p><p>The kubelet also writes
<a href="/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a>
into the first filesystem, and treats these similarly to ephemeral local storage.</p><p>You also use a separate filesystem, backed by a different logical storage device.
In this configuration, the directory where you tell the kubelet to place
container image layers and writeable layers is on this second filesystem.</p><p>The first filesystem does not hold any image layers or writeable layers.</p><p>Your node can have as many other filesystems, not used for Kubernetes,
as you like.</p></p></div></div></p><p>The kubelet can measure how much local storage it is using. It does this provided
that you have set up the node using one of the supported configurations for local
ephemeral storage.</p><p>If you have a different configuration, then the kubelet does not apply resource
limits for ephemeral local storage.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The kubelet tracks <code>tmpfs</code> emptyDir volumes as container memory use, rather
than as local ephemeral storage.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The kubelet will only track the root filesystem for ephemeral storage. OS layouts that mount a separate disk to <code>/var/lib/kubelet</code> or <code>/var/lib/containers</code> will not report ephemeral storage correctly.</div><h3 id="setting-requests-and-limits-for-local-ephemeral-storage">Setting requests and limits for local ephemeral storage</h3><p>You can specify <code>ephemeral-storage</code> for managing local ephemeral storage. Each
container of a Pod can specify either or both of the following:</p><ul><li><code>spec.containers[].resources.limits.ephemeral-storage</code></li><li><code>spec.containers[].resources.requests.ephemeral-storage</code></li></ul><p>Limits and requests for <code>ephemeral-storage</code> are measured in byte quantities.
You can express storage as a plain integer or as a fixed-point number using one of these suffixes:
E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following quantities all represent roughly the same value:</p><ul><li><code>128974848</code></li><li><code>129e6</code></li><li><code>129M</code></li><li><code>123Mi</code></li></ul><p>Pay attention to the case of the suffixes. If you request <code>400m</code> of ephemeral-storage, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (<code>400Mi</code>)
or 400 megabytes (<code>400M</code>).</p><p>In the following example, the Pod has two containers. Each container has a request of
2GiB of local ephemeral storage. Each container has a limit of 4GiB of local ephemeral
storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and
a limit of 8GiB of local ephemeral storage. 500Mi of that limit could be
consumed by the <code>emptyDir</code> volume.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>frontend<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>app<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>images.my-company.example/app:v4<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>ephemeral-storage</span>:<span> </span><span>"2Gi"</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>ephemeral-storage</span>:<span> </span><span>"4Gi"</span><span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>ephemeral<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/tmp"</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>log-aggregator<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>images.my-company.example/log-aggregator:v6<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>ephemeral-storage</span>:<span> </span><span>"2Gi"</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>ephemeral-storage</span>:<span> </span><span>"4Gi"</span><span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>ephemeral<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span><span>"/tmp"</span><span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>ephemeral<span>
</span></span></span><span><span><span>      </span><span>emptyDir</span>:<span>
</span></span></span><span><span><span>        </span><span>sizeLimit</span>:<span> </span>500Mi<span>
</span></span></span></code></pre></div><h3 id="how-pods-with-ephemeral-storage-requests-are-scheduled">How Pods with ephemeral-storage requests are scheduled</h3><p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods.
For more information, see
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">Node Allocatable</a>.</p><p>The scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.</p><h3 id="resource-emphemeralstorage-consumption">Ephemeral storage consumption management</h3><p>If the kubelet is managing local ephemeral storage as a resource, then the
kubelet measures storage use in:</p><ul><li><code>emptyDir</code> volumes, except <em>tmpfs</em> <code>emptyDir</code> volumes</li><li>directories holding node-level logs</li><li>writeable container layers</li></ul><p>If a Pod is using more ephemeral storage than you allow it to, the kubelet
sets an eviction signal that triggers Pod eviction.</p><p>For container-level isolation, if a container's writable layer and log
usage exceeds its storage limit, the kubelet marks the Pod for eviction.</p><p>For pod-level isolation the kubelet works out an overall Pod storage limit by
summing the limits for the containers in that Pod. In this case, if the sum of
the local ephemeral storage usage from all containers and also the Pod's <code>emptyDir</code>
volumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod
for eviction.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>If the kubelet is not measuring local ephemeral storage, then a Pod
that exceeds its local storage limit will not be evicted for breaching
local storage resource limits.</p><p>However, if the filesystem space for writeable container layers, node-level logs,
or <code>emptyDir</code> volumes falls low, the node
<a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">taints</a> itself as short on local storage
and this taint triggers eviction for any Pods that don't specifically tolerate the taint.</p><p>See the supported <a href="#configurations-for-local-ephemeral-storage">configurations</a>
for ephemeral local storage.</p></div><p>The kubelet supports different ways to measure Pod storage use:</p><ul class="nav nav-tabs" id="resource-emphemeralstorage-measurement"><li class="nav-item"><a class="nav-link active" href="#resource-emphemeralstorage-measurement-0">Periodic scanning</a></li><li class="nav-item"><a class="nav-link" href="#resource-emphemeralstorage-measurement-1">Filesystem project quota</a></li></ul><div class="tab-content" id="resource-emphemeralstorage-measurement"><div id="resource-emphemeralstorage-measurement-0" class="tab-pane show active"><p><p>The kubelet performs regular, scheduled checks that scan each
<code>emptyDir</code> volume, container log directory, and writeable container layer.</p><p>The scan measures how much space is used.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>In this mode, the kubelet does not track open file descriptors
for deleted files.</p><p>If you (or a container) create a file inside an <code>emptyDir</code> volume,
something then opens that file, and you delete the file while it is
still open, then the inode for the deleted file stays until you close
that file but the kubelet does not categorize the space as in use.</p></div></p></div><div id="resource-emphemeralstorage-measurement-1" class="tab-pane"><p><div class="feature-state-notice feature-beta" title="Feature Gate: LocalStorageCapacityIsolationFSQuotaMonitoring"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [beta]</code> (enabled by default: false)</div><p>Project quotas are an operating-system level feature for managing
storage use on filesystems. With Kubernetes, you can enable project
quotas for monitoring storage use. Make sure that the filesystem
backing the <code>emptyDir</code> volumes, on the node, provides project quota support.
For example, XFS and ext4fs offer project quotas.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Project quotas let you monitor storage use; they do not enforce limits.</div><p>Kubernetes uses project IDs starting from <code>1048576</code>. The IDs in use are
registered in <code>/etc/projects</code> and <code>/etc/projid</code>. If project IDs in
this range are used for other purposes on the system, those project
IDs must be registered in <code>/etc/projects</code> and <code>/etc/projid</code> so that
Kubernetes does not use them.</p><p>Quotas are faster and more accurate than directory scanning. When a
directory is assigned to a project, all files created under a
directory are created in that project, and the kernel merely has to
keep track of how many blocks are in use by files in that project.
If a file is created and deleted, but has an open file descriptor,
it continues to consume space. Quota tracking records that space accurately
whereas directory scans overlook the storage used by deleted files.</p><p>To use quotas to track a pod's resource usage, the pod must be in
a user namespace. Within user namespaces, the kernel restricts changes
to projectIDs on the filesystem, ensuring the reliability of storage
metrics calculated by quotas.</p><p>If you want to use project quotas, you should:</p><ul><li><p>Enable the <code>LocalStorageCapacityIsolationFSQuotaMonitoring=true</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
using the <code>featureGates</code> field in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration</a>.</p></li><li><p>Ensure the <code>UserNamespacesSupport</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled, and that the kernel, CRI implementation and OCI runtime support user namespaces.</p></li><li><p>Ensure that the root filesystem (or optional runtime filesystem)
has project quotas enabled. All XFS filesystems support project quotas.
For ext4 filesystems, you need to enable the project quota tracking feature
while the filesystem is not mounted.</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># For ext4, with /dev/block-device not mounted</span>
</span></span><span><span>sudo tune2fs -O project -Q prjquota /dev/block-device
</span></span></code></pre></div></li><li><p>Ensure that the root filesystem (or optional runtime filesystem) is
mounted with project quotas enabled. For both XFS and ext4fs, the
mount option is named <code>prjquota</code>.</p></li></ul><p>If you don't want to use project quotas, you should:</p><ul><li>Disable the <code>LocalStorageCapacityIsolationFSQuotaMonitoring</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
using the <code>featureGates</code> field in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration</a>.</li></ul></p></div></div><h2 id="extended-resources">Extended resources</h2><p>Extended resources are fully-qualified resource names outside the
<code>kubernetes.io</code> domain. They allow cluster operators to advertise and users to
consume the non-Kubernetes-built-in resources.</p><p>There are two steps required to use Extended Resources. First, the cluster
operator must advertise an Extended Resource. Second, users must request the
Extended Resource in Pods.</p><h3 id="managing-extended-resources">Managing extended resources</h3><h4 id="node-level-extended-resources">Node-level extended resources</h4><p>Node-level extended resources are tied to nodes.</p><h5 id="device-plugin-managed-resources">Device plugin managed resources</h5><p>See <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">Device
Plugin</a>
for how to advertise device plugin managed resources on each node.</p><h5 id="other-resources">Other resources</h5><p>To advertise a new node-level extended resource, the cluster operator can
submit a <code>PATCH</code> HTTP request to the API server to specify the available
quantity in the <code>status.capacity</code> for a node in the cluster. After this
operation, the node's <code>status.capacity</code> will include a new resource. The
<code>status.allocatable</code> field is updated automatically with the new resource
asynchronously by the kubelet.</p><p>Because the scheduler uses the node's <code>status.allocatable</code> value when
evaluating Pod fitness, the scheduler only takes account of the new value after
that asynchronous update. There may be a short delay between patching the
node capacity with a new resource and the time when the first Pod that requests
the resource can be scheduled on that node.</p><p><strong>Example:</strong></p><p>Here is an example showing how to use <code>curl</code> to form an HTTP request that
advertises five "example.com/foo" resources on node <code>k8s-node-1</code> whose master
is <code>k8s-master</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>curl --header <span>"Content-Type: application/json-patch+json"</span> <span>\
</span></span></span><span><span><span></span>--request PATCH <span>\
</span></span></span><span><span><span></span>--data <span>'[{"op": "add", "path": "/status/capacity/example.com~1foo", "value": "5"}]'</span> <span>\
</span></span></span><span><span><span></span>http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
</span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In the preceding request, <code>~1</code> is the encoding for the character <code>/</code>
in the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
<a href="https://tools.ietf.org/html/rfc6901#section-3">IETF RFC 6901, section 3</a>.</div><h4 id="cluster-level-extended-resources">Cluster-level extended resources</h4><p>Cluster-level extended resources are not tied to nodes. They are usually managed
by scheduler extenders, which handle the resource consumption and resource quota.</p><p>You can specify the extended resources that are handled by scheduler extenders
in <a href="/docs/reference/config-api/kube-scheduler-config.v1/">scheduler configuration</a></p><p><strong>Example:</strong></p><p>The following configuration for a scheduler policy indicates that the
cluster-level extended resource "example.com/foo" is handled by the scheduler
extender.</p><ul><li>The scheduler sends a Pod to the scheduler extender only if the Pod requests
"example.com/foo".</li><li>The <code>ignoredByScheduler</code> field specifies that the scheduler does not check
the "example.com/foo" resource in its <code>PodFitsResources</code> predicate.</li></ul><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"kind"</span>: <span>"Policy"</span>,
</span></span><span><span>  <span>"apiVersion"</span>: <span>"v1"</span>,
</span></span><span><span>  <span>"extenders"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"urlPrefix"</span>:<span>"&lt;extender-endpoint&gt;"</span>,
</span></span><span><span>      <span>"bindVerb"</span>: <span>"bind"</span>,
</span></span><span><span>      <span>"managedResources"</span>: [
</span></span><span><span>        {
</span></span><span><span>          <span>"name"</span>: <span>"example.com/foo"</span>,
</span></span><span><span>          <span>"ignoredByScheduler"</span>: <span>true</span>
</span></span><span><span>        }
</span></span><span><span>      ]
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><h4 id="extended-resources-allocation-by-dra">Extended resources allocation by DRA</h4><p>Extended resources allocation by DRA allows cluster administrators to specify an <code>extendedResourceName</code>
in DeviceClass, then the devices matching the DeviceClass can be requested from a pod's extended
resource requests. Read more about
<a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#extended-resource">Extended Resource allocation by DRA</a>.</p><h3 id="consuming-extended-resources">Consuming extended resources</h3><p>Users can consume extended resources in Pod specs like CPU and memory.
The scheduler takes care of the resource accounting so that no more than the
available amount is simultaneously allocated to Pods.</p><p>The API server restricts quantities of extended resources to whole numbers.
Examples of <em>valid</em> quantities are <code>3</code>, <code>3000m</code> and <code>3Ki</code>. Examples of
<em>invalid</em> quantities are <code>0.5</code> and <code>1500m</code> (because <code>1500m</code> would result in <code>1.5</code>).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Extended resources replace Opaque Integer Resources.
Users can use any domain name prefix other than <code>kubernetes.io</code> which is reserved.</div><p>To consume an extended resource in a Pod, include the resource name as a key
in the <code>spec.containers[].resources.limits</code> map in the container spec.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Extended resources cannot be overcommitted, so request and limit
must be equal if both are present in a container spec.</div><p>A Pod is scheduled only if all of the resource requests are satisfied, including
CPU, memory and any extended resources. The Pod remains in the <code>PENDING</code> state
as long as the resource request cannot be satisfied.</p><p><strong>Example:</strong></p><p>The Pod below requests 2 CPUs and 1 "example.com/foo" (an extended resource).</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>my-container<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>myimage<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>        </span><span>example.com/foo</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>example.com/foo</span>:<span> </span><span>1</span><span>
</span></span></span></code></pre></div><h2 id="pid-limiting">PID limiting</h2><p>Process ID (PID) limits allow for the configuration of a kubelet
to limit the number of PIDs that a given Pod can consume. See
<a href="/docs/concepts/policy/pid-limiting/">PID Limiting</a> for information.</p><h2 id="troubleshooting">Troubleshooting</h2><h3 id="my-pods-are-pending-with-event-message-failedscheduling">My Pods are pending with event message <code>FailedScheduling</code></h3><p>If the scheduler cannot find any node where a Pod can fit, the Pod remains
unscheduled until a place can be found. An
<a href="/docs/reference/kubernetes-api/cluster-resources/event-v1/">Event</a> is produced
each time the scheduler fails to find a place for the Pod. You can use <code>kubectl</code>
to view the events for a Pod; for example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod frontend | grep -A <span>9999999999</span> Events
</span></span></code></pre></div><pre tabindex="0"><code>Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu
</code></pre><p>In the preceding example, the Pod named "frontend" fails to be scheduled due to
insufficient CPU resource on any node. Similar error messages can also suggest
failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod
is pending with a message of this type, there are several things to try:</p><ul><li>Add more nodes to the cluster.</li><li>Terminate unneeded Pods to make room for pending Pods.</li><li>Check that the Pod is not larger than all the nodes. For example, if all the
nodes have a capacity of <code>cpu: 1</code>, then a Pod with a request of <code>cpu: 1.1</code> will
never be scheduled.</li><li>Check for node taints. If most of your nodes are tainted, and the new Pod does
not tolerate that taint, the scheduler only considers placements onto the
remaining nodes that don't have that taint.</li></ul><p>You can check node capacities and amounts allocated with the
<code>kubectl describe nodes</code> command. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe nodes e2e-test-node-pool-4lw4
</span></span></code></pre></div><pre tabindex="0"><code>Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)
</code></pre><p>In the preceding output, you can see that if a Pod requests more than 1.120 CPUs
or more than 6.23Gi of memory, that Pod will not fit on the node.</p><p>By looking at the &#8220;Pods&#8221; section, you can see which Pods are taking up space on
the node.</p><p>The amount of resources available to Pods is less than the node capacity because
system daemons use a portion of the available resources. Within the Kubernetes API,
each Node has a <code>.status.allocatable</code> field
(see <a href="/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeStatus">NodeStatus</a>
for details).</p><p>The <code>.status.allocatable</code> field describes the amount of resources that are available
to Pods on that node (for example: 15 virtual CPUs and 7538 MiB of memory).
For more information on node allocatable resources in Kubernetes, see
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons</a>.</p><p>You can configure <a href="/docs/concepts/policy/resource-quotas/">resource quotas</a>
to limit the total amount of resources that a namespace can consume.
Kubernetes enforces quotas for objects in particular namespace when there is a
ResourceQuota in that namespace.
For example, if you assign specific namespaces to different teams, you
can add ResourceQuotas into those namespaces. Setting resource quotas helps to
prevent one team from using so much of any resource that this over-use affects other teams.</p><p>You should also consider what access you grant to that namespace:
<strong>full</strong> write access to a namespace allows someone with that access to remove any
resource, including a configured ResourceQuota.</p><h3 id="my-container-is-terminated">My container is terminated</h3><p>Your container might get terminated because it is resource-starved. To check
whether a container is being killed because it is hitting a resource limit, call
<code>kubectl describe pod</code> on the Pod of interest:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe pod simmemleak-hra99
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex="0"><code>Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Containers:
  simmemleak:
    Image:  saadali/simmemleak:latest
    Limits:
      cpu:          100m
      memory:       50Mi
    State:          Running
      Started:      Tue, 07 Jul 2019 12:54:41 -0700
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Fri, 07 Jul 2019 12:54:30 -0700
      Finished:     Fri, 07 Jul 2019 12:54:33 -0700
    Ready:          False
    Restart Count:  5
Conditions:
  Type      Status
  Ready     False
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Normal  Pulled     41s   kubelet            Container image "saadali/simmemleak:latest" already present on machine
  Normal  Created    41s   kubelet            Created container simmemleak
  Normal  Started    40s   kubelet            Started container simmemleak
  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod
</code></pre><p>In the preceding example, the <code>Restart Count: 5</code> indicates that the <code>simmemleak</code>
container in the Pod was terminated and restarted five times (so far).
The <code>OOMKilled</code> reason shows that the container tried to use more memory than its limit.</p><p>Your next step might be to check the application code for a memory leak. If you
find that the application is behaving how you expect, consider setting a higher
memory limit (and possibly request) for that container.</p><h2 id="what-s-next">What's next</h2><ul><li>Get hands-on experience <a href="/docs/tasks/configure-pod-container/assign-memory-resource/">assigning Memory resources to containers and Pods</a>.</li><li>Get hands-on experience <a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">assigning CPU resources to containers and Pods</a>.</li><li>Read how the API reference defines a <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">container</a>
and its <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#resources">resource requirements</a></li><li>Read about <a href="https://www.linux.org/docs/man8/xfs_quota.html">project quotas</a> in XFS</li><li>Read more about the <a href="/docs/reference/config-api/kube-scheduler-config.v1/">kube-scheduler configuration reference (v1)</a></li><li>Read more about <a href="/docs/concepts/workloads/pods/pod-qos/">Quality of Service classes for Pods</a></li><li>Read more about <a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#extended-resource">Extended Resource allocation by DRA</a></li></ul></div></div><div><div class="td-content"><h1>Organizing Cluster Access Using kubeconfig Files</h1><p>Use kubeconfig files to organize information about clusters, users, namespaces, and
authentication mechanisms. The <code>kubectl</code> command-line tool uses kubeconfig files to
find the information it needs to choose a cluster and communicate with the API server
of a cluster.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A file that is used to configure access to clusters is called
a <em>kubeconfig file</em>. This is a generic way of referring to configuration files.
It does not mean that there is a file named <code>kubeconfig</code>.</div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig file could result in malicious code execution or file exposure.
If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script.</div><p>By default, <code>kubectl</code> looks for a file named <code>config</code> in the <code>$HOME/.kube</code> directory.
You can specify other kubeconfig files by setting the <code>KUBECONFIG</code> environment
variable or by setting the
<a href="/docs/reference/generated/kubectl/kubectl/"><code>--kubeconfig</code></a> flag.</p><p>For step-by-step instructions on creating and specifying kubeconfig files, see
<a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Configure Access to Multiple Clusters</a>.</p><h2 id="supporting-multiple-clusters-users-and-authentication-mechanisms">Supporting multiple clusters, users, and authentication mechanisms</h2><p>Suppose you have several clusters, and your users and components authenticate
in a variety of ways. For example:</p><ul><li>A running kubelet might authenticate using certificates.</li><li>A user might authenticate using tokens.</li><li>Administrators might have sets of certificates that they provide to individual users.</li></ul><p>With kubeconfig files, you can organize your clusters, users, and namespaces.
You can also define contexts to quickly and easily switch between
clusters and namespaces.</p><h2 id="context">Context</h2><p>A <em>context</em> element in a kubeconfig file is used to group access parameters
under a convenient name. Each context has three parameters: cluster, namespace, and user.
By default, the <code>kubectl</code> command-line tool uses parameters from
the <em>current context</em> to communicate with the cluster.</p><p>To choose the current context:</p><pre tabindex="0"><code>kubectl config use-context
</code></pre><h2 id="the-kubeconfig-environment-variable">The KUBECONFIG environment variable</h2><p>The <code>KUBECONFIG</code> environment variable holds a list of kubeconfig files.
For Linux and Mac, the list is colon-delimited. For Windows, the list
is semicolon-delimited. The <code>KUBECONFIG</code> environment variable is not
required. If the <code>KUBECONFIG</code> environment variable doesn't exist,
<code>kubectl</code> uses the default kubeconfig file, <code>$HOME/.kube/config</code>.</p><p>If the <code>KUBECONFIG</code> environment variable does exist, <code>kubectl</code> uses
an effective configuration that is the result of merging the files
listed in the <code>KUBECONFIG</code> environment variable.</p><h2 id="merging-kubeconfig-files">Merging kubeconfig files</h2><p>To see your configuration, enter this command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl config view
</span></span></code></pre></div><p>As described previously, the output might be from a single kubeconfig file,
or it might be the result of merging several kubeconfig files.</p><p>Here are the rules that <code>kubectl</code> uses when it merges kubeconfig files:</p><ol><li><p>If the <code>--kubeconfig</code> flag is set, use only the specified file. Do not merge.
Only one instance of this flag is allowed.</p><p>Otherwise, if the <code>KUBECONFIG</code> environment variable is set, use it as a
list of files that should be merged.
Merge the files listed in the <code>KUBECONFIG</code> environment variable
according to these rules:</p><ul><li>Ignore empty filenames.</li><li>Produce errors for files with content that cannot be deserialized.</li><li>The first file to set a particular value or map key wins.</li><li>Never change the value or map key.
Example: Preserve the context of the first file to set <code>current-context</code>.
Example: If two files specify a <code>red-user</code>, use only values from the first file's <code>red-user</code>.
Even if the second file has non-conflicting entries under <code>red-user</code>, discard them.</li></ul><p>For an example of setting the <code>KUBECONFIG</code> environment variable, see
<a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable">Setting the KUBECONFIG environment variable</a>.</p><p>Otherwise, use the default kubeconfig file, <code>$HOME/.kube/config</code>, with no merging.</p></li><li><p>Determine the context to use based on the first hit in this chain:</p><ol><li>Use the <code>--context</code> command-line flag if it exists.</li><li>Use the <code>current-context</code> from the merged kubeconfig files.</li></ol><p>An empty context is allowed at this point.</p></li><li><p>Determine the cluster and user. At this point, there might or might not be a context.
Determine the cluster and user based on the first hit in this chain,
which is run twice: once for user and once for cluster:</p><ol><li>Use a command-line flag if it exists: <code>--user</code> or <code>--cluster</code>.</li><li>If the context is non-empty, take the user or cluster from the context.</li></ol><p>The user and cluster can be empty at this point.</p></li><li><p>Determine the actual cluster information to use. At this point, there might or
might not be cluster information.
Build each piece of the cluster information based on this chain; the first hit wins:</p><ol><li>Use command line flags if they exist: <code>--server</code>, <code>--certificate-authority</code>, <code>--insecure-skip-tls-verify</code>.</li><li>If any cluster information attributes exist from the merged kubeconfig files, use them.</li><li>If there is no server location, fail.</li></ol></li><li><p>Determine the actual user information to use. Build user information using the same
rules as cluster information, except allow only one authentication
technique per user:</p><ol><li>Use command line flags if they exist: <code>--client-certificate</code>, <code>--client-key</code>, <code>--username</code>, <code>--password</code>, <code>--token</code>.</li><li>Use the <code>user</code> fields from the merged kubeconfig files.</li><li>If there are two conflicting techniques, fail.</li></ol></li><li><p>For any information still missing, use default values and potentially
prompt for authentication information.</p></li></ol><h2 id="file-references">File references</h2><p>File and path references in a kubeconfig file are relative to the location of the kubeconfig file.
File references on the command line are relative to the current working directory.
In <code>$HOME/.kube/config</code>, relative paths are stored relatively, and absolute paths
are stored absolutely.</p><h2 id="proxy">Proxy</h2><p>You can configure <code>kubectl</code> to use a proxy per cluster using <code>proxy-url</code> in your kubeconfig file, like this:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Config<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>clusters</span>:<span>
</span></span></span><span><span><span></span>- <span>cluster</span>:<span>
</span></span></span><span><span><span>    </span><span>proxy-url</span>:<span> </span>http://proxy.example.org:3128<span>
</span></span></span><span><span><span>    </span><span>server</span>:<span> </span>https://k8s.example.org/k8s/clusters/c-xxyyzz<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>development<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>users</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>developer<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>contexts</span>:<span>
</span></span></span><span><span><span></span>- <span>context</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>development<span>
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Configure Access to Multiple Clusters</a></li><li><a href="/docs/reference/generated/kubectl/kubectl-commands#config"><code>kubectl config</code></a></li></ul></div></div><div><div class="td-content"><h1>Resource Management for Windows nodes</h1><p>This page outlines the differences in how resources are managed between Linux and Windows.</p><p>On Linux nodes, <a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." href="/docs/reference/glossary/?all=true#term-cgroup" target="_blank">cgroups</a> are used
as a pod boundary for resource control. Containers are created within that boundary
for network, process and file system isolation. The Linux cgroup APIs can be used to
gather CPU, I/O, and memory use statistics.</p><p>In contrast, Windows uses a <a href="https://docs.microsoft.com/windows/win32/procthread/job-objects"><em>job object</em></a> per container with a system namespace filter
to contain all processes in a container and provide logical isolation from the
host.
(Job objects are a Windows process isolation mechanism and are different from
what Kubernetes refers to as a <a class="glossary-tooltip" title="A finite or batch task that runs to completion." href="/docs/concepts/workloads/controllers/job/" target="_blank">Job</a>).</p><p>There is no way to run a Windows container without the namespace filtering in
place. This means that system privileges cannot be asserted in the context of the
host, and thus privileged containers are not available on Windows.
Containers cannot assume an identity from the host because the Security Account Manager
(SAM) is separate.</p><h2 id="resource-management-memory">Memory management</h2><p>Windows does not have an out-of-memory process killer as Linux does. Windows always
treats all user-mode memory allocations as virtual, and pagefiles are mandatory.</p><p>Windows nodes do not overcommit memory for processes. The
net effect is that Windows won't reach out of memory conditions the same way Linux
does, and processes page to disk instead of being subject to out of memory (OOM)
termination. If memory is over-provisioned and all physical memory is exhausted,
then paging can slow down performance.</p><h2 id="resource-management-cpu">CPU management</h2><p>Windows can limit the amount of CPU time allocated for different processes but cannot
guarantee a minimum amount of CPU time.</p><p>On Windows, the kubelet supports a command-line flag to set the
<a href="https://docs.microsoft.com/windows/win32/procthread/scheduling-priorities">scheduling priority</a> of the
kubelet process: <code>--windows-priorityclass</code>. This flag allows the kubelet process to get
more CPU time slices when compared to other processes running on the Windows host.
More information on the allowable values and their meaning is available at
<a href="https://docs.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities#priority-class">Windows Priority Classes</a>.
To ensure that running Pods do not starve the kubelet of CPU cycles, set this flag to <code>ABOVE_NORMAL_PRIORITY_CLASS</code> or above.</p><h2 id="resource-reservation">Resource reservation</h2><p>To account for memory and CPU used by the operating system, the container runtime, and by
Kubernetes host processes such as the kubelet, you can (and should) reserve
memory and CPU resources with the <code>--kube-reserved</code> and/or <code>--system-reserved</code> kubelet flags.
On Windows these values are only used to calculate the node's
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">allocatable</a> resources.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>As you deploy workloads, set resource memory and CPU limits on containers.
This also subtracts from <code>NodeAllocatable</code> and helps the cluster-wide scheduler in determining which pods to place on which nodes.</p><p>Scheduling pods without limits may over-provision the Windows nodes and in extreme
cases can cause the nodes to become unhealthy.</p></div><p>On Windows, a good practice is to reserve at least 2GiB of memory.</p><p>To determine how much CPU to reserve,
identify the maximum pod density for each node and monitor the CPU usage of
the system services running there, then choose a value that meets your workload needs.</p></div></div><div><div class="td-content"><h1>Security</h1><div class="lead">Concepts for keeping your cloud-native workload secure.</div><p>This section of the Kubernetes documentation aims to help you learn to run
workloads more securely, and about the essential aspects of keeping a
Kubernetes cluster secure.</p><p>Kubernetes is based on a cloud-native architecture, and draws on advice from the
<a class="glossary-tooltip" title="Cloud Native Computing Foundation" href="https://cncf.io/" target="_blank">CNCF</a> about good practice for
cloud native information security.</p><p>Read <a href="/docs/concepts/security/cloud-native-security/">Cloud Native Security and Kubernetes</a>
for the broader context about how to secure your cluster and the applications that
you're running on it.</p><h2 id="security-mechanisms">Kubernetes security mechanisms</h2><p>Kubernetes includes several APIs and security controls, as well as ways to
define <a href="#policies">policies</a> that can form part of how you manage information security.</p><h3 id="control-plane-protection">Control plane protection</h3><p>A key security mechanism for any Kubernetes cluster is to
<a href="/docs/concepts/security/controlling-access/">control access to the Kubernetes API</a>.</p><p>Kubernetes expects you to configure and use TLS to provide
<a href="/docs/tasks/tls/managing-tls-in-a-cluster/">data encryption in transit</a>
within the control plane, and between the control plane and its clients.
You can also enable <a href="/docs/tasks/administer-cluster/encrypt-data/">encryption at rest</a>
for the data stored within Kubernetes control plane; this is separate from using
encryption at rest for your own workloads' data, which might also be a good idea.</p><h3 id="secrets">Secrets</h3><p>The <a href="/docs/concepts/configuration/secret/">Secret</a> API provides basic protection for
configuration values that require confidentiality.</p><h3 id="workload-protection">Workload protection</h3><p>Enforce <a href="/docs/concepts/security/pod-security-standards/">Pod security standards</a> to
ensure that Pods and their containers are isolated appropriately. You can also use
<a href="/docs/concepts/containers/runtime-class/">RuntimeClasses</a> to define custom isolation
if you need it.</p><p><a href="/docs/concepts/services-networking/network-policies/">Network policies</a> let you control
network traffic between Pods, or between Pods and the network outside your cluster.</p><p>You can deploy security controls from the wider ecosystem to implement preventative
or detective controls around Pods, their containers, and the images that run in them.</p><h3 id="admission-control">Admission control</h3><p><a href="/docs/reference/access-authn-authz/admission-controllers/">Admission controllers</a>
are plugins that intercept Kubernetes API requests and can validate or mutate
the requests based on specific fields in the request. Thoughtfully designing
these controllers helps to avoid unintended disruptions as Kubernetes APIs
change across version updates. For design considerations, see
<a href="/docs/concepts/cluster-administration/admission-webhooks-good-practices/">Admission Webhook Good Practices</a>.</p><h3 id="auditing">Auditing</h3><p>Kubernetes <a href="/docs/tasks/debug/debug-cluster/audit/">audit logging</a> provides a
security-relevant, chronological set of records documenting the sequence of actions
in a cluster. The cluster audits the activities generated by users, by applications
that use the Kubernetes API, and by the control plane itself.</p><h2 id="cloud-provider-security">Cloud provider security</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;Items on this page refer to vendors external to Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. To add a vendor, product or project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>If you are running a Kubernetes cluster on your own hardware or a different cloud provider,
consult your documentation for security best practices.
Here are links to some of the popular cloud providers' security documentation:</p><table><caption>Cloud provider security</caption><thead><tr><th>IaaS Provider</th><th>Link</th></tr></thead><tbody><tr><td>Alibaba Cloud</td><td><a href="https://www.alibabacloud.com/trust-center">https://www.alibabacloud.com/trust-center</a></td></tr><tr><td>Amazon Web Services</td><td><a href="https://aws.amazon.com/security">https://aws.amazon.com/security</a></td></tr><tr><td>Google Cloud Platform</td><td><a href="https://cloud.google.com/security">https://cloud.google.com/security</a></td></tr><tr><td>Huawei Cloud</td><td><a href="https://www.huaweicloud.com/intl/en-us/securecenter/overallsafety">https://www.huaweicloud.com/intl/en-us/securecenter/overallsafety</a></td></tr><tr><td>IBM Cloud</td><td><a href="https://www.ibm.com/cloud/security">https://www.ibm.com/cloud/security</a></td></tr><tr><td>Microsoft Azure</td><td><a href="https://docs.microsoft.com/en-us/azure/security/azure-security">https://docs.microsoft.com/en-us/azure/security/azure-security</a></td></tr><tr><td>Oracle Cloud Infrastructure</td><td><a href="https://www.oracle.com/security">https://www.oracle.com/security</a></td></tr><tr><td>Tencent Cloud</td><td><a href="https://www.tencentcloud.com/solutions/data-security-and-information-protection">https://www.tencentcloud.com/solutions/data-security-and-information-protection</a></td></tr><tr><td>VMware vSphere</td><td><a href="https://www.vmware.com/solutions/security/hardening-guides">https://www.vmware.com/solutions/security/hardening-guides</a></td></tr></tbody></table><h2 id="policies">Policies</h2><p>You can define security policies using Kubernetes-native mechanisms,
such as <a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy</a>
(declarative control over network packet filtering) or
<a href="/docs/reference/access-authn-authz/validating-admission-policy/">ValidatingAdmissionPolicy</a> (declarative restrictions on what changes
someone can make using the Kubernetes API).</p><p>However, you can also rely on policy implementations from the wider
ecosystem around Kubernetes. Kubernetes provides extension mechanisms
to let those ecosystem projects implement their own policy controls
on source code review, container image approval, API access controls,
networking, and more.</p><p>For more information about policy mechanisms and Kubernetes,
read <a href="/docs/concepts/policy/">Policies</a>.</p><h2 id="what-s-next">What's next</h2><p>Learn about related Kubernetes security topics:</p><ul><li><a href="/docs/tasks/administer-cluster/securing-a-cluster/">Securing your cluster</a></li><li><a href="/docs/reference/issues-security/official-cve-feed/">Known vulnerabilities</a>
in Kubernetes (and links to further information)</li><li><a href="/docs/tasks/tls/managing-tls-in-a-cluster/">Data encryption in transit</a> for the control plane</li><li><a href="/docs/tasks/administer-cluster/encrypt-data/">Data encryption at rest</a></li><li><a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a></li><li><a href="/docs/concepts/services-networking/network-policies/">Network policies</a> for Pods</li><li><a href="/docs/concepts/configuration/secret/">Secrets in Kubernetes</a></li><li><a href="/docs/concepts/security/pod-security-standards/">Pod security standards</a></li><li><a href="/docs/concepts/containers/runtime-class/">RuntimeClasses</a></li></ul><p>Learn the context:</p><ul><li><a href="/docs/concepts/security/cloud-native-security/">Cloud Native Security and Kubernetes</a></li></ul><p>Get certified:</p><ul><li><a href="https://training.linuxfoundation.org/certification/certified-kubernetes-security-specialist/">Certified Kubernetes Security Specialist</a>
certification and official training course.</li></ul><p>Read more in this section:</p><div class="section-index"><ul><li><a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a></li><li><a href="/docs/concepts/security/pod-security-admission/">Pod Security Admission</a></li><li><a href="/docs/concepts/security/service-accounts/">Service Accounts</a></li><li><a href="/docs/concepts/security/pod-security-policy/">Pod Security Policies</a></li><li><a href="/docs/concepts/security/linux-security/">Security For Linux Nodes</a></li><li><a href="/docs/concepts/security/windows-security/">Security For Windows Nodes</a></li><li><a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a></li><li><a href="/docs/concepts/security/rbac-good-practices/">Role Based Access Control Good Practices</a></li><li><a href="/docs/concepts/security/secrets-good-practices/">Good practices for Kubernetes Secrets</a></li><li><a href="/docs/concepts/security/multi-tenancy/">Multi-tenancy</a></li><li><a href="/docs/concepts/security/hardening-guide/authentication-mechanisms/">Hardening Guide - Authentication Mechanisms</a></li><li><a href="/docs/concepts/security/hardening-guide/scheduler/">Hardening Guide - Scheduler Configuration</a></li><li><a href="/docs/concepts/security/api-server-bypass-risks/">Kubernetes API Server Bypass Risks</a></li><li><a href="/docs/concepts/security/linux-kernel-security-constraints/">Linux kernel security constraints for Pods and containers</a></li><li><a href="/docs/concepts/security/security-checklist/">Security Checklist</a></li><li><a href="/docs/concepts/security/application-security-checklist/">Application Security Checklist</a></li></ul></div></div></div><div><div class="td-content"><h1>Cloud Native Security and Kubernetes</h1><div class="lead">Concepts for keeping your cloud-native workload secure.</div><p>Kubernetes is based on a cloud-native architecture, and draws on advice from the
<a class="glossary-tooltip" title="Cloud Native Computing Foundation" href="https://cncf.io/" target="_blank">CNCF</a> about good practice for
cloud native information security.</p><p>Read on through this page for an overview of how Kubernetes is designed to
help you deploy a secure cloud native platform.</p><h2 id="cloud-native-information-security">Cloud native information security</h2><p>The CNCF <a href="https://github.com/cncf/tag-security/blob/main/community/resources/security-whitepaper/v2/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf">white paper</a>
on cloud native security defines security controls and practices that are
appropriate to different <em>lifecycle phases</em>.</p><h2 id="lifecycle-phase-develop"><em>Develop</em> lifecycle phase</h2><ul><li>Ensure the integrity of development environments.</li><li>Design applications following good practice for information security,
appropriate for your context.</li><li>Consider end user security as part of solution design.</li></ul><p>To achieve this, you can:</p><ol><li>Adopt an architecture, such as <a href="https://glossary.cncf.io/zero-trust-architecture/">zero trust</a>,
that minimizes attack surfaces, even for internal threats.</li><li>Define a code review process that considers security concerns.</li><li>Build a <em>threat model</em> of your system or application that identifies
trust boundaries. Use that to model to identify risks and to help find
ways to treat those risks.</li><li>Incorporate advanced security automation, such as <em>fuzzing</em> and
<a href="https://glossary.cncf.io/security-chaos-engineering/">security chaos engineering</a>,
where it's justified.</li></ol><h2 id="lifecycle-phase-distribute"><em>Distribute</em> lifecycle phase</h2><ul><li>Ensure the security of the supply chain for container images you execute.</li><li>Ensure the security of the supply chain for the cluster and other components
that execute your application. An example of another component might be an
external database that your cloud-native application uses for persistence.</li></ul><p>To achieve this, you can:</p><ol><li>Scan container images and other artifacts for known vulnerabilities.</li><li>Ensure that software distribution uses encryption in transit, with
a chain of trust for the software source.</li><li>Adopt and follow processes to update dependencies when updates are
available, especially in response to security announcements.</li><li>Use validation mechanisms such as digital certificates for supply
chain assurance.</li><li>Subscribe to feeds and other mechanisms to alert you to security
risks.</li><li>Restrict access to artifacts. Place container images in a
<a href="/docs/concepts/containers/images/#using-a-private-registry">private registry</a>
that only allows authorized clients to pull images.</li></ol><h2 id="lifecycle-phase-deploy"><em>Deploy</em> lifecycle phase</h2><p>Ensure appropriate restrictions on what can be deployed, who can deploy it,
and where it can be deployed to.
You can enforce measures from the <em>distribute</em> phase, such as verifying the
cryptographic identity of container image artifacts.</p><p>You can deploy different applications and cluster components into different
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespaces</a>. Containers
themselves, and namespaces, both provide isolation mechanisms that are
relevant to information security.</p><p>When you deploy Kubernetes, you also set the foundation for your
applications' runtime environment: a Kubernetes cluster (or
multiple clusters).
That IT infrastructure must provide the security guarantees that higher
layers expect.</p><h2 id="lifecycle-phase-runtime"><em>Runtime</em> lifecycle phase</h2><p>The Runtime phase comprises three critical areas: <a href="#protection-runtime-access">access</a>,
<a href="#protection-runtime-compute">compute</a>, and <a href="#protection-runtime-storage">storage</a>.</p><h3 id="protection-runtime-access">Runtime protection: access</h3><p>The Kubernetes API is what makes your cluster work. Protecting this API is key
to providing effective cluster security.</p><p>Other pages in the Kubernetes documentation have more detail about how to set up
specific aspects of access control. The <a href="/docs/concepts/security/security-checklist/">security checklist</a>
has a set of suggested basic checks for your cluster.</p><p>Beyond that, securing your cluster means implementing effective
<a href="/docs/concepts/security/controlling-access/#authentication">authentication</a> and
<a href="/docs/concepts/security/controlling-access/#authorization">authorization</a> for API access. Use <a href="/docs/concepts/security/service-accounts/">ServiceAccounts</a> to
provide and manage security identities for workloads and cluster
components.</p><p>Kubernetes uses TLS to protect API traffic; make sure to deploy the cluster using
TLS (including for traffic between nodes and the control plane), and protect the
encryption keys. If you use Kubernetes' own API for
<a href="/docs/reference/access-authn-authz/certificate-signing-requests/#certificate-signing-requests">CertificateSigningRequests</a>,
pay special attention to restricting misuse there.</p><h3 id="protection-runtime-compute">Runtime protection: compute</h3><p><a class="glossary-tooltip" title="A lightweight and portable executable image that contains software and all of its dependencies." href="/docs/concepts/containers/" target="_blank">Containers</a> provide two
things: isolation between different applications, and a mechanism to combine
those isolated applications to run on the same host computer. Those two
aspects, isolation and aggregation, mean that runtime security involves
identifying trade-offs and finding an appropriate balance.</p><p>Kubernetes relies on a <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>
to actually set up and run containers. The Kubernetes project does
not recommend a specific container runtime and you should make sure that
the runtime(s) that you choose meet your information security needs.</p><p>To protect your compute at runtime, you can:</p><ol><li><p>Enforce <a href="/docs/concepts/security/pod-security-standards/">Pod security standards</a>
for applications, to help ensure they run with only the necessary privileges.</p></li><li><p>Run a specialized operating system on your nodes that is designed specifically
for running containerized workloads. This is typically based on a read-only
operating system (<em>immutable image</em>) that provides only the services
essential for running containers.</p><p>Container-specific operating systems help to isolate system components and
present a reduced attack surface in case of a container escape.</p></li><li><p>Define <a href="/docs/concepts/policy/resource-quotas/">ResourceQuotas</a> to
fairly allocate shared resources, and use
mechanisms such as <a href="/docs/concepts/policy/limit-range/">LimitRanges</a>
to ensure that Pods specify their resource requirements.</p></li><li><p>Partition workloads across different nodes.
Use <a href="/docs/concepts/scheduling-eviction/assign-pod-node/#node-isolation-restriction">node isolation</a>
mechanisms, either from Kubernetes itself or from the ecosystem, to ensure that
Pods with different trust contexts are run on separate sets of nodes.</p></li><li><p>Use a <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>
that provides security restrictions.</p></li><li><p>On Linux nodes, use a Linux security module such as <a href="/docs/tutorials/security/apparmor/">AppArmor</a>
or <a href="/docs/tutorials/security/seccomp/">seccomp</a>.</p></li></ol><h3 id="protection-runtime-storage">Runtime protection: storage</h3><p>To protect storage for your cluster and the applications that run there, you can:</p><ol><li>Integrate your cluster with an external storage plugin that provides encryption at
rest for volumes.</li><li>Enable <a href="/docs/tasks/administer-cluster/encrypt-data/">encryption at rest</a> for
API objects.</li><li>Protect data durability using backups. Verify that you can restore these, whenever you need to.</li><li>Authenticate connections between cluster nodes and any network storage they rely
upon.</li><li>Implement data encryption within your own application.</li></ol><p>For encryption keys, generating these within specialized hardware provides
the best protection against disclosure risks. A <em>hardware security module</em>
can let you perform cryptographic operations without allowing the security
key to be copied elsewhere.</p><h3 id="networking-and-security">Networking and security</h3><p>You should also consider network security measures, such as
<a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy</a> or a
<a href="https://glossary.cncf.io/service-mesh/">service mesh</a>.
Some network plugins for Kubernetes provide encryption for your
cluster network, using technologies such as a virtual
private network (VPN) overlay.
By design, Kubernetes lets you use your own networking plugin for your
cluster (if you use managed Kubernetes, the person or organization
managing your cluster may have chosen a network plugin for you).</p><p>The network plugin you choose and the way you integrate it can have a
strong impact on the security of information in transit.</p><h3 id="observability-and-runtime-security">Observability and runtime security</h3><p>Kubernetes lets you extend your cluster with extra tooling. You can set up third
party solutions to help you monitor or troubleshoot your applications and the
clusters they are running. You also get some basic observability features built
in to Kubernetes itself. Your code running in containers can generate logs,
publish metrics or provide other observability data; at deploy time, you need to
make sure your cluster provides an appropriate level of protection there.</p><p>If you set up a metrics dashboard or something similar, review the chain of components
that populate data into that dashboard, as well as the dashboard itself. Make sure
that the whole chain is designed with enough resilience and enough integrity protection
that you can rely on it even during an incident where your cluster might be degraded.</p><p>Where appropriate, deploy security measures below the level of Kubernetes
itself, such as cryptographically measured boot, or authenticated distribution
of time (which helps ensure the fidelity of logs and audit records).</p><p>For a high assurance environment, deploy cryptographic protections to ensure that
logs are both tamper-proof and confidential.</p><h2 id="what-s-next">What's next</h2><h3 id="further-reading-cloud-native">Cloud native security</h3><ul><li>CNCF <a href="https://github.com/cncf/tag-security/blob/main/community/resources/security-whitepaper/v2/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf">white paper</a>
on cloud native security.</li><li>CNCF <a href="https://github.com/cncf/tag-security/blob/f80844baaea22a358f5b20dca52cd6f72a32b066/supply-chain-security/supply-chain-security-paper/CNCF_SSCP_v1.pdf">white paper</a>
on good practices for securing a software supply chain.</li><li><a href="https://archive.fosdem.org/2020/schedule/event/kubernetes/">Fixing the Kubernetes clusterf**k: Understanding security from the kernel up</a> (FOSDEM 2020)</li><li><a href="https://www.youtube.com/watch?v=wqsUfvRyYpw">Kubernetes Security Best Practices</a> (Kubernetes Forum Seoul 2019)</li><li><a href="https://www.youtube.com/watch?v=EzSkU3Oecuw">Towards Measured Boot Out of the Box</a> (Linux Security Summit 2016)</li></ul><h3 id="further-reading-k8s">Kubernetes and information security</h3><ul><li><a href="/docs/concepts/security/">Kubernetes security</a></li><li><a href="/docs/tasks/administer-cluster/securing-a-cluster/">Securing your cluster</a></li><li><a href="/docs/tasks/tls/managing-tls-in-a-cluster/">Data encryption in transit</a> for the control plane</li><li><a href="/docs/tasks/administer-cluster/encrypt-data/">Data encryption at rest</a></li><li><a href="/docs/concepts/configuration/secret/">Secrets in Kubernetes</a></li><li><a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a></li><li><a href="/docs/concepts/services-networking/network-policies/">Network policies</a> for Pods</li><li><a href="/docs/concepts/security/pod-security-standards/">Pod security standards</a></li><li><a href="/docs/concepts/containers/runtime-class/">RuntimeClasses</a></li></ul></div></div><div><div class="td-content"><h1>Pod Security Standards</h1><div class="lead">A detailed look at the different policy levels defined in the Pod Security Standards.</div><p>The Pod Security Standards define three different <em>policies</em> to broadly cover the security
spectrum. These policies are <em>cumulative</em> and range from highly-permissive to highly-restrictive.
This guide outlines the requirements of each policy.</p><table><thead><tr><th>Profile</th><th>Description</th></tr></thead><tbody><tr><td><strong>Privileged</strong></td><td>Unrestricted policy, providing the widest possible level of permissions. This policy allows for known privilege escalations.</td></tr><tr><td><strong>Baseline</strong></td><td>Minimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) Pod configuration.</td></tr><tr><td><strong>Restricted</strong></td><td>Heavily restricted policy, following current Pod hardening best practices.</td></tr></tbody></table><h2 id="profile-details">Profile Details</h2><h3 id="privileged">Privileged</h3><p><strong>The <em>Privileged</em> policy is purposely-open, and entirely unrestricted.</strong> This type of policy is
typically aimed at system- and infrastructure-level workloads managed by privileged, trusted users.</p><p>The Privileged policy is defined by an absence of restrictions. If you define a Pod where the Privileged
security policy applies, the Pod you define is able to bypass typical container isolation mechanisms.
For example, you can define a Pod that has access to the node's host network.</p><h3 id="baseline">Baseline</h3><p><strong>The <em>Baseline</em> policy is aimed at ease of adoption for common containerized workloads while
preventing known privilege escalations.</strong> This policy is targeted at application operators and
developers of non-critical applications. The following listed controls should be
enforced/disallowed:</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In this table, wildcards (<code>*</code>) indicate all elements in a list. For example,
<code>spec.containers[*].securityContext</code> refers to the Security Context object for <em>all defined
containers</em>. If any of the listed containers fails to meet the requirements, the entire pod will
fail validation.</div><table><caption>Baseline policy specification</caption><tbody><tr><th>Control</th><th>Policy</th></tr><tr><td>HostProcess</td><td><p>Windows Pods offer the ability to run <a href="/docs/tasks/configure-pod-container/create-hostprocess-pod">HostProcess containers</a> which enables privileged access to the Windows host machine. Privileged access to the host is disallowed in the Baseline policy.<div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.windowsOptions.hostProcess</code></li><li><code>spec.containers[*].securityContext.windowsOptions.hostProcess</code></li><li><code>spec.initContainers[*].securityContext.windowsOptions.hostProcess</code></li><li><code>spec.ephemeralContainers[*].securityContext.windowsOptions.hostProcess</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td>Host Namespaces</td><td><p>Sharing the host namespaces must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.hostNetwork</code></li><li><code>spec.hostPID</code></li><li><code>spec.hostIPC</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td>Privileged Containers</td><td><p>Privileged Pods disable most security mechanisms and must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.privileged</code></li><li><code>spec.initContainers[*].securityContext.privileged</code></li><li><code>spec.ephemeralContainers[*].securityContext.privileged</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td>Capabilities</td><td><p>Adding additional capabilities beyond those listed below must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.add</code></li><li><code>spec.initContainers[*].securityContext.capabilities.add</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>AUDIT_WRITE</code></li><li><code>CHOWN</code></li><li><code>DAC_OVERRIDE</code></li><li><code>FOWNER</code></li><li><code>FSETID</code></li><li><code>KILL</code></li><li><code>MKNOD</code></li><li><code>NET_BIND_SERVICE</code></li><li><code>SETFCAP</code></li><li><code>SETGID</code></li><li><code>SETPCAP</code></li><li><code>SETUID</code></li><li><code>SYS_CHROOT</code></li></ul></td></tr><tr><td>HostPath Volumes</td><td><p>HostPath volumes must be forbidden.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.volumes[*].hostPath</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li></ul></td></tr><tr><td>Host Ports</td><td><p>HostPorts should be disallowed entirely (recommended) or restricted to a known list</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].ports[*].hostPort</code></li><li><code>spec.initContainers[*].ports[*].hostPort</code></li><li><code>spec.ephemeralContainers[*].ports[*].hostPort</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li>Known list (not supported by the built-in <a href="/docs/concepts/security/pod-security-admission/">Pod Security Admission controller</a>)</li><li><code>0</code></li></ul></td></tr><tr><td>Host Probes / Lifecycle Hooks (v1.34+)</td><td><p>The Host field in probes and lifecycle hooks must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].livenessProbe.httpGet.host</code></li><li><code>spec.containers[*].readinessProbe.httpGet.host</code></li><li><code>spec.containers[*].startupProbe.httpGet.host</code></li><li><code>spec.containers[*].livenessProbe.tcpSocket.host</code></li><li><code>spec.containers[*].readinessProbe.tcpSocket.host</code></li><li><code>spec.containers[*].startupProbe.tcpSocket.host</code></li><li><code>spec.containers[*].lifecycle.postStart.tcpSocket.host</code><li><code>spec.containers[*].lifecycle.preStop.tcpSocket.host</code><li><code>spec.containers[*].lifecycle.postStart.httpGet.host</code></li><li><code>spec.containers[*].lifecycle.preStop.httpGet.host</code></li><li><code>spec.initContainers[*].livenessProbe.httpGet.host</code></li><li><code>spec.initContainers[*].readinessProbe.httpGet.host</code></li><li><code>spec.initContainers[*].startupProbe.httpGet.host</code></li><li><code>spec.initContainers[*].livenessProbe.tcpSocket.host</code></li><li><code>spec.initContainers[*].readinessProbe.tcpSocket.host</code></li><li><code>spec.initContainers[*].startupProbe.tcpSocket.host</code></li><li><code>spec.initContainers[*].lifecycle.postStart.tcpSocket.host</code><li><code>spec.initContainers[*].lifecycle.preStop.tcpSocket.host</code><li><code>spec.initContainers[*].lifecycle.postStart.httpGet.host</code></li><li><code>spec.initContainers[*].lifecycle.preStop.httpGet.host</code></li></li></li></li></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li>""</li></ul></td></tr><tr><td>AppArmor</td><td><p>On supported hosts, the <code>RuntimeDefault</code> AppArmor profile is applied by default. The baseline policy should prevent overriding or disabling the default AppArmor profile, or restrict overrides to an allowed set of profiles.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.appArmorProfile.type</code></li><li><code>spec.containers[*].securityContext.appArmorProfile.type</code></li><li><code>spec.initContainers[*].securityContext.appArmorProfile.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.appArmorProfile.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>RuntimeDefault</code></li><li><code>Localhost</code></li></ul><hr><ul><li><code>metadata.annotations["container.apparmor.security.beta.kubernetes.io/*"]</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>runtime/default</code></li><li><code>localhost/*</code></li></ul></td></tr><tr><td>SELinux</td><td><p>Setting the SELinux type is restricted, and setting a custom SELinux user or role option is forbidden.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seLinuxOptions.type</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.type</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/""</li><li><code>container_t</code></li><li><code>container_init_t</code></li><li><code>container_kvm_t</code></li><li><code>container_engine_t</code> (since Kubernetes 1.31)</li></ul><hr><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seLinuxOptions.user</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.securityContext.seLinuxOptions.role</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.role</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.role</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.role</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/""</li></ul></td></tr><tr><td><code>/proc</code> Mount Type</td><td><p>The default <code>/proc</code> masks are set up to reduce attack surface, and should be required.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.procMount</code></li><li><code>spec.initContainers[*].securityContext.procMount</code></li><li><code>spec.ephemeralContainers[*].securityContext.procMount</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>Default</code></li></ul></td></tr><tr><td>Seccomp</td><td><p>Seccomp profile must not be explicitly set to <code>Unconfined</code>.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seccompProfile.type</code></li><li><code>spec.containers[*].securityContext.seccompProfile.type</code></li><li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>RuntimeDefault</code></li><li><code>Localhost</code></li></ul></td></tr><tr><td>Sysctls</td><td><p>Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed "safe" subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.sysctls[*].name</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>kernel.shm_rmid_forced</code></li><li><code>net.ipv4.ip_local_port_range</code></li><li><code>net.ipv4.ip_unprivileged_port_start</code></li><li><code>net.ipv4.tcp_syncookies</code></li><li><code>net.ipv4.ping_group_range</code></li><li><code>net.ipv4.ip_local_reserved_ports</code> (since Kubernetes 1.27)</li><li><code>net.ipv4.tcp_keepalive_time</code> (since Kubernetes 1.29)</li><li><code>net.ipv4.tcp_fin_timeout</code> (since Kubernetes 1.29)</li><li><code>net.ipv4.tcp_keepalive_intvl</code> (since Kubernetes 1.29)</li><li><code>net.ipv4.tcp_keepalive_probes</code> (since Kubernetes 1.29)</li></ul></td></tr></tbody></table><h3 id="restricted">Restricted</h3><p><strong>The <em>Restricted</em> policy is aimed at enforcing current Pod hardening best practices, at the
expense of some compatibility.</strong> It is targeted at operators and developers of security-critical
applications, as well as lower-trust users. The following listed controls should be
enforced/disallowed:</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In this table, wildcards (<code>*</code>) indicate all elements in a list. For example,
<code>spec.containers[*].securityContext</code> refers to the Security Context object for <em>all defined
containers</em>. If any of the listed containers fails to meet the requirements, the entire pod will
fail validation.</div><table><caption>Restricted policy specification</caption><tbody><tr><td><strong>Control</strong></td><td><strong>Policy</strong></td></tr><tr><td colspan="2"><em>Everything from the Baseline policy</em></td></tr><tr><td>Volume Types</td><td><p>The Restricted policy only permits the following volume types.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.volumes[*]</code></li></ul><p><strong>Allowed Values</strong></p>Every item in the <code>spec.volumes[*]</code> list must set one of the following fields to a non-null value:<ul><li><code>spec.volumes[*].configMap</code></li><li><code>spec.volumes[*].csi</code></li><li><code>spec.volumes[*].downwardAPI</code></li><li><code>spec.volumes[*].emptyDir</code></li><li><code>spec.volumes[*].ephemeral</code></li><li><code>spec.volumes[*].persistentVolumeClaim</code></li><li><code>spec.volumes[*].projected</code></li><li><code>spec.volumes[*].secret</code></li></ul></td></tr><tr><td>Privilege Escalation (v1.8+)</td><td><p>Privilege escalation (such as via set-user-ID or set-group-ID file mode) should not be allowed. <em><a href="#os-specific-policy-controls">This is Linux only policy</a> in v1.25+ <code>(spec.os.name != windows)</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.initContainers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.ephemeralContainers[*].securityContext.allowPrivilegeEscalation</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>false</code></li></ul></td></tr><tr><td>Running as Non-root</td><td><p>Containers must be required to run as non-root users.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.runAsNonRoot</code></li><li><code>spec.containers[*].securityContext.runAsNonRoot</code></li><li><code>spec.initContainers[*].securityContext.runAsNonRoot</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsNonRoot</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>true</code></li></ul><small>The container fields may be undefined/<code>nil</code> if the pod-level
<code>spec.securityContext.runAsNonRoot</code> is set to <code>true</code>.</small></td></tr><tr><td>Running as Non-root user (v1.23+)</td><td><p>Containers must not set <tt>runAsUser</tt> to 0</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.runAsUser</code></li><li><code>spec.containers[*].securityContext.runAsUser</code></li><li><code>spec.initContainers[*].securityContext.runAsUser</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsUser</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>any non-zero value</li><li><code>undefined/null</code></li></ul></td></tr><tr><td>Seccomp (v1.19+)</td><td><p>Seccomp profile must be explicitly set to one of the allowed values. Both the <code>Unconfined</code> profile and the <em>absence</em> of a profile are prohibited. <em><a href="#os-specific-policy-controls">This is Linux only policy</a> in v1.25+ <code>(spec.os.name != windows)</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seccompProfile.type</code></li><li><code>spec.containers[*].securityContext.seccompProfile.type</code></li><li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>RuntimeDefault</code></li><li><code>Localhost</code></li></ul><small>The container fields may be undefined/<code>nil</code> if the pod-level
<code>spec.securityContext.seccompProfile.type</code> field is set appropriately.
Conversely, the pod-level field may be undefined/<code>nil</code> if _all_ container-
level fields are set.</small></td></tr><tr><td>Capabilities (v1.22+)</td><td><p>Containers must drop <code>ALL</code> capabilities, and are only permitted to add back
the <code>NET_BIND_SERVICE</code> capability. <em><a href="#os-specific-policy-controls">This is Linux only policy</a> in v1.25+ <code>(.spec.os.name != "windows")</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.drop</code></li><li><code>spec.initContainers[*].securityContext.capabilities.drop</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.drop</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Any list of capabilities that includes <code>ALL</code></li></ul><hr><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.add</code></li><li><code>spec.initContainers[*].securityContext.capabilities.add</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>NET_BIND_SERVICE</code></li></ul></td></tr></tbody></table><h2 id="policy-instantiation">Policy Instantiation</h2><p>Decoupling policy definition from policy instantiation allows for a common understanding and
consistent language of policies across clusters, independent of the underlying enforcement
mechanism.</p><p>As mechanisms mature, they will be defined below on a per-policy basis. The methods of enforcement
of individual policies are not defined here.</p><p><a href="/docs/concepts/security/pod-security-admission/"><strong>Pod Security Admission Controller</strong></a></p><ul><li><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-privileged.yaml">Privileged namespace</a></li><li><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-baseline.yaml">Baseline namespace</a></li><li><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-restricted.yaml">Restricted namespace</a></li></ul><h3 id="alternatives">Alternatives</h3><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Other alternatives for enforcing policies are being developed in the Kubernetes ecosystem, such as:</p><ul><li><a href="https://github.com/kubewarden">Kubewarden</a></li><li><a href="https://kyverno.io/policies/pod-security/">Kyverno</a></li><li><a href="https://github.com/open-policy-agent/gatekeeper">OPA Gatekeeper</a></li></ul><h2 id="pod-os-field">Pod OS field</h2><p>Kubernetes lets you use nodes that run either Linux or Windows. You can mix both kinds of
node in one cluster.
Windows in Kubernetes has some limitations and differentiators from Linux-based
workloads. Specifically, many of the Pod <code>securityContext</code> fields
<a href="/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext">have no effect on Windows</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubelets prior to v1.24 don't enforce the pod OS field, and if a cluster has nodes on versions earlier than v1.24 the Restricted policies should be pinned to a version prior to v1.25.</div><h3 id="restricted-pod-security-standard-changes">Restricted Pod Security Standard changes</h3><p>Another important change, made in Kubernetes v1.25 is that the <em>Restricted</em> policy
has been updated to use the <code>pod.spec.os.name</code> field. Based on the OS name, certain policies that are specific
to a particular OS can be relaxed for the other OS.</p><h4 id="os-specific-policy-controls">OS-specific policy controls</h4><p>Restrictions on the following controls are only required if <code>.spec.os.name</code> is not <code>windows</code>:</p><ul><li>Privilege Escalation</li><li>Seccomp</li><li>Linux Capabilities</li></ul><h2 id="user-namespaces">User namespaces</h2><p>User Namespaces are a Linux-only feature to run workloads with increased
isolation. How they work together with Pod Security Standards is described in
the <a href="/docs/concepts/workloads/pods/user-namespaces/#integration-with-pod-security-admission-checks">documentation</a> for Pods that use user namespaces.</p><h2 id="faq">FAQ</h2><h3 id="why-isn-t-there-a-profile-between-privileged-and-baseline">Why isn't there a profile between Privileged and Baseline?</h3><p>The three profiles defined here have a clear linear progression from most secure (Restricted) to least
secure (Privileged), and cover a broad set of workloads. Privileges required above the Baseline
policy are typically very application specific, so we do not offer a standard profile in this
niche. This is not to say that the privileged profile should always be used in this case, but that
policies in this space need to be defined on a case-by-case basis.</p><p>SIG Auth may reconsider this position in the future, should a clear need for other profiles arise.</p><h3 id="what-s-the-difference-between-a-security-profile-and-a-security-context">What's the difference between a security profile and a security context?</h3><p><a href="/docs/tasks/configure-pod-container/security-context/">Security Contexts</a> configure Pods and
Containers at runtime. Security contexts are defined as part of the Pod and container specifications
in the Pod manifest, and represent parameters to the container runtime.</p><p>Security profiles are control plane mechanisms to enforce specific settings in the Security Context,
as well as other related parameters outside the Security Context. As of July 2021,
<a href="/docs/concepts/security/pod-security-policy/">Pod Security Policies</a> are deprecated in favor of the
built-in <a href="/docs/concepts/security/pod-security-admission/">Pod Security Admission Controller</a>.</p><h3 id="what-about-sandboxed-pods">What about sandboxed Pods?</h3><p>There is currently no API standard that controls whether a Pod is considered sandboxed or
not. Sandbox Pods may be identified by the use of a sandboxed runtime (such as gVisor or Kata
Containers), but there is no standard definition of what a sandboxed runtime is.</p><p>The protections necessary for sandboxed workloads can differ from others. For example, the need to
restrict privileged permissions is lessened when the workload is isolated from the underlying
kernel. This allows for workloads requiring heightened permissions to still be isolated.</p><p>Additionally, the protection of sandboxed workloads is highly dependent on the method of
sandboxing. As such, no single recommended profile is recommended for all sandboxed workloads.</p></div></div><div><div class="td-content"><h1>Pod Security Admission</h1><div class="lead">An overview of the Pod Security Admission Controller, which can enforce the Pod Security Standards.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.25 [stable]</code></div><p>The Kubernetes <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a> define
different isolation levels for Pods. These standards let you define how you want to restrict the
behavior of pods in a clear, consistent fashion.</p><p>Kubernetes offers a built-in <em>Pod Security</em> <a class="glossary-tooltip" title="A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object." href="/docs/reference/access-authn-authz/admission-controllers/" target="_blank">admission controller</a> to enforce the Pod Security Standards. Pod security restrictions
are applied at the <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a> level when pods are
created.</p><h3 id="built-in-pod-security-admission-enforcement">Built-in Pod Security admission enforcement</h3><p>This page is part of the documentation for Kubernetes v1.34.
If you are running a different version of Kubernetes, consult the documentation for that release.</p><h2 id="pod-security-levels">Pod Security levels</h2><p>Pod Security admission places requirements on a Pod's <a href="/docs/tasks/configure-pod-container/security-context/">Security
Context</a> and other related fields according
to the three levels defined by the <a href="/docs/concepts/security/pod-security-standards/">Pod Security
Standards</a>: <code>privileged</code>, <code>baseline</code>, and
<code>restricted</code>. Refer to the <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>
page for an in-depth look at those requirements.</p><h2 id="pod-security-admission-labels-for-namespaces">Pod Security Admission labels for namespaces</h2><p>Once the feature is enabled or the webhook is installed, you can configure namespaces to define the admission
control mode you want to use for pod security in each namespace. Kubernetes defines a set of
<a class="glossary-tooltip" title="Tags objects with identifying attributes that are meaningful and relevant to users." href="/docs/concepts/overview/working-with-objects/labels" target="_blank">labels</a> that you can set to define which of the
predefined Pod Security Standard levels you want to use for a namespace. The label you select
defines what action the <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>
takes if a potential violation is detected:</p><table><caption>Pod Security Admission modes</caption><thead><tr><th>Mode</th><th>Description</th></tr></thead><tbody><tr><td><strong>enforce</strong></td><td>Policy violations will cause the pod to be rejected.</td></tr><tr><td><strong>audit</strong></td><td>Policy violations will trigger the addition of an audit annotation to the event recorded in the <a href="/docs/tasks/debug/debug-cluster/audit/">audit log</a>, but are otherwise allowed.</td></tr><tr><td><strong>warn</strong></td><td>Policy violations will trigger a user-facing warning, but are otherwise allowed.</td></tr></tbody></table><p>A namespace can configure any or all modes, or even set a different level for different modes.</p><p>For each mode, there are two labels that determine the policy used:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># The per-mode level label indicates which policy level to apply for the mode.</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span># MODE must be one of `enforce`, `audit`, or `warn`.</span><span>
</span></span></span><span><span><span></span><span># LEVEL must be one of `privileged`, `baseline`, or `restricted`.</span><span>
</span></span></span><span><span><span></span><span>pod-security.kubernetes.io/&lt;MODE&gt;</span>:<span> </span>&lt;LEVEL&gt;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># Optional: per-mode version label that can be used to pin the policy to the</span><span>
</span></span></span><span><span><span></span><span># version that shipped with a given Kubernetes minor version (for example v1.34).</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span># MODE must be one of `enforce`, `audit`, or `warn`.</span><span>
</span></span></span><span><span><span></span><span># VERSION must be a valid Kubernetes minor version, or `latest`.</span><span>
</span></span></span><span><span><span></span><span>pod-security.kubernetes.io/&lt;MODE&gt;-version</span>:<span> </span>&lt;VERSION&gt;<span>
</span></span></span></code></pre></div><p>Check out <a href="/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/">Enforce Pod Security Standards with Namespace Labels</a> to see example usage.</p><h2 id="workload-resources-and-pod-templates">Workload resources and Pod templates</h2><p>Pods are often created indirectly, by creating a <a href="/docs/concepts/workloads/controllers/">workload
object</a> such as a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a> or <a class="glossary-tooltip" title="A finite or batch task that runs to completion." href="/docs/concepts/workloads/controllers/job/" target="_blank">Job</a>. The workload object defines a
<em>Pod template</em> and a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> for the
workload resource creates Pods based on that template. To help catch violations early, both the
audit and warning modes are applied to the workload resources. However, enforce mode is <strong>not</strong>
applied to workload resources, only to the resulting pod objects.</p><h2 id="exemptions">Exemptions</h2><p>You can define <em>exemptions</em> from pod security enforcement in order to allow the creation of pods that
would have otherwise been prohibited due to the policy associated with a given namespace.
Exemptions can be statically configured in the
<a href="/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller">Admission Controller configuration</a>.</p><p>Exemptions must be explicitly enumerated. Requests meeting exemption criteria are <em>ignored</em> by the
Admission Controller (all <code>enforce</code>, <code>audit</code> and <code>warn</code> behaviors are skipped). Exemption dimensions include:</p><ul><li><strong>Usernames:</strong> requests from users with an exempt authenticated (or impersonated) username are
ignored.</li><li><strong>RuntimeClassNames:</strong> pods and <a href="#workload-resources-and-pod-templates">workload resources</a> specifying an exempt runtime class name are
ignored.</li><li><strong>Namespaces:</strong> pods and <a href="#workload-resources-and-pod-templates">workload resources</a> in an exempt namespace are ignored.</li></ul><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Most pods are created by a controller in response to a <a href="#workload-resources-and-pod-templates">workload
resource</a>, meaning that exempting an end user will only
exempt them from enforcement when creating pods directly, but not when creating a workload resource.
Controller service accounts (such as <code>system:serviceaccount:kube-system:replicaset-controller</code>)
should generally not be exempted, as doing so would implicitly exempt any user that can create the
corresponding workload resource.</div><p>Updates to the following pod fields are exempt from policy checks, meaning that if a pod update
request only changes these fields, it will not be denied even if the pod is in violation of the
current policy level:</p><ul><li>Any metadata updates <strong>except</strong> changes to the seccomp or AppArmor annotations:<ul><li><code>seccomp.security.alpha.kubernetes.io/pod</code> (deprecated)</li><li><code>container.seccomp.security.alpha.kubernetes.io/*</code> (deprecated)</li><li><code>container.apparmor.security.beta.kubernetes.io/*</code> (deprecated)</li></ul></li><li>Valid updates to <code>.spec.activeDeadlineSeconds</code></li><li>Valid updates to <code>.spec.tolerations</code></li></ul><h2 id="metrics">Metrics</h2><p>Here are the Prometheus metrics exposed by kube-apiserver:</p><ul><li><code>pod_security_errors_total</code>: This metric indicates the number of errors preventing normal evaluation.
Non-fatal errors may result in the latest restricted profile being used for enforcement.</li><li><code>pod_security_evaluations_total</code>: This metric indicates the number of policy evaluations that have occurred,
not counting ignored or exempt requests during exporting.</li><li><code>pod_security_exemptions_total</code>: This metric indicates the number of exempt requests, not counting ignored
or out of scope requests.</li></ul><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a></li><li><a href="/docs/setup/best-practices/enforcing-pod-security-standards/">Enforcing Pod Security Standards</a></li><li><a href="/docs/tasks/configure-pod-container/enforce-standards-admission-controller/">Enforce Pod Security Standards by Configuring the Built-in Admission Controller</a></li><li><a href="/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/">Enforce Pod Security Standards with Namespace Labels</a></li></ul><p>If you are running an older version of Kubernetes and want to upgrade
to a version of Kubernetes that does not include PodSecurityPolicies,
read <a href="/docs/tasks/configure-pod-container/migrate-from-psp/">migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a>.</p></div></div><div><div class="td-content"><h1>Service Accounts</h1><div class="lead">Learn about ServiceAccount objects in Kubernetes.</div><p>This page introduces the ServiceAccount object in Kubernetes, providing
information about how service accounts work, use cases, limitations,
alternatives, and links to resources for additional guidance.</p><h2 id="what-are-service-accounts">What are service accounts?</h2><p>A service account is a type of non-human account that, in Kubernetes, provides
a distinct identity in a Kubernetes cluster. Application Pods, system
components, and entities inside and outside the cluster can use a specific
ServiceAccount's credentials to identify as that ServiceAccount. This identity
is useful in various situations, including authenticating to the API server or
implementing identity-based security policies.</p><p>Service accounts exist as ServiceAccount objects in the API server. Service
accounts have the following properties:</p><ul><li><p><strong>Namespaced:</strong> Each service account is bound to a Kubernetes
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>. Every namespace
gets a <a href="#default-service-accounts"><code>default</code> ServiceAccount</a> upon creation.</p></li><li><p><strong>Lightweight:</strong> Service accounts exist in the cluster and are
defined in the Kubernetes API. You can quickly create service accounts to
enable specific tasks.</p></li><li><p><strong>Portable:</strong> A configuration bundle for a complex containerized workload
might include service account definitions for the system's components. The
lightweight nature of service accounts and the namespaced identities make
the configurations portable.</p></li></ul><p>Service accounts are different from user accounts, which are authenticated
human users in the cluster. By default, user accounts don't exist in the Kubernetes
API server; instead, the API server treats user identities as opaque
data. You can authenticate as a user account using multiple methods. Some
Kubernetes distributions might add custom extension APIs to represent user
accounts in the API server.</p><table><caption>Comparison between service accounts and users</caption><thead><tr><th>Description</th><th>ServiceAccount</th><th>User or group</th></tr></thead><tbody><tr><td>Location</td><td>Kubernetes API (ServiceAccount object)</td><td>External</td></tr><tr><td>Access control</td><td>Kubernetes RBAC or other <a href="/docs/reference/access-authn-authz/authorization/#authorization-modules">authorization mechanisms</a></td><td>Kubernetes RBAC or other identity and access management mechanisms</td></tr><tr><td>Intended use</td><td>Workloads, automation</td><td>People</td></tr></tbody></table><h3 id="default-service-accounts">Default service accounts</h3><p>When you create a cluster, Kubernetes automatically creates a ServiceAccount
object named <code>default</code> for every namespace in your cluster. The <code>default</code>
service accounts in each namespace get no permissions by default other than the
<a href="/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings">default API discovery permissions</a>
that Kubernetes grants to all authenticated principals if role-based access control (RBAC) is enabled.
If you delete the <code>default</code> ServiceAccount object in a namespace, the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>
replaces it with a new one.</p><p>If you deploy a Pod in a namespace, and you don't
<a href="#assign-to-pod">manually assign a ServiceAccount to the Pod</a>, Kubernetes
assigns the <code>default</code> ServiceAccount for that namespace to the Pod.</p><h2 id="use-cases">Use cases for Kubernetes service accounts</h2><p>As a general guideline, you can use service accounts to provide identities in
the following scenarios:</p><ul><li>Your Pods need to communicate with the Kubernetes API server, for example in
situations such as the following:<ul><li>Providing read-only access to sensitive information stored in Secrets.</li><li>Granting <a href="#cross-namespace">cross-namespace access</a>, such as allowing a
Pod in namespace <code>example</code> to read, list, and watch for Lease objects in
the <code>kube-node-lease</code> namespace.</li></ul></li><li>Your Pods need to communicate with an external service. For example, a
workload Pod requires an identity for a commercially available cloud API,
and the commercial provider allows configuring a suitable trust relationship.</li><li><a href="/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">Authenticating to a private image registry using an <code>imagePullSecret</code></a>.</li><li>An external service needs to communicate with the Kubernetes API server. For
example, authenticating to the cluster as part of a CI/CD pipeline.</li><li>You use third-party security software in your cluster that relies on the
ServiceAccount identity of different Pods to group those Pods into different
contexts.</li></ul><h2 id="how-to-use">How to use service accounts</h2><p>To use a Kubernetes service account, you do the following:</p><ol><li><p>Create a ServiceAccount object using a Kubernetes
client like <code>kubectl</code> or a manifest that defines the object.</p></li><li><p>Grant permissions to the ServiceAccount object using an authorization
mechanism such as
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a>.</p></li><li><p>Assign the ServiceAccount object to Pods during Pod creation.</p><p>If you're using the identity from an external service,
<a href="#get-a-token">retrieve the ServiceAccount token</a> and use it from that
service instead.</p></li></ol><p>For instructions, refer to
<a href="/docs/tasks/configure-pod-container/configure-service-account/">Configure Service Accounts for Pods</a>.</p><h3 id="grant-permissions">Grant permissions to a ServiceAccount</h3><p>You can use the built-in Kubernetes
<a href="/docs/reference/access-authn-authz/rbac/">role-based access control (RBAC)</a>
mechanism to grant the minimum permissions required by each service account.
You create a <em>role</em>, which grants access, and then <em>bind</em> the role to your
ServiceAccount. RBAC lets you define a minimum set of permissions so that the
service account permissions follow the principle of least privilege. Pods that
use that service account don't get more permissions than are required to
function correctly.</p><p>For instructions, refer to
<a href="/docs/reference/access-authn-authz/rbac/#service-account-permissions">ServiceAccount permissions</a>.</p><h4 id="cross-namespace">Cross-namespace access using a ServiceAccount</h4><p>You can use RBAC to allow service accounts in one namespace to perform actions
on resources in a different namespace in the cluster. For example, consider a
scenario where you have a service account and Pod in the <code>dev</code> namespace and
you want your Pod to see Jobs running in the <code>maintenance</code> namespace. You could
create a Role object that grants permissions to list Job objects. Then,
you'd create a RoleBinding object in the <code>maintenance</code> namespace to bind the
Role to the ServiceAccount object. Now, Pods in the <code>dev</code> namespace can list
Job objects in the <code>maintenance</code> namespace using that service account.</p><h3 id="assign-to-pod">Assign a ServiceAccount to a Pod</h3><p>To assign a ServiceAccount to a Pod, you set the <code>spec.serviceAccountName</code>
field in the Pod specification. Kubernetes then automatically provides the
credentials for that ServiceAccount to the Pod. In v1.22 and later, Kubernetes
gets a short-lived, <strong>automatically rotating</strong> token using the <code>TokenRequest</code>
API and mounts the token as a
<a href="/docs/concepts/storage/projected-volumes/#serviceaccounttoken">projected volume</a>.</p><p>By default, Kubernetes provides the Pod
with the credentials for an assigned ServiceAccount, whether that is the
<code>default</code> ServiceAccount or a custom ServiceAccount that you specify.</p><p>To prevent Kubernetes from automatically injecting
credentials for a specified ServiceAccount or the <code>default</code> ServiceAccount, set the
<code>automountServiceAccountToken</code> field in your Pod specification to <code>false</code>.</p><p>In versions earlier than 1.22, Kubernetes provides a long-lived, static token
to the Pod as a Secret.</p><h4 id="get-a-token">Manually retrieve ServiceAccount credentials</h4><p>If you need the credentials for a ServiceAccount to mount in a non-standard
location, or for an audience that isn't the API server, use one of the
following methods:</p><ul><li><a href="/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">TokenRequest API</a>
(recommended): Request a short-lived service account token from within
your own <em>application code</em>. The token expires automatically and can rotate
upon expiration.
If you have a legacy application that is not aware of Kubernetes, you
could use a sidecar container within the same pod to fetch these tokens
and make them available to the application workload.</li><li><a href="/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection">Token Volume Projection</a>
(also recommended): In Kubernetes v1.20 and later, use the Pod specification to
tell the kubelet to add the service account token to the Pod as a
<em>projected volume</em>. Projected tokens expire automatically, and the kubelet
rotates the token before it expires.</li><li><a href="/docs/tasks/configure-pod-container/configure-service-account/#manually-create-an-api-token-for-a-serviceaccount">Service Account Token Secrets</a>
(not recommended): You can mount service account tokens as Kubernetes
Secrets in Pods. These tokens don't expire and don't rotate. In versions prior to v1.24, a permanent token was automatically created for each service account.
This method is not recommended anymore, especially at scale, because of the risks associated
with static, long-lived credentials. The <a href="/docs/reference/command-line-tools-reference/feature-gates-removed/">LegacyServiceAccountTokenNoAutoGeneration feature gate</a>
(which was enabled by default from Kubernetes v1.24 to v1.26), prevented Kubernetes from automatically creating these tokens for
ServiceAccounts. The feature gate is removed in v1.27, because it was elevated to GA status; you can still create indefinite service account tokens manually, but should take into account the security implications.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>For applications running outside your Kubernetes cluster, you might be considering
creating a long-lived ServiceAccount token that is stored in a Secret. This allows authentication, but the Kubernetes project recommends you avoid this approach.
Long-lived bearer tokens represent a security risk as, once disclosed, the token
can be misused. Instead, consider using an alternative. For example, your external
application can authenticate using a well-protected private key <code>and</code> a certificate,
or using a custom mechanism such as an <a href="/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">authentication webhook</a> that you implement yourself.</p><p>You can also use TokenRequest to obtain short-lived tokens for your external application.</p></div><h3 id="enforce-mountable-secrets">Restricting access to Secrets (deprecated)</h3><div class="feature-state-notice feature-deprecated"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [deprecated]</code></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>kubernetes.io/enforce-mountable-secrets</code> is deprecated since Kubernetes v1.32. Use separate namespaces to isolate access to mounted secrets.</div><p>Kubernetes provides an annotation called <code>kubernetes.io/enforce-mountable-secrets</code>
that you can add to your ServiceAccounts. When this annotation is applied,
the ServiceAccount's secrets can only be mounted on specified types of resources,
enhancing the security posture of your cluster.</p><p>You can add the annotation to a ServiceAccount using a manifest:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/enforce-mountable-secrets</span>:<span> </span><span>"true"</span><span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>my-serviceaccount<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>my-namespace<span>
</span></span></span></code></pre></div><p>When this annotation is set to "true", the Kubernetes control plane ensures that
the Secrets from this ServiceAccount are subject to certain mounting restrictions.</p><ol><li>The name of each Secret that is mounted as a volume in a Pod must appear in the <code>secrets</code> field of the
Pod's ServiceAccount.</li><li>The name of each Secret referenced using <code>envFrom</code> in a Pod must also appear in the <code>secrets</code>
field of the Pod's ServiceAccount.</li><li>The name of each Secret referenced using <code>imagePullSecrets</code> in a Pod must also appear in the <code>secrets</code>
field of the Pod's ServiceAccount.</li></ol><p>By understanding and enforcing these restrictions, cluster administrators can maintain a tighter security profile and ensure that secrets are accessed only by the appropriate resources.</p><h2 id="authenticating-credentials">Authenticating service account credentials</h2><p>ServiceAccounts use signed
<a class="glossary-tooltip" title="A means of representing claims to be transferred between two parties." href="https://www.rfc-editor.org/rfc/rfc7519" target="_blank">JSON Web Tokens</a> (JWTs)
to authenticate to the Kubernetes API server, and to any other system where a
trust relationship exists. Depending on how the token was issued
(either time-limited using a <code>TokenRequest</code> or using a legacy mechanism with
a Secret), a ServiceAccount token might also have an expiry time, an audience,
and a time after which the token <em>starts</em> being valid. When a client that is
acting as a ServiceAccount tries to communicate with the Kubernetes API server,
the client includes an <code>Authorization: Bearer &lt;token&gt;</code> header with the HTTP
request. The API server checks the validity of that bearer token as follows:</p><ol><li>Checks the token signature.</li><li>Checks whether the token has expired.</li><li>Checks whether object references in the token claims are currently valid.</li><li>Checks whether the token is currently valid.</li><li>Checks the audience claims.</li></ol><p>The TokenRequest API produces <em>bound tokens</em> for a ServiceAccount. This
binding is linked to the lifetime of the client, such as a Pod, that is acting
as that ServiceAccount. See <a href="/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection">Token Volume Projection</a>
for an example of a bound pod service account token's JWT schema and payload.</p><p>For tokens issued using the <code>TokenRequest</code> API, the API server also checks that
the specific object reference that is using the ServiceAccount still exists,
matching by the <a class="glossary-tooltip" title="A Kubernetes systems-generated string to uniquely identify objects." href="/docs/concepts/overview/working-with-objects/names" target="_blank">unique ID</a> of that
object. For legacy tokens that are mounted as Secrets in Pods, the API server
checks the token against the Secret.</p><p>For more information about the authentication process, refer to
<a href="/docs/reference/access-authn-authz/authentication/#service-account-tokens">Authentication</a>.</p><h3 id="authenticating-in-code">Authenticating service account credentials in your own code</h3><p>If you have services of your own that need to validate Kubernetes service
account credentials, you can use the following methods:</p><ul><li><a href="/docs/reference/kubernetes-api/authentication-resources/token-review-v1/">TokenReview API</a>
(recommended)</li><li>OIDC discovery</li></ul><p>The Kubernetes project recommends that you use the TokenReview API, because
this method invalidates tokens that are bound to API objects such as Secrets,
ServiceAccounts, Pods or Nodes when those objects are deleted. For example, if you
delete the Pod that contains a projected ServiceAccount token, the cluster
invalidates that token immediately and a TokenReview immediately fails.
If you use OIDC validation instead, your clients continue to treat the token
as valid until the token reaches its expiration timestamp.</p><p>Your application should always define the audience that it accepts, and should
check that the token's audiences match the audiences that the application
expects. This helps to minimize the scope of the token so that it can only be
used in your application and nowhere else.</p><h2 id="alternatives">Alternatives</h2><ul><li>Issue your own tokens using another mechanism, and then use
<a href="/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">Webhook Token Authentication</a>
to validate bearer tokens using your own validation service.</li><li>Provide your own identities to Pods.<ul><li><p><a href="https://cert-manager.io/docs/projects/csi-driver-spiffe/">Use the SPIFFE CSI driver plugin to provide SPIFFE SVIDs as X.509 certificate pairs to Pods</a>.</p><div class="alert alert-secondary callout third-party-content">&#128711; This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div></li><li><p><a href="https://istio.io/latest/docs/tasks/security/cert-management/plugin-ca-cert/">Use a service mesh such as Istio to provide certificates to Pods</a>.</p></li></ul></li><li>Authenticate from outside the cluster to the API server without using service account tokens:<ul><li><a href="/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">Configure the API server to accept OpenID Connect (OIDC) tokens from your identity provider</a>.</li><li>Use service accounts or user accounts created using an external Identity
and Access Management (IAM) service, such as from a cloud provider, to
authenticate to your cluster.</li><li><a href="/docs/tasks/tls/managing-tls-in-a-cluster/">Use the CertificateSigningRequest API with client certificates</a>.</li></ul></li><li><a href="/docs/tasks/administer-cluster/kubelet-credential-provider/">Configure the kubelet to retrieve credentials from an image registry</a>.</li><li>Use a Device Plugin to access a virtual Trusted Platform Module (TPM), which
then allows authentication using a private key.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn how to <a href="/docs/reference/access-authn-authz/service-accounts-admin/">manage your ServiceAccounts as a cluster administrator</a>.</li><li>Learn how to <a href="/docs/tasks/configure-pod-container/configure-service-account/">assign a ServiceAccount to a Pod</a>.</li><li>Read the <a href="/docs/reference/kubernetes-api/authentication-resources/service-account-v1/">ServiceAccount API reference</a>.</li></ul></div></div><div><div class="td-content"><h1>Pod Security Policies</h1><div class="alert alert-warning"><h4 class="alert-heading">Removed feature</h4>PodSecurityPolicy was <a href="/blog/2021/04/08/kubernetes-1-21-release-announcement/#podsecuritypolicy-deprecation">deprecated</a>
in Kubernetes v1.21, and removed from Kubernetes in v1.25.</div><p>Instead of using PodSecurityPolicy, you can enforce similar restrictions on Pods using
either or both:</p><ul><li><a href="/docs/concepts/security/pod-security-admission/">Pod Security Admission</a></li><li>a 3rd party admission plugin, that you deploy and configure yourself</li></ul><p>For a migration guide, see <a href="/docs/tasks/configure-pod-container/migrate-from-psp/">Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a>.
For more information on the removal of this API,
see <a href="/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future</a>.</p><p>If you are not running Kubernetes v1.34, check the documentation for
your version of Kubernetes.</p></div></div><div><div class="td-content"><h1>Security For Linux Nodes</h1><p>This page describes security considerations and best practices specific to the Linux operating system.</p><h2 id="protection-for-secret-data-on-nodes">Protection for Secret data on nodes</h2><p>On Linux nodes, memory-backed volumes (such as <a href="/docs/concepts/configuration/secret/"><code>secret</code></a>
volume mounts, or <a href="/docs/concepts/storage/volumes/#emptydir"><code>emptyDir</code></a> with <code>medium: Memory</code>)
are implemented with a <code>tmpfs</code> filesystem.</p><p>If you have swap configured and use an older Linux kernel (or a current kernel and an unsupported configuration of Kubernetes),
<strong>memory</strong> backed volumes can have data written to persistent storage.</p><p>The Linux kernel officially supports the <code>noswap</code> option from version 6.3,
therefore it is recommended the used kernel version is 6.3 or later,
or supports the <code>noswap</code> option via a backport, if swap is enabled on the node.</p><p>Read <a href="/docs/concepts/cluster-administration/swap-memory-management/#memory-backed-volumes">swap memory management</a>
for more info.</p></div></div><div><div class="td-content"><h1>Security For Windows Nodes</h1><p>This page describes security considerations and best practices specific to the Windows operating system.</p><h2 id="protection-for-secret-data-on-nodes">Protection for Secret data on nodes</h2><p>On Windows, data from Secrets are written out in clear text onto the node's local
storage (as compared to using tmpfs / in-memory filesystems on Linux). As a cluster
operator, you should take both of the following additional measures:</p><ol><li>Use file ACLs to secure the Secrets' file location.</li><li>Apply volume-level encryption using
<a href="https://docs.microsoft.com/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server">BitLocker</a>.</li></ol><h2 id="container-users">Container users</h2><p><a href="/docs/tasks/configure-pod-container/configure-runasusername/">RunAsUsername</a>
can be specified for Windows Pods or containers to execute the container
processes as specific user. This is roughly equivalent to
<a href="/docs/concepts/security/pod-security-policy/#users-and-groups">RunAsUser</a>.</p><p>Windows containers offer two default user accounts, ContainerUser and ContainerAdministrator.
The differences between these two user accounts are covered in
<a href="https://docs.microsoft.com/virtualization/windowscontainers/manage-containers/container-security#when-to-use-containeradmin-and-containeruser-user-accounts">When to use ContainerAdmin and ContainerUser user accounts</a>
within Microsoft's <em>Secure Windows containers</em> documentation.</p><p>Local users can be added to container images during the container build process.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li><a href="https://hub.docker.com/_/microsoft-windows-nanoserver">Nano Server</a> based images run as
<code>ContainerUser</code> by default</li><li><a href="https://hub.docker.com/_/microsoft-windows-servercore">Server Core</a> based images run as
<code>ContainerAdministrator</code> by default</li></ul></div><p>Windows containers can also run as Active Directory identities by utilizing
<a href="/docs/tasks/configure-pod-container/configure-gmsa/">Group Managed Service Accounts</a></p><h2 id="pod-level-security-isolation">Pod-level security isolation</h2><p>Linux-specific pod security context mechanisms (such as SELinux, AppArmor, Seccomp, or custom
POSIX capabilities) are not supported on Windows nodes.</p><p>Privileged containers are <a href="/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext">not supported</a>
on Windows.
Instead <a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">HostProcess containers</a>
can be used on Windows to perform many of the tasks performed by privileged containers on Linux.</p></div></div><div><div class="td-content"><h1>Controlling Access to the Kubernetes API</h1><p>This page provides an overview of controlling access to the Kubernetes API.</p><p>Users access the <a href="/docs/concepts/overview/kubernetes-api/">Kubernetes API</a> using <code>kubectl</code>,
client libraries, or by making REST requests. Both human users and
<a href="/docs/tasks/configure-pod-container/configure-service-account/">Kubernetes service accounts</a> can be
authorized for API access.
When a request reaches the API, it goes through several stages, illustrated in the
following diagram:</p><p><img alt="Diagram of request handling steps for Kubernetes API request" src="/images/docs/admin/access-control-overview.svg"></p><h2 id="transport-security">Transport security</h2><p>By default, the Kubernetes API server listens on port 6443 on the first non-localhost
network interface, protected by TLS. In a typical production Kubernetes cluster, the
API serves on port 443. The port can be changed with the <code>--secure-port</code>, and the
listening IP address with the <code>--bind-address</code> flag.</p><p>The API server presents a certificate. This certificate may be signed using
a private certificate authority (CA), or based on a public key infrastructure linked
to a generally recognized CA. The certificate and corresponding private key can be set
by using the <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> flags.</p><p>If your cluster uses a private certificate authority, you need a copy of that CA
certificate configured into your <code>~/.kube/config</code> on the client, so that you can
trust the connection and be confident it was not intercepted.</p><p>Your client can present a TLS client certificate at this stage.</p><h2 id="authentication">Authentication</h2><p>Once TLS is established, the HTTP request moves to the Authentication step.
This is shown as step <strong>1</strong> in the diagram.
The cluster creation script or cluster admin configures the API server to run
one or more Authenticator modules.
Authenticators are described in more detail in
<a href="/docs/reference/access-authn-authz/authentication/">Authentication</a>.</p><p>The input to the authentication step is the entire HTTP request; however, it typically
examines the headers and/or client certificate.</p><p>Authentication modules include client certificates, password, and plain tokens,
bootstrap tokens, and JSON Web Tokens (used for service accounts).</p><p>Multiple authentication modules can be specified, in which case each one is tried in sequence,
until one of them succeeds.</p><p>If the request cannot be authenticated, it is rejected with HTTP status code 401.
Otherwise, the user is authenticated as a specific <code>username</code>, and the user name
is available to subsequent steps to use in their decisions. Some authenticators
also provide the group memberships of the user, while other authenticators
do not.</p><p>While Kubernetes uses usernames for access control decisions and in request logging,
it does not have a <code>User</code> object nor does it store usernames or other information about
users in its API.</p><h2 id="authorization">Authorization</h2><p>After the request is authenticated as coming from a specific user, the request must
be authorized. This is shown as step&#160;<strong>2</strong>&#160;in the diagram.</p><p>A request must include the username of the requester, the requested action, and
the object affected by the action. The request is authorized if an existing policy
declares that the user has permissions to complete the requested action.</p><p>For example, if Bob has the policy below, then he can read pods only in the namespace <code>projectCaribou</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>    <span>"apiVersion"</span>: <span>"abac.authorization.kubernetes.io/v1beta1"</span>,
</span></span><span><span>    <span>"kind"</span>: <span>"Policy"</span>,
</span></span><span><span>    <span>"spec"</span>: {
</span></span><span><span>        <span>"user"</span>: <span>"bob"</span>,
</span></span><span><span>        <span>"namespace"</span>: <span>"projectCaribou"</span>,
</span></span><span><span>        <span>"resource"</span>: <span>"pods"</span>,
</span></span><span><span>        <span>"readonly"</span>: <span>true</span>
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>If Bob makes the following request, the request is authorized because he is
allowed to read objects in the <code>projectCaribou</code> namespace:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"apiVersion"</span>: <span>"authorization.k8s.io/v1beta1"</span>,
</span></span><span><span>  <span>"kind"</span>: <span>"SubjectAccessReview"</span>,
</span></span><span><span>  <span>"spec"</span>: {
</span></span><span><span>    <span>"resourceAttributes"</span>: {
</span></span><span><span>      <span>"namespace"</span>: <span>"projectCaribou"</span>,
</span></span><span><span>      <span>"verb"</span>: <span>"get"</span>,
</span></span><span><span>      <span>"group"</span>: <span>"unicorn.example.org"</span>,
</span></span><span><span>      <span>"resource"</span>: <span>"pods"</span>
</span></span><span><span>    }
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><p>If Bob makes a request to write (<code>create</code> or <code>update</code>) to the objects in the
<code>projectCaribou</code> namespace, his authorization is denied. If Bob makes a request
to read (<code>get</code>) objects in a different namespace such as <code>projectFish</code>, then his authorization is denied.</p><p>Kubernetes authorization requires that you use common REST attributes to interact
with existing organization-wide or cloud-provider-wide access control systems.
It is important to use REST formatting because these control systems might
interact with other APIs besides the Kubernetes API.</p><p>Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode,
and Webhook mode. When an administrator creates a cluster, they configure the
authorization modules that should be used in the API server. If more than one
authorization modules are configured, Kubernetes checks each module, and if
any module authorizes the request, then the request can proceed. If all of
the modules deny the request, then the request is denied (HTTP status code 403).</p><p>To learn more about Kubernetes authorization, including details about creating
policies using the supported authorization modules, see <a href="/docs/reference/access-authn-authz/authorization/">Authorization</a>.</p><h2 id="admission-control">Admission control</h2><p>Admission Control modules are software modules that can modify or reject requests.
In addition to the attributes available to Authorization modules, Admission
Control modules can access the contents of the object that is being created or modified.</p><p>Admission controllers act on requests that create, modify, delete, or connect to (proxy) an object.
Admission controllers do not act on requests that merely read objects.
When multiple admission controllers are configured, they are called in order.</p><p>This is shown as step <strong>3</strong> in the diagram.</p><p>Unlike Authentication and Authorization modules, if any admission controller module
rejects, then the request is immediately rejected.</p><p>In addition to rejecting objects, admission controllers can also set complex defaults for
fields.</p><p>The available Admission Control modules are described in <a href="/docs/reference/access-authn-authz/admission-controllers/">Admission Controllers</a>.</p><p>Once a request passes all admission controllers, it is validated using the validation routines
for the corresponding API object, and then written to the object store (shown as step <strong>4</strong>).</p><h2 id="auditing">Auditing</h2><p>Kubernetes auditing provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster.
The cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.</p><p>For more information, see <a href="/docs/tasks/debug/debug-cluster/audit/">Auditing</a>.</p><h2 id="what-s-next">What's next</h2><p>Read more documentation on authentication, authorization and API access control:</p><ul><li><a href="/docs/reference/access-authn-authz/authentication/">Authenticating</a><ul><li><a href="/docs/reference/access-authn-authz/bootstrap-tokens/">Authenticating with Bootstrap Tokens</a></li></ul></li><li><a href="/docs/reference/access-authn-authz/admission-controllers/">Admission Controllers</a><ul><li><a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">Dynamic Admission Control</a></li></ul></li><li><a href="/docs/reference/access-authn-authz/authorization/">Authorization</a><ul><li><a href="/docs/reference/access-authn-authz/rbac/">Role Based Access Control</a></li><li><a href="/docs/reference/access-authn-authz/abac/">Attribute Based Access Control</a></li><li><a href="/docs/reference/access-authn-authz/node/">Node Authorization</a></li><li><a href="/docs/reference/access-authn-authz/webhook/">Webhook Authorization</a></li></ul></li><li><a href="/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a><ul><li>including <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection">CSR approval</a>
and <a href="/docs/reference/access-authn-authz/certificate-signing-requests/#signing">certificate signing</a></li></ul></li><li>Service accounts<ul><li><a href="/docs/tasks/configure-pod-container/configure-service-account/">Developer guide</a></li><li><a href="/docs/reference/access-authn-authz/service-accounts-admin/">Administration</a></li></ul></li></ul><p>You can learn about:</p><ul><li>how Pods can use
<a href="/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials">Secrets</a>
to obtain API credentials.</li></ul></div></div><div><div class="td-content"><h1>Role Based Access Control Good Practices</h1><div class="lead">Principles and practices for good RBAC design for cluster operators.</div><p>Kubernetes <a class="glossary-tooltip" title="Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API." href="/docs/reference/access-authn-authz/rbac/" target="_blank">RBAC</a> is a key security control
to ensure that cluster users and workloads have only the access to resources required to
execute their roles. It is important to ensure that, when designing permissions for cluster
users, the cluster administrator understands the areas where privilege escalation could occur,
to reduce the risk of excessive access leading to security incidents.</p><p>The good practices laid out here should be read in conjunction with the general
<a href="/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update">RBAC documentation</a>.</p><h2 id="general-good-practice">General good practice</h2><h3 id="least-privilege">Least privilege</h3><p>Ideally, minimal RBAC rights should be assigned to users and service accounts. Only permissions
explicitly required for their operation should be used. While each cluster will be different,
some general rules that can be applied are :</p><ul><li>Assign permissions at the namespace level where possible. Use RoleBindings as opposed to
ClusterRoleBindings to give users rights only within a specific namespace.</li><li>Avoid providing wildcard permissions when possible, especially to all resources.
As Kubernetes is an extensible system, providing wildcard access gives rights
not just to all object types that currently exist in the cluster, but also to all object types
which are created in the future.</li><li>Administrators should not use <code>cluster-admin</code> accounts except where specifically needed.
Providing a low privileged account with
<a href="/docs/reference/access-authn-authz/authentication/#user-impersonation">impersonation rights</a>
can avoid accidental modification of cluster resources.</li><li>Avoid adding users to the <code>system:masters</code> group. Any user who is a member of this group
bypasses all RBAC rights checks and will always have unrestricted superuser access, which cannot be
revoked by removing RoleBindings or ClusterRoleBindings. As an aside, if a cluster is
using an authorization webhook, membership of this group also bypasses that webhook (requests
from users who are members of that group are never sent to the webhook)</li></ul><h3 id="minimize-distribution-of-privileged-tokens">Minimize distribution of privileged tokens</h3><p>Ideally, pods shouldn't be assigned service accounts that have been granted powerful permissions
(for example, any of the rights listed under <a href="#privilege-escalation-risks">privilege escalation risks</a>).
In cases where a workload requires powerful permissions, consider the following practices:</p><ul><li>Limit the number of nodes running powerful pods. Ensure that any DaemonSets you run
are necessary and are run with least privilege to limit the blast radius of container escapes.</li><li>Avoid running powerful pods alongside untrusted or publicly-exposed ones. Consider using
<a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">Taints and Toleration</a>,
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity">NodeAffinity</a>, or
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">PodAntiAffinity</a>
to ensure pods don't run alongside untrusted or less-trusted Pods. Pay special attention to
situations where less-trustworthy Pods are not meeting the <strong>Restricted</strong> Pod Security Standard.</li></ul><h3 id="hardening">Hardening</h3><p>Kubernetes defaults to providing access which may not be required in every cluster. Reviewing
the RBAC rights provided by default can provide opportunities for security hardening.
In general, changes should not be made to rights provided to <code>system:</code> accounts some options
to harden cluster rights exist:</p><ul><li>Review bindings for the <code>system:unauthenticated</code> group and remove them where possible, as this gives
access to anyone who can contact the API server at a network level.</li><li>Avoid the default auto-mounting of service account tokens by setting
<code>automountServiceAccountToken: false</code>. For more details, see
<a href="/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server">using default service account token</a>.
Setting this value for a Pod will overwrite the service account setting, workloads
which require service account tokens can still mount them.</li></ul><h3 id="periodic-review">Periodic review</h3><p>It is vital to periodically review the Kubernetes RBAC settings for redundant entries and
possible privilege escalations.
If an attacker is able to create a user account with the same name as a deleted user,
they can automatically inherit all the rights of the deleted user, especially the
rights assigned to that user.</p><h2 id="privilege-escalation-risks">Kubernetes RBAC - privilege escalation risks</h2><p>Within Kubernetes RBAC there are a number of privileges which, if granted, can allow a user or a service account
to escalate their privileges in the cluster or affect systems outside the cluster.</p><p>This section is intended to provide visibility of the areas where cluster operators
should take care, to ensure that they do not inadvertently allow for more access to clusters than intended.</p><h3 id="listing-secrets">Listing secrets</h3><p>It is generally clear that allowing <code>get</code> access on Secrets will allow a user to read their contents.
It is also important to note that <code>list</code> and <code>watch</code> access also effectively allow for users to reveal the Secret contents.
For example, when a List response is returned (for example, via <code>kubectl get secrets -A -o yaml</code>), the response
includes the contents of all Secrets.</p><h3 id="workload-creation">Workload creation</h3><p>Permission to create workloads (either Pods, or
<a href="/docs/concepts/workloads/controllers/">workload resources</a> that manage Pods) in a namespace
implicitly grants access to many other resources in that namespace, such as Secrets, ConfigMaps, and
PersistentVolumes that can be mounted in Pods. Additionally, since Pods can run as any
<a href="/docs/reference/access-authn-authz/service-accounts-admin/">ServiceAccount</a>, granting permission
to create workloads also implicitly grants the API access levels of any service account in that
namespace.</p><p>Users who can run privileged Pods can use that access to gain node access and potentially to
further elevate their privileges. Where you do not fully trust a user or other principal
with the ability to create suitably secure and isolated Pods, you should enforce either the
<strong>Baseline</strong> or <strong>Restricted</strong> Pod Security Standard.
You can use <a href="/docs/concepts/security/pod-security-admission/">Pod Security admission</a>
or other (third party) mechanisms to implement that enforcement.</p><p>For these reasons, namespaces should be used to separate resources requiring different levels of
trust or tenancy. It is still considered best practice to follow <a href="#least-privilege">least privilege</a>
principles and assign the minimum set of permissions, but boundaries within a namespace should be
considered weak.</p><h3 id="persistent-volume-creation">Persistent volume creation</h3><p>If someone - or some application - is allowed to create arbitrary PersistentVolumes, that access
includes the creation of <code>hostPath</code> volumes, which then means that a Pod would get access
to the underlying host filesystem(s) on the associated node. Granting that ability is a security risk.</p><p>There are many ways a container with unrestricted access to the host filesystem can escalate privileges, including
reading data from other containers, and abusing the credentials of system services, such as Kubelet.</p><p>You should only allow access to create PersistentVolume objects for:</p><ul><li>Users (cluster operators) that need this access for their work, and who you trust.</li><li>The Kubernetes control plane components which creates PersistentVolumes based on PersistentVolumeClaims
that are configured for automatic provisioning.
This is usually setup by the Kubernetes provider or by the operator when installing a CSI driver.</li></ul><p>Where access to persistent storage is required trusted administrators should create
PersistentVolumes, and constrained users should use PersistentVolumeClaims to access that storage.</p><h3 id="access-to-proxy-subresource-of-nodes">Access to <code>proxy</code> subresource of Nodes</h3><p>Users with access to the proxy sub-resource of node objects have rights to the Kubelet API,
which allows for command execution on every pod on the node(s) to which they have rights.
This access bypasses audit logging and admission control, so care should be taken before
granting rights to this resource.</p><h3 id="escalate-verb">Escalate verb</h3><p>Generally, the RBAC system prevents users from creating clusterroles with more rights than the user possesses.
The exception to this is the <code>escalate</code> verb. As noted in the <a href="/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update">RBAC documentation</a>,
users with this right can effectively escalate their privileges.</p><h3 id="bind-verb">Bind verb</h3><p>Similar to the <code>escalate</code> verb, granting users this right allows for the bypass of Kubernetes
in-built protections against privilege escalation, allowing users to create bindings to
roles with rights they do not already have.</p><h3 id="impersonate-verb">Impersonate verb</h3><p>This verb allows users to impersonate and gain the rights of other users in the cluster.
Care should be taken when granting it, to ensure that excessive permissions cannot be gained
via one of the impersonated accounts.</p><h3 id="csrs-and-certificate-issuing">CSRs and certificate issuing</h3><p>The CSR API allows for users with <code>create</code> rights to CSRs and <code>update</code> rights on <code>certificatesigningrequests/approval</code>
where the signer is <code>kubernetes.io/kube-apiserver-client</code> to create new client certificates
which allow users to authenticate to the cluster. Those client certificates can have arbitrary
names including duplicates of Kubernetes system components. This will effectively allow for privilege escalation.</p><h3 id="token-request">Token request</h3><p>Users with <code>create</code> rights on <code>serviceaccounts/token</code> can create TokenRequests to issue
tokens for existing service accounts.</p><h3 id="control-admission-webhooks">Control admission webhooks</h3><p>Users with control over <code>validatingwebhookconfigurations</code> or <code>mutatingwebhookconfigurations</code>
can control webhooks that can read any object admitted to the cluster, and in the case of
mutating webhooks, also mutate admitted objects.</p><h3 id="namespace-modification">Namespace modification</h3><p>Users who can perform <strong>patch</strong> operations on Namespace objects (through a namespaced RoleBinding to a Role with that access) can modify
labels on that namespace. In clusters where Pod Security Admission is used, this may allow a user to configure the namespace
for a more permissive policy than intended by the administrators.
For clusters where NetworkPolicy is used, users may be set labels that indirectly allow
access to services that an administrator did not intend to allow.</p><h2 id="denial-of-service-risks">Kubernetes RBAC - denial of service risks</h2><h3 id="object-creation-dos">Object creation denial-of-service</h3><p>Users who have rights to create objects in a cluster may be able to create sufficient large
objects to create a denial of service condition either based on the size or number of objects, as discussed in
<a href="https://github.com/kubernetes/kubernetes/issues/107325">etcd used by Kubernetes is vulnerable to OOM attack</a>. This may be
specifically relevant in multi-tenant clusters if semi-trusted or untrusted users
are allowed limited access to a system.</p><p>One option for mitigation of this issue would be to use
<a href="/docs/concepts/policy/resource-quotas/#object-count-quota">resource quotas</a>
to limit the quantity of objects which can be created.</p><h2 id="what-s-next">What's next</h2><ul><li>To learn more about RBAC, see the <a href="/docs/reference/access-authn-authz/rbac/">RBAC documentation</a>.</li></ul></div></div><div><div class="td-content"><h1>Good practices for Kubernetes Secrets</h1><div class="lead">Principles and practices for good Secret management for cluster administrators and application developers.</div><p><p>In Kubernetes, a Secret is an object that stores sensitive information, such as passwords, OAuth tokens, and SSH keys.</p></p><p>Secrets give you more control over how sensitive information is used and reduces
the risk of accidental exposure. Secret values are encoded as base64 strings and
are stored unencrypted by default, but can be configured to be
<a href="/docs/tasks/administer-cluster/encrypt-data/#ensure-all-secrets-are-encrypted">encrypted at rest</a>.</p><p>A <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a> can reference the Secret in
a variety of ways, such as in a volume mount or as an environment variable.
Secrets are designed for confidential data and
<a href="/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a> are
designed for non-confidential data.</p><p>The following good practices are intended for both cluster administrators and
application developers. Use these guidelines to improve the security of your
sensitive information in Secret objects, as well as to more effectively manage
your Secrets.</p><h2 id="cluster-administrators">Cluster administrators</h2><p>This section provides good practices that cluster administrators can use to
improve the security of confidential information in the cluster.</p><h3 id="configure-encryption-at-rest">Configure encryption at rest</h3><p>By default, Secret objects are stored unencrypted in <a class="glossary-tooltip" title="Consistent and highly-available key value store used as backing store of Kubernetes for all cluster data." href="/docs/tasks/administer-cluster/configure-upgrade-etcd/" target="_blank">etcd</a>. You should configure encryption of your Secret
data in <code>etcd</code>. For instructions, refer to
<a href="/docs/tasks/administer-cluster/encrypt-data/">Encrypt Secret Data at Rest</a>.</p><h3 id="least-privilege-secrets">Configure least-privilege access to Secrets</h3><p>When planning your access control mechanism, such as Kubernetes
<a class="glossary-tooltip" title="Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API." href="/docs/reference/access-authn-authz/rbac/" target="_blank">Role-based Access Control</a> <a href="/docs/reference/access-authn-authz/rbac/">(RBAC)</a>,
consider the following guidelines for access to <code>Secret</code> objects. You should
also follow the other guidelines in
<a href="/docs/concepts/security/rbac-good-practices/">RBAC good practices</a>.</p><ul><li><strong>Components</strong>: Restrict <code>watch</code> or <code>list</code> access to only the most
privileged, system-level components. Only grant <code>get</code> access for Secrets if
the component's normal behavior requires it.</li><li><strong>Humans</strong>: Restrict <code>get</code>, <code>watch</code>, or <code>list</code> access to Secrets. Only allow
cluster administrators to access <code>etcd</code>. This includes read-only access. For
more complex access control, such as restricting access to Secrets with
specific annotations, consider using third-party authorization mechanisms.</li></ul><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Granting <code>list</code> access to Secrets implicitly lets the subject fetch the
contents of the Secrets.</div><p>A user who can create a Pod that uses a Secret can also see the value of that
Secret. Even if cluster policies do not allow a user to read the Secret
directly, the same user could have access to run a Pod that then exposes the
Secret. You can detect or limit the impact caused by Secret data being exposed,
either intentionally or unintentionally, by a user with this access. Some
recommendations include:</p><ul><li>Use short-lived Secrets</li><li>Implement audit rules that alert on specific events, such as concurrent
reading of multiple Secrets by a single user</li></ul><h4 id="restrict-access-for-secrets">Restrict Access for Secrets</h4><p>Use separate namespaces to isolate access to mounted secrets.</p><h3 id="improve-etcd-management-policies">Improve etcd management policies</h3><p>Consider wiping or shredding the durable storage used by <code>etcd</code> once it is
no longer in use.</p><p>If there are multiple <code>etcd</code> instances, configure encrypted SSL/TLS
communication between the instances to protect the Secret data in transit.</p><h3 id="configure-access-to-external-secrets">Configure access to external Secrets</h3><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>You can use third-party Secrets store providers to keep your confidential data
outside your cluster and then configure Pods to access that information.
The <a href="https://secrets-store-csi-driver.sigs.k8s.io/">Kubernetes Secrets Store CSI Driver</a>
is a DaemonSet that lets the kubelet retrieve Secrets from external stores, and
mount the Secrets as a volume into specific Pods that you authorize to access
the data.</p><p>For a list of supported providers, refer to
<a href="https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver">Providers for the Secret Store CSI Driver</a>.</p><h2 id="good-practices-for-using-swap-memory">Good practices for using swap memory</h2><p>For best practices for setting swap memory for Linux nodes, please refer to
<a href="/docs/concepts/cluster-administration/swap-memory-management/#good-practice-for-using-swap-in-a-kubernetes-cluster">swap memory management</a>.</p><h2 id="developers">Developers</h2><p>This section provides good practices for developers to use to improve the
security of confidential data when building and deploying Kubernetes resources.</p><h3 id="restrict-secret-access-to-specific-containers">Restrict Secret access to specific containers</h3><p>If you are defining multiple containers in a Pod, and only one of those
containers needs access to a Secret, define the volume mount or environment
variable configuration so that the other containers do not have access to that
Secret.</p><h3 id="protect-secret-data-after-reading">Protect Secret data after reading</h3><p>Applications still need to protect the value of confidential information after
reading it from an environment variable or volume. For example, your
application must avoid logging the secret data in the clear or transmitting it
to an untrusted party.</p><h3 id="avoid-sharing-secret-manifests">Avoid sharing Secret manifests</h3><p>If you configure a Secret through a
<a class="glossary-tooltip" title="A serialized specification of one or more Kubernetes API objects." href="/docs/reference/glossary/?all=true#term-manifest" target="_blank">manifest</a>, with the secret
data encoded as base64, sharing this file or checking it in to a source
repository means the secret is available to everyone who can read the manifest.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Base64 encoding is <em>not</em> an encryption method, it provides no additional
confidentiality over plain text.</div></div></div><div><div class="td-content"><h1>Multi-tenancy</h1><p>This page provides an overview of available configuration options and best practices for cluster
multi-tenancy.</p><p>Sharing clusters saves costs and simplifies administration. However, sharing clusters also
presents challenges such as security, fairness, and managing <em>noisy neighbors</em>.</p><p>Clusters can be shared in many ways. In some cases, different applications may run in the same
cluster. In other cases, multiple instances of the same application may run in the same cluster,
one for each end user. All these types of sharing are frequently described using the umbrella term
<em>multi-tenancy</em>.</p><p>While Kubernetes does not have first-class concepts of end users or tenants, it provides several
features to help manage different tenancy requirements. These are discussed below.</p><h2 id="use-cases">Use cases</h2><p>The first step to determining how to share your cluster is understanding your use case, so you can
evaluate the patterns and tools available. In general, multi-tenancy in Kubernetes clusters falls
into two broad categories, though many variations and hybrids are also possible.</p><h3 id="multiple-teams">Multiple teams</h3><p>A common form of multi-tenancy is to share a cluster between multiple teams within an
organization, each of whom may operate one or more workloads. These workloads frequently need to
communicate with each other, and with other workloads located on the same or different clusters.</p><p>In this scenario, members of the teams often have direct access to Kubernetes resources via tools
such as <code>kubectl</code>, or indirect access through GitOps controllers or other types of release
automation tools. There is often some level of trust between members of different teams, but
Kubernetes policies such as RBAC, quotas, and network policies are essential to safely and fairly
share clusters.</p><h3 id="multiple-customers">Multiple customers</h3><p>The other major form of multi-tenancy frequently involves a Software-as-a-Service (SaaS) vendor
running multiple instances of a workload for customers. This business model is so strongly
associated with this deployment style that many people call it "SaaS tenancy." However, a better
term might be "multi-customer tenancy," since SaaS vendors may also use other deployment models,
and this deployment model can also be used outside of SaaS.</p><p>In this scenario, the customers do not have access to the cluster; Kubernetes is invisible from
their perspective and is only used by the vendor to manage the workloads. Cost optimization is
frequently a critical concern, and Kubernetes policies are used to ensure that the workloads are
strongly isolated from each other.</p><h2 id="terminology">Terminology</h2><h3 id="tenants">Tenants</h3><p>When discussing multi-tenancy in Kubernetes, there is no single definition for a "tenant".
Rather, the definition of a tenant will vary depending on whether multi-team or multi-customer
tenancy is being discussed.</p><p>In multi-team usage, a tenant is typically a team, where each team typically deploys a small
number of workloads that scales with the complexity of the service. However, the definition of
"team" may itself be fuzzy, as teams may be organized into higher-level divisions or subdivided
into smaller teams.</p><p>By contrast, if each team deploys dedicated workloads for each new client, they are using a
multi-customer model of tenancy. In this case, a "tenant" is simply a group of users who share a
single workload. This may be as large as an entire company, or as small as a single team at that
company.</p><p>In many cases, the same organization may use both definitions of "tenants" in different contexts.
For example, a platform team may offer shared services such as security tools and databases to
multiple internal &#8220;customers&#8221; and a SaaS vendor may also have multiple teams sharing a development
cluster. Finally, hybrid architectures are also possible, such as a SaaS provider using a
combination of per-customer workloads for sensitive data, combined with multi-tenant shared
services.</p><figure class="diagram-large"><img src="/images/docs/multi-tenancy.png"><figcaption><h4>A cluster showing coexisting tenancy models</h4></figcaption></figure><h3 id="isolation">Isolation</h3><p>There are several ways to design and build multi-tenant solutions with Kubernetes. Each of these
methods comes with its own set of tradeoffs that impact the isolation level, implementation
effort, operational complexity, and cost of service.</p><p>A Kubernetes cluster consists of a control plane which runs Kubernetes software, and a data plane
consisting of worker nodes where tenant workloads are executed as pods. Tenant isolation can be
applied in both the control plane and the data plane based on organizational requirements.</p><p>The level of isolation offered is sometimes described using terms like &#8220;hard&#8221; multi-tenancy, which
implies strong isolation, and &#8220;soft&#8221; multi-tenancy, which implies weaker isolation. In particular,
"hard" multi-tenancy is often used to describe cases where the tenants do not trust each other,
often from security and resource sharing perspectives (e.g. guarding against attacks such as data
exfiltration or DoS). Since data planes typically have much larger attack surfaces, "hard"
multi-tenancy often requires extra attention to isolating the data-plane, though control plane
isolation also remains critical.</p><p>However, the terms "hard" and "soft" can often be confusing, as there is no single definition that
will apply to all users. Rather, "hardness" or "softness" is better understood as a broad
spectrum, with many different techniques that can be used to maintain different types of isolation
in your clusters, based on your requirements.</p><p>In more extreme cases, it may be easier or necessary to forgo any cluster-level sharing at all and
assign each tenant their dedicated cluster, possibly even running on dedicated hardware if VMs are
not considered an adequate security boundary. This may be easier with managed Kubernetes clusters,
where the overhead of creating and operating clusters is at least somewhat taken on by a cloud
provider. The benefit of stronger tenant isolation must be evaluated against the cost and
complexity of managing multiple clusters. The <a href="https://git.k8s.io/community/sig-multicluster/README.md">Multi-cluster SIG</a>
is responsible for addressing these types of use cases.</p><p>The remainder of this page focuses on isolation techniques used for shared Kubernetes clusters.
However, even if you are considering dedicated clusters, it may be valuable to review these
recommendations, as it will give you the flexibility to shift to shared clusters in the future if
your needs or capabilities change.</p><h2 id="control-plane-isolation">Control plane isolation</h2><p>Control plane isolation ensures that different tenants cannot access or affect each others'
Kubernetes API resources.</p><h3 id="namespaces">Namespaces</h3><p>In Kubernetes, a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">Namespace</a> provides a
mechanism for isolating groups of API resources within a single cluster. This isolation has two
key dimensions:</p><ol><li><p>Object names within a namespace can overlap with names in other namespaces, similar to files in
folders. This allows tenants to name their resources without having to consider what other
tenants are doing.</p></li><li><p>Many Kubernetes security policies are scoped to namespaces. For example, RBAC Roles and Network
Policies are namespace-scoped resources. Using RBAC, Users and Service Accounts can be
restricted to a namespace.</p></li></ol><p>In a multi-tenant environment, a Namespace helps segment a tenant's workload into a logical and
distinct management unit. In fact, a common practice is to isolate every workload in its own
namespace, even if multiple workloads are operated by the same tenant. This ensures that each
workload has its own identity and can be configured with an appropriate security policy.</p><p>The namespace isolation model requires configuration of several other Kubernetes resources,
networking plugins, and adherence to security best practices to properly isolate tenant workloads.
These considerations are discussed below.</p><h3 id="access-controls">Access controls</h3><p>The most important type of isolation for the control plane is authorization. If teams or their
workloads can access or modify each others' API resources, they can change or disable all other
types of policies thereby negating any protection those policies may offer. As a result, it is
critical to ensure that each tenant has the appropriate access to only the namespaces they need,
and no more. This is known as the "Principle of Least Privilege."</p><p>Role-based access control (RBAC) is commonly used to enforce authorization in the Kubernetes
control plane, for both users and workloads (service accounts).
<a href="/docs/reference/access-authn-authz/rbac/#role-and-clusterrole">Roles</a> and
<a href="/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding">RoleBindings</a> are
Kubernetes objects that are used at a namespace level to enforce access control in your
application; similar objects exist for authorizing access to cluster-level objects, though these
are less useful for multi-tenant clusters.</p><p>In a multi-team environment, RBAC must be used to restrict tenants' access to the appropriate
namespaces, and ensure that cluster-wide resources can only be accessed or modified by privileged
users such as cluster administrators.</p><p>If a policy ends up granting a user more permissions than they need, this is likely a signal that
the namespace containing the affected resources should be refactored into finer-grained
namespaces. Namespace management tools may simplify the management of these finer-grained
namespaces by applying common RBAC policies to different namespaces, while still allowing
fine-grained policies where necessary.</p><h3 id="quotas">Quotas</h3><p>Kubernetes workloads consume node resources, like CPU and memory. In a multi-tenant environment,
you can use <a href="/docs/concepts/policy/resource-quotas/">Resource Quotas</a> to manage resource usage of
tenant workloads. For the multiple teams use case, where tenants have access to the Kubernetes
API, you can use resource quotas to limit the number of API resources (for example: the number of
Pods, or the number of ConfigMaps) that a tenant can create. Limits on object count ensure
fairness and aim to avoid <em>noisy neighbor</em> issues from affecting other tenants that share a
control plane.</p><p>Resource quotas are namespaced objects. By mapping tenants to namespaces, cluster admins can use
quotas to ensure that a tenant cannot monopolize a cluster's resources or overwhelm its control
plane. Namespace management tools simplify the administration of quotas. In addition, while
Kubernetes quotas only apply within a single namespace, some namespace management tools allow
groups of namespaces to share quotas, giving administrators far more flexibility with less effort
than built-in quotas.</p><p>Quotas prevent a single tenant from consuming greater than their allocated share of resources
hence minimizing the &#8220;noisy neighbor&#8221; issue, where one tenant negatively impacts the performance
of other tenants' workloads.</p><p>When you apply a quota to namespace, Kubernetes requires you to also specify resource requests and
limits for each container. Limits are the upper bound for the amount of resources that a container
can consume. Containers that attempt to consume resources that exceed the configured limits will
either be throttled or killed, based on the resource type. When resource requests are set lower
than limits, each container is guaranteed the requested amount but there may still be some
potential for impact across workloads.</p><p>Quotas cannot protect against all kinds of resource sharing, such as network traffic.
Node isolation (described below) may be a better solution for this problem.</p><h2 id="data-plane-isolation">Data Plane Isolation</h2><p>Data plane isolation ensures that pods and workloads for different tenants are sufficiently
isolated.</p><h3 id="network-isolation">Network isolation</h3><p>By default, all pods in a Kubernetes cluster are allowed to communicate with each other, and all
network traffic is unencrypted. This can lead to security vulnerabilities where traffic is
accidentally or maliciously sent to an unintended destination, or is intercepted by a workload on
a compromised node.</p><p>Pod-to-pod communication can be controlled using <a href="/docs/concepts/services-networking/network-policies/">Network Policies</a>,
which restrict communication between pods using namespace labels or IP address ranges.
In a multi-tenant environment where strict network isolation between tenants is required, starting
with a default policy that denies communication between pods is recommended with another rule that
allows all pods to query the DNS server for name resolution. With such a default policy in place,
you can begin adding more permissive rules that allow for communication within a namespace.
It is also recommended not to use empty label selector '{}' for namespaceSelector field in network policy definition,
in case traffic need to be allowed between namespaces.
This scheme can be further refined as required. Note that this only applies to pods within a single
control plane; pods that belong to different virtual control planes cannot talk to each other via
Kubernetes networking.</p><p>Namespace management tools may simplify the creation of default or common network policies.
In addition, some of these tools allow you to enforce a consistent set of namespace labels across
your cluster, ensuring that they are a trusted basis for your policies.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>Network policies require a <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni">CNI plugin</a>
that supports the implementation of network policies. Otherwise, NetworkPolicy resources will be ignored.</div><p>More advanced network isolation may be provided by service meshes, which provide OSI Layer 7
policies based on workload identity, in addition to namespaces. These higher-level policies can
make it easier to manage namespace-based multi-tenancy, especially when multiple namespaces are
dedicated to a single tenant. They frequently also offer encryption using mutual TLS, protecting
your data even in the presence of a compromised node, and work across dedicated or virtual clusters.
However, they can be significantly more complex to manage and may not be appropriate for all users.</p><h3 id="storage-isolation">Storage isolation</h3><p>Kubernetes offers several types of volumes that can be used as persistent storage for workloads.
For security and data-isolation, <a href="/docs/concepts/storage/dynamic-provisioning/">dynamic volume provisioning</a>
is recommended and volume types that use node resources should be avoided.</p><p><a href="/docs/concepts/storage/storage-classes/">StorageClasses</a> allow you to describe custom "classes"
of storage offered by your cluster, based on quality-of-service levels, backup policies, or custom
policies determined by the cluster administrators.</p><p>Pods can request storage using a <a href="/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaim</a>.
A PersistentVolumeClaim is a namespaced resource, which enables isolating portions of the storage
system and dedicating it to tenants within the shared Kubernetes cluster.
However, it is important to note that a PersistentVolume is a cluster-wide resource and has a
lifecycle independent of workloads and namespaces.</p><p>For example, you can configure a separate StorageClass for each tenant and use this to strengthen isolation.
If a StorageClass is shared, you should set a <a href="/docs/concepts/storage/storage-classes/#reclaim-policy">reclaim policy of <code>Delete</code></a>
to ensure that a PersistentVolume cannot be reused across different namespaces.</p><h3 id="sandboxing-containers">Sandboxing containers</h3><p>Kubernetes pods are composed of one or more containers that execute on worker nodes.
Containers utilize OS-level virtualization and hence offer a weaker isolation boundary than
virtual machines that utilize hardware-based virtualization.</p><p>In a shared environment, unpatched vulnerabilities in the application and system layers can be
exploited by attackers for container breakouts and remote code execution that allow access to host
resources. In some applications, like a Content Management System (CMS), customers may be allowed
the ability to upload and execute untrusted scripts or code. In either case, mechanisms to further
isolate and protect workloads using strong isolation are desirable.</p><p>Sandboxing provides a way to isolate workloads running in a shared cluster. It typically involves
running each pod in a separate execution environment such as a virtual machine or a userspace
kernel. Sandboxing is often recommended when you are running untrusted code, where workloads are
assumed to be malicious. Part of the reason this type of isolation is necessary is because
containers are processes running on a shared kernel; they mount file systems like <code>/sys</code> and <code>/proc</code>
from the underlying host, making them less secure than an application that runs on a virtual
machine which has its own kernel. While controls such as seccomp, AppArmor, and SELinux can be
used to strengthen the security of containers, it is hard to apply a universal set of rules to all
workloads running in a shared cluster. Running workloads in a sandbox environment helps to
insulate the host from container escapes, where an attacker exploits a vulnerability to gain
access to the host system and all the processes/files running on that host.</p><p>Virtual machines and userspace kernels are two popular approaches to sandboxing.</p><h3 id="node-isolation">Node Isolation</h3><p>Node isolation is another technique that you can use to isolate tenant workloads from each other.
With node isolation, a set of nodes is dedicated to running pods from a particular tenant and
co-mingling of tenant pods is prohibited. This configuration reduces the noisy tenant issue, as
all pods running on a node will belong to a single tenant. The risk of information disclosure is
slightly lower with node isolation because an attacker that manages to escape from a container
will only have access to the containers and volumes mounted to that node.</p><p>Although workloads from different tenants are running on different nodes, it is important to be
aware that the kubelet and (unless using virtual control planes) the API service are still shared
services. A skilled attacker could use the permissions assigned to the kubelet or other pods
running on the node to move laterally within the cluster and gain access to tenant workloads
running on other nodes. If this is a major concern, consider implementing compensating controls
such as seccomp, AppArmor or SELinux or explore using sandboxed containers or creating separate
clusters for each tenant.</p><p>Node isolation is a little easier to reason about from a billing standpoint than sandboxing
containers since you can charge back per node rather than per pod. It also has fewer compatibility
and performance issues and may be easier to implement than sandboxing containers.
For example, nodes for each tenant can be configured with taints so that only pods with the
corresponding toleration can run on them. A mutating webhook could then be used to automatically
add tolerations and node affinities to pods deployed into tenant namespaces so that they run on a
specific set of nodes designated for that tenant.</p><p>Node isolation can be implemented using <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">pod node selectors</a>.</p><h2 id="additional-considerations">Additional Considerations</h2><p>This section discusses other Kubernetes constructs and patterns that are relevant for multi-tenancy.</p><h3 id="api-priority-and-fairness">API Priority and Fairness</h3><p><a href="/docs/concepts/cluster-administration/flow-control/">API priority and fairness</a> is a Kubernetes
feature that allows you to assign a priority to certain pods running within the cluster.
When an application calls the Kubernetes API, the API server evaluates the priority assigned to pod.
Calls from pods with higher priority are fulfilled before those with a lower priority.
When contention is high, lower priority calls can be queued until the server is less busy or you
can reject the requests.</p><p>Using API priority and fairness will not be very common in SaaS environments unless you are
allowing customers to run applications that interface with the Kubernetes API, for example,
a controller.</p><h3 id="qos">Quality-of-Service (QoS)</h3><p>When you&#8217;re running a SaaS application, you may want the ability to offer different
Quality-of-Service (QoS) tiers of service to different tenants. For example, you may have freemium
service that comes with fewer performance guarantees and features and a for-fee service tier with
specific performance guarantees. Fortunately, there are several Kubernetes constructs that can
help you accomplish this within a shared cluster, including network QoS, storage classes, and pod
priority and preemption. The idea with each of these is to provide tenants with the quality of
service that they paid for. Let&#8217;s start by looking at networking QoS.</p><p>Typically, all pods on a node share a network interface. Without network QoS, some pods may
consume an unfair share of the available bandwidth at the expense of other pods.
The Kubernetes <a href="https://www.cni.dev/plugins/current/meta/bandwidth/">bandwidth plugin</a> creates an
<a href="/docs/concepts/configuration/manage-resources-containers/#extended-resources">extended resource</a>
for networking that allows you to use Kubernetes resources constructs, i.e. requests/limits, to
apply rate limits to pods by using Linux tc queues.
Be aware that the plugin is considered experimental as per the
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping">Network Plugins</a>
documentation and should be thoroughly tested before use in production environments.</p><p>For storage QoS, you will likely want to create different storage classes or profiles with
different performance characteristics. Each storage profile can be associated with a different
tier of service that is optimized for different workloads such IO, redundancy, or throughput.
Additional logic might be necessary to allow the tenant to associate the appropriate storage
profile with their workload.</p><p>Finally, there&#8217;s <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">pod priority and preemption</a>
where you can assign priority values to pods. When scheduling pods, the scheduler will try
evicting pods with lower priority when there are insufficient resources to schedule pods that are
assigned a higher priority. If you have a use case where tenants have different service tiers in a
shared cluster e.g. free and paid, you may want to give higher priority to certain tiers using
this feature.</p><h3 id="dns">DNS</h3><p>Kubernetes clusters include a Domain Name System (DNS) service to provide translations from names
to IP addresses, for all Services and Pods. By default, the Kubernetes DNS service allows lookups
across all namespaces in the cluster.</p><p>In multi-tenant environments where tenants can access pods and other Kubernetes resources, or where
stronger isolation is required, it may be necessary to prevent pods from looking up services in other
Namespaces.
You can restrict cross-namespace DNS lookups by configuring security rules for the DNS service.
For example, CoreDNS (the default DNS service for Kubernetes) can leverage Kubernetes metadata
to restrict queries to Pods and Services within a namespace. For more information, read an
<a href="https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy">example</a> of
configuring this within the CoreDNS documentation.</p><p>When a <a href="#virtual-control-plane-per-tenant">Virtual Control Plane per tenant</a> model is used, a DNS
service must be configured per tenant or a multi-tenant DNS service must be used.
Here is an example of a <a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested/blob/main/virtualcluster/doc/tenant-dns.md">customized version of CoreDNS</a>
that supports multiple tenants.</p><h3 id="operators">Operators</h3><p><a href="/docs/concepts/extend-kubernetes/operator/">Operators</a> are Kubernetes controllers that manage
applications. Operators can simplify the management of multiple instances of an application, like
a database service, which makes them a common building block in the multi-consumer (SaaS)
multi-tenancy use case.</p><p>Operators used in a multi-tenant environment should follow a stricter set of guidelines.
Specifically, the Operator should:</p><ul><li>Support creating resources within different tenant namespaces, rather than just in the namespace
in which the Operator is deployed.</li><li>Ensure that the Pods are configured with resource requests and limits, to ensure scheduling and fairness.</li><li>Support configuration of Pods for data-plane isolation techniques such as node isolation and
sandboxed containers.</li></ul><h2 id="implementations">Implementations</h2><p>There are two primary ways to share a Kubernetes cluster for multi-tenancy: using Namespaces
(that is, a Namespace per tenant) or by virtualizing the control plane (that is, virtual control
plane per tenant).</p><p>In both cases, data plane isolation, and management of additional considerations such as API
Priority and Fairness, is also recommended.</p><p>Namespace isolation is well-supported by Kubernetes, has a negligible resource cost, and provides
mechanisms to allow tenants to interact appropriately, such as by allowing service-to-service
communication. However, it can be difficult to configure, and doesn't apply to Kubernetes
resources that can't be namespaced, such as Custom Resource Definitions, Storage Classes, and Webhooks.</p><p>Control plane virtualization allows for isolation of non-namespaced resources at the cost of
somewhat higher resource usage and more difficult cross-tenant sharing. It is a good option when
namespace isolation is insufficient but dedicated clusters are undesirable, due to the high cost
of maintaining them (especially on-prem) or due to their higher overhead and lack of resource
sharing. However, even within a virtualized control plane, you will likely see benefits by using
namespaces as well.</p><p>The two options are discussed in more detail in the following sections.</p><h3 id="namespace-per-tenant">Namespace per tenant</h3><p>As previously mentioned, you should consider isolating each workload in its own namespace, even if
you are using dedicated clusters or virtualized control planes. This ensures that each workload
only has access to its own resources, such as ConfigMaps and Secrets, and allows you to tailor
dedicated security policies for each workload. In addition, it is a best practice to give each
namespace names that are unique across your entire fleet (that is, even if they are in separate
clusters), as this gives you the flexibility to switch between dedicated and shared clusters in
the future, or to use multi-cluster tooling such as service meshes.</p><p>Conversely, there are also advantages to assigning namespaces at the tenant level, not just the
workload level, since there are often policies that apply to all workloads owned by a single
tenant. However, this raises its own problems. Firstly, this makes it difficult or impossible to
customize policies to individual workloads, and secondly, it may be challenging to come up with a
single level of "tenancy" that should be given a namespace. For example, an organization may have
divisions, teams, and subteams - which should be assigned a namespace?</p><p>One possible approach is to organize your namespaces into hierarchies, and share certain policies and
resources between them. This could include managing namespace labels, namespace lifecycles,
delegated access, and shared resource quotas across related namespaces. These capabilities can
be useful in both multi-team and multi-customer scenarios.</p><h3 id="virtual-control-plane-per-tenant">Virtual control plane per tenant</h3><p>Another form of control-plane isolation is to use Kubernetes extensions to provide each tenant a
virtual control-plane that enables segmentation of cluster-wide API resources.
<a href="#data-plane-isolation">Data plane isolation</a> techniques can be used with this model to securely
manage worker nodes across tenants.</p><p>The virtual control plane based multi-tenancy model extends namespace-based multi-tenancy by
providing each tenant with dedicated control plane components, and hence complete control over
cluster-wide resources and add-on services. Worker nodes are shared across all tenants, and are
managed by a Kubernetes cluster that is normally inaccessible to tenants.
This cluster is often referred to as a <em>super-cluster</em> (or sometimes as a <em>host-cluster</em>).
Since a tenant&#8217;s control-plane is not directly associated with underlying compute resources it is
referred to as a <em>virtual control plane</em>.</p><p>A virtual control plane typically consists of the Kubernetes API server, the controller manager,
and the etcd data store. It interacts with the super cluster via a metadata synchronization
controller which coordinates changes across tenant control planes and the control plane of the
super-cluster.</p><p>By using per-tenant dedicated control planes, most of the isolation problems due to sharing one
API server among all tenants are solved. Examples include noisy neighbors in the control plane,
uncontrollable blast radius of policy misconfigurations, and conflicts between cluster scope
objects such as webhooks and CRDs. Hence, the virtual control plane model is particularly
suitable for cases where each tenant requires access to a Kubernetes API server and expects the
full cluster manageability.</p><p>The improved isolation comes at the cost of running and maintaining an individual virtual control
plane per tenant. In addition, per-tenant control planes do not solve isolation problems in the
data plane, such as node-level noisy neighbors or security threats. These must still be addressed
separately.</p></div></div><div><div class="td-content"><h1>Hardening Guide - Authentication Mechanisms</h1><div class="lead">Information on authentication options in Kubernetes and their security properties.</div><p>Selecting the appropriate authentication mechanism(s) is a crucial aspect of securing your cluster.
Kubernetes provides several built-in mechanisms, each with its own strengths and weaknesses that
should be carefully considered when choosing the best authentication mechanism for your cluster.</p><p>In general, it is recommended to enable as few authentication mechanisms as possible to simplify
user management and prevent cases where users retain access to a cluster that is no longer required.</p><p>It is important to note that Kubernetes does not have an in-built user database within the cluster.
Instead, it takes user information from the configured authentication system and uses that to make
authorization decisions. Therefore, to audit user access, you need to review credentials from every
configured authentication source.</p><p>For production clusters with multiple users directly accessing the Kubernetes API, it is
recommended to use external authentication sources such as OIDC. The internal authentication
mechanisms, such as client certificates and service account tokens, described below, are not
suitable for this use case.</p><h2 id="x509-client-certificate-authentication">X.509 client certificate authentication</h2><p>Kubernetes leverages <a href="/docs/reference/access-authn-authz/authentication/#x509-client-certificates">X.509 client certificate</a>
authentication for system components, such as when the kubelet authenticates to the API Server.
While this mechanism can also be used for user authentication, it might not be suitable for
production use due to several restrictions:</p><ul><li>Client certificates cannot be individually revoked. Once compromised, a certificate can be used
by an attacker until it expires. To mitigate this risk, it is recommended to configure short
lifetimes for user authentication credentials created using client certificates.</li><li>If a certificate needs to be invalidated, the certificate authority must be re-keyed, which
can introduce availability risks to the cluster.</li><li>There is no permanent record of client certificates created in the cluster. Therefore, all
issued certificates must be recorded if you need to keep track of them.</li><li>Private keys used for client certificate authentication cannot be password-protected. Anyone
who can read the file containing the key will be able to make use of it.</li><li>Using client certificate authentication requires a direct connection from the client to the
API server without any intervening TLS termination points, which can complicate network architectures.</li><li>Group data is embedded in the <code>O</code> value of the client certificate, which means the user's group
memberships cannot be changed for the lifetime of the certificate.</li></ul><h2 id="static-token-file">Static token file</h2><p>Although Kubernetes allows you to load credentials from a
<a href="/docs/reference/access-authn-authz/authentication/#static-token-file">static token file</a> located
on the control plane node disks, this approach is not recommended for production servers due to
several reasons:</p><ul><li>Credentials are stored in clear text on control plane node disks, which can be a security risk.</li><li>Changing any credential requires a restart of the API server process to take effect, which can
impact availability.</li><li>There is no mechanism available to allow users to rotate their credentials. To rotate a
credential, a cluster administrator must modify the token on disk and distribute it to the users.</li><li>There is no lockout mechanism available to prevent brute-force attacks.</li></ul><h2 id="bootstrap-tokens">Bootstrap tokens</h2><p><a href="/docs/reference/access-authn-authz/bootstrap-tokens/">Bootstrap tokens</a> are used for joining
nodes to clusters and are not recommended for user authentication due to several reasons:</p><ul><li>They have hard-coded group memberships that are not suitable for general use, making them
unsuitable for authentication purposes.</li><li>Manually generating bootstrap tokens can lead to weak tokens that can be guessed by an attacker,
which can be a security risk.</li><li>There is no lockout mechanism available to prevent brute-force attacks, making it easier for
attackers to guess or crack the token.</li></ul><h2 id="serviceaccount-secret-tokens">ServiceAccount secret tokens</h2><p><a href="/docs/reference/access-authn-authz/service-accounts-admin/#manual-secret-management-for-serviceaccounts">Service account secrets</a>
are available as an option to allow workloads running in the cluster to authenticate to the
API server. In Kubernetes &lt; 1.23, these were the default option, however, they are being replaced
with TokenRequest API tokens. While these secrets could be used for user authentication, they are
generally unsuitable for a number of reasons:</p><ul><li>They cannot be set with an expiry and will remain valid until the associated service account is deleted.</li><li>The authentication tokens are visible to any cluster user who can read secrets in the namespace
that they are defined in.</li><li>Service accounts cannot be added to arbitrary groups complicating RBAC management where they are used.</li></ul><h2 id="tokenrequest-api-tokens">TokenRequest API tokens</h2><p>The TokenRequest API is a useful tool for generating short-lived credentials for service
authentication to the API server or third-party systems. However, it is not generally recommended
for user authentication as there is no revocation method available, and distributing credentials
to users in a secure manner can be challenging.</p><p>When using TokenRequest tokens for service authentication, it is recommended to implement a short
lifespan to reduce the impact of compromised tokens.</p><h2 id="openid-connect-token-authentication">OpenID Connect token authentication</h2><p>Kubernetes supports integrating external authentication services with the Kubernetes API using
<a href="/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">OpenID Connect (OIDC)</a>.
There is a wide variety of software that can be used to integrate Kubernetes with an identity
provider. However, when using OIDC authentication in Kubernetes, it is important to consider the
following hardening measures:</p><ul><li>The software installed in the cluster to support OIDC authentication should be isolated from
general workloads as it will run with high privileges.</li><li>Some Kubernetes managed services are limited in the OIDC providers that can be used.</li><li>As with TokenRequest tokens, OIDC tokens should have a short lifespan to reduce the impact of
compromised tokens.</li></ul><h2 id="webhook-token-authentication">Webhook token authentication</h2><p><a href="/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">Webhook token authentication</a>
is another option for integrating external authentication providers into Kubernetes. This mechanism
allows for an authentication service, either running inside the cluster or externally, to be
contacted for an authentication decision over a webhook. It is important to note that the suitability
of this mechanism will likely depend on the software used for the authentication service, and there
are some Kubernetes-specific considerations to take into account.</p><p>To configure Webhook authentication, access to control plane server filesystems is required. This
means that it will not be possible with Managed Kubernetes unless the provider specifically makes it
available. Additionally, any software installed in the cluster to support this access should be
isolated from general workloads, as it will run with high privileges.</p><h2 id="authenticating-proxy">Authenticating proxy</h2><p>Another option for integrating external authentication systems into Kubernetes is to use an
<a href="/docs/reference/access-authn-authz/authentication/#authenticating-proxy">authenticating proxy</a>.
With this mechanism, Kubernetes expects to receive requests from the proxy with specific header
values set, indicating the username and group memberships to assign for authorization purposes.
It is important to note that there are specific considerations to take into account when using
this mechanism.</p><p>Firstly, securely configured TLS must be used between the proxy and Kubernetes API server to
mitigate the risk of traffic interception or sniffing attacks. This ensures that the communication
between the proxy and Kubernetes API server is secure.</p><p>Secondly, it is important to be aware that an attacker who is able to modify the headers of the
request may be able to gain unauthorized access to Kubernetes resources. As such, it is important
to ensure that the headers are properly secured and cannot be tampered with.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/reference/access-authn-authz/authentication/">User Authentication</a></li><li><a href="/docs/reference/access-authn-authz/bootstrap-tokens/">Authenticating with Bootstrap Tokens</a></li><li><a href="/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication">kubelet Authentication</a></li><li><a href="/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-tokens">Authenticating with Service Account Tokens</a></li></ul></div></div><div><div class="td-content"><h1>Hardening Guide - Scheduler Configuration</h1><div class="lead">Information about how to make the Kubernetes scheduler more secure.</div><p>The Kubernetes <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">scheduler</a> is
one of the critical components of the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>.</p><p>This document covers how to improve the security posture of the Scheduler.</p><p>A misconfigured scheduler can have security implications.
Such a scheduler can target specific nodes and evict the workloads or applications that are sharing the node and its resources.
This can aid an attacker with a <a href="https://arxiv.org/abs/2105.00542">Yo-Yo attack</a>: an attack on a vulnerable autoscaler.</p><h2 id="kube-scheduler-configuration">kube-scheduler configuration</h2><h3 id="scheduler-authentication-authorization-command-line-options">Scheduler authentication &amp; authorization command line options</h3><p>When setting up authentication configuration, it should be made sure that kube-scheduler's authentication remains consistent with kube-api-server's authentication.
If any request has missing authentication headers,
the <a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/#original-request-username-and-group">authentication should happen through the kube-api-server allowing all authentication to be consistent in the cluster</a>.</p><ul><li><code>authentication-kubeconfig</code>: Make sure to provide a proper kubeconfig so that the scheduler can retrieve authentication configuration options from the API Server. This kubeconfig file should be protected with strict file permissions.</li><li><code>authentication-tolerate-lookup-failure</code>: Set this to <code>false</code> to make sure the scheduler <em>always</em> looks up its authentication configuration from the API server.</li><li><code>authentication-skip-lookup</code>: Set this to <code>false</code> to make sure the scheduler <em>always</em> looks up its authentication configuration from the API server.</li><li><code>authorization-always-allow-paths</code>: These paths should respond with data that is appropriate for anonymous authorization. Defaults to <code>/healthz,/readyz,/livez</code>.</li><li><code>profiling</code>: Set to <code>false</code> to disable the profiling endpoints which are provide debugging information but which should not be enabled on production clusters as they present a risk of denial of service or information leakage. The <code>--profiling</code> argument is deprecated and can now be provided through the <a href="https://kubernetes.io/docs/reference/config-api/kube-scheduler-config.v1/#DebuggingConfiguration">KubeScheduler DebuggingConfiguration</a>. Profiling can be disabled through the kube-scheduler config by setting <code>enableProfiling</code> to <code>false</code>.</li><li><code>requestheader-client-ca-file</code>: Avoid passing this argument.</li></ul><h3 id="scheduler-networking-command-line-options">Scheduler networking command line options</h3><ul><li><code>bind-address</code>: In most cases, the kube-scheduler does not need to be externally accessible. Setting the bind address to <code>localhost</code> is a secure practice.</li><li><code>permit-address-sharing</code>: Set this to <code>false</code> to disable connection sharing through <code>SO_REUSEADDR</code>. <code>SO_REUSEADDR</code> can lead to reuse of terminated connections that are in <code>TIME_WAIT</code> state.</li><li><code>permit-port-sharing</code>: Default <code>false</code>. Use the default unless you are confident you understand the security implications.</li></ul><h3 id="scheduler-tls-command-line-options">Scheduler TLS command line options</h3><ul><li><code>tls-cipher-suites</code>: Always provide a list of preferred cipher suites. This ensures encryption never happens with insecure cipher suites.</li></ul><h2 id="scheduling-configurations-for-custom-schedulers">Scheduling configurations for custom schedulers</h2><p>When using custom schedulers based on the Kubernetes scheduling code, cluster administrators need to be careful with
plugins that use the <code>queueSort</code>, <code>prefilter</code>, <code>filter</code>, or <code>permit</code> <a href="/docs/reference/scheduling/config/#extension-points">extension points</a>.
These extension points control various stages of a scheduling process, and the wrong configuration can impact the kube-scheduler's behavior in your cluster.</p><h3 id="key-considerations">Key considerations</h3><ul><li>Exactly one plugin that uses the <code>queueSort</code> extension point can be enabled at a time. Any plugins that use <code>queueSort</code> should be scrutinized.</li><li>Plugins that implement the <code>prefilter</code> or <code>filter</code> extension point can potentially mark all nodes as unschedulable. This can bring scheduling of new pods to a halt.</li><li>Plugins that implement the <code>permit</code> extension point can prevent or delay the binding of a Pod. Such plugins should be thoroughly reviewed by the cluster administrator.</li></ul><p>When using a plugin that is not one of the <a href="/docs/reference/scheduling/config/#scheduling-plugins">default plugins</a>, consider disabling the <code>queueSort</code>, <code>filter</code> and <code>permit</code> extension points as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubescheduler.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeSchedulerConfiguration<span>
</span></span></span><span><span><span></span><span>profiles</span>:<span>
</span></span></span><span><span><span>  </span>- <span>schedulerName</span>:<span> </span>my-scheduler<span>
</span></span></span><span><span><span>    </span><span>plugins</span>:<span>
</span></span></span><span><span><span>      </span><span># Disable specific plugins for different extension points</span><span>
</span></span></span><span><span><span>      </span><span># You can disable all plugins for an extension point using "*"</span><span>
</span></span></span><span><span><span>      </span><span>queueSort</span>:<span>
</span></span></span><span><span><span>        </span><span>disabled</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span><span>"*"</span><span>             </span><span># Disable all queueSort plugins</span><span>
</span></span></span><span><span><span>      </span><span># - name: "PrioritySort"  # Disable specific queueSort plugin</span><span>
</span></span></span><span><span><span>      </span><span>filter</span>:<span>
</span></span></span><span><span><span>        </span><span>disabled</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span><span>"*"</span><span>                 </span><span># Disable all filter plugins</span><span>
</span></span></span><span><span><span>      </span><span># - name: "NodeResourcesFit"  # Disable specific filter plugin</span><span>
</span></span></span><span><span><span>      </span><span>permit</span>:<span>
</span></span></span><span><span><span>        </span><span>disabled</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span><span>"*"</span><span>               </span><span># Disables all permit plugins</span><span>
</span></span></span><span><span><span>      </span><span># - name: "TaintToleration" # Disable specific permit plugin</span><span>
</span></span></span></code></pre></div><p>This creates a scheduler profile <code>my-custom-scheduler</code>.
Whenever the <code>.spec</code> of a Pod does not have a value for <code>.spec.schedulerName</code>, the kube-scheduler runs for that Pod,
using its main configuration, and default plugins.
If you define a Pod with <code>.spec.schedulerName</code> set to <code>my-custom-scheduler</code>, the kube-scheduler runs but with a custom configuration; in that custom configuration,
the <code>queueSort</code>, <code>filter</code> and <code>permit</code> extension points are disabled.
If you use this KubeSchedulerConfiguration, and don't run any custom scheduler,
and you then define a Pod with <code>.spec.schedulerName</code> set to <code>nonexistent-scheduler</code>
(or any other scheduler name that doesn't exist in your cluster), no events would be generated for a pod.</p><h2 id="disallow-labeling-nodes">Disallow labeling nodes</h2><p>A cluster administrator should ensure that cluster users cannot label the nodes.
A malicious actor can use <code>nodeSelector</code> to schedule workloads on nodes where those workloads should not be present.</p></div></div><div><div class="td-content"><h1>Kubernetes API Server Bypass Risks</h1><div class="lead">Security architecture information relating to the API server and other components</div><p>The Kubernetes API server is the main point of entry to a cluster for external parties
(users and services) interacting with it.</p><p>As part of this role, the API server has several key built-in security controls, such as
audit logging and <a class="glossary-tooltip" title="A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object." href="/docs/reference/access-authn-authz/admission-controllers/" target="_blank">admission controllers</a>.
However, there are ways to modify the configuration
or content of the cluster that bypass these controls.</p><p>This page describes the ways in which the security controls built into the
Kubernetes API server can be bypassed, so that cluster operators
and security architects can ensure that these bypasses are appropriately restricted.</p><h2 id="static-pods">Static Pods</h2><p>The <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> on each node loads and
directly manages any manifests that are stored in a named directory or fetched from
a specific URL as <a href="/docs/tasks/configure-pod-container/static-pod/"><em>static Pods</em></a> in
your cluster. The API server doesn't manage these static Pods. An attacker with write
access to this location could modify the configuration of static pods loaded from that
source, or could introduce new static Pods.</p><p>Static Pods are restricted from accessing other objects in the Kubernetes API. For example,
you can't configure a static Pod to mount a Secret from the cluster. However, these Pods can
take other security sensitive actions, such as using <code>hostPath</code> mounts from the underlying
node.</p><p>By default, the kubelet creates a <a class="glossary-tooltip" title="An object in the API server that tracks a static pod on a kubelet." href="/docs/reference/glossary/?all=true#term-mirror-pod" target="_blank">mirror pod</a>
so that the static Pods are visible in the Kubernetes API. However, if the attacker uses an invalid
namespace name when creating the Pod, it will not be visible in the Kubernetes API and can only
be discovered by tooling that has access to the affected host(s).</p><p>If a static Pod fails admission control, the kubelet won't register the Pod with the
API server. However, the Pod still runs on the node. For more information, refer to
<a href="https://github.com/kubernetes/kubeadm/issues/1541#issuecomment-487331701">kubeadm issue #1541</a>.</p><h3 id="static-pods-mitigations">Mitigations</h3><ul><li>Only <a href="/docs/tasks/configure-pod-container/static-pod/#static-pod-creation">enable the kubelet static Pod manifest functionality</a>
if required by the node.</li><li>If a node uses the static Pod functionality, restrict filesystem access to the static Pod manifest directory
or URL to users who need the access.</li><li>Restrict access to kubelet configuration parameters and files to prevent an attacker setting
a static Pod path or URL.</li><li>Regularly audit and centrally report all access to directories or web storage locations that host
static Pod manifests and kubelet configuration files.</li></ul><h2 id="kubelet-api">The kubelet API</h2><p>The kubelet provides an HTTP API that is typically exposed on TCP port 10250 on cluster
worker nodes. The API might also be exposed on control plane nodes depending on the Kubernetes
distribution in use. Direct access to the API allows for disclosure of information about
the pods running on a node, the logs from those pods, and execution of commands in
every container running on the node.</p><p>When Kubernetes cluster users have RBAC access to <code>Node</code> object sub-resources, that access
serves as authorization to interact with the kubelet API. The exact access depends on
which sub-resource access has been granted, as detailed in
<a href="/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authorization">kubelet authorization</a>.</p><p>Direct access to the kubelet API is not subject to admission control and is not logged
by Kubernetes audit logging. An attacker with direct access to this API may be able to
bypass controls that detect or prevent certain actions.</p><p>The kubelet API can be configured to authenticate requests in a number of ways.
By default, the kubelet configuration allows anonymous access. Most Kubernetes providers
change the default to use webhook and certificate authentication. This lets the control plane
ensure that the caller is authorized to access the <code>nodes</code> API resource or sub-resources.
The default anonymous access doesn't make this assertion with the control plane.</p><h3 id="mitigations">Mitigations</h3><ul><li>Restrict access to sub-resources of the <code>nodes</code> API object using mechanisms such as
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a>. Only grant this access when required,
such as by monitoring services.</li><li>Restrict access to the kubelet port. Only allow specified and trusted IP address
ranges to access the port.</li><li>Ensure that <a href="/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication">kubelet authentication</a>.
is set to webhook or certificate mode.</li><li>Ensure that the unauthenticated "read-only" Kubelet port is not enabled on the cluster.</li></ul><h2 id="the-etcd-api">The etcd API</h2><p>Kubernetes clusters use etcd as a datastore. The <code>etcd</code> service listens on TCP port 2379.
The only clients that need access are the Kubernetes API server and any backup tooling
that you use. Direct access to this API allows for disclosure or modification of any
data held in the cluster.</p><p>Access to the etcd API is typically managed by client certificate authentication.
Any certificate issued by a certificate authority that etcd trusts allows full access
to the data stored inside etcd.</p><p>Direct access to etcd is not subject to Kubernetes admission control and is not logged
by Kubernetes audit logging. An attacker who has read access to the API server's
etcd client certificate private key (or can create a new trusted client certificate) can gain
cluster admin rights by accessing cluster secrets or modifying access rules. Even without
elevating their Kubernetes RBAC privileges, an attacker who can modify etcd can retrieve any API object
or create new workloads inside the cluster.</p><p>Many Kubernetes providers configure
etcd to use mutual TLS (both client and server verify each other's certificate for authentication).
There is no widely accepted implementation of authorization for the etcd API, although
the feature exists. Since there is no authorization model, any certificate
with client access to etcd can be used to gain full access to etcd. Typically, etcd client certificates
that are only used for health checking can also grant full read and write access.</p><h3 id="etcd-api-mitigations">Mitigations</h3><ul><li>Ensure that the certificate authority trusted by etcd is used only for the purposes of
authentication to that service.</li><li>Control access to the private key for the etcd server certificate, and to the API server's
client certificate and key.</li><li>Consider restricting access to the etcd port at a network level, to only allow access
from specified and trusted IP address ranges.</li></ul><h2 id="runtime-socket">Container runtime socket</h2><p>On each node in a Kubernetes cluster, access to interact with containers is controlled
by the container runtime (or runtimes, if you have configured more than one). Typically,
the container runtime exposes a Unix socket that the kubelet can access. An attacker with
access to this socket can launch new containers or interact with running containers.</p><p>At the cluster level, the impact of this access depends on whether the containers that
run on the compromised node have access to Secrets or other confidential
data that an attacker could use to escalate privileges to other worker nodes or to
control plane components.</p><h3 id="runtime-socket-mitigations">Mitigations</h3><ul><li>Ensure that you tightly control filesystem access to container runtime sockets.
When possible, restrict this access to the <code>root</code> user.</li><li>Isolate the kubelet from other components running on the node, using
mechanisms such as Linux kernel namespaces.</li><li>Ensure that you restrict or forbid the use of <a href="/docs/concepts/storage/volumes/#hostpath"><code>hostPath</code> mounts</a>
that include the container runtime socket, either directly or by mounting a parent
directory. Also <code>hostPath</code> mounts must be set as read-only to mitigate risks
of attackers bypassing directory restrictions.</li><li>Restrict user access to nodes, and especially restrict superuser access to nodes.</li></ul></div></div><div><div class="td-content"><h1>Linux kernel security constraints for Pods and containers</h1><div class="lead">Overview of Linux kernel security modules and constraints that you can use to harden your Pods and containers.</div><p>This page describes some of the security features that are built into the Linux
kernel that you can use in your Kubernetes workloads. To learn how to apply
these features to your Pods and containers, refer to
<a href="/docs/tasks/configure-pod-container/security-context/">Configure a SecurityContext for a Pod or Container</a>.
You should already be familiar with Linux and with the basics of Kubernetes
workloads.</p><h2 id="run-without-root">Run workloads without root privileges</h2><p>When you deploy a workload in Kubernetes, use the Pod specification to restrict
that workload from running as the root user on the node. You can use the Pod
<code>securityContext</code> to define the specific Linux user and group for the processes in
the Pod, and explicitly restrict containers from running as root users. Setting
these values in the Pod manifest takes precedence over similar values in the
container image, which is especially useful if you're running images that you
don't own.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Ensure that the user or group that you assign to the workload has the permissions
required for the application to function correctly. Changing the user or group
to one that doesn't have the correct permissions could lead to file access
issues or failed operations.</div><p>Configuring the kernel security features on this page provides fine-grained
control over the actions that processes in your cluster can take, but managing
these configurations can be challenging at scale. Running containers as
non-root, or in user namespaces if you need root privileges, helps to reduce the
chance that you'll need to enforce your configured kernel security capabilities.</p><h2 id="linux-security-features">Security features in the Linux kernel</h2><p>Kubernetes lets you configure and use Linux kernel features to improve isolation
and harden your containerized workloads. Common features include the following:</p><ul><li><strong>Secure computing mode (seccomp)</strong>: Filter which system calls a process can
make</li><li><strong>AppArmor</strong>: Restrict the access privileges of individual programs</li><li><strong>Security Enhanced Linux (SELinux)</strong>: Assign security labels to objects for
more manageable security policy enforcement</li></ul><p>To configure settings for one of these features, the operating system that you
choose for your nodes must enable the feature in the kernel. For example,
Ubuntu 7.10 and later enable AppArmor by default. To learn whether your OS
enables a specific feature, consult the OS documentation.</p><p>You use the <code>securityContext</code> field in your Pod specification to define the
constraints that apply to those processes. The <code>securityContext</code> field also
supports other security settings, such as specific Linux capabilities or file
access permissions using UIDs and GIDs. To learn more, refer to
<a href="/docs/tasks/configure-pod-container/security-context/">Configure a SecurityContext for a Pod or Container</a>.</p><h3 id="seccomp">seccomp</h3><p>Some of your workloads might need privileges to perform specific actions as the
root user on your node's host machine. Linux uses <em>capabilities</em> to divide the
available privileges into categories, so that processes can get the privileges
required to perform specific actions without being granted all privileges. Each
capability has a set of system calls (syscalls) that a process can make. seccomp
lets you restrict these individual syscalls. It can be used to sandbox the privileges of a process, restricting the calls it
is able to make from userspace into the kernel.</p><p>In Kubernetes, you use a <em>container runtime</em> on each node to run your
containers. Example runtimes include CRI-O, Docker, or containerd. Each runtime
allows only a subset of Linux capabilities by default. You can further limit the
allowed syscalls individually by using a seccomp profile. Container runtimes
usually include a default seccomp profile. Kubernetes lets you automatically
apply seccomp profiles loaded onto a node to your Pods and containers.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubernetes also has the <code>allowPrivilegeEscalation</code> setting for Pods and
containers. When set to <code>false</code>, this prevents processes from gaining new
capabilities and restricts unprivileged users from changing the applied seccomp
profile to a more permissive profile.</div><p>To learn how to implement seccomp in Kubernetes, refer to
<a href="/docs/tutorials/security/seccomp/">Restrict a Container's Syscalls with seccomp</a>
or the <a href="/docs/reference/node/seccomp/">Seccomp node reference</a></p><p>To learn more about seccomp, see
<a href="https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html">Seccomp BPF</a>
in the Linux kernel documentation.</p><h4 id="seccomp-considerations">Considerations for seccomp</h4><p>seccomp is a low-level security configuration that you should only configure
yourself if you require fine-grained control over Linux syscalls. Using
seccomp, especially at scale, has the following risks:</p><ul><li>Configurations might break during application updates</li><li>Attackers can still use allowed syscalls to exploit vulnerabilities</li><li>Profile management for individual applications becomes challenging at scale</li></ul><p><strong>Recommendation</strong>: Use the default seccomp profile that's bundled with your
container runtime. If you need a more isolated environment, consider using a
sandbox, such as gVisor. Sandboxes solve the preceding risks with custom
seccomp profiles, but require more compute resources on your nodes and might
have compatibility issues with GPUs and other specialized hardware.</p><h3 id="policy-based-mac">AppArmor and SELinux: policy-based mandatory access control</h3><p>You can use Linux policy-based mandatory access control (MAC) mechanisms, such
as AppArmor and SELinux, to harden your Kubernetes workloads.</p><h4 id="apparmor">AppArmor</h4><p><a href="https://apparmor.net/">AppArmor</a> is a Linux kernel security module that
supplements the standard Linux user and group based permissions to confine
programs to a limited set of resources. AppArmor can be configured for any
application to reduce its potential attack surface and provide greater in-depth
defense. It is configured through profiles tuned to allow the access needed by a
specific program or container, such as Linux capabilities, network access, and
file permissions. Each profile can be run in either enforcing mode, which blocks
access to disallowed resources, or complain mode, which only reports violations.</p><p>AppArmor can help you to run a more secure deployment by restricting what
containers are allowed to do, and/or provide better auditing through system
logs. The container runtime that you use might ship with a default AppArmor
profile, or you can use a custom profile.</p><p>To learn how to use AppArmor in Kubernetes, refer to
<a href="/docs/tutorials/security/apparmor/">Restrict a Container's Access to Resources with AppArmor</a>.</p><h4 id="selinux">SELinux</h4><p>SELinux is a Linux kernel security module that lets you restrict the access
that a specific <em>subject</em>, such as a process, has to the files on your system.
You define security policies that apply to subjects that have specific SELinux
labels. When a process that has an SELinux label attempts to access a file, the
SELinux server checks whether that process' security policy allows the access
and makes an authorization decision.</p><p>In Kubernetes, you can set an SELinux label in the <code>securityContext</code> field of
your manifest. The specified labels are assigned to those processes. If you
have configured security policies that affect those labels, the host OS kernel
enforces these policies.</p><p>To learn how to use SELinux in Kubernetes, refer to
<a href="/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container">Assign SELinux labels to a container</a>.</p><h4 id="apparmor-selinux-diff">Differences between AppArmor and SELinux</h4><p>The operating system on your Linux nodes usually includes one of either
AppArmor or SELinux. Both mechanisms provide similar types of protection, but
have differences such as the following:</p><ul><li><strong>Configuration</strong>: AppArmor uses profiles to define access to resources.
SELinux uses policies that apply to specific labels.</li><li><strong>Policy application</strong>: In AppArmor, you define resources using file paths.
SELinux uses the index node (inode) of a resource to identify the resource.</li></ul><h3 id="summary">Summary of features</h3><p>The following table describes the use cases and scope of each security control.
You can use all of these controls together to build a more hardened system.</p><table><caption>Summary of Linux kernel security features</caption><thead><tr><th>Security feature</th><th>Description</th><th>How to use</th><th>Example</th></tr></thead><tbody><tr><td>seccomp</td><td>Restrict individual kernel calls in the userspace. Reduces the
likelihood that a vulnerability that uses a restricted syscall would
compromise the system.</td><td>Specify a loaded seccomp profile in the Pod or container specification
to apply its constraints to the processes in the Pod.</td><td>Reject the <code>unshare</code> syscall, which was used in
<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-0185">CVE-2022-0185</a>.</td></tr><tr><td>AppArmor</td><td>Restrict program access to specific resources. Reduces the attack
surface of the program. Improves audit logging.</td><td>Specify a loaded AppArmor profile in the container specification.</td><td>Restrict a read-only program from writing to any file path
in the system.</td></tr><tr><td>SELinux</td><td>Restrict access to resources such as files, applications, ports, and
processes using labels and security policies.</td><td>Specify access restrictions for specific labels. Tag processes with
those labels to enforce the access restrictions related to the label.</td><td>Restrict a container from accessing files outside its own filesystem.</td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Mechanisms like AppArmor and SELinux can provide protection that extends beyond
the container. For example, you can use SELinux to help mitigate
<a href="https://access.redhat.com/security/cve/cve-2019-5736">CVE-2019-5736</a>.</div><h3 id="considerations-custom-configurations">Considerations for managing custom configurations</h3><p>seccomp, AppArmor, and SELinux usually have a default configuration that offers
basic protections. You can also create custom profiles and policies that meet
the requirements of your workloads. Managing and distributing these custom
configurations at scale might be challenging, especially if you use all three
features together. To help you to manage these configurations at scale, use a
tool like the
<a href="https://github.com/kubernetes-sigs/security-profiles-operator">Kubernetes Security Profiles Operator</a>.</p><h2 id="kernel-security-features-privileged-containers">Kernel-level security features and privileged containers</h2><p>Kubernetes lets you specify that some trusted containers can run in
<em>privileged</em> mode. Any container in a Pod can run in privileged mode to use
operating system administrative capabilities that would otherwise be
inaccessible. This is available for both Windows and Linux.</p><p>Privileged containers explicitly override some of the Linux kernel constraints
that you might use in your workloads, as follows:</p><ul><li><strong>seccomp</strong>: Privileged containers run as the <code>Unconfined</code> seccomp profile,
overriding any seccomp profile that you specified in your manifest.</li><li><strong>AppArmor</strong>: Privileged containers ignore any applied AppArmor profiles.</li><li><strong>SELinux</strong>: Privileged containers run as the <code>unconfined_t</code> domain.</li></ul><h3 id="privileged-containers">Privileged containers</h3><p>Any container in a Pod can enable <em>Privileged mode</em> if you set the
<code>privileged: true</code> field in the
<a href="/docs/tasks/configure-pod-container/security-context/"><code>securityContext</code></a>
field for the container. Privileged containers override or undo many other hardening settings such as the applied seccomp profile, AppArmor profile, or
SELinux constraints. Privileged containers are given all Linux capabilities,
including capabilities that they don't require. For example, a root user in a
privileged container might be able to use the <code>CAP_SYS_ADMIN</code> and
<code>CAP_NET_ADMIN</code> capabilities on the node, bypassing the runtime seccomp
configuration and other restrictions.</p><p>In most cases, you should avoid using privileged containers, and instead grant
the specific capabilities required by your container using the <code>capabilities</code>
field in the <code>securityContext</code> field. Only use privileged mode if you have a
capability that you can't grant with the securityContext. This is useful for
containers that want to use operating system administrative capabilities such
as manipulating the network stack or accessing hardware devices.</p><p>In Kubernetes version 1.26 and later, you can also run Windows containers in a
similarly privileged mode by setting the <code>windowsOptions.hostProcess</code> flag on
the security context of the Pod spec. For details and instructions, see
<a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create a Windows HostProcess Pod</a>.</p><h2 id="recommendations-best-practices">Recommendations and best practices</h2><ul><li>Before configuring kernel-level security capabilities, you should consider
implementing network-level isolation. For more information, read the
<a href="/docs/concepts/security/security-checklist/#network-security">Security Checklist</a>.</li><li>Unless necessary, run Linux workloads as non-root by setting specific user and
group IDs in your Pod manifest and by specifying <code>runAsNonRoot: true</code>.</li></ul><p>Additionally, you can run workloads in user namespaces by setting
<code>hostUsers: false</code> in your Pod manifest. This lets you run containers as root
users in the user namespace, but as non-root users in the host namespace on the
node. This is still in early stages of development and might not have the level
of support that you need. For instructions, refer to
<a href="/docs/tasks/configure-pod-container/user-namespaces/">Use a User Namespace With a Pod</a>.</p><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tutorials/security/apparmor/">Learn how to use AppArmor</a></li><li><a href="/docs/tutorials/security/seccomp/">Learn how to use seccomp</a></li><li><a href="/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container">Learn how to use SELinux</a></li><li><a href="/docs/reference/node/seccomp/">Seccomp Node Reference</a></li></ul></div></div><div><div class="td-content"><h1>Security Checklist</h1><div class="lead">Baseline checklist for ensuring security in Kubernetes clusters.</div><p>This checklist aims at providing a basic list of guidance with links to more
comprehensive documentation on each topic. It does not claim to be exhaustive
and is meant to evolve.</p><p>On how to read and use this document:</p><ul><li>The order of topics does not reflect an order of priority.</li><li>Some checklist items are detailed in the paragraph below the list of each section.</li></ul><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Checklists are <strong>not</strong> sufficient for attaining a good security posture on their
own. A good security posture requires constant attention and improvement, but a
checklist can be the first step on the never-ending journey towards security
preparedness. Some of the recommendations in this checklist may be too
restrictive or too lax for your specific security needs. Since Kubernetes
security is not "one size fits all", each category of checklist items should be
evaluated on its merits.</div><h2 id="authentication-authorization">Authentication &amp; Authorization</h2><ul><li> <code>system:masters</code> group is not used for user or component authentication after bootstrapping.</li><li> The kube-controller-manager is running with <code>--use-service-account-credentials</code>
enabled.</li><li> The root certificate is protected (either an offline CA, or a managed
online CA with effective access controls).</li><li> Intermediate and leaf certificates have an expiry date no more than 3
years in the future.</li><li> A process exists for periodic access review, and reviews occur no more
than 24 months apart.</li><li> The <a href="/docs/concepts/security/rbac-good-practices/">Role Based Access Control Good Practices</a>
are followed for guidance related to authentication and authorization.</li></ul><p>After bootstrapping, neither users nor components should authenticate to the
Kubernetes API as <code>system:masters</code>. Similarly, running all of
kube-controller-manager as <code>system:masters</code> should be avoided. In fact,
<code>system:masters</code> should only be used as a break-glass mechanism, as opposed to
an admin user.</p><h2 id="network-security">Network security</h2><ul><li> CNI plugins in use support network policies.</li><li> Ingress and egress network policies are applied to all workloads in the
cluster.</li><li> Default network policies within each namespace, selecting all pods, denying
everything, are in place.</li><li> If appropriate, a service mesh is used to encrypt all communications inside of the cluster.</li><li> The Kubernetes API, kubelet API and etcd are not exposed publicly on Internet.</li><li> Access from the workloads to the cloud metadata API is filtered.</li><li> Use of LoadBalancer and ExternalIPs is restricted.</li></ul><p>A number of <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">Container Network Interface (CNI) plugins</a>
plugins provide the functionality to
restrict network resources that pods may communicate with. This is most commonly done
through <a href="/docs/concepts/services-networking/network-policies/">Network Policies</a>
which provide a namespaced resource to define rules. Default network policies
that block all egress and ingress, in each namespace, selecting all pods, can be
useful to adopt an allow list approach to ensure that no workloads are missed.</p><p>Not all CNI plugins provide encryption in transit. If the chosen plugin lacks this
feature, an alternative solution could be to use a service mesh to provide that
functionality.</p><p>The etcd datastore of the control plane should have controls to limit access and
not be publicly exposed on the Internet. Furthermore, mutual TLS (mTLS) should
be used to communicate securely with it. The certificate authority for this
should be unique to etcd.</p><p>External Internet access to the Kubernetes API server should be restricted to
not expose the API publicly. Be careful, as many managed Kubernetes distributions
are publicly exposing the API server by default. You can then use a bastion host
to access the server.</p><p>The <a href="/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> API access
should be restricted and not exposed publicly, the default authentication and
authorization settings, when no configuration file specified with the <code>--config</code>
flag, are overly permissive.</p><p>If a cloud provider is used for hosting Kubernetes, the access from pods to the cloud
metadata API <code>169.254.169.254</code> should also be restricted or blocked if not needed
because it may leak information.</p><p>For restricted LoadBalancer and ExternalIPs use, see
<a href="https://github.com/kubernetes/kubernetes/issues/97076">CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs</a>
and the <a href="/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips">DenyServiceExternalIPs admission controller</a>
for further information.</p><h2 id="pod-security">Pod security</h2><ul><li> RBAC rights to <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code> workloads is only granted if necessary.</li><li> Appropriate Pod Security Standards policy is applied for all namespaces and enforced.</li><li> Memory limit is set for the workloads with a limit equal or inferior to the request.</li><li> CPU limit might be set on sensitive workloads.</li><li> For nodes that support it, Seccomp is enabled with appropriate syscalls
profile for programs.</li><li> For nodes that support it, AppArmor or SELinux is enabled with appropriate
profile for programs.</li></ul><p>RBAC authorization is crucial but
<a href="/docs/concepts/security/rbac-good-practices/#workload-creation">cannot be granular enough to have authorization on the Pods' resources</a>
(or on any resource that manages Pods). The only granularity is the API verbs
on the resource itself, for example, <code>create</code> on Pods. Without
additional admission, the authorization to create these resources allows direct
unrestricted access to the schedulable nodes of a cluster.</p><p>The <a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>
define three different policies, privileged, baseline and restricted that limit
how fields can be set in the <code>PodSpec</code> regarding security.
These standards can be enforced at the namespace level with the new
<a href="/docs/concepts/security/pod-security-admission/">Pod Security</a> admission,
enabled by default, or by third-party admission webhook. Please note that,
contrary to the removed PodSecurityPolicy admission it replaces,
<a href="/docs/concepts/security/pod-security-admission/">Pod Security</a>
admission can be easily combined with admission webhooks and external services.</p><p>Pod Security admission <code>restricted</code> policy, the most restrictive policy of the
<a href="/docs/concepts/security/pod-security-standards/">Pod Security Standards</a> set,
<a href="/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces">can operate in several modes</a>,
<code>warn</code>, <code>audit</code> or <code>enforce</code> to gradually apply the most appropriate
<a href="/docs/tasks/configure-pod-container/security-context/">security context</a>
according to security best practices. Nevertheless, pods'
<a href="/docs/tasks/configure-pod-container/security-context/">security context</a>
should be separately investigated to limit the privileges and access pods may
have on top of the predefined security standards, for specific use cases.</p><p>For a hands-on tutorial on <a href="/docs/concepts/security/pod-security-admission/">Pod Security</a>,
see the blog post
<a href="/blog/2021/12/09/pod-security-admission-beta/">Kubernetes 1.23: Pod Security Graduates to Beta</a>.</p><p><a href="/docs/concepts/configuration/manage-resources-containers/">Memory and CPU limits</a>
should be set in order to restrict the memory and CPU resources a pod can
consume on a node, and therefore prevent potential DoS attacks from malicious or
breached workloads. Such policy can be enforced by an admission controller.
Please note that CPU limits will throttle usage and thus can have unintended
effects on auto-scaling features or efficiency i.e. running the process in best
effort with the CPU resource available.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Memory limit superior to request can expose the whole node to OOM issues.</div><h3 id="enabling-seccomp">Enabling Seccomp</h3><p>Seccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12.
It can be used to sandbox the privileges of a process, restricting the calls it is able to make
from userspace into the kernel. Kubernetes lets you automatically apply seccomp profiles loaded onto
a node to your Pods and containers.</p><p>Seccomp can improve the security of your workloads by reducing the Linux kernel syscall attack
surface available inside containers. The seccomp filter mode leverages BPF to create an allow or
deny list of specific syscalls, named profiles.</p><p>Since Kubernetes 1.27, you can enable the use of <code>RuntimeDefault</code> as the default seccomp profile
for all workloads. A <a href="/docs/tutorials/security/seccomp/">security tutorial</a> is available on this
topic. In addition, the
<a href="https://github.com/kubernetes-sigs/security-profiles-operator">Kubernetes Security Profiles Operator</a>
is a project that facilitates the management and use of seccomp in clusters.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Seccomp is only available on Linux nodes.</div><h3 id="enabling-apparmor-or-selinux">Enabling AppArmor or SELinux</h3><h4 id="apparmor">AppArmor</h4><p><a href="/docs/tutorials/security/apparmor/">AppArmor</a> is a Linux kernel security module that can
provide an easy way to implement Mandatory Access Control (MAC) and better
auditing through system logs. A default AppArmor profile is enforced on nodes that support it, or a custom profile can be configured.
Like seccomp, AppArmor is also configured
through profiles, where each profile is either running in enforcing mode, which
blocks access to disallowed resources or complain mode, which only reports
violations. AppArmor profiles are enforced on a per-container basis, with an
annotation, allowing for processes to gain just the right privileges.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>AppArmor is only available on Linux nodes, and enabled in
<a href="https://gitlab.com/apparmor/apparmor/-/wikis/home#distributions-and-ports">some Linux distributions</a>.</div><h4 id="selinux">SELinux</h4><p><a href="https://github.com/SELinuxProject/selinux-notebook/blob/main/src/selinux_overview.md">SELinux</a> is also a
Linux kernel security module that can provide a mechanism for supporting access
control security policies, including Mandatory Access Controls (MAC). SELinux
labels can be assigned to containers or pods
<a href="/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container">via their <code>securityContext</code> section</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>SELinux is only available on Linux nodes, and enabled in
<a href="https://en.wikipedia.org/wiki/Security-Enhanced_Linux#Implementations">some Linux distributions</a>.</div><h2 id="logs-and-auditing">Logs and auditing</h2><ul><li> Audit logs, if enabled, are protected from general access.</li></ul><h2 id="pod-placement">Pod placement</h2><ul><li> Pod placement is done in accordance with the tiers of sensitivity of the
application.</li><li> Sensitive applications are running isolated on nodes or with specific
sandboxed runtimes.</li></ul><p>Pods that are on different tiers of sensitivity, for example, an application pod
and the Kubernetes API server, should be deployed onto separate nodes. The
purpose of node isolation is to prevent an application container breakout to
directly providing access to applications with higher level of sensitivity to easily
pivot within the cluster. This separation should be enforced to prevent pods
accidentally being deployed onto the same node. This could be enforced with the
following features:</p><dl><dt><a href="/docs/concepts/scheduling-eviction/assign-pod-node/">Node Selectors</a></dt><dd>Key-value pairs, as part of the pod specification, that specify which nodes to
deploy onto. These can be enforced at the namespace and cluster level with the
<a href="/docs/reference/access-authn-authz/admission-controllers/#podnodeselector">PodNodeSelector</a>
admission controller.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#podtolerationrestriction">PodTolerationRestriction</a></dt><dd>An admission controller that allows administrators to restrict permitted
<a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerations</a> within a
namespace. Pods within a namespace may only utilize the tolerations specified on
the namespace object annotation keys that provide a set of default and allowed
tolerations.</dd><dt><a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a></dt><dd>RuntimeClass is a feature for selecting the container runtime configuration.
The container runtime configuration is used to run a Pod's containers and can
provide more or less isolation from the host at the cost of performance
overhead.</dd></dl><h2 id="secrets">Secrets</h2><ul><li> ConfigMaps are not used to hold confidential data.</li><li> Encryption at rest is configured for the Secret API.</li><li> If appropriate, a mechanism to inject secrets stored in third-party storage
is deployed and available.</li><li> Service account tokens are not mounted in pods that don't require them.</li><li> <a href="/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume">Bound service account token volume</a>
is in-use instead of non-expiring tokens.</li></ul><p>Secrets required for pods should be stored within Kubernetes Secrets as opposed
to alternatives such as ConfigMap. Secret resources stored within etcd should
be <a href="/docs/tasks/administer-cluster/encrypt-data/">encrypted at rest</a>.</p><p>Pods needing secrets should have these automatically mounted through volumes,
preferably stored in memory like with the <a href="/docs/concepts/storage/volumes/#emptydir"><code>emptyDir.medium</code> option</a>.
Mechanism can be used to also inject secrets from third-party storages as
volume, like the <a href="https://secrets-store-csi-driver.sigs.k8s.io/">Secrets Store CSI Driver</a>.
This should be done preferentially as compared to providing the pods service
account RBAC access to secrets. This would allow adding secrets into the pod as
environment variables or files. Please note that the environment variable method
might be more prone to leakage due to crash dumps in logs and the
non-confidential nature of environment variable in Linux, as opposed to the
permission mechanism on files.</p><p>Service account tokens should not be mounted into pods that do not require them. This can be configured by setting
<a href="/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server"><code>automountServiceAccountToken</code></a>
to <code>false</code> either within the service account to apply throughout the namespace
or specifically for a pod. For Kubernetes v1.22 and above, use
<a href="/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume">Bound Service Accounts</a>
for time-bound service account credentials.</p><h2 id="images">Images</h2><ul><li> Minimize unnecessary content in container images.</li><li> Container images are configured to be run as unprivileged user.</li><li> References to container images are made by sha256 digests (rather than
tags) or the provenance of the image is validated by verifying the image's
digital signature at deploy time <a href="/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures-with-admission-controller">via admission control</a>.</li><li> Container images are regularly scanned during creation and in deployment, and
known vulnerable software is patched.</li></ul><p>Container image should contain the bare minimum to run the program they
package. Preferably, only the program and its dependencies, building the image
from the minimal possible base. In particular, image used in production should not
contain shells or debugging utilities, as an
<a href="/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container">ephemeral debug container</a>
can be used for troubleshooting.</p><p>Build images to directly start with an unprivileged user by using the
<a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user"><code>USER</code> instruction in Dockerfile</a>.
The <a href="/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod">Security Context</a>
allows a container image to be started with a specific user and group with
<code>runAsUser</code> and <code>runAsGroup</code>, even if not specified in the image manifest.
However, the file permissions in the image layers might make it impossible to just
start the process with a new unprivileged user without image modification.</p><p>Avoid using image tags to reference an image, especially the <code>latest</code> tag, the
image behind a tag can be easily modified in a registry. Prefer using the
complete <code>sha256</code> digest which is unique to the image manifest. This policy can be
enforced via an <a href="/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook">ImagePolicyWebhook</a>.
Image signatures can also be automatically <a href="/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures-with-admission-controller">verified with an admission controller</a>
at deploy time to validate their authenticity and integrity.</p><p>Scanning a container image can prevent critical vulnerabilities from being
deployed to the cluster alongside the container image. Image scanning should be
completed before deploying a container image to a cluster and is usually done
as part of the deployment process in a CI/CD pipeline. The purpose of an image
scan is to obtain information about possible vulnerabilities and their
prevention in the container image, such as a
<a href="https://www.first.org/cvss/">Common Vulnerability Scoring System (CVSS)</a>
score. If the result of the image scans is combined with the pipeline
compliance rules, only properly patched container images will end up in
Production.</p><h2 id="admission-controllers">Admission controllers</h2><ul><li> An appropriate selection of admission controllers is enabled.</li><li> A pod security policy is enforced by the Pod Security Admission or/and a
webhook admission controller.</li><li> The admission chain plugins and webhooks are securely configured.</li></ul><p>Admission controllers can help improve the security of the cluster. However,
they can present risks themselves as they extend the API server and
<a href="/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/">should be properly secured</a>.</p><p>The following lists present a number of admission controllers that could be
considered to enhance the security posture of your cluster and application. It
includes controllers that may be referenced in other parts of this document.</p><p>This first group of admission controllers includes plugins
<a href="/docs/reference/access-authn-authz/admission-controllers/#which-plugins-are-enabled-by-default">enabled by default</a>,
consider to leave them enabled unless you know what you are doing:</p><dl><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#certificateapproval"><code>CertificateApproval</code></a></dt><dd>Performs additional authorization checks to ensure the approving user has
permission to approve certificate request.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#certificatesigning"><code>CertificateSigning</code></a></dt><dd>Performs additional authorization checks to ensure the signing user has
permission to sign certificate requests.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#certificatesubjectrestriction"><code>CertificateSubjectRestriction</code></a></dt><dd>Rejects any certificate request that specifies a 'group' (or 'organization
attribute') of <code>system:masters</code>.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#limitranger"><code>LimitRanger</code></a></dt><dd>Enforces the LimitRange API constraints.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook"><code>MutatingAdmissionWebhook</code></a></dt><dd>Allows the use of custom controllers through webhooks, these controllers may
mutate requests that they review.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#podsecurity"><code>PodSecurity</code></a></dt><dd>Replacement for Pod Security Policy, restricts security contexts of deployed
Pods.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#resourcequota"><code>ResourceQuota</code></a></dt><dd>Enforces resource quotas to prevent over-usage of resources.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook"><code>ValidatingAdmissionWebhook</code></a></dt><dd>Allows the use of custom controllers through webhooks, these controllers do
not mutate requests that it reviews.</dd></dl><p>The second group includes plugins that are not enabled by default but are in general
availability state and are recommended to improve your security posture:</p><dl><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips"><code>DenyServiceExternalIPs</code></a></dt><dd>Rejects all net-new usage of the <code>Service.spec.externalIPs</code> field. This is a mitigation for
<a href="https://github.com/kubernetes/kubernetes/issues/97076">CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs</a>.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#noderestriction"><code>NodeRestriction</code></a></dt><dd>Restricts kubelet's permissions to only modify the pods API resources they own
or the node API resource that represent themselves. It also prevents kubelet
from using the <code>node-restriction.kubernetes.io/</code> annotation, which can be used
by an attacker with access to the kubelet's credentials to influence pod
placement to the controlled node.</dd></dl><p>The third group includes plugins that are not enabled by default but could be
considered for certain use cases:</p><dl><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages"><code>AlwaysPullImages</code></a></dt><dd>Enforces the usage of the latest version of a tagged image and ensures that the deployer
has permissions to use the image.</dd><dt><a href="/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook"><code>ImagePolicyWebhook</code></a></dt><dd>Allows enforcing additional controls for images through webhooks.</dd></dl><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/reference/access-authn-authz/authorization/#privilege-escalation-via-pod-creation">Privilege escalation via Pod creation</a>
warns you about a specific access control risk; check how you're managing that
threat.<ul><li>If you use Kubernetes RBAC, read
<a href="/docs/concepts/security/rbac-good-practices/">RBAC Good Practices</a> for
further information on authorization.</li></ul></li><li><a href="/docs/tasks/administer-cluster/securing-a-cluster/">Securing a Cluster</a> for
information on protecting a cluster from accidental or malicious access.</li><li><a href="/docs/concepts/security/multi-tenancy/">Cluster Multi-tenancy guide</a> for
configuration options recommendations and best practices on multi-tenancy.</li><li><a href="/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/#building-secure-container-images">Blog post "A Closer Look at NSA/CISA Kubernetes Hardening Guidance"</a>
for complementary resource on hardening Kubernetes clusters.</li></ul></div></div><div><div class="td-content"><h1>Application Security Checklist</h1><div class="lead">Baseline guidelines around ensuring application security on Kubernetes, aimed at application developers</div><p>This checklist aims to provide basic guidelines on securing applications
running in Kubernetes from a developer's perspective.
This list is not meant to be exhaustive and is intended to evolve over time.</p><p>On how to read and use this document:</p><ul><li>The order of topics does not reflect an order of priority.</li><li>Some checklist items are detailed in the paragraph below the list of each section.</li><li>This checklist assumes that a <code>developer</code> is a Kubernetes cluster user who
interacts with namespaced scope objects.</li></ul><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Checklists are <strong>not</strong> sufficient for attaining a good security posture on their own.
A good security posture requires constant attention and improvement, but a checklist
can be the first step on the never-ending journey towards security preparedness.
Some recommendations in this checklist may be too restrictive or too lax for
your specific security needs. Since Kubernetes security is not "one size fits all",
each category of checklist items should be evaluated on its merits.</div><h2 id="base-security-hardening">Base security hardening</h2><p>The following checklist provides base security hardening recommendations that
would apply to most applications deploying to Kubernetes.</p><h3 id="application-design">Application design</h3><ul><li> Follow the right
<a href="https://www.cncf.io/wp-content/uploads/2022/06/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf">security principles</a>
when designing applications.</li><li> Application configured with appropriate <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." href="/docs/concepts/workloads/pods/pod-qos/" target="_blank">QoS class</a>
through resource request and limits.<ul><li> Memory limit is set for the workloads with a limit equal to or greater than the request.</li><li> CPU limit might be set on sensitive workloads.</li></ul></li></ul><h3 id="service-account">Service account</h3><ul><li> Avoid using the <code>default</code> ServiceAccount. Instead, create ServiceAccounts for
each workload or microservice.</li><li> <code>automountServiceAccountToken</code> should be set to <code>false</code> unless the pod
specifically requires access to the Kubernetes API to operate.</li></ul><h3 id="security-context-pod">Pod-level <code>securityContext</code> recommendations</h3><ul><li> Set <code>runAsNonRoot: true</code>.</li><li> Configure the container to execute as a less privileged user
(for example, using <code>runAsUser</code> and <code>runAsGroup</code>), and configure appropriate
permissions on files or directories inside the container image.</li><li> Optionally add a supplementary group with <code>fsGroup</code> to access persistent volumes.</li><li> The application deploys into a namespace that enforces an appropriate
<a href="/docs/concepts/security/pod-security-standards/">Pod security standard</a>.
If you cannot control this enforcement for the cluster(s) where the application is
deployed, take this into account either through documentation or additional defense in depth.</li></ul><h3 id="security-context-container">Container-level <code>securityContext</code> recommendations</h3><ul><li> Disable privilege escalations using <code>allowPrivilegeEscalation: false</code>.</li><li> Configure the root filesystem to be read-only with <code>readOnlyRootFilesystem: true</code>.</li><li> Avoid running privileged containers (set <code>privileged: false</code>).</li><li> Drop all capabilities from the containers and add back only specific ones
that are needed for operation of the container.</li></ul><h3 id="rbac">Role Based Access Control (RBAC)</h3><ul><li> Permissions such as <strong>create</strong>, <strong>patch</strong>, <strong>update</strong> and <strong>delete</strong>
should be only granted if necessary.</li><li> Avoid creating RBAC permissions to create or update roles which can lead to
<a href="/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping">privilege escalation</a>.</li><li> Review bindings for the <code>system:unauthenticated</code> group and remove them where
possible, as this gives access to anyone who can contact the API server at a network level.</li></ul><p>The <strong>create</strong>, <strong>update</strong> and <strong>delete</strong> verbs should be permitted judiciously.
The <strong>patch</strong> verb if allowed on a Namespace can
<a href="/docs/concepts/security/rbac-good-practices/#namespace-modification">allow users to update labels on the namespace or deployments</a>
which can increase the attack surface.</p><p>For sensitive workloads, consider providing a recommended ValidatingAdmissionPolicy
that further restricts the permitted write actions.</p><h3 id="image-security">Image security</h3><ul><li> Using an image scanning tool to scan an image before deploying containers in the Kubernetes cluster.</li><li> Use container signing to validate the container image signature before deploying to the Kubernetes cluster.</li></ul><h3 id="network-policies">Network policies</h3><ul><li> Configure <a href="/docs/concepts/services-networking/network-policies/">NetworkPolicies</a>
to only allow expected ingress and egress traffic from the pods.</li></ul><p>Make sure that your cluster provides and enforces NetworkPolicy.
If you are writing an application that users will deploy to different clusters,
consider whether you can assume that NetworkPolicy is available and enforced.</p><h2 id="advanced">Advanced security hardening</h2><p>This section of this guide covers some advanced security hardening points
which might be valuable based on different Kubernetes environment setup.</p><h3 id="linux-container-security">Linux container security</h3><p>Configure <a class="glossary-tooltip" title="The securityContext field defines privilege and access control settings for a Pod or container." href="/docs/tasks/configure-pod-container/security-context/" target="_blank">Security Context</a>
for the pod-container.</p><ul><li> <a href="/docs/tasks/configure-pod-container/security-context/#set-the-seccomp-profile-for-a-container">Set the Seccomp Profile for a Container</a>.</li><li> <a href="/docs/tutorials/security/apparmor/">Restrict a Container's Access to Resources with AppArmor</a>.</li><li> <a href="/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container">Assign SELinux Labels to a Container</a>.</li></ul><h3 id="runtime-classes">Runtime classes</h3><ul><li> Configure appropriate runtime classes for containers.</li></ul><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Some containers may require a different isolation level from what is provided by
the default runtime of the cluster. <code>runtimeClassName</code> can be used in a podspec
to define a different runtime class.</p><p>For sensitive workloads consider using kernel emulation tools like
<a href="https://gvisor.dev/docs/">gVisor</a>, or virtualized isolation using a mechanism
such as <a href="https://katacontainers.io/">kata-containers</a>.</p><p>In high trust environments, consider using
<a href="/blog/2023/07/06/confidential-kubernetes/">confidential virtual machines</a>
to improve cluster security even further.</p></div></div><div><div class="td-content"><h1>Policies</h1><div class="lead">Manage security and best-practices with policies.</div><p>Kubernetes policies are configurations that manage other configurations or runtime behaviors. Kubernetes offers various forms of policies, described below:</p><h2 id="apply-policies-using-api-objects">Apply policies using API objects</h2><p>Some API objects act as policies. Here are some examples:</p><ul><li><a href="/docs/concepts/services-networking/network-policies/">NetworkPolicies</a> can be used to restrict ingress and egress traffic for a workload.</li><li><a href="/docs/concepts/policy/limit-range/">LimitRanges</a> manage resource allocation constraints across different object kinds.</li><li><a href="/docs/concepts/policy/resource-quotas/">ResourceQuotas</a> limit resource consumption for a <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>.</li></ul><h2 id="apply-policies-using-admission-controllers">Apply policies using admission controllers</h2><p>An <a class="glossary-tooltip" title="A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object." href="/docs/reference/access-authn-authz/admission-controllers/" target="_blank">admission controller</a>
runs in the API server
and can validate or mutate API requests. Some admission controllers act to apply policies.
For example, the <a href="/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages">AlwaysPullImages</a> admission controller modifies a new Pod to set the image pull policy to <code>Always</code>.</p><p>Kubernetes has several built-in admission controllers that are configurable via the API server <code>--enable-admission-plugins</code> flag.</p><p>Details on admission controllers, with the complete list of available admission controllers, are documented in a dedicated section:</p><ul><li><a href="/docs/reference/access-authn-authz/admission-controllers/">Admission Controllers</a></li></ul><h2 id="apply-policies-using-validatingadmissionpolicy">Apply policies using ValidatingAdmissionPolicy</h2><p>Validating admission policies allow configurable validation checks to be executed in the API server using the Common Expression Language (CEL). For example, a <code>ValidatingAdmissionPolicy</code> can be used to disallow use of the <code>latest</code> image tag.</p><p>A <code>ValidatingAdmissionPolicy</code> operates on an API request and can be used to block, audit, and warn users about non-compliant configurations.</p><p>Details on the <code>ValidatingAdmissionPolicy</code> API, with examples, are documented in a dedicated section:</p><ul><li><a href="/docs/reference/access-authn-authz/validating-admission-policy/">Validating Admission Policy</a></li></ul><h2 id="apply-policies-using-dynamic-admission-control">Apply policies using dynamic admission control</h2><p>Dynamic admission controllers (or admission webhooks) run outside the API server as separate applications that register to receive webhooks requests to perform validation or mutation of API requests.</p><p>Dynamic admission controllers can be used to apply policies on API requests and trigger other policy-based workflows. A dynamic admission controller can perform complex checks including those that require retrieval of other cluster resources and external data. For example, an image verification check can lookup data from OCI registries to validate the container image signatures and attestations.</p><p>Details on dynamic admission control are documented in a dedicated section:</p><ul><li><a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">Dynamic Admission Control</a></li></ul><h3 id="implementations-admission-control">Implementations</h3><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Dynamic Admission Controllers that act as flexible policy engines are being developed in the Kubernetes ecosystem, such as:</p><ul><li><a href="https://github.com/kubewarden">Kubewarden</a></li><li><a href="https://kyverno.io">Kyverno</a></li><li><a href="https://github.com/open-policy-agent/gatekeeper">OPA Gatekeeper</a></li><li><a href="https://polaris.docs.fairwinds.com/admission-controller/">Polaris</a></li></ul><h2 id="apply-policies-using-kubelet-configurations">Apply policies using Kubelet configurations</h2><p>Kubernetes allows configuring the Kubelet on each worker node. Some Kubelet configurations act as policies:</p><ul><li><a href="/docs/concepts/policy/pid-limiting/">Process ID limits and reservations</a> are used to limit and reserve allocatable PIDs.</li><li><a href="/docs/concepts/policy/node-resource-managers/">Node Resource Managers</a> can manage compute, memory, and device resources for latency-critical and high-throughput workloads.</li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Limit Ranges</h1><p>By default, containers run with unbounded
<a href="/docs/concepts/configuration/manage-resources-containers/">compute resources</a> on a Kubernetes cluster.
Using Kubernetes <a href="/docs/concepts/policy/resource-quotas/">resource quotas</a>,
administrators (also termed <em>cluster operators</em>) can restrict consumption and creation
of cluster resources (such as CPU time, memory, and persistent storage) within a specified
<a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>.
Within a namespace, a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a> can consume as much CPU and memory as is allowed by the ResourceQuotas that apply to that namespace.
As a cluster operator, or as a namespace-level administrator, you might also be concerned
about making sure that a single object cannot monopolize all available resources within a namespace.</p><p>A LimitRange is a policy to constrain the resource allocations (limits and requests) that you can specify for
each applicable object kind (such as Pod or <a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank">PersistentVolumeClaim</a>) in a namespace.</p><p>A <em>LimitRange</em> provides constraints that can:</p><ul><li>Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.</li><li>Enforce minimum and maximum storage request per
<a class="glossary-tooltip" title="Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container." href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank">PersistentVolumeClaim</a> in a namespace.</li><li>Enforce a ratio between request and limit for a resource in a namespace.</li><li>Set default request/limit for compute resources in a namespace and automatically
inject them to Containers at runtime.</li></ul><p>Kubernetes constrains resource allocations to Pods in a particular namespace
whenever there is at least one LimitRange object in that namespace.</p><p>The name of a LimitRange object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><h2 id="constraints-on-resource-limits-and-requests">Constraints on resource limits and requests</h2><ul><li>The administrator creates a LimitRange in a namespace.</li><li>Users create (or try to create) objects in that namespace, such as Pods or
PersistentVolumeClaims.</li><li>First, the LimitRange admission controller applies default request and limit values
for all Pods (and their containers) that do not set compute resource requirements.</li><li>Second, the LimitRange tracks usage to ensure it does not exceed resource minimum,
maximum and ratio defined in any LimitRange present in the namespace.</li><li>If you attempt to create or update an object (Pod or PersistentVolumeClaim) that violates
a LimitRange constraint, your request to the API server will fail with anHTTP status
code <code>403 Forbidden</code> and a message explaining the constraint that has been violated.</li><li>If you add a LimitRange in a namespace that applies to compute-related resources
such as <code>cpu</code> and <code>memory</code>, you must specify requests or limits for those values.
Otherwise, the system may reject Pod creation.</li><li>LimitRange validations occur only at Pod admission stage, not on running Pods.
If you add or modify a LimitRange, the Pods that already exist in that namespace
continue unchanged.</li><li>If two or more LimitRange objects exist in the namespace, it is not deterministic
which default value will be applied.</li></ul><h2 id="limitrange-and-admission-checks-for-pods">LimitRange and admission checks for Pods</h2><p>A LimitRange does <strong>not</strong> check the consistency of the default values it applies.
This means that a default value for the <em>limit</em> that is set by LimitRange may be
less than the <em>request</em> value specified for the container in the spec that a client
submits to the API server. If that happens, the final Pod will not be schedulable.</p><p>For example, you define a LimitRange with below manifest:<div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The following examples operate within the default namespace of your cluster, as the namespace
parameter is undefined and the LimitRange scope is limited to the namespace level.
This implies that any references or operations within these examples will interact
with elements within the default namespace of your cluster. You can override the
operating namespace by configuring namespace in the <code>metadata.namespace</code> field.</div></p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/concepts/policy/limit-range/problematic-limit-range.yaml"><code>concepts/policy/limit-range/problematic-limit-range.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy concepts/policy/limit-range/problematic-limit-range.yaml to clipboard"></div><div class="includecode" id="concepts-policy-limit-range-problematic-limit-range-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>LimitRange<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cpu-resource-constraint<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>limits</span>:<span>
</span></span></span><span><span><span>  </span>- <span>default</span>:<span> </span><span># this section defines default limits</span><span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span><span><span><span>    </span><span>defaultRequest</span>:<span> </span><span># this section defines default requests</span><span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span><span><span><span>    </span><span>max</span>:<span> </span><span># max and min define the limit range</span><span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span><span><span><span>    </span><span>min</span>:<span>
</span></span></span><span><span><span>      </span><span>cpu</span>:<span> </span>100m<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>Container<span>
</span></span></span></code></pre></div></div></div><p>along with a Pod that declares a CPU resource request of <code>700m</code>, but not a limit:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/concepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml"><code>concepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy concepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml to clipboard"></div><div class="includecode" id="concepts-policy-limit-range-example-conflict-with-limitrange-cpu-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-conflict-with-limitrange-cpu<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>demo<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.8<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>700m<span>
</span></span></span></code></pre></div></div></div><p>then that Pod will not be scheduled, failing with an error similar to:</p><pre tabindex="0"><code>Pod "example-conflict-with-limitrange-cpu" is invalid: spec.containers[0].resources.requests: Invalid value: "700m": must be less than or equal to cpu limit
</code></pre><p>If you set both <code>request</code> and <code>limit</code>, then that new Pod will be scheduled successfully
even with the same LimitRange in place:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/concepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml"><code>concepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy concepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml to clipboard"></div><div class="includecode" id="concepts-policy-limit-range-example-no-conflict-with-limitrange-cpu-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-no-conflict-with-limitrange-cpu<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>demo<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.8<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>700m<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>700m<span>
</span></span></span></code></pre></div></div></div><h2 id="example-resource-constraints">Example resource constraints</h2><p>Examples of policies that could be created using LimitRange are:</p><ul><li>In a 2 node cluster with a capacity of 8 GiB RAM and 16 cores, constrain Pods in a
namespace to request 100m of CPU with a max limit of 500m for CPU and request 200Mi
for Memory with a max limit of 600Mi for Memory.</li><li>Define default CPU limit and request to 150m and memory default request to 300Mi for
Containers started with no cpu and memory requests in their specs.</li></ul><p>In the case where the total limits of the namespace is less than the sum of the limits
of the Pods/Containers, there may be contention for resources. In this case, the
Containers or Pods will not be created.</p><p>Neither contention nor changes to a LimitRange will affect already created resources.</p><h2 id="what-s-next">What's next</h2><p>For examples on using limits, see:</p><ul><li><a href="/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">how to configure minimum and maximum CPU constraints per namespace</a>.</li><li><a href="/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">how to configure minimum and maximum Memory constraints per namespace</a>.</li><li><a href="/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">how to configure default CPU Requests and Limits per namespace</a>.</li><li><a href="/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">how to configure default Memory Requests and Limits per namespace</a>.</li><li><a href="/docs/tasks/administer-cluster/limit-storage-consumption/#limitrange-to-limit-requests-for-storage">how to configure minimum and maximum Storage consumption per namespace</a>.</li><li>a <a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">detailed example on configuring quota per namespace</a>.</li></ul><p>Refer to the <a href="https://git.k8s.io/design-proposals-archive/resource-management/admission_control_limit_range.md">LimitRanger design document</a>
for context and historical information.</p></div></div><div><div class="td-content"><h1>Resource Quotas</h1><p>When several users or teams share a cluster with a fixed number of nodes,
there is a concern that one team could use more than its fair share of resources.</p><p><em>Resource quotas</em> are a tool for administrators to address this concern.</p><p>A resource quota, defined by a ResourceQuota object, provides constraints that limit
aggregate resource consumption per <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a>. A ResourceQuota can also
limit the <a href="#quota-on-object-count">quantity of objects that can be created in a namespace</a> by API kind, as well as the total
amount of <a class="glossary-tooltip" title="A defined amount of infrastructure available for consumption (CPU, memory, etc)." href="/docs/reference/glossary/?all=true#term-infrastructure-resource" target="_blank">infrastructure resources</a> that may be consumed by
API objects found in that namespace.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Neither contention nor changes to quota will affect already created resources.</div><h2 id="how-kubernetes-resourcequotas-work">How Kubernetes ResourceQuotas work</h2><p>ResourceQuotas work like this:</p><ul><li><p>Different teams work in different namespaces. This separation can be enforced with
<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a> or any other <a href="/docs/reference/access-authn-authz/authorization/">authorization</a>
mechanism.</p></li><li><p>A cluster administrator creates at least one ResourceQuota for each namespace.</p><ul><li>To make sure the enforcement stays enforced, the cluster administrator should also restrict access to delete or update
that ResourceQuota; for example, by defining a <a href="/docs/reference/access-authn-authz/validating-admission-policy/">ValidatingAdmissionPolicy</a>.</li></ul></li><li><p>Users create resources (pods, services, etc.) in the namespace, and the quota system
tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.</p><p>You can apply a <a href="#quota-scopes">scope</a> to a ResourceQuota to limit where it applies,</p></li><li><p>If creating or updating a resource violates a quota constraint, the control plane rejects that request with HTTP
status code <code>403 Forbidden</code>. The error includes a message explaining the constraint that would have been violated.</p></li><li><p>If quotas are enabled in a namespace for <a class="glossary-tooltip" title="A defined amount of infrastructure available for consumption (CPU, memory, etc)." href="/docs/reference/glossary/?all=true#term-infrastructure-resource" target="_blank">resource</a>
such as <code>cpu</code> and <code>memory</code>, users must specify requests or limits for those values when they define a Pod; otherwise,
the quota system may reject pod creation.</p><p>The resource quota <a href="/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">walkthrough</a>
shows an example of how to avoid this problem.</p></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li>You can define a <a href="/docs/concepts/policy/limit-range/">LimitRange</a>
to force defaults on pods that make no compute resource requirements (so that users don't have to remember to do that).</li></ul></div><p>You often do not create Pods directly; for example, you more usually create a <a href="/docs/concepts/workloads/controllers/">workload management</a>
object such as a <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a>. If you create a Deployment that tries to use more
resources than are available, the creation of the Deployment (or other workload management object) <strong>succeeds</strong>, but
the Deployment may not be able to get all of the Pods it manages to exist. In that case you can check the status of
the Deployment, for example with <code>kubectl describe</code>, to see what has happened.</p><ul><li>For <code>cpu</code> and <code>memory</code> resources, ResourceQuotas enforce that <strong>every</strong>
(new) pod in that namespace sets a limit for that resource.
If you enforce a resource quota in a namespace for either <code>cpu</code> or <code>memory</code>,
you and other clients, <strong>must</strong> specify either <code>requests</code> or <code>limits</code> for that resource,
for every new Pod you submit. If you don't, the control plane may reject admission
for that Pod.</li><li>For other resources: ResourceQuota works and will ignore pods in the namespace without
setting a limit or request for that resource. It means that you can create a new pod
without limit/request for ephemeral storage if the resource quota limits the ephemeral
storage of this namespace.</li></ul><p>You can use a <a href="/docs/concepts/policy/limit-range/">LimitRange</a> to automatically set
a default request for these resources.</p><p>The name of a ResourceQuota object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>.</p><p>Examples of policies that could be created using namespaces and quotas are:</p><ul><li>In a cluster with a capacity of 32 GiB RAM, and 16 cores, let team A use 20 GiB and 10 cores,
let B use 10GiB and 4 cores, and hold 2GiB and 2 cores in reserve for future allocation.</li><li>Limit the "testing" namespace to using 1 core and 1GiB RAM. Let the "production" namespace
use any amount.</li></ul><p>In the case where the total capacity of the cluster is less than the sum of the quotas of the namespaces,
there may be contention for resources. This is handled on a first-come-first-served basis.</p><h2 id="enabling-resource-quota">Enabling Resource Quota</h2><p>ResourceQuota support is enabled by default for many Kubernetes distributions. It is
enabled when the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a>
<code>--enable-admission-plugins=</code> flag has <code>ResourceQuota</code> as
one of its arguments.</p><p>A resource quota is enforced in a particular namespace when there is a
ResourceQuota in that namespace.</p><h2 id="types-of-resource-quota">Types of resource quota</h2><p>The ResourceQuota mechanism lets you enforce different kinds of limits. This
section describes the types of limit that you can enforce.</p><h3 id="compute-resource-quota">Quota for infrastructure resources</h3><p>You can limit the total sum of
<a href="/docs/concepts/configuration/manage-resources-containers/">compute resources</a>
that can be requested in a given namespace.</p><p>The following resource types are supported:</p><table><thead><tr><th>Resource Name</th><th>Description</th></tr></thead><tbody><tr><td><code>limits.cpu</code></td><td>Across all pods in a non-terminal state, the sum of CPU limits cannot exceed this value.</td></tr><tr><td><code>limits.memory</code></td><td>Across all pods in a non-terminal state, the sum of memory limits cannot exceed this value.</td></tr><tr><td><code>requests.cpu</code></td><td>Across all pods in a non-terminal state, the sum of CPU requests cannot exceed this value.</td></tr><tr><td><code>requests.memory</code></td><td>Across all pods in a non-terminal state, the sum of memory requests cannot exceed this value.</td></tr><tr><td><code>hugepages-&lt;size&gt;</code></td><td>Across all pods in a non-terminal state, the number of huge page requests of the specified size cannot exceed this value.</td></tr><tr><td><code>cpu</code></td><td>Same as <code>requests.cpu</code></td></tr><tr><td><code>memory</code></td><td>Same as <code>requests.memory</code></td></tr></tbody></table><h3 id="quota-for-extended-resources">Quota for extended resources</h3><p>In addition to the resources mentioned above, in release 1.10, quota support for
<a href="/docs/concepts/configuration/manage-resources-containers/#extended-resources">extended resources</a> is added.</p><p>As overcommit is not allowed for extended resources, it makes no sense to specify both <code>requests</code>
and <code>limits</code> for the same extended resource in a quota. So for extended resources, only quota items
with prefix <code>requests.</code> are allowed.</p><p>Take the GPU resource as an example, if the resource name is <code>nvidia.com/gpu</code>, and you want to
limit the total number of GPUs requested in a namespace to 4, you can define a quota as follows:</p><ul><li><code>requests.nvidia.com/gpu: 4</code></li></ul><p>See <a href="#viewing-and-setting-quotas">Viewing and Setting Quotas</a> for more details.</p><h3 id="quota-for-storage">Quota for storage</h3><p>You can limit the total sum of <a href="/docs/concepts/storage/persistent-volumes/">storage</a> for volumes
that can be requested in a given namespace.</p><p>In addition, you can limit consumption of storage resources based on associated
<a href="/docs/concepts/storage/storage-classes/">StorageClass</a>.</p><table><thead><tr><th>Resource Name</th><th>Description</th></tr></thead><tbody><tr><td><code>requests.storage</code></td><td>Across all persistent volume claims, the sum of storage requests cannot exceed this value.</td></tr><tr><td><code>persistentvolumeclaims</code></td><td>The total number of <a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a> that can exist in the namespace.</td></tr><tr><td><code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/requests.storage</code></td><td>Across all persistent volume claims associated with the <code>&lt;storage-class-name&gt;</code>, the sum of storage requests cannot exceed this value.</td></tr><tr><td><code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/persistentvolumeclaims</code></td><td>Across all persistent volume claims associated with the <code>&lt;storage-class-name&gt;</code>, the total number of <a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">persistent volume claims</a> that can exist in the namespace.</td></tr></tbody></table><p>For example, if you want to quota storage with <code>gold</code> StorageClass separate from
a <code>bronze</code> StorageClass, you can define a quota as follows:</p><ul><li><code>gold.storageclass.storage.k8s.io/requests.storage: 500Gi</code></li><li><code>bronze.storageclass.storage.k8s.io/requests.storage: 100Gi</code></li></ul><h4 id="quota-for-local-ephemeral-storage">Quota for local ephemeral storage</h4><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.8 [alpha]</code></div><table><thead><tr><th>Resource Name</th><th>Description</th></tr></thead><tbody><tr><td><code>requests.ephemeral-storage</code></td><td>Across all pods in the namespace, the sum of local ephemeral storage requests cannot exceed this value.</td></tr><tr><td><code>limits.ephemeral-storage</code></td><td>Across all pods in the namespace, the sum of local ephemeral storage limits cannot exceed this value.</td></tr><tr><td><code>ephemeral-storage</code></td><td>Same as <code>requests.ephemeral-storage</code>.</td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>When using a CRI container runtime, container logs will count against the ephemeral storage quota.
This can result in the unexpected eviction of pods that have exhausted their storage quotas.</p><p>Refer to <a href="/docs/concepts/cluster-administration/logging/">Logging Architecture</a> for details.</p></div><h3 id="quota-on-object-count">Quota on object count</h3><p>You can set quota for <em>the total number of one particular <a class="glossary-tooltip" title="A Kubernetes entity, representing an endpoint on the Kubernetes API server." href="/docs/reference/using-api/api-concepts/#standard-api-terminology" target="_blank">resource</a> kind</em> in the Kubernetes API,
using the following syntax:</p><ul><li><code>count/&lt;resource&gt;.&lt;group&gt;</code> for resources from non-core API groups</li><li><code>count/&lt;resource&gt;</code> for resources from the core API group</li></ul><p>For example, the PodTemplate API is in the core API group and so if you want to limit the number of
PodTemplate objects in a namespace, you use <code>count/podtemplates</code>.</p><p>These types of quotas are useful to protect against exhaustion of control plane storage. For example, you may
want to limit the number of Secrets in a server given their large size. Too many Secrets in a cluster can
actually prevent servers and controllers from starting. You can set a quota for Jobs to protect against
a poorly configured CronJob. CronJobs that create too many Jobs in a namespace can lead to a denial of service.</p><p>If you define a quota this way, it applies to Kubernetes' APIs that are part of the API server, and
to any custom resources backed by a CustomResourceDefinition.
For example, to create a quota on a <code>widgets</code> custom resource in the <code>example.com</code> API group,
use <code>count/widgets.example.com</code>.
If you use <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">API aggregation</a> to
add additional, custom APIs that are not defined as CustomResourceDefinitions, the core Kubernetes
control plane does not enforce quota for the aggregated API. The extension API server is expected to
provide quota enforcement if that's appropriate for the custom API.</p><h5 id="resource-quota-object-count-generic">Generic syntax</h5><p>This is a list of common examples of object kinds that you may want to put under object count quota,
listed by the configuration string that you would use.</p><ul><li><code>count/pods</code></li><li><code>count/persistentvolumeclaims</code></li><li><code>count/services</code></li><li><code>count/secrets</code></li><li><code>count/configmaps</code></li><li><code>count/deployments.apps</code></li><li><code>count/replicasets.apps</code></li><li><code>count/statefulsets.apps</code></li><li><code>count/jobs.batch</code></li><li><code>count/cronjobs.batch</code></li></ul><h5 id="resource-quota-object-count-specialized">Specialized syntax</h5><p>There is another syntax only to set the same type of quota, that only works for certain API kinds.
The following types are supported:</p><table><thead><tr><th>Resource Name</th><th>Description</th></tr></thead><tbody><tr><td><code>configmaps</code></td><td>The total number of ConfigMaps that can exist in the namespace.</td></tr><tr><td><code>persistentvolumeclaims</code></td><td>The total number of <a href="/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a> that can exist in the namespace.</td></tr><tr><td><code>pods</code></td><td>The total number of Pods in a non-terminal state that can exist in the namespace. A pod is in a terminal state if <code>.status.phase in (Failed, Succeeded)</code> is true.</td></tr><tr><td><code>replicationcontrollers</code></td><td>The total number of ReplicationControllers that can exist in the namespace.</td></tr><tr><td><code>resourcequotas</code></td><td>The total number of ResourceQuotas that can exist in the namespace.</td></tr><tr><td><code>services</code></td><td>The total number of Services that can exist in the namespace.</td></tr><tr><td><code>services.loadbalancers</code></td><td>The total number of Services of type <code>LoadBalancer</code> that can exist in the namespace.</td></tr><tr><td><code>services.nodeports</code></td><td>The total number of <code>NodePorts</code> allocated to Services of type <code>NodePort</code> or <code>LoadBalancer</code> that can exist in the namespace.</td></tr><tr><td><code>secrets</code></td><td>The total number of Secrets that can exist in the namespace.</td></tr></tbody></table><p>For example, <code>pods</code> quota counts and enforces a maximum on the number of <code>pods</code>
created in a single namespace that are not terminal. You might want to set a <code>pods</code>
quota on a namespace to avoid the case where a user creates many small pods and
exhausts the cluster's supply of Pod IPs.</p><p>You can find more examples on <a href="#viewing-and-setting-quotas">Viewing and Setting Quotas</a>.</p><h2 id="viewing-and-setting-quotas">Viewing and Setting Quotas</h2><p>kubectl supports creating, updating, and viewing quotas:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace myspace
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF &gt; compute-resources.yaml
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: ResourceQuota
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: compute-resources
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  hard:
</span></span></span><span><span><span>    requests.cpu: "1"
</span></span></span><span><span><span>    requests.memory: "1Gi"
</span></span></span><span><span><span>    limits.cpu: "2"
</span></span></span><span><span><span>    limits.memory: "2Gi"
</span></span></span><span><span><span>    requests.nvidia.com/gpu: 4
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f ./compute-resources.yaml --namespace<span>=</span>myspace
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat <span>&lt;&lt;EOF &gt; object-counts.yaml
</span></span></span><span><span><span>apiVersion: v1
</span></span></span><span><span><span>kind: ResourceQuota
</span></span></span><span><span><span>metadata:
</span></span></span><span><span><span>  name: object-counts
</span></span></span><span><span><span>spec:
</span></span></span><span><span><span>  hard:
</span></span></span><span><span><span>    configmaps: "10"
</span></span></span><span><span><span>    persistentvolumeclaims: "4"
</span></span></span><span><span><span>    pods: "4"
</span></span></span><span><span><span>    replicationcontrollers: "20"
</span></span></span><span><span><span>    secrets: "10"
</span></span></span><span><span><span>    services: "10"
</span></span></span><span><span><span>    services.loadbalancers: "2"
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f ./object-counts.yaml --namespace<span>=</span>myspace
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get quota --namespace<span>=</span>myspace
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">NAME                    AGE
compute-resources       30s
object-counts           32s
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe quota compute-resources --namespace<span>=</span>myspace
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Name:                    compute-resources
Namespace:               myspace
Resource                 Used  Hard
--------                 ----  ----
limits.cpu               0     2
limits.memory            0     2Gi
requests.cpu             0     1
requests.memory          0     1Gi
requests.nvidia.com/gpu  0     4
</code></pre><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe quota object-counts --namespace<span>=</span>myspace
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">Name:                   object-counts
Namespace:              myspace
Resource                Used    Hard
--------                ----    ----
configmaps              0       10
persistentvolumeclaims  0       4
pods                    0       4
replicationcontrollers  0       20
secrets                 1       10
services                0       10
services.loadbalancers  0       2
</code></pre><p>kubectl also supports object count quota for all standard namespaced resources
using the syntax <code>count/&lt;resource&gt;.&lt;group&gt;</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create namespace myspace
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create quota <span>test</span> --hard<span>=</span>count/deployments.apps<span>=</span>2,count/replicasets.apps<span>=</span>4,count/pods<span>=</span>3,count/secrets<span>=</span><span>4</span> --namespace<span>=</span>myspace
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create deployment nginx --image<span>=</span>nginx --namespace<span>=</span>myspace --replicas<span>=</span><span>2</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe quota --namespace<span>=</span>myspace
</span></span></code></pre></div><pre tabindex="0"><code>Name:                         test
Namespace:                    myspace
Resource                      Used  Hard
--------                      ----  ----
count/deployments.apps        1     2
count/pods                    2     3
count/replicasets.apps        1     4
count/secrets                 1     4
</code></pre><h2 id="quota-and-cluster-capacity">Quota and Cluster Capacity</h2><p>ResourceQuotas are independent of the cluster capacity. They are
expressed in absolute units. So, if you add nodes to your cluster, this does <em>not</em>
automatically give each namespace the ability to consume more resources.</p><p>Sometimes more complex policies may be desired, such as:</p><ul><li>Proportionally divide total cluster resources among several teams.</li><li>Allow each tenant to grow resource usage as needed, but have a generous
limit to prevent accidental resource exhaustion.</li><li>Detect demand from one namespace, add nodes, and increase quota.</li></ul><p>Such policies could be implemented using <code>ResourceQuotas</code> as building blocks, by
writing a "controller" that watches the quota usage and adjusts the quota
hard limits of each namespace according to other signals.</p><p>Note that resource quota divides up aggregate cluster resources, but it creates no
restrictions around nodes: pods from several namespaces may run on the same node.</p><h2 id="quota-scopes">Quota scopes</h2><p>Each quota can have an associated set of <code>scopes</code>. A quota will only measure usage for a resource if it matches
the intersection of enumerated scopes.</p><p>When a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope.
Resources specified on the quota outside of the allowed set results in a validation error.</p><p>Kubernetes 1.34 supports the following scopes:</p><table><thead><tr><th>Scope</th><th>Description</th></tr></thead><tbody><tr><td><a href="#quota-scope-best-effort"><code>BestEffort</code></a></td><td>Match pods that have best effort quality of service.</td></tr><tr><td><a href="#cross-namespace-pod-affinity-scope"><code>CrossNamespacePodAffinity</code></a></td><td>Match pods that have cross-namespace pod <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">(anti)affinity terms</a>.</td></tr><tr><td><a href="#quota-scope-non-best-effort"><code>NotBestEffort</code></a></td><td>Match pods that do not have best effort quality of service.</td></tr><tr><td><a href="#quota-scope-non-terminating"><code>NotTerminating</code></a></td><td>Match pods where <code>.spec.activeDeadlineSeconds</code>]() is <code>nil</code>]()</td></tr><tr><td><a href="#resource-quota-per-priorityclass"><code>PriorityClass</code></a></td><td>Match pods that references the specified <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">priority class</a>.</td></tr><tr><td><a href="#quota-scope-terminating"><code>Terminating</code></a></td><td>Match pods where <code>.spec.activeDeadlineSeconds</code>]() &gt;= <code>0</code>]()</td></tr><tr><td><a href="#quota-scope-volume-attributes-class"><code>VolumeAttributesClass</code></a></td><td>Match PersistentVolumeClaims that reference the specified <a href="/docs/concepts/storage/volume-attributes-classes/">volume attributes class</a>.</td></tr></tbody></table><p>ResourceQuotas with a scope set can also have a optional <code>scopeSelector</code> field. You define one or more <em>match expressions</em>
that specify an <code>operators</code> and, if relevant, a set of <code>values</code> to match. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span>scopeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>      </span>- <span>scopeName</span>:<span> </span>BestEffort<span> </span><span># Match pods that have best effort quality of service</span><span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>Exists<span> </span><span># optional; "Exists" is implied for BestEffort scope</span><span>
</span></span></span></code></pre></div><p>The <code>scopeSelector</code> supports the following values in the <code>operator</code> field:</p><ul><li><code>In</code></li><li><code>NotIn</code></li><li><code>Exists</code></li><li><code>DoesNotExist</code></li></ul><p>If the <code>operator</code> is <code>In</code> or <code>NotIn</code>, the <code>values</code> field must have at least
one value. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span>scopeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>      </span>- <span>scopeName</span>:<span> </span>PriorityClass<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>        </span><span>values</span>:<span>
</span></span></span><span><span><span>          </span>- middle<span>
</span></span></span></code></pre></div><p>If the <code>operator</code> is <code>Exists</code> or <code>DoesNotExist</code>, the <code>values</code> field must <em>NOT</em> be
specified.</p><h3 id="quota-scope-best-effort">Best effort Pods scope</h3><p>This scope only tracks quota consumed by Pods.
It only matches pods that have the <a href="/docs/concepts/workloads/pods/pod-qos/#besteffort">best effort</a>
<a href="/docs/concepts/workloads/pods/pod-qos/">QoS class</a>.</p><p>The <code>operator</code> for a <code>scopeSelector</code> must be <code>Exists</code>.</p><h3 id="quota-scope-non-best-effort">Not-best-effort Pods scope</h3><p>This scope only tracks quota consumed by Pods.
It only matches pods that have the <a href="/docs/concepts/workloads/pods/pod-qos/#guaranteed">Guaranteed</a>
or <a href="/docs/concepts/workloads/pods/pod-qos/#burstable">Burstable</a>
<a href="/docs/concepts/workloads/pods/pod-qos/">QoS class</a>.</p><p>The <code>operator</code> for a <code>scopeSelector</code> must be <code>Exists</code>.</p><h3 id="quota-scope-non-terminating">Non-terminating Pods scope</h3><p>This scope only tracks quota consumed by Pods that are not terminating. The <code>operator</code> for a <code>scopeSelector</code>
must be <code>Exists</code>.</p><p>A Pod is not terminating if the <code>.spec.activeDeadlineSeconds</code> field is unset.</p><p>You can use a ResourceQuota with this scope to manage the following resources:</p><ul><li><code>count.pods</code></li><li><code>pods</code></li><li><code>cpu</code></li><li><code>memory</code></li><li><code>requests.cpu</code></li><li><code>requests.memory</code></li><li><code>limits.cpu</code></li><li><code>limits.memory</code></li></ul><h3 id="quota-scope-terminating">Terminating Pods scope</h3><p>This scope only tracks quota consumed by Pods that are terminating. The <code>operator</code> for a <code>scopeSelector</code>
must be <code>Exists</code>.</p><p>A Pod is considered as <em>terminating</em> if the <code>.spec.activeDeadlineSeconds</code> field is set to any number.</p><p>You can use a ResourceQuota with this scope to manage the following resources:</p><ul><li><code>count.pods</code></li><li><code>pods</code></li><li><code>cpu</code></li><li><code>memory</code></li><li><code>requests.cpu</code></li><li><code>requests.memory</code></li><li><code>limits.cpu</code></li><li><code>limits.memory</code></li></ul><h3 id="cross-namespace-pod-affinity-scope">Cross-namespace pod affinity scope</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>You can use <code>CrossNamespacePodAffinity</code> <a href="#quota-scopes">quota scope</a> to limit which namespaces are allowed to
have pods with affinity terms that cross namespaces. Specifically, it controls which pods are allowed
to set <code>namespaces</code> or <code>namespaceSelector</code> fields in pod <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">(anti)affinity terms</a>.</p><p>Preventing users from using cross-namespace affinity terms might be desired since a pod
with anti-affinity constraints can block pods from all other namespaces
from getting scheduled in a failure domain.</p><p>Using this scope, you (as a cluster administrator) can prevent certain namespaces - such as <code>foo-ns</code> in the example below -
from having pods that use cross-namespace pod affinity. You configure this creating a ResourceQuota object in
that namespace with <code>CrossNamespacePodAffinity</code> scope and hard limit of 0:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceQuota<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>disable-cross-namespace-affinity<span>
</span></span></span><span><span><span>  </span><span>namespace</span>:<span> </span>foo-ns<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span> </span><span>"0"</span><span>
</span></span></span><span><span><span>  </span><span>scopeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>scopeName</span>:<span> </span>CrossNamespacePodAffinity<span>
</span></span></span><span><span><span>      </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span></code></pre></div><p>If you want to disallow using <code>namespaces</code> and <code>namespaceSelector</code> by default, and
only allow it for specific namespaces, you could configure <code>CrossNamespacePodAffinity</code>
as a limited resource by setting the kube-apiserver flag <code>--admission-control-config-file</code>
to the path of the following configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>AdmissionConfiguration<span>
</span></span></span><span><span><span></span><span>plugins</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span><span>"ResourceQuota"</span><span>
</span></span></span><span><span><span>  </span><span>configuration</span>:<span>
</span></span></span><span><span><span>    </span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>ResourceQuotaConfiguration<span>
</span></span></span><span><span><span>    </span><span>limitedResources</span>:<span>
</span></span></span><span><span><span>    </span>- <span>resource</span>:<span> </span>pods<span>
</span></span></span><span><span><span>      </span><span>matchScopes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>scopeName</span>:<span> </span>CrossNamespacePodAffinity<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span></code></pre></div><p>With the above configuration, pods can use <code>namespaces</code> and <code>namespaceSelector</code> in pod affinity only
if the namespace where they are created have a resource quota object with
<code>CrossNamespacePodAffinity</code> scope and a hard limit greater than or equal to the number of pods using those fields.</p><h3 id="resource-quota-per-priorityclass">PriorityClass scope</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.17 [stable]</code></div><p>A ResourceQuota with a PriorityClass scope only matches Pods that have a particular
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">priority class</a>, and only
if any <code>scopeSelector</code> in the quota spec selects a particular Pod.</p><p>Pods can be created at a specific <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority">priority</a>.
You can control a pod's consumption of system resources based on a pod's priority, by using the <code>scopeSelector</code>
field in the quota spec.</p><p>When quota is scoped for PriorityClass using the <code>scopeSelector</code> field, the ResourceQuota
can only track (and limit) the following resources:</p><ul><li><code>pods</code></li><li><code>cpu</code></li><li><code>memory</code></li><li><code>ephemeral-storage</code></li><li><code>limits.cpu</code></li><li><code>limits.memory</code></li><li><code>limits.ephemeral-storage</code></li><li><code>requests.cpu</code></li><li><code>requests.memory</code></li><li><code>requests.ephemeral-storage</code></li></ul><h4 id="quota-scope-priorityclass-example">Example</h4><p>This example creates a ResourceQuota matches it with pods at specific priorities. The example
works as follows:</p><ul><li>Pods in the cluster have one of the three <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">PriorityClasses</a>, "low", "medium", "high".<ul><li>If you want to try this out, use a testing cluster and set up those three PriorityClasses before you continue.</li></ul></li><li>One quota object is created for each priority.</li></ul><p>Inspect this set of ResourceQuotas:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/policy/quota.yaml"><code>policy/quota.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy policy/quota.yaml to clipboard"></div><div class="includecode" id="policy-quota-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceQuota<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pods-high<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span><span>"1000"</span><span>
</span></span></span><span><span><span>    </span><span>memory</span>:<span> </span><span>"200Gi"</span><span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span> </span><span>"10"</span><span>
</span></span></span><span><span><span>  </span><span>scopeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>      </span><span>scopeName</span>:<span> </span>PriorityClass<span>
</span></span></span><span><span><span>      </span><span>values</span>:<span> </span>[<span>"high"</span>]<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceQuota<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pods-medium<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span><span>"10"</span><span>
</span></span></span><span><span><span>    </span><span>memory</span>:<span> </span><span>"20Gi"</span><span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span> </span><span>"10"</span><span>
</span></span></span><span><span><span>  </span><span>scopeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>      </span><span>scopeName</span>:<span> </span>PriorityClass<span>
</span></span></span><span><span><span>      </span><span>values</span>:<span> </span>[<span>"medium"</span>]<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceQuota<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pods-low<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>hard</span>:<span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span><span>"5"</span><span>
</span></span></span><span><span><span>    </span><span>memory</span>:<span> </span><span>"10Gi"</span><span>
</span></span></span><span><span><span>    </span><span>pods</span>:<span> </span><span>"10"</span><span>
</span></span></span><span><span><span>  </span><span>scopeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>    </span>- <span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>      </span><span>scopeName</span>:<span> </span>PriorityClass<span>
</span></span></span><span><span><span>      </span><span>values</span>:<span> </span>[<span>"low"</span>]<span>
</span></span></span></code></pre></div></div></div><p>Apply the YAML using <code>kubectl create</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/policy/quota.yaml
</span></span></code></pre></div><pre tabindex="0"><code>resourcequota/pods-high created
resourcequota/pods-medium created
resourcequota/pods-low created
</code></pre><p>Verify that <code>Used</code> quota is <code>0</code> using <code>kubectl describe quota</code>.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe quota
</span></span></code></pre></div><pre tabindex="0"><code>Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     1k
memory      0     200Gi
pods        0     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
</code></pre><p>Create a pod with priority "high".</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/policy/high-priority-pod.yaml"><code>policy/high-priority-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy policy/high-priority-pod.yaml to clipboard"></div><div class="includecode" id="policy-high-priority-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>high-priority<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>high-priority<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>ubuntu<span>
</span></span></span><span><span><span>    </span><span>command</span>:<span> </span>[<span>"/bin/sh"</span>]<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[<span>"-c"</span>,<span> </span><span>"while true; do echo hello; sleep 10;done"</span>]<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"10Gi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"500m"</span><span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"10Gi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"500m"</span><span>
</span></span></span><span><span><span>  </span><span>priorityClassName</span>:<span> </span>high<span>
</span></span></span></code></pre></div></div></div><p>To create the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl create -f https://k8s.io/examples/policy/high-priority-pod.yaml
</span></span></code></pre></div><p>Verify that "Used" stats for "high" priority quota, <code>pods-high</code>, has changed and that
the other two quotas are unchanged.</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl describe quota
</span></span></code></pre></div><pre tabindex="0"><code>Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         500m  1k
memory      10Gi  200Gi
pods        1     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
</code></pre><h4 id="limiting-priorityclass-consumption-by-default">Limiting PriorityClass consumption by default</h4><p>It may be desired that pods at a particular priority, such as "cluster-services",
should be allowed in a namespace, if and only if, a matching quota object exists.</p><p>With this mechanism, operators are able to restrict usage of certain high
priority classes to a limited number of namespaces and not every namespace
will be able to consume these priority classes by default.</p><p>To enforce this, <code>kube-apiserver</code> flag <code>--admission-control-config-file</code> should be
used to pass path to the following configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>AdmissionConfiguration<span>
</span></span></span><span><span><span></span><span>plugins</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span><span>"ResourceQuota"</span><span>
</span></span></span><span><span><span>  </span><span>configuration</span>:<span>
</span></span></span><span><span><span>    </span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>ResourceQuotaConfiguration<span>
</span></span></span><span><span><span>    </span><span>limitedResources</span>:<span>
</span></span></span><span><span><span>    </span>- <span>resource</span>:<span> </span>pods<span>
</span></span></span><span><span><span>      </span><span>matchScopes</span>:<span>
</span></span></span><span><span><span>      </span>- <span>scopeName</span>:<span> </span>PriorityClass<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>        </span><span>values</span>:<span> </span>[<span>"cluster-services"</span>]<span>
</span></span></span></code></pre></div><p>Then, create a resource quota object in the <code>kube-system</code> namespace:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/policy/priority-class-resourcequota.yaml"><code>policy/priority-class-resourcequota.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy policy/priority-class-resourcequota.yaml to clipboard"></div><div class="includecode" id="policy-priority-class-resourcequota-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceQuota<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pods-cluster-services<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>scopeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>      </span>- <span>operator </span>:<span> </span>In<span>
</span></span></span><span><span><span>        </span><span>scopeName</span>:<span> </span>PriorityClass<span>
</span></span></span><span><span><span>        </span><span>values</span>:<span> </span>[<span>"cluster-services"</span>]</span></span></code></pre></div></div></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system
</span></span></code></pre></div><pre tabindex="0"><code class="language-none">resourcequota/pods-cluster-services created
</code></pre><p>In this case, a pod creation will be allowed if:</p><ol><li>the Pod's <code>priorityClassName</code> is not specified.</li><li>the Pod's <code>priorityClassName</code> is specified to a value other than <code>cluster-services</code>.</li><li>the Pod's <code>priorityClassName</code> is set to <code>cluster-services</code>, it is to be created
in the <code>kube-system</code> namespace, and it has passed the resource quota check.</li></ol><p>A Pod creation request is rejected if its <code>priorityClassName</code> is set to <code>cluster-services</code>
and it is to be created in a namespace other than <code>kube-system</code>.</p><h3 id="quota-scope-volume-attributes-class">VolumeAttributesClass scope</h3><div class="feature-state-notice feature-stable" title="Feature Gate: VolumeAttributesClass"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>This scope only tracks quota consumed by PersistentVolumeClaims.</p><p>PersistentVolumeClaims can be created with a specific
<a href="/docs/concepts/storage/volume-attributes-classes/">VolumeAttributesClass</a>, and might be modified after creation.
You can control a PVC's consumption of storage resources based on the associated
VolumeAttributesClasses, by using the <code>scopeSelector</code> field in the quota spec.</p><p>The PVC references the associated VolumeAttributesClass by the following fields:</p><ul><li><code>spec.volumeAttributesClassName</code></li><li><code>status.currentVolumeAttributesClassName</code></li><li><code>status.modifyVolumeStatus.targetVolumeAttributesClassName</code></li></ul><p>A relevant ResourceQuota is matched and consumed only if the ResourceQuota has a <code>scopeSelector</code> that selects the PVC.</p><p>When the quota is scoped for the volume attributes class using the <code>scopeSelector</code> field, the quota object is restricted to track only the following resources:</p><ul><li><code>persistentvolumeclaims</code></li><li><code>requests.storage</code></li></ul><p>Read <a href="/docs/tasks/administer-cluster/limit-storage-consumption/">Limit Storage Consumption</a> to learn more about this.</p><h2 id="what-s-next">What's next</h2><ul><li>See a <a href="/docs/tasks/administer-cluster/quota-api-object/">detailed example for how to use resource quota</a>.</li><li>Read the ResourceQuota <a href="/docs/reference/kubernetes-api/policy-resources/resource-quota-v1/">API reference</a></li><li>Learn about <a href="/docs/concepts/policy/limit-range/">LimitRanges</a></li><li>You can read the historical <a href="https://git.k8s.io/design-proposals-archive/resource-management/admission_control_resource_quota.md">ResourceQuota design document</a>
for more information.</li><li>You can also read the <a href="https://git.k8s.io/design-proposals-archive/scheduling/pod-priority-resourcequota.md">Quota support for priority class design document</a>.</li></ul></div></div><div><div class="td-content"><h1>Process ID Limits And Reservations</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><p>Kubernetes allow you to limit the number of process IDs (PIDs) that a
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a> can use.
You can also reserve a number of allocatable PIDs for each <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a>
for use by the operating system and daemons (rather than by Pods).</p><p>Process IDs (PIDs) are a fundamental resource on nodes. It is trivial to hit the
task limit without hitting any other resource limits, which can then cause
instability to a host machine.</p><p>Cluster administrators require mechanisms to ensure that Pods running in the
cluster cannot induce PID exhaustion that prevents host daemons (such as the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> or
<a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank">kube-proxy</a>,
and potentially also the container runtime) from running.
In addition, it is important to ensure that PIDs are limited among Pods in order
to ensure they have limited impact on other workloads on the same node.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>On certain Linux installations, the operating system sets the PIDs limit to a low default,
such as <code>32768</code>. Consider raising the value of <code>/proc/sys/kernel/pid_max</code>.</div><p>You can configure a kubelet to limit the number of PIDs a given Pod can consume.
For example, if your node's host OS is set to use a maximum of <code>262144</code> PIDs and
expect to host less than <code>250</code> Pods, one can give each Pod a budget of <code>1000</code>
PIDs to prevent using up that node's overall number of available PIDs. If the
admin wants to overcommit PIDs similar to CPU or memory, they may do so as well
with some additional risks. Either way, a single Pod will not be able to bring
the whole machine down. This kind of resource limiting helps to prevent simple
fork bombs from affecting operation of an entire cluster.</p><p>Per-Pod PID limiting allows administrators to protect one Pod from another, but
does not ensure that all Pods scheduled onto that host are unable to impact the node overall.
Per-Pod limiting also does not protect the node agents themselves from PID exhaustion.</p><p>You can also reserve an amount of PIDs for node overhead, separate from the
allocation to Pods. This is similar to how you can reserve CPU, memory, or other
resources for use by the operating system and other facilities outside of Pods
and their containers.</p><p>PID limiting is an important sibling to <a href="/docs/concepts/configuration/manage-resources-containers/">compute
resource</a> requests
and limits. However, you specify it in a different way: rather than defining a
Pod's resource limit in the <code>.spec</code> for a Pod, you configure the limit as a
setting on the kubelet. Pod-defined PID limits are not currently supported.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>This means that the limit that applies to a Pod may be different depending on
where the Pod is scheduled. To make things simple, it's easiest if all Nodes use
the same PID resource limits and reservations.</div><h2 id="node-pid-limits">Node PID limits</h2><p>Kubernetes allows you to reserve a number of process IDs for the system use. To
configure the reservation, use the parameter <code>pid=&lt;number&gt;</code> in the
<code>--system-reserved</code> and <code>--kube-reserved</code> command line options to the kubelet.
The value you specified declares that the specified number of process IDs will
be reserved for the system as a whole and for Kubernetes system daemons
respectively.</p><h2 id="pod-pid-limits">Pod PID limits</h2><p>Kubernetes allows you to limit the number of processes running in a Pod. You
specify this limit at the node level, rather than configuring it as a resource
limit for a particular Pod. Each Node can have a different PID limit.<br>To configure the limit, you can specify the command line parameter <code>--pod-max-pids</code>
to the kubelet, or set <code>PodPidsLimit</code> in the kubelet
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">configuration file</a>.</p><h2 id="pid-based-eviction">PID based eviction</h2><p>You can configure kubelet to start terminating a Pod when it is misbehaving and consuming abnormal amount of resources.
This feature is called eviction. You can
<a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Configure Out of Resource Handling</a>
for various eviction signals.
Use <code>pid.available</code> eviction signal to configure the threshold for number of PIDs used by Pod.
You can set soft and hard eviction policies.
However, even with the hard eviction policy, if the number of PIDs growing very fast,
node can still get into unstable state by hitting the node PIDs limit.
Eviction signal value is calculated periodically and does NOT enforce the limit.</p><p>PID limiting - per Pod and per Node sets the hard limit.
Once the limit is hit, workload will start experiencing failures when trying to get a new PID.
It may or may not lead to rescheduling of a Pod,
depending on how workload reacts on these failures and how liveness and readiness
probes are configured for the Pod. However, if limits were set correctly,
you can guarantee that other Pods workload and system processes will not run out of PIDs
when one Pod is misbehaving.</p><h2 id="what-s-next">What's next</h2><ul><li>Refer to the <a href="https://github.com/kubernetes/enhancements/blob/097b4d8276bc9564e56adf72505d43ce9bc5e9e8/keps/sig-node/20190129-pid-limiting.md">PID Limiting enhancement document</a> for more information.</li><li>For historical context, read
<a href="/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/">Process ID Limiting for Stability Improvements in Kubernetes 1.14</a>.</li><li>Read <a href="/docs/concepts/configuration/manage-resources-containers/">Managing Resources for Containers</a>.</li><li>Learn how to <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Configure Out of Resource Handling</a>.</li></ul></div></div><div><div class="td-content"><h1>Node Resource Managers</h1><p>In order to support latency-critical and high-throughput workloads, Kubernetes offers a suite of
Resource Managers. The managers aim to co-ordinate and optimise the alignment of node's resources for pods
configured with a specific requirement for CPUs, devices, and memory (hugepages) resources.</p><h2 id="hardware-topology-alignment-policies">Hardware topology alignment policies</h2><p><em>Topology Manager</em> is a kubelet component that aims to coordinate the set of components that are
responsible for these optimizations. The overall resource management process is governed using
the policy you specify. To learn more, read
<a href="/docs/tasks/administer-cluster/topology-manager/">Control Topology Management Policies on a Node</a>.</p><h2 id="policies-for-assigning-cpus-to-pods">Policies for assigning CPUs to Pods</h2><div class="feature-state-notice feature-stable" title="Feature Gate: CPUManager"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code> (enabled by default: true)</div><p>Once a Pod is bound to a Node, the kubelet on that node may need to either multiplex the existing
hardware (for example, sharing CPUs across multiple Pods) or allocate hardware by dedicating some
resource (for example, assigning one of more CPUs for a Pod's exclusive use).</p><p>By default, the kubelet uses <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">CFS quota</a>
to enforce pod CPU limits. &#160;When the node runs many CPU-bound pods, the workload can move to
different CPU cores depending on whether the pod is throttled and which CPU cores are available
at scheduling time. Many workloads are not sensitive to this migration and thus
work fine without any intervention.</p><p>However, in workloads where CPU cache affinity and scheduling latency significantly affect
workload performance, the kubelet allows alternative CPU
management policies to determine some placement preferences on the node.
This is implemented using the <em>CPU Manager</em> and its policy.
There are two available policies:</p><ul><li><code>none</code>: the <code>none</code> policy explicitly enables the existing default CPU
affinity scheme, providing no affinity beyond what the OS scheduler does
automatically. &#160;Limits on CPU usage for
<a href="/docs/concepts/workloads/pods/pod-qos/">Guaranteed pods</a> and
<a href="/docs/concepts/workloads/pods/pod-qos/">Burstable pods</a>
are enforced using CFS quota.</li><li><code>static</code>: the <code>static</code> policy allows containers in <code>Guaranteed</code> pods with integer CPU
<code>requests</code> access to exclusive CPUs on the node. This exclusivity is enforced
using the <a href="https://www.kernel.org/doc/Documentation/cgroup-v2.txt">cpuset cgroup controller</a>.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>System services such as the container runtime and the kubelet itself can continue to run on
these exclusive CPUs. &#160;The exclusivity only extends to other pods.</div><p>CPU Manager doesn't support offlining and onlining of CPUs at runtime.</p><h3 id="static-policy">Static policy</h3><p>The static policy enables finer-grained CPU management and exclusive CPU assignment.
This policy manages a shared pool of CPUs that initially contains all CPUs in the
node. The amount of exclusively allocatable CPUs is equal to the total
number of CPUs in the node minus any CPU reservations set by the kubelet configuration.
CPUs reserved by these options are taken, in integer quantity, from the initial shared pool in ascending order by physical
core ID. &#160;This shared pool is the set of CPUs on which any containers in
<code>BestEffort</code> and <code>Burstable</code> pods run. Containers in <code>Guaranteed</code> pods with fractional
CPU <code>requests</code> also run on CPUs in the shared pool. Only containers that are
part of a <code>Guaranteed</code> pod and have integer CPU <code>requests</code> are assigned
exclusive CPUs.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The kubelet requires a CPU reservation greater than zero when the static policy is enabled.
This is because a zero CPU reservation would allow the shared pool to become empty.</div><p>As <code>Guaranteed</code> pods whose containers fit the requirements for being statically
assigned are scheduled to the node, CPUs are removed from the shared pool and
placed in the cpuset for the container. CFS quota is not used to bound
the CPU usage of these containers as their usage is bound by the scheduling domain
itself. In others words, the number of CPUs in the container cpuset is equal to the integer
CPU <code>limit</code> specified in the pod spec.&#160;This static assignment increases CPU
affinity and decreases context switches due to throttling for the CPU-bound
workload.</p><p>Consider the containers in the following pod specs:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span></code></pre></div><p>The pod above runs in the <code>BestEffort</code> QoS class because no resource <code>requests</code> or
<code>limits</code> are specified. It runs in the shared pool.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span></code></pre></div><p>The pod above runs in the <code>Burstable</code> QoS class because resource <code>requests</code> do not
equal <code>limits</code> and the <code>cpu</code> quantity is not specified. It runs in the shared
pool.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"100Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"1"</span><span>
</span></span></span></code></pre></div><p>The pod above runs in the <code>Burstable</code> QoS class because resource <code>requests</code> do not
equal <code>limits</code>. It runs in the shared pool.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span></code></pre></div><p>The pod above runs in the <code>Guaranteed</code> QoS class because <code>requests</code> are equal to <code>limits</code>.
And the container's resource limit for the CPU resource is an integer greater than
or equal to one. The <code>nginx</code> container is granted 2 exclusive CPUs.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"1.5"</span><span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"1.5"</span><span>
</span></span></span></code></pre></div><p>The pod above runs in the <code>Guaranteed</code> QoS class because <code>requests</code> are equal to <code>limits</code>.
But the container's resource limit for the CPU resource is a fraction. It runs in
the shared pool.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span><span>"200Mi"</span><span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span><span>"2"</span><span>
</span></span></span></code></pre></div><p>The pod above runs in the <code>Guaranteed</code> QoS class because only <code>limits</code> are specified
and <code>requests</code> are set equal to <code>limits</code> when not explicitly specified. And the
container's resource limit for the CPU resource is an integer greater than or
equal to one. The <code>nginx</code> container is granted 2 exclusive CPUs.</p><h4 id="cpu-policy-static--options">Static policy options</h4><p>Here are the available policy options for the static CPU management policy,
listed in alphabetical order:</p><dl><dt><code>align-by-socket</code> (alpha, hidden by default)</dt><dd>Align CPUs by physical package / socket boundary, rather than logical NUMA boundaries
(available since Kubernetes v1.25)</dd><dt><code>distribute-cpus-across-cores</code> (alpha, hidden by default)</dt><dd>Allocate virtual cores, sometimes called hardware threads, across different physical cores
(available since Kubernetes v1.31)</dd><dt><code>distribute-cpus-across-numa</code> (beta, visible by default)</dt><dd>Spread CPUs across different NUMA domains, aiming for an even balance between the selected domains
(available since Kubernetes v1.23)</dd><dt><code>full-pcpus-only</code> (GA, visible by default)</dt><dd>Always allocate full physical cores (available since Kubernetes v1.22, GA since Kubernetes v1.33)</dd><dt><code>strict-cpu-reservation</code> (beta, visible by default)</dt><dd>Prevent all the pods regardless of their Quality of Service class to run on reserved CPUs
(available since Kubernetes v1.32)</dd><dt><code>prefer-align-cpus-by-uncorecache</code> (beta, visible by default)</dt><dd>Align CPUs by uncore (Last-Level) cache boundary on a best-effort way
(available since Kubernetes v1.32)</dd></dl><p>You can toggle groups of options on and off based upon their maturity level
using the following feature gates:</p><ul><li><code>CPUManagerPolicyBetaOptions</code> (default enabled). Disable to hide beta-level options.</li><li><code>CPUManagerPolicyAlphaOptions</code> (default disabled). Enable to show alpha-level options.</li></ul><p>You will still have to enable each option using the <code>cpuManagerPolicyOptions</code> field in the
kubelet configuration file.</p><p>For more detail about the individual options you can configure, read on.</p><h5 id="full-pcpus-only"><code>full-pcpus-only</code></h5><p>If the <code>full-pcpus-only</code> policy option is specified, the static policy will always allocate full physical cores.
By default, without this option, the static policy allocates CPUs using a topology-aware best-fit allocation.
On SMT enabled systems, the policy can allocate individual virtual cores, which correspond to hardware threads.
This can lead to different containers sharing the same physical cores; this behaviour in turn contributes
to the <a href="https://en.wikipedia.org/wiki/Cloud_computing_issues#Performance_interference_and_noisy_neighbors">noisy neighbours problem</a>.
With the option enabled, the pod will be admitted by the kubelet only if the CPU request of all its containers
can be fulfilled by allocating full physical cores.
If the pod does not pass the admission, it will be put in Failed state with the message <code>SMTAlignmentError</code>.</p><h5 id="distribute-cpus-across-numa"><code>distribute-cpus-across-numa</code></h5><p>If the <code>distribute-cpus-across-numa</code>policy option is specified, the static
policy will evenly distribute CPUs across NUMA nodes in cases where more than
one NUMA node is required to satisfy the allocation.
By default, the <code>CPUManager</code> will pack CPUs onto one NUMA node until it is
filled, with any remaining CPUs simply spilling over to the next NUMA node.
This can cause undesired bottlenecks in parallel code relying on barriers (and
similar synchronization primitives), as this type of code tends to run only as
fast as its slowest worker (which is slowed down by the fact that fewer CPUs
are available on at least one NUMA node).
By distributing CPUs evenly across NUMA nodes, application developers can more
easily ensure that no single worker suffers from NUMA effects more than any
other, improving the overall performance of these types of applications.</p><h5 id="align-by-socket"><code>align-by-socket</code></h5><p>If the <code>align-by-socket</code> policy option is specified, CPUs will be considered
aligned at the socket boundary when deciding how to allocate CPUs to a
container. By default, the <code>CPUManager</code> aligns CPU allocations at the NUMA
boundary, which could result in performance degradation if CPUs need to be
pulled from more than one NUMA node to satisfy the allocation. Although it
tries to ensure that all CPUs are allocated from the <em>minimum</em> number of NUMA
nodes, there is no guarantee that those NUMA nodes will be on the same socket.
By directing the <code>CPUManager</code> to explicitly align CPUs at the socket boundary
rather than the NUMA boundary, we are able to avoid such issues. Note, this
policy option is not compatible with <code>TopologyManager</code> <code>single-numa-node</code>
policy and does not apply to hardware where the number of sockets is greater
than number of NUMA nodes.</p><h5 id="distribute-cpus-across-cores"><code>distribute-cpus-across-cores</code></h5><p>If the <code>distribute-cpus-across-cores</code> policy option is specified, the static policy
will attempt to allocate virtual cores (hardware threads) across different physical cores.
By default, the <code>CPUManager</code> tends to pack CPUs onto as few physical cores as possible,
which can lead to contention among CPUs on the same physical core and result
in performance bottlenecks. By enabling the <code>distribute-cpus-across-cores</code> policy,
the static policy ensures that CPUs are distributed across as many physical cores
as possible, reducing the contention on the same physical core and thereby
improving overall performance. However, it is important to note that this strategy
might be less effective when the system is heavily loaded. Under such conditions,
the benefit of reducing contention diminishes. Conversely, default behavior
can help in reducing inter-core communication overhead, potentially providing
better performance under high load conditions.</p><h5 id="strict-cpu-reservation"><code>strict-cpu-reservation</code></h5><p>The <code>reservedSystemCPUs</code> parameter in <a href="/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration</a>,
or the deprecated kubelet command line option <code>--reserved-cpus</code>, defines an explicit CPU set for OS system daemons
and kubernetes system daemons. More details of this parameter can be found on the
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#explicitly-reserved-cpu-list">Explicitly Reserved CPU List</a> page.
By default, this isolation is implemented only for guaranteed pods with integer CPU requests not for burstable and best-effort pods
(and guaranteed pods with fractional CPU requests). Admission is only comparing the CPU requests against the allocatable CPUs.
Since the CPU limit is higher than the request, the default behaviour allows burstable and best-effort pods to use up the capacity
of <code>reservedSystemCPUs</code> and cause host OS services to starve in real life deployments.
If the <code>strict-cpu-reservation</code> policy option is enabled, the static policy will not allow
any workload to use the CPU cores specified in <code>reservedSystemCPUs</code>.</p><h5 id="prefer-align-cpus-by-uncorecache"><code>prefer-align-cpus-by-uncorecache</code></h5><p>If the <code>prefer-align-cpus-by-uncorecache</code> policy is specified, the static policy
will allocate CPU resources for individual containers such that all CPUs assigned
to a container share the same uncore cache block (also known as the Last-Level Cache
or LLC). By default, the <code>CPUManager</code> will tightly pack CPU assignments which can
result in containers being assigned CPUs from multiple uncore caches. This option
enables the <code>CPUManager</code> to allocate CPUs in a way that maximizes the efficient use
of the uncore cache. Allocation is performed on a best-effort basis, aiming to
affine as many CPUs as possible within the same uncore cache. If the container's
CPU requirement exceeds the CPU capacity of a single uncore cache, the <code>CPUManager</code>
minimizes the number of uncore caches used in order to maintain optimal uncore
cache alignment. Specific workloads can benefit in performance from the reduction
of inter-cache latency and noisy neighbors at the cache level. If the <code>CPUManager</code>
cannot align optimally while the node has sufficient resources, the container will
still be admitted using the default packed behavior.</p><h2 id="memory-management-policies">Memory Management Policies</h2><div class="feature-state-notice feature-stable" title="Feature Gate: MemoryManager"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>The Kubernetes <em>Memory Manager</em> enables the feature of guaranteed memory (and hugepages)
allocation for pods in the <code>Guaranteed</code> <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." href="/docs/concepts/workloads/pods/pod-qos/" target="_blank">QoS class</a>.</p><p>The Memory Manager employs hint generation protocol to yield the most suitable NUMA affinity for a pod.
The Memory Manager feeds the central manager (<em>Topology Manager</em>) with these affinity hints.
Based on both the hints and Topology Manager policy, the pod is rejected or admitted to the node.</p><p>Moreover, the Memory Manager ensures that the memory which a pod requests
is allocated from a minimum number of NUMA nodes.</p><h2 id="other-resource-managers">Other resource managers</h2><p>The configuration of individual managers is elaborated in dedicated documents:</p><ul><li><a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager">Device Manager</a></li></ul></div></div><div><div class="td-content"><h1>Scheduling, Preemption and Eviction</h1><p>In Kubernetes, scheduling refers to making sure that <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>
are matched to <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">Nodes</a> so that the
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> can run them. Preemption
is the process of terminating Pods with lower <a class="glossary-tooltip" title="Pod Priority indicates the importance of a Pod relative to other Pods." href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority" target="_blank">Priority</a>
so that Pods with higher Priority can schedule on Nodes. Eviction is the process
of terminating one or more Pods on Nodes.</p><h2 id="scheduling">Scheduling</h2><ul><li><a href="/docs/concepts/scheduling-eviction/kube-scheduler/">Kubernetes Scheduler</a></li><li><a href="/docs/concepts/scheduling-eviction/assign-pod-node/">Assigning Pods to Nodes</a></li><li><a href="/docs/concepts/scheduling-eviction/pod-overhead/">Pod Overhead</a></li><li><a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod Topology Spread Constraints</a></li><li><a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">Taints and Tolerations</a></li><li><a href="/docs/concepts/scheduling-eviction/scheduling-framework/">Scheduling Framework</a></li><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Dynamic Resource Allocation</a></li><li><a href="/docs/concepts/scheduling-eviction/scheduler-perf-tuning/">Scheduler Performance Tuning</a></li><li><a href="/docs/concepts/scheduling-eviction/resource-bin-packing/">Resource Bin Packing for Extended Resources</a></li><li><a href="/docs/concepts/scheduling-eviction/pod-scheduling-readiness/">Pod Scheduling Readiness</a></li><li><a href="https://github.com/kubernetes-sigs/descheduler#descheduler-for-kubernetes">Descheduler</a></li></ul><h2 id="pod-disruption">Pod Disruption</h2><p><a href="/docs/concepts/workloads/pods/disruptions/">Pod disruption</a> is the process by which
Pods on Nodes are terminated either voluntarily or involuntarily.</p><p>Voluntary disruptions are started intentionally by application owners or cluster
administrators. Involuntary disruptions are unintentional and can be triggered by
unavoidable issues like Nodes running out of <a class="glossary-tooltip" title="A defined amount of infrastructure available for consumption (CPU, memory, etc)." href="/docs/reference/glossary/?all=true#term-infrastructure-resource" target="_blank">resources</a>,
or by accidental deletions.</p><ul><li><a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod Priority and Preemption</a></li><li><a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node-pressure Eviction</a></li><li><a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated Eviction</a></li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Kubernetes Scheduler</h1><p>In Kubernetes, <em>scheduling</em> refers to making sure that <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>
are matched to <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">Nodes</a> so that
<a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">Kubelet</a> can run them.</p><h2 id="scheduling">Scheduling overview</h2><p>A scheduler watches for newly created Pods that have no Node assigned. For
every Pod that the scheduler discovers, the scheduler becomes responsible
for finding the best Node for that Pod to run on. The scheduler reaches
this placement decision taking into account the scheduling principles
described below.</p><p>If you want to understand why Pods are placed onto a particular Node,
or if you're planning to implement a custom scheduler yourself, this
page will help you learn about scheduling.</p><h2 id="kube-scheduler">kube-scheduler</h2><p><a href="/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler</a>
is the default scheduler for Kubernetes and runs as part of the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>.
kube-scheduler is designed so that, if you want and need to, you can
write your own scheduling component and use that instead.</p><p>Kube-scheduler selects an optimal node to run newly created or not yet
scheduled (unscheduled) pods. Since containers in pods - and pods themselves -
can have different requirements, the scheduler filters out any nodes that
don't meet a Pod's specific scheduling needs. Alternatively, the API lets
you specify a node for a Pod when you create it, but this is unusual
and is only done in special cases.</p><p>In a cluster, Nodes that meet the scheduling requirements for a Pod
are called <em>feasible</em> nodes. If none of the nodes are suitable, the pod
remains unscheduled until the scheduler is able to place it.</p><p>The scheduler finds feasible Nodes for a Pod and then runs a set of
functions to score the feasible Nodes and picks a Node with the highest
score among the feasible ones to run the Pod. The scheduler then notifies
the API server about this decision in a process called <em>binding</em>.</p><p>Factors that need to be taken into account for scheduling decisions include
individual and collective resource requirements, hardware / software /
policy constraints, affinity and anti-affinity specifications, data
locality, inter-workload interference, and so on.</p><h3 id="kube-scheduler-implementation">Node selection in kube-scheduler</h3><p>kube-scheduler selects a node for the pod in a 2-step operation:</p><ol><li>Filtering</li><li>Scoring</li></ol><p>The <em>filtering</em> step finds the set of Nodes where it's feasible to
schedule the Pod. For example, the PodFitsResources filter checks whether a
candidate Node has enough available resources to meet a Pod's specific
resource requests. After this step, the node list contains any suitable
Nodes; often, there will be more than one. If the list is empty, that
Pod isn't (yet) schedulable.</p><p>In the <em>scoring</em> step, the scheduler ranks the remaining nodes to choose
the most suitable Pod placement. The scheduler assigns a score to each Node
that survived filtering, basing this score on the active scoring rules.</p><p>Finally, kube-scheduler assigns the Pod to the Node with the highest ranking.
If there is more than one node with equal scores, kube-scheduler selects
one of these at random.</p><p>There are two supported ways to configure the filtering and scoring behavior
of the scheduler:</p><ol><li><a href="/docs/reference/scheduling/policies/">Scheduling Policies</a> allow you to configure <em>Predicates</em> for filtering and <em>Priorities</em> for scoring.</li><li><a href="/docs/reference/scheduling/config/#profiles">Scheduling Profiles</a> allow you to configure Plugins that implement different scheduling stages, including: <code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code>, and others. You can also configure the kube-scheduler to run different profiles.</li></ol><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/scheduling-eviction/scheduler-perf-tuning/">scheduler performance tuning</a></li><li>Read about <a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod topology spread constraints</a></li><li>Read the <a href="/docs/reference/command-line-tools-reference/kube-scheduler/">reference documentation</a> for kube-scheduler</li><li>Read the <a href="/docs/reference/config-api/kube-scheduler-config.v1/">kube-scheduler config (v1)</a> reference</li><li>Learn about <a href="/docs/tasks/extend-kubernetes/configure-multiple-schedulers/">configuring multiple schedulers</a></li><li>Learn about <a href="/docs/tasks/administer-cluster/topology-manager/">topology management policies</a></li><li>Learn about <a href="/docs/concepts/scheduling-eviction/pod-overhead/">Pod Overhead</a></li><li>Learn about scheduling of Pods that use volumes in:<ul><li><a href="/docs/concepts/storage/storage-classes/#volume-binding-mode">Volume Topology Support</a></li><li><a href="/docs/concepts/storage/storage-capacity/">Storage Capacity Tracking</a></li><li><a href="/docs/concepts/storage/storage-limits/">Node-specific Volume Limits</a></li></ul></li></ul></div></div><div><div class="td-content"><h1>Assigning Pods to Nodes</h1><p>You can constrain a <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pod</a> so that it is
<em>restricted</em> to run on particular <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node(s)</a>,
or to <em>prefer</em> to run on particular nodes.
There are several ways to do this and the recommended approaches all use
<a href="/docs/concepts/overview/working-with-objects/labels/">label selectors</a> to facilitate the selection.
Often, you do not need to set any such constraints; the
<a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">scheduler</a> will automatically do a reasonable placement
(for example, spreading your Pods across nodes so as not place Pods on a node with insufficient free resources).
However, there are some circumstances where you may want to control which node
the Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it,
or to co-locate Pods from two different services that communicate a lot into the same availability zone.</p><p>You can use any of the following methods to choose where Kubernetes schedules
specific Pods:</p><ul><li><a href="#nodeselector">nodeSelector</a> field matching against <a href="#built-in-node-labels">node labels</a></li><li><a href="#affinity-and-anti-affinity">Affinity and anti-affinity</a></li><li><a href="#nodename">nodeName</a> field</li><li><a href="#pod-topology-spread-constraints">Pod topology spread constraints</a></li></ul><h2 id="built-in-node-labels">Node labels</h2><p>Like many other Kubernetes objects, nodes have
<a href="/docs/concepts/overview/working-with-objects/labels/">labels</a>. You can
<a href="/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node">attach labels manually</a>.
Kubernetes also populates a <a href="/docs/reference/node/node-labels/">standard set of labels</a>
on all nodes in a cluster.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The value of these labels is cloud provider specific and is not guaranteed to be reliable.
For example, the value of <code>kubernetes.io/hostname</code> may be the same as the node name in some environments
and a different value in other environments.</div><h3 id="node-isolation-restriction">Node isolation/restriction</h3><p>Adding labels to nodes allows you to target Pods for scheduling on specific
nodes or groups of nodes. You can use this functionality to ensure that specific
Pods only run on nodes with certain isolation, security, or regulatory
properties.</p><p>If you use labels for node isolation, choose label keys that the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a>
cannot modify. This prevents a compromised node from setting those labels on
itself so that the scheduler schedules workloads onto the compromised node.</p><p>The <a href="/docs/reference/access-authn-authz/admission-controllers/#noderestriction"><code>NodeRestriction</code> admission plugin</a>
prevents the kubelet from setting or modifying labels with a
<code>node-restriction.kubernetes.io/</code> prefix.</p><p>To make use of that label prefix for node isolation:</p><ol><li>Ensure you are using the <a href="/docs/reference/access-authn-authz/node/">Node authorizer</a> and have <em>enabled</em> the <code>NodeRestriction</code> admission plugin.</li><li>Add labels with the <code>node-restriction.kubernetes.io/</code> prefix to your nodes, and use those labels in your <a href="#nodeselector">node selectors</a>.
For example, <code>example.com.node-restriction.kubernetes.io/fips=true</code> or <code>example.com.node-restriction.kubernetes.io/pci-dss=true</code>.</li></ol><h2 id="nodeselector">nodeSelector</h2><p><code>nodeSelector</code> is the simplest recommended form of node selection constraint.
You can add the <code>nodeSelector</code> field to your Pod specification and specify the
<a href="#built-in-node-labels">node labels</a> you want the target node to have.
Kubernetes only schedules the Pod onto nodes that have each of the labels you
specify.</p><p>See <a href="/docs/tasks/configure-pod-container/assign-pods-nodes/">Assign Pods to Nodes</a> for more
information.</p><h2 id="affinity-and-anti-affinity">Affinity and anti-affinity</h2><p><code>nodeSelector</code> is the simplest way to constrain Pods to nodes with specific
labels. Affinity and anti-affinity expand the types of constraints you can
define. Some of the benefits of affinity and anti-affinity include:</p><ul><li>The affinity/anti-affinity language is more expressive. <code>nodeSelector</code> only
selects nodes with all the specified labels. Affinity/anti-affinity gives you
more control over the selection logic.</li><li>You can indicate that a rule is <em>soft</em> or <em>preferred</em>, so that the scheduler
still schedules the Pod even if it can't find a matching node.</li><li>You can constrain a Pod using labels on other Pods running on the node (or other topological domain),
instead of just node labels, which allows you to define rules for which Pods
can be co-located on a node.</li></ul><p>The affinity feature consists of two types of affinity:</p><ul><li><em>Node affinity</em> functions like the <code>nodeSelector</code> field but is more expressive and
allows you to specify soft rules.</li><li><em>Inter-pod affinity/anti-affinity</em> allows you to constrain Pods against labels
on other Pods.</li></ul><h3 id="node-affinity">Node affinity</h3><p>Node affinity is conceptually similar to <code>nodeSelector</code>, allowing you to constrain which nodes your
Pod can be scheduled on based on node labels. There are two types of node
affinity:</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code>: The scheduler can't
schedule the Pod unless the rule is met. This functions like <code>nodeSelector</code>,
but with a more expressive syntax.</li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code>: The scheduler tries to
find a node that meets the rule. If a matching node is not available, the
scheduler still schedules the Pod.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In the preceding types, <code>IgnoredDuringExecution</code> means that if the node labels
change after Kubernetes schedules the Pod, the Pod continues to run.</div><p>You can specify node affinities using the <code>.spec.affinity.nodeAffinity</code> field in
your Pod spec.</p><p>For example, consider the following Pod spec:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-node-affinity.yaml"><code>pods/pod-with-node-affinity.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-with-node-affinity.yaml to clipboard"></div><div class="includecode" id="pods-pod-with-node-affinity-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>with-node-affinity<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>affinity</span>:<span>
</span></span></span><span><span><span>    </span><span>nodeAffinity</span>:<span>
</span></span></span><span><span><span>      </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>        </span><span>nodeSelectorTerms</span>:<span>
</span></span></span><span><span><span>        </span>- <span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>topology.kubernetes.io/zone<span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>            </span><span>values</span>:<span>
</span></span></span><span><span><span>            </span>- antarctica-east1<span>
</span></span></span><span><span><span>            </span>- antarctica-west1<span>
</span></span></span><span><span><span>      </span><span>preferredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>      </span>- <span>weight</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>        </span><span>preference</span>:<span>
</span></span></span><span><span><span>          </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>another-node-label-key<span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>            </span><span>values</span>:<span>
</span></span></span><span><span><span>            </span>- another-node-label-value<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>with-node-affinity<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.8</span></span></code></pre></div></div></div><p>In this example, the following rules apply:</p><ul><li>The node <em>must</em> have a label with the key <code>topology.kubernetes.io/zone</code> and
the value of that label <em>must</em> be either <code>antarctica-east1</code> or <code>antarctica-west1</code>.</li><li>The node <em>preferably</em> has a label with the key <code>another-node-label-key</code> and
the value <code>another-node-label-value</code>.</li></ul><p>You can use the <code>operator</code> field to specify a logical operator for Kubernetes to use when
interpreting the rules. You can use <code>In</code>, <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>,
<code>Gt</code> and <code>Lt</code>.</p><p>Read <a href="#operators">Operators</a>
to learn more about how these work.</p><p><code>NotIn</code> and <code>DoesNotExist</code> allow you to define node anti-affinity behavior.
Alternatively, you can use <a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">node taints</a>
to repel Pods from specific nodes.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If you specify both <code>nodeSelector</code> and <code>nodeAffinity</code>, <em>both</em> must be satisfied
for the Pod to be scheduled onto a node.</p><p>If you specify multiple terms in <code>nodeSelectorTerms</code> associated with <code>nodeAffinity</code>
types, then the Pod can be scheduled onto a node if one of the specified terms
can be satisfied (terms are ORed).</p><p>If you specify multiple expressions in a single <code>matchExpressions</code> field associated with a
term in <code>nodeSelectorTerms</code>, then the Pod can be scheduled onto a node only
if all the expressions are satisfied (expressions are ANDed).</p></div><p>See <a href="/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/">Assign Pods to Nodes using Node Affinity</a>
for more information.</p><h4 id="node-affinity-weight">Node affinity weight</h4><p>You can specify a <code>weight</code> between 1 and 100 for each instance of the
<code>preferredDuringSchedulingIgnoredDuringExecution</code> affinity type. When the
scheduler finds nodes that meet all the other scheduling requirements of the Pod, the
scheduler iterates through every preferred rule that the node satisfies and adds the
value of the <code>weight</code> for that expression to a sum.</p><p>The final sum is added to the score of other priority functions for the node.
Nodes with the highest total score are prioritized when the scheduler makes a
scheduling decision for the Pod.</p><p>For example, consider the following Pod spec:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-affinity-preferred-weight.yaml"><code>pods/pod-with-affinity-preferred-weight.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-with-affinity-preferred-weight.yaml to clipboard"></div><div class="includecode" id="pods-pod-with-affinity-preferred-weight-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>with-affinity-preferred-weight<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>affinity</span>:<span>
</span></span></span><span><span><span>    </span><span>nodeAffinity</span>:<span>
</span></span></span><span><span><span>      </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>        </span><span>nodeSelectorTerms</span>:<span>
</span></span></span><span><span><span>        </span>- <span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>kubernetes.io/os<span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>            </span><span>values</span>:<span>
</span></span></span><span><span><span>            </span>- linux<span>
</span></span></span><span><span><span>      </span><span>preferredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>      </span>- <span>weight</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>        </span><span>preference</span>:<span>
</span></span></span><span><span><span>          </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>label-1<span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>            </span><span>values</span>:<span>
</span></span></span><span><span><span>            </span>- key-1<span>
</span></span></span><span><span><span>      </span>- <span>weight</span>:<span> </span><span>50</span><span>
</span></span></span><span><span><span>        </span><span>preference</span>:<span>
</span></span></span><span><span><span>          </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>label-2<span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>            </span><span>values</span>:<span>
</span></span></span><span><span><span>            </span>- key-2<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>with-node-affinity<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.8<span>
</span></span></span></code></pre></div></div></div><p>If there are two possible nodes that match the
<code>preferredDuringSchedulingIgnoredDuringExecution</code> rule, one with the
<code>label-1:key-1</code> label and another with the <code>label-2:key-2</code> label, the scheduler
considers the <code>weight</code> of each node and adds the weight to the other scores for
that node, and schedules the Pod onto the node with the highest final score.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you want Kubernetes to successfully schedule the Pods in this example, you
must have existing nodes with the <code>kubernetes.io/os=linux</code> label.</div><h4 id="node-affinity-per-scheduling-profile">Node affinity per scheduling profile</h4><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [beta]</code></div><p>When configuring multiple <a href="/docs/reference/scheduling/config/#multiple-profiles">scheduling profiles</a>, you can associate
a profile with a node affinity, which is useful if a profile only applies to a specific set of nodes.
To do so, add an <code>addedAffinity</code> to the <code>args</code> field of the <a href="/docs/reference/scheduling/config/#scheduling-plugins"><code>NodeAffinity</code> plugin</a>
in the <a href="/docs/reference/scheduling/config/">scheduler configuration</a>. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubescheduler.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeSchedulerConfiguration<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>profiles</span>:<span>
</span></span></span><span><span><span>  </span>- <span>schedulerName</span>:<span> </span>default-scheduler<span>
</span></span></span><span><span><span>  </span>- <span>schedulerName</span>:<span> </span>foo-scheduler<span>
</span></span></span><span><span><span>    </span><span>pluginConfig</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>NodeAffinity<span>
</span></span></span><span><span><span>        </span><span>args</span>:<span>
</span></span></span><span><span><span>          </span><span>addedAffinity</span>:<span>
</span></span></span><span><span><span>            </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>              </span><span>nodeSelectorTerms</span>:<span>
</span></span></span><span><span><span>              </span>- <span>matchExpressions</span>:<span>
</span></span></span><span><span><span>                </span>- <span>key</span>:<span> </span>scheduler-profile<span>
</span></span></span><span><span><span>                  </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>                  </span><span>values</span>:<span>
</span></span></span><span><span><span>                  </span>- foo<span>
</span></span></span></code></pre></div><p>The <code>addedAffinity</code> is applied to all Pods that set <code>.spec.schedulerName</code> to <code>foo-scheduler</code>, in addition to the
NodeAffinity specified in the PodSpec.
That is, in order to match the Pod, nodes need to satisfy <code>addedAffinity</code> and
the Pod's <code>.spec.NodeAffinity</code>.</p><p>Since the <code>addedAffinity</code> is not visible to end users, its behavior might be
unexpected to them. Use node labels that have a clear correlation to the
scheduler profile name.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The DaemonSet controller, which <a href="/docs/concepts/workloads/controllers/daemonset/#how-daemon-pods-are-scheduled">creates Pods for DaemonSets</a>,
does not support scheduling profiles. When the DaemonSet controller creates
Pods, the default Kubernetes scheduler places those Pods and honors any
<code>nodeAffinity</code> rules in the DaemonSet controller.</div><h3 id="inter-pod-affinity-and-anti-affinity">Inter-pod affinity and anti-affinity</h3><p>Inter-pod affinity and anti-affinity allow you to constrain which nodes your
Pods can be scheduled on based on the labels of Pods already running on that
node, instead of the node labels.</p><h4 id="types-of-inter-pod-affinity-and-anti-affinity">Types of Inter-pod Affinity and Anti-affinity</h4><p>Inter-pod affinity and anti-affinity take the form "this
Pod should (or, in the case of anti-affinity, should not) run in an X if that X
is already running one or more Pods that meet rule Y", where X is a topology
domain like node, rack, cloud provider zone or region, or similar and Y is the
rule Kubernetes tries to satisfy.</p><p>You express these rules (Y) as <a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">label selectors</a>
with an optional associated list of namespaces. Pods are namespaced objects in
Kubernetes, so Pod labels also implicitly have namespaces. Any label selectors
for Pod labels should specify the namespaces in which Kubernetes should look for those
labels.</p><p>You express the topology domain (X) using a <code>topologyKey</code>, which is the key for
the node label that the system uses to denote the domain. For examples, see
<a href="/docs/reference/labels-annotations-taints/">Well-Known Labels, Annotations and Taints</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Inter-pod affinity and anti-affinity require substantial amounts of
processing which can slow down scheduling in large clusters significantly. We do
not recommend using them in clusters larger than several hundred nodes.</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Pod anti-affinity requires nodes to be consistently labeled, in other words,
every node in the cluster must have an appropriate label matching <code>topologyKey</code>.
If some or all nodes are missing the specified <code>topologyKey</code> label, it can lead
to unintended behavior.</div><p>Similar to <a href="#node-affinity">node affinity</a> are two types of Pod affinity and
anti-affinity as follows:</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code></li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code></li></ul><p>For example, you could use
<code>requiredDuringSchedulingIgnoredDuringExecution</code> affinity to tell the scheduler to
co-locate Pods of two services in the same cloud provider zone because they
communicate with each other a lot. Similarly, you could use
<code>preferredDuringSchedulingIgnoredDuringExecution</code> anti-affinity to spread Pods
from a service across multiple cloud provider zones.</p><p>To use inter-pod affinity, use the <code>affinity.podAffinity</code> field in the Pod spec.
For inter-pod anti-affinity, use the <code>affinity.podAntiAffinity</code> field in the Pod
spec.</p><h4 id="scheduling-behavior">Scheduling Behavior</h4><p>When scheduling a new Pod, the Kubernetes scheduler evaluates the Pod's affinity/anti-affinity rules in the context of the current cluster state:</p><ol><li><p>Hard Constraints (Node Filtering):</p><ul><li><code>podAffinity.requiredDuringSchedulingIgnoredDuringExecution</code> and <code>podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution</code>:<ul><li>The scheduler ensures the new Pod is assigned to nodes that satisfy these required affinity and anti-affinity rules based on existing Pods.</li></ul></li></ul></li><li><p>Soft Constraints (Scoring):</p><ul><li><code>podAffinity.preferredDuringSchedulingIgnoredDuringExecution</code> and <code>podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution</code>:<ul><li>The scheduler scores nodes based on how well they meet these preferred affinity and anti-affinity rules to optimize Pod placement.</li></ul></li></ul></li><li><p>Ignored Fields:</p><ul><li>Existing Pods' <code>podAffinity.preferredDuringSchedulingIgnoredDuringExecution</code>:<ul><li>These preferred affinity rules are not considered during the scheduling decision for new Pods.</li></ul></li><li>Existing Pods' <code>podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution</code>:<ul><li>Similarly, preferred anti-affinity rules of existing Pods are ignored during scheduling.</li></ul></li></ul></li></ol><h4 id="scheduling-a-group-of-pods-with-inter-pod-affinity-to-themselves">Scheduling a Group of Pods with Inter-pod Affinity to Themselves</h4><p>If the current Pod being scheduled is the first in a series that have affinity to themselves,
it is allowed to be scheduled if it passes all other affinity checks. This is determined by
verifying that no other Pod in the cluster matches the namespace and selector of this Pod,
that the Pod matches its own terms, and the chosen node matches all requested topologies.
This ensures that there will not be a deadlock even if all the Pods have inter-pod affinity
specified.</p><h4 id="an-example-of-a-pod-that-uses-pod-affinity">Pod Affinity Example</h4><p>Consider the following Pod spec:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-pod-affinity.yaml"><code>pods/pod-with-pod-affinity.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-with-pod-affinity.yaml to clipboard"></div><div class="includecode" id="pods-pod-with-pod-affinity-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>with-pod-affinity<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>affinity</span>:<span>
</span></span></span><span><span><span>    </span><span>podAffinity</span>:<span>
</span></span></span><span><span><span>      </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>      </span>- <span>labelSelector</span>:<span>
</span></span></span><span><span><span>          </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>security<span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>            </span><span>values</span>:<span>
</span></span></span><span><span><span>            </span>- S1<span>
</span></span></span><span><span><span>        </span><span>topologyKey</span>:<span> </span>topology.kubernetes.io/zone<span>
</span></span></span><span><span><span>    </span><span>podAntiAffinity</span>:<span>
</span></span></span><span><span><span>      </span><span>preferredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>      </span>- <span>weight</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>        </span><span>podAffinityTerm</span>:<span>
</span></span></span><span><span><span>          </span><span>labelSelector</span>:<span>
</span></span></span><span><span><span>            </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>            </span>- <span>key</span>:<span> </span>security<span>
</span></span></span><span><span><span>              </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>              </span><span>values</span>:<span>
</span></span></span><span><span><span>              </span>- S2<span>
</span></span></span><span><span><span>          </span><span>topologyKey</span>:<span> </span>topology.kubernetes.io/zone<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>with-pod-affinity<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.8<span>
</span></span></span></code></pre></div></div></div><p>This example defines one Pod affinity rule and one Pod anti-affinity rule. The
Pod affinity rule uses the "hard"
<code>requiredDuringSchedulingIgnoredDuringExecution</code>, while the anti-affinity rule
uses the "soft" <code>preferredDuringSchedulingIgnoredDuringExecution</code>.</p><p>The affinity rule specifies that the scheduler is allowed to place the example Pod
on a node only if that node belongs to a specific <a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">zone</a>
where other Pods have been labeled with <code>security=S1</code>.
For instance, if we have a cluster with a designated zone, let's call it "Zone V,"
consisting of nodes labeled with <code>topology.kubernetes.io/zone=V</code>, the scheduler can
assign the Pod to any node within Zone V, as long as there is at least one Pod within
Zone V already labeled with <code>security=S1</code>. Conversely, if there are no Pods with <code>security=S1</code>
labels in Zone V, the scheduler will not assign the example Pod to any node in that zone.</p><p>The anti-affinity rule specifies that the scheduler should try to avoid scheduling the Pod
on a node if that node belongs to a specific <a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">zone</a>
where other Pods have been labeled with <code>security=S2</code>.
For instance, if we have a cluster with a designated zone, let's call it "Zone R,"
consisting of nodes labeled with <code>topology.kubernetes.io/zone=R</code>, the scheduler should avoid
assigning the Pod to any node within Zone R, as long as there is at least one Pod within
Zone R already labeled with <code>security=S2</code>. Conversely, the anti-affinity rule does not impact
scheduling into Zone R if there are no Pods with <code>security=S2</code> labels.</p><p>To get yourself more familiar with the examples of Pod affinity and anti-affinity,
refer to the <a href="https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md">design proposal</a>.</p><p>You can use the <code>In</code>, <code>NotIn</code>, <code>Exists</code> and <code>DoesNotExist</code> values in the
<code>operator</code> field for Pod affinity and anti-affinity.</p><p>Read <a href="#operators">Operators</a>
to learn more about how these work.</p><p>In principle, the <code>topologyKey</code> can be any allowed label key with the following
exceptions for performance and security reasons:</p><ul><li>For Pod affinity and anti-affinity, an empty <code>topologyKey</code> field is not allowed in both
<code>requiredDuringSchedulingIgnoredDuringExecution</code>
and <code>preferredDuringSchedulingIgnoredDuringExecution</code>.</li><li>For <code>requiredDuringSchedulingIgnoredDuringExecution</code> Pod anti-affinity rules,
the admission controller <code>LimitPodHardAntiAffinityTopology</code> limits
<code>topologyKey</code> to <code>kubernetes.io/hostname</code>. You can modify or disable the
admission controller if you want to allow custom topologies.</li></ul><p>In addition to <code>labelSelector</code> and <code>topologyKey</code>, you can optionally specify a list
of namespaces which the <code>labelSelector</code> should match against using the
<code>namespaces</code> field at the same level as <code>labelSelector</code> and <code>topologyKey</code>.
If omitted or empty, <code>namespaces</code> defaults to the namespace of the Pod where the
affinity/anti-affinity definition appears.</p><h4 id="namespace-selector">Namespace Selector</h4><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>You can also select matching namespaces using <code>namespaceSelector</code>, which is a label query over the set of namespaces.
The affinity term is applied to namespaces selected by both <code>namespaceSelector</code> and the <code>namespaces</code> field.
Note that an empty <code>namespaceSelector</code> ({}) matches all namespaces, while a null or empty <code>namespaces</code> list and
null <code>namespaceSelector</code> matches the namespace of the Pod where the rule is defined.</p><h4 id="matchlabelkeys">matchLabelKeys</h4><div class="feature-state-notice feature-stable" title="Feature Gate: MatchLabelKeysInPodAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The <code>matchLabelKeys</code> field is a beta-level field and is enabled by default in
Kubernetes 1.34.
When you want to disable it, you have to disable it explicitly via the
<code>MatchLabelKeysInPodAffinity</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p></div><p>Kubernetes includes an optional <code>matchLabelKeys</code> field for Pod affinity
or anti-affinity. The field specifies keys for the labels that should match with the incoming Pod's labels,
when satisfying the Pod (anti)affinity.</p><p>The keys are used to look up values from the Pod labels; those key-value labels are combined
(using <code>AND</code>) with the match restrictions defined using the <code>labelSelector</code> field. The combined
filtering selects the set of existing Pods that will be taken into Pod (anti)affinity calculation.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>It's not recommended to use <code>matchLabelKeys</code> with labels that might be updated directly on pods.
Even if you edit the pod's label that is specified at <code>matchLabelKeys</code> <strong>directly</strong>, (that is, not via a deployment),
kube-apiserver doesn't reflect the label update onto the merged <code>labelSelector</code>.</div><p>A common use case is to use <code>matchLabelKeys</code> with <code>pod-template-hash</code> (set on Pods
managed as part of a Deployment, where the value is unique for each revision).
Using <code>pod-template-hash</code> in <code>matchLabelKeys</code> allows you to target the Pods that belong
to the same revision as the incoming Pod, so that a rolling upgrade won't break affinity.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>application-server<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>affinity</span>:<span>
</span></span></span><span><span><span>        </span><span>podAffinity</span>:<span>
</span></span></span><span><span><span>          </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>          </span>- <span>labelSelector</span>:<span>
</span></span></span><span><span><span>              </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>              </span>- <span>key</span>:<span> </span>app<span>
</span></span></span><span><span><span>                </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>                </span><span>values</span>:<span>
</span></span></span><span><span><span>                </span>- database<span>
</span></span></span><span><span><span>            </span><span>topologyKey</span>:<span> </span>topology.kubernetes.io/zone<span>
</span></span></span><span><span><span>            </span><span># Only Pods from a given rollout are taken into consideration when calculating pod affinity.</span><span>
</span></span></span><span><span><span>            </span><span># If you update the Deployment, the replacement Pods follow their own affinity rules</span><span>
</span></span></span><span><span><span>            </span><span># (if there are any defined in the new Pod template)</span><span>
</span></span></span><span><span><span>            </span><span>matchLabelKeys</span>:<span>
</span></span></span><span><span><span>            </span>- pod-template-hash<span>
</span></span></span></code></pre></div><h4 id="mismatchlabelkeys">mismatchLabelKeys</h4><div class="feature-state-notice feature-stable" title="Feature Gate: MatchLabelKeysInPodAffinity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [stable]</code> (enabled by default: true)</div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The <code>mismatchLabelKeys</code> field is a beta-level field and is enabled by default in
Kubernetes 1.34.
When you want to disable it, you have to disable it explicitly via the
<code>MatchLabelKeysInPodAffinity</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p></div><p>Kubernetes includes an optional <code>mismatchLabelKeys</code> field for Pod affinity
or anti-affinity. The field specifies keys for the labels that should not match with the incoming Pod's labels,
when satisfying the Pod (anti)affinity.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>It's not recommended to use <code>mismatchLabelKeys</code> with labels that might be updated directly on pods.
Even if you edit the pod's label that is specified at <code>mismatchLabelKeys</code> <strong>directly</strong>, (that is, not via a deployment),
kube-apiserver doesn't reflect the label update onto the merged <code>labelSelector</code>.</div><p>One example use case is to ensure Pods go to the topology domain (node, zone, etc) where only Pods from the same tenant or team are scheduled in.
In other words, you want to avoid running Pods from two different tenants on the same topology domain at the same time.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span># Assume that all relevant Pods have a "tenant" label set</span><span>
</span></span></span><span><span><span>    </span><span>tenant</span>:<span> </span>tenant-a<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>affinity</span>:<span>
</span></span></span><span><span><span>    </span><span>podAffinity</span>:<span>
</span></span></span><span><span><span>      </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>      </span><span># ensure that Pods associated with this tenant land on the correct node pool</span><span>
</span></span></span><span><span><span>      </span>- <span>matchLabelKeys</span>:<span>
</span></span></span><span><span><span>          </span>- tenant<span>
</span></span></span><span><span><span>        </span><span>topologyKey</span>:<span> </span>node-pool<span>
</span></span></span><span><span><span>    </span><span>podAntiAffinity</span>:<span>
</span></span></span><span><span><span>      </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>      </span><span># ensure that Pods associated with this tenant can't schedule to nodes used for another tenant</span><span>
</span></span></span><span><span><span>      </span>- <span>mismatchLabelKeys</span>:<span>
</span></span></span><span><span><span>        </span>- tenant<span> </span><span># whatever the value of the "tenant" label for this Pod, prevent</span><span>
</span></span></span><span><span><span>                 </span><span># scheduling to nodes in any pool where any Pod from a different</span><span>
</span></span></span><span><span><span>                 </span><span># tenant is running.</span><span>
</span></span></span><span><span><span>        </span><span>labelSelector</span>:<span>
</span></span></span><span><span><span>          </span><span># We have to have the labelSelector which selects only Pods with the tenant label,</span><span>
</span></span></span><span><span><span>          </span><span># otherwise this Pod would have anti-affinity against Pods from daemonsets as well, for example,</span><span>
</span></span></span><span><span><span>          </span><span># which aren't supposed to have the tenant label.</span><span>
</span></span></span><span><span><span>          </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>tenant<span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>Exists<span>
</span></span></span><span><span><span>        </span><span>topologyKey</span>:<span> </span>node-pool<span>
</span></span></span></code></pre></div><h4 id="more-practical-use-cases">More practical use-cases</h4><p>Inter-pod affinity and anti-affinity can be even more useful when they are used with higher
level collections such as ReplicaSets, StatefulSets, Deployments, etc. These
rules allow you to configure that a set of workloads should
be co-located in the same defined topology; for example, preferring to place two related
Pods onto the same node.</p><p>For example: imagine a three-node cluster. You use the cluster to run a web application
and also an in-memory cache (such as Redis). For this example, also assume that latency between
the web application and the memory cache should be as low as is practical. You could use inter-pod
affinity and anti-affinity to co-locate the web servers with the cache as much as possible.</p><p>In the following example Deployment for the Redis cache, the replicas get the label <code>app=store</code>. The
<code>podAntiAffinity</code> rule tells the scheduler to avoid placing multiple replicas
with the <code>app=store</code> label on a single node. This creates each cache in a
separate node.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>redis-cache<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>store<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>store<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>affinity</span>:<span>
</span></span></span><span><span><span>        </span><span>podAntiAffinity</span>:<span>
</span></span></span><span><span><span>          </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>          </span>- <span>labelSelector</span>:<span>
</span></span></span><span><span><span>              </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>              </span>- <span>key</span>:<span> </span>app<span>
</span></span></span><span><span><span>                </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>                </span><span>values</span>:<span>
</span></span></span><span><span><span>                </span>- store<span>
</span></span></span><span><span><span>            </span><span>topologyKey</span>:<span> </span><span>"kubernetes.io/hostname"</span><span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>redis-server<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>redis:3.2-alpine<span>
</span></span></span></code></pre></div><p>The following example Deployment for the web servers creates replicas with the label <code>app=web-store</code>.
The Pod affinity rule tells the scheduler to place each replica on a node that has a Pod
with the label <code>app=store</code>. The Pod anti-affinity rule tells the scheduler never to place
multiple <code>app=web-store</code> servers on a single node.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>web-server<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>web-store<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>web-store<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>affinity</span>:<span>
</span></span></span><span><span><span>        </span><span>podAntiAffinity</span>:<span>
</span></span></span><span><span><span>          </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>          </span>- <span>labelSelector</span>:<span>
</span></span></span><span><span><span>              </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>              </span>- <span>key</span>:<span> </span>app<span>
</span></span></span><span><span><span>                </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>                </span><span>values</span>:<span>
</span></span></span><span><span><span>                </span>- web-store<span>
</span></span></span><span><span><span>            </span><span>topologyKey</span>:<span> </span><span>"kubernetes.io/hostname"</span><span>
</span></span></span><span><span><span>        </span><span>podAffinity</span>:<span>
</span></span></span><span><span><span>          </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>          </span>- <span>labelSelector</span>:<span>
</span></span></span><span><span><span>              </span><span>matchExpressions</span>:<span>
</span></span></span><span><span><span>              </span>- <span>key</span>:<span> </span>app<span>
</span></span></span><span><span><span>                </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>                </span><span>values</span>:<span>
</span></span></span><span><span><span>                </span>- store<span>
</span></span></span><span><span><span>            </span><span>topologyKey</span>:<span> </span><span>"kubernetes.io/hostname"</span><span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>web-app<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>nginx:1.16-alpine<span>
</span></span></span></code></pre></div><p>Creating the two preceding Deployments results in the following cluster layout,
where each web server is co-located with a cache, on three separate nodes.</p><table><thead><tr><th>node-1</th><th>node-2</th><th>node-3</th></tr></thead><tbody><tr><td><em>webserver-1</em></td><td><em>webserver-2</em></td><td><em>webserver-3</em></td></tr><tr><td><em>cache-1</em></td><td><em>cache-2</em></td><td><em>cache-3</em></td></tr></tbody></table><p>The overall effect is that each cache instance is likely to be accessed by a single client that
is running on the same node. This approach aims to minimize both skew (imbalanced load) and latency.</p><p>You might have other reasons to use Pod anti-affinity.
See the <a href="/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure">ZooKeeper tutorial</a>
for an example of a StatefulSet configured with anti-affinity for high
availability, using the same technique as this example.</p><h2 id="nodename">nodeName</h2><p><code>nodeName</code> is a more direct form of node selection than affinity or
<code>nodeSelector</code>. <code>nodeName</code> is a field in the Pod spec. If the <code>nodeName</code> field
is not empty, the scheduler ignores the Pod and the kubelet on the named node
tries to place the Pod on that node. Using <code>nodeName</code> overrules using
<code>nodeSelector</code> or affinity and anti-affinity rules.</p><p>Some of the limitations of using <code>nodeName</code> to select nodes are:</p><ul><li>If the named node does not exist, the Pod will not run, and in
some cases may be automatically deleted.</li><li>If the named node does not have the resources to accommodate the
Pod, the Pod will fail and its reason will indicate why,
for example OutOfmemory or OutOfcpu.</li><li>Node names in cloud environments are not always predictable or stable.</li></ul><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4><code>nodeName</code> is intended for use by custom schedulers or advanced use cases where
you need to bypass any configured schedulers. Bypassing the schedulers might lead to
failed Pods if the assigned Nodes get oversubscribed. You can use <a href="#node-affinity">node affinity</a>
or the <a href="#nodeselector"><code>nodeSelector</code> field</a> to assign a Pod to a specific Node without bypassing the schedulers.</div><p>Here is an example of a Pod spec using the <code>nodeName</code> field:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>nodeName</span>:<span> </span>kube-01<span>
</span></span></span></code></pre></div><p>The above Pod will only run on the node <code>kube-01</code>.</p><h2 id="nominatednodename">nominatedNodeName</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: NominatedNodeNameForExpectation"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p><code>nominatedNodeName</code> can be used for external components to nominate node for a pending pod.
This nomination is best effort: it might be ignored if the scheduler determines the pod cannot go to a nominated node.</p><p>Also, this field can be (over)written by the scheduler:</p><ul><li>If the scheduler finds a node to nominate via the preemption.</li><li>If the scheduler decides where the pod is going, and move it to the binding cycle.<ul><li>Note that, in this case, <code>nominatedNodeName</code> is put only when the pod has to go through <code>WaitOnPermit</code> or <code>PreBind</code> extension points.</li></ul></li></ul><p>Here is an example of a Pod status using the <code>nominatedNodeName</code> field:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>nominatedNodeName</span>:<span> </span>kube-01<span>
</span></span></span></code></pre></div><h2 id="pod-topology-spread-constraints">Pod topology spread constraints</h2><p>You can use <em>topology spread constraints</em> to control how <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a>
are spread across your cluster among failure-domains such as regions, zones, nodes, or among any other
topology domains that you define. You might do this to improve performance, expected availability, or
overall utilization.</p><p>Read <a href="/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod topology spread constraints</a>
to learn more about how these work.</p><h2 id="operators">Operators</h2><p>The following are all the logical operators that you can use in the <code>operator</code> field for <code>nodeAffinity</code> and <code>podAffinity</code> mentioned above.</p><table><thead><tr><th>Operator</th><th>Behavior</th></tr></thead><tbody><tr><td><code>In</code></td><td>The label value is present in the supplied set of strings</td></tr><tr><td><code>NotIn</code></td><td>The label value is not contained in the supplied set of strings</td></tr><tr><td><code>Exists</code></td><td>A label with this key exists on the object</td></tr><tr><td><code>DoesNotExist</code></td><td>No label with this key exists on the object</td></tr></tbody></table><p>The following operators can only be used with <code>nodeAffinity</code>.</p><table><thead><tr><th>Operator</th><th>Behavior</th></tr></thead><tbody><tr><td><code>Gt</code></td><td>The field value will be parsed as an integer, and that integer is less than the integer that results from parsing the value of a label named by this selector</td></tr><tr><td><code>Lt</code></td><td>The field value will be parsed as an integer, and that integer is greater than the integer that results from parsing the value of a label named by this selector</td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>Gt</code> and <code>Lt</code> operators will not work with non-integer values. If the given value
doesn't parse as an integer, the Pod will fail to get scheduled. Also, <code>Gt</code> and <code>Lt</code>
are not available for <code>podAffinity</code>.</div><h2 id="what-s-next">What's next</h2><ul><li>Read more about <a href="/docs/concepts/scheduling-eviction/taint-and-toleration/">taints and tolerations</a>.</li><li>Read the design docs for <a href="https://git.k8s.io/design-proposals-archive/scheduling/nodeaffinity.md">node affinity</a>
and for <a href="https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md">inter-pod affinity/anti-affinity</a>.</li><li>Learn about how the <a href="/docs/tasks/administer-cluster/topology-manager/">topology manager</a> takes part in node-level
resource allocation decisions.</li><li>Learn how to use <a href="/docs/tasks/configure-pod-container/assign-pods-nodes/">nodeSelector</a>.</li><li>Learn how to use <a href="/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/">affinity and anti-affinity</a>.</li></ul></div></div><div><div class="td-content"><h1>Pod Overhead</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>When you run a Pod on a Node, the Pod itself takes an amount of system resources. These
resources are additional to the resources needed to run the container(s) inside the Pod.
In Kubernetes, <em>Pod Overhead</em> is a way to account for the resources consumed by the Pod
infrastructure on top of the container requests &amp; limits.</p><p>In Kubernetes, the Pod's overhead is set at
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">admission</a>
time according to the overhead associated with the Pod's
<a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a>.</p><p>A pod's overhead is considered in addition to the sum of container resource requests when
scheduling a Pod. Similarly, the kubelet will include the Pod overhead when sizing the Pod cgroup,
and when carrying out Pod eviction ranking.</p><h2 id="set-up">Configuring Pod overhead</h2><p>You need to make sure a <code>RuntimeClass</code> is utilized which defines the <code>overhead</code> field.</p><h2 id="usage-example">Usage example</h2><p>To work with Pod overhead, you need a RuntimeClass that defines the <code>overhead</code> field. As
an example, you could use the following RuntimeClass definition with a virtualization container
runtime (in this example, Kata Containers combined with the Firecracker virtual machine monitor)
that uses around 120MiB per Pod for the virtual machine and the guest OS:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span># You need to change this example to match the actual runtime name, and per-Pod</span><span>
</span></span></span><span><span><span></span><span># resource overhead, that the container runtime is adding in your cluster.</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>node.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>RuntimeClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>kata-fc<span>
</span></span></span><span><span><span></span><span>handler</span>:<span> </span>kata-fc<span>
</span></span></span><span><span><span></span><span>overhead</span>:<span>
</span></span></span><span><span><span>  </span><span>podFixed</span>:<span>
</span></span></span><span><span><span>    </span><span>memory</span>:<span> </span><span>"120Mi"</span><span>
</span></span></span><span><span><span>    </span><span>cpu</span>:<span> </span><span>"250m"</span><span>
</span></span></span></code></pre></div><p>Workloads which are created which specify the <code>kata-fc</code> RuntimeClass handler will take the memory and
cpu overheads into account for resource quota calculations, node scheduling, as well as Pod cgroup sizing.</p><p>Consider running the given example workload, test-pod:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>runtimeClassName</span>:<span> </span>kata-fc<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>busybox-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>stdin</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>tty</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>500m<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx-ctr<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>resources</span>:<span>
</span></span></span><span><span><span>      </span><span>limits</span>:<span>
</span></span></span><span><span><span>        </span><span>cpu</span>:<span> </span>1500m<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span> </span>100Mi<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If only <code>limits</code> are specified in the pod definition, kubelet will deduce <code>requests</code> from those limits and set them to be the same as the defined <code>limits</code>.</div><p>At admission time the RuntimeClass <a href="/docs/reference/access-authn-authz/admission-controllers/">admission controller</a>
updates the workload's PodSpec to include the <code>overhead</code> as described in the RuntimeClass. If the PodSpec already has this field defined,
the Pod will be rejected. In the given example, since only the RuntimeClass name is specified, the admission controller mutates the Pod
to include an <code>overhead</code>.</p><p>After the RuntimeClass admission controller has made modifications, you can check the updated
Pod overhead value:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get pod test-pod -o <span>jsonpath</span><span>=</span><span>'{.spec.overhead}'</span>
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code>map[cpu:250m memory:120Mi]
</code></pre><p>If a <a href="/docs/concepts/policy/resource-quotas/">ResourceQuota</a> is defined, the sum of container requests as well as the
<code>overhead</code> field are counted.</p><p>When the kube-scheduler is deciding which node should run a new Pod, the scheduler considers that Pod's
<code>overhead</code> as well as the sum of container requests for that Pod. For this example, the scheduler adds the
requests and the overhead, then looks for a node that has 2.25 CPU and 320 MiB of memory available.</p><p>Once a Pod is scheduled to a node, the kubelet on that node creates a new <a class="glossary-tooltip" title="A group of Linux processes with optional resource isolation, accounting and limits." href="/docs/reference/glossary/?all=true#term-cgroup" target="_blank">cgroup</a> for the Pod. It is within this pod that the underlying
container runtime will create containers.</p><p>If the resource has a limit defined for each container (Guaranteed QoS or Burstable QoS with limits defined),
the kubelet will set an upper limit for the pod cgroup associated with that resource (cpu.cfs_quota_us for CPU
and memory.limit_in_bytes memory). This upper limit is based on the sum of the container limits plus the <code>overhead</code>
defined in the PodSpec.</p><p>For CPU, if the Pod is Guaranteed or Burstable QoS, the kubelet will set <code>cpu.shares</code> based on the
sum of container requests plus the <code>overhead</code> defined in the PodSpec.</p><p>Looking at our example, verify the container requests for the workload:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get pod test-pod -o <span>jsonpath</span><span>=</span><span>'{.spec.containers[*].resources.limits}'</span>
</span></span></code></pre></div><p>The total container requests are 2000m CPU and 200MiB of memory:</p><pre tabindex="0"><code>map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]
</code></pre><p>Check this against what is observed by the node:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl describe node | grep test-pod -B2
</span></span></code></pre></div><p>The output shows requests for 2250m CPU, and for 320MiB of memory. The requests include Pod overhead:</p><pre tabindex="0"><code>  Namespace    Name       CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------    ----       ------------  ----------   ---------------  -------------  ---
  default      test-pod   2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m
</code></pre><h2 id="verify-pod-cgroup-limits">Verify Pod cgroup limits</h2><p>Check the Pod's memory cgroups on the node where the workload is running. In the following example,
<a href="https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md"><code>crictl</code></a>
is used on the node, which provides a CLI for CRI-compatible container runtimes. This is an
advanced example to show Pod overhead behavior, and it is not expected that users should need to check
cgroups directly on the node.</p><p>First, on the particular node, determine the Pod identifier:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># Run this on the node where the Pod is scheduled</span>
</span></span><span><span><span>POD_ID</span><span>=</span><span>"</span><span>$(</span>sudo crictl pods --name test-pod -q<span>)</span><span>"</span>
</span></span></code></pre></div><p>From this, you can determine the cgroup path for the Pod:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># Run this on the node where the Pod is scheduled</span>
</span></span><span><span>sudo crictl inspectp -o<span>=</span>json <span>$POD_ID</span> | grep cgroupsPath
</span></span></code></pre></div><p>The resulting cgroup path includes the Pod's <code>pause</code> container. The Pod level cgroup is one directory above.</p><pre tabindex="0"><code>  "cgroupsPath": "/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a"
</code></pre><p>In this specific case, the pod cgroup path is <code>kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2</code>.
Verify the Pod level cgroup setting for memory:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span><span># Run this on the node where the Pod is scheduled.</span>
</span></span><span><span><span># Also, change the name of the cgroup to match the cgroup allocated for your pod.</span>
</span></span><span><span> cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes
</span></span></code></pre></div><p>This is 320 MiB, as expected:</p><pre tabindex="0"><code>335544320
</code></pre><h3 id="observability">Observability</h3><p>Some <code>kube_pod_overhead_*</code> metrics are available in <a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a>
to help identify when Pod overhead is being utilized and to help observe stability of workloads
running with a defined overhead.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a></li><li>Read the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead">PodOverhead Design</a>
enhancement proposal for extra context</li></ul></div></div><div><div class="td-content"><h1>Pod Scheduling Readiness</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [stable]</code></div><p>Pods were considered ready for scheduling once created. Kubernetes scheduler
does its due diligence to find nodes to place all pending Pods. However, in a
real-world case, some Pods may stay in a "miss-essential-resources" state for a long period.
These Pods actually churn the scheduler (and downstream integrators like Cluster AutoScaler)
in an unnecessary manner.</p><p>By specifying/removing a Pod's <code>.spec.schedulingGates</code>, you can control when a Pod is ready
to be considered for scheduling.</p><h2 id="configuring-pod-schedulinggates">Configuring Pod schedulingGates</h2><p>The <code>schedulingGates</code> field contains a list of strings, and each string literal is perceived as a
criteria that Pod should be satisfied before considered schedulable. This field can be initialized
only when a Pod is created (either by the client, or mutated during admission). After creation,
each schedulingGate can be removed in arbitrary order, but addition of a new scheduling gate is disallowed.</p><figure class="diagram-large"><a href="https://mermaid.live/edit#pako:eNplkktTwyAUhf8KgzuHWpukaYszutGlK3caFxQuCVMCGSDVTKf_XfKyPlhxz4HDB9wT5lYAptgHFuBRsdKxenFMClMYFIdfUdRYgbiD6ItJTEbR8wpEq5UpUfnDTf-5cbPoJjcbXdcaE61RVJIiqJvQ_Y30D-OCt-t3tFjcR5wZayiVnIGmkv4NiEfX9jijKTmmRH5jf0sRugOP0HyHUc1m6KGMFP27cM28fwSJDluPpNKaXqVJzmFNfHD2APRKSjnNFx9KhIpmzSfhVls3eHdTRrwG8QnxKfEZUUNeYTDBNbiaKRF_5dSfX-BQQQ0FpnEqQLJWhwIX5hyXsjbYl85wTINrgeC2EZd_xFQy7b_VJ6GCdd-itkxALE84dE3fAqXyIUZya6Qqe711OspVCI2ny2Vv35QqVO3-htt66ZWomAvVcZcv8yTfsiSFfJOydZoKvl_ttjLJVlJsblcJw-czwQ0zr9ZeqGDgeR77b2jD8xdtjtDn"><img src="/docs/images/podSchedulingGates.svg" alt="pod-scheduling-gates-diagram"></a><figcaption><p>Figure. Pod SchedulingGates</p></figcaption></figure><h2 id="usage-example">Usage example</h2><p>To mark a Pod not-ready for scheduling, you can create it with one or more scheduling gates like this:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-scheduling-gates.yaml"><code>pods/pod-with-scheduling-gates.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-with-scheduling-gates.yaml to clipboard"></div><div class="includecode" id="pods-pod-with-scheduling-gates-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>schedulingGates</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example.com/foo<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>example.com/bar<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pause<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.6<span>
</span></span></span></code></pre></div></div></div><p>After the Pod's creation, you can check its state using:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get pod test-pod
</span></span></code></pre></div><p>The output reveals it's in <code>SchedulingGated</code> state:</p><pre tabindex="0"><code class="language-none">NAME       READY   STATUS            RESTARTS   AGE
test-pod   0/1     SchedulingGated   0          7s
</code></pre><p>You can also check its <code>schedulingGates</code> field by running:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get pod test-pod -o <span>jsonpath</span><span>=</span><span>'{.spec.schedulingGates}'</span>
</span></span></code></pre></div><p>The output is:</p><pre tabindex="0"><code class="language-none">[{"name":"example.com/foo"},{"name":"example.com/bar"}]
</code></pre><p>To inform scheduler this Pod is ready for scheduling, you can remove its <code>schedulingGates</code> entirely
by reapplying a modified manifest:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-without-scheduling-gates.yaml"><code>pods/pod-without-scheduling-gates.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-without-scheduling-gates.yaml to clipboard"></div><div class="includecode" id="pods-pod-without-scheduling-gates-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>test-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pause<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.6<span>
</span></span></span></code></pre></div></div></div><p>You can check if the <code>schedulingGates</code> is cleared by running:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get pod test-pod -o <span>jsonpath</span><span>=</span><span>'{.spec.schedulingGates}'</span>
</span></span></code></pre></div><p>The output is expected to be empty. And you can check its latest status by running:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get pod test-pod -o wide
</span></span></code></pre></div><p>Given the test-pod doesn't request any CPU/memory resources, it's expected that this Pod's state get
transited from previous <code>SchedulingGated</code> to <code>Running</code>:</p><pre tabindex="0"><code class="language-none">NAME       READY   STATUS    RESTARTS   AGE   IP         NODE
test-pod   1/1     Running   0          15s   10.0.0.4   node-2
</code></pre><h2 id="observability">Observability</h2><p>The metric <code>scheduler_pending_pods</code> comes with a new label <code>"gated"</code> to distinguish whether a Pod
has been tried scheduling but claimed as unschedulable, or explicitly marked as not ready for
scheduling. You can use <code>scheduler_pending_pods{queue="gated"}</code> to check the metric result.</p><h2 id="mutable-pod-scheduling-directives">Mutable Pod scheduling directives</h2><p>You can mutate scheduling directives of Pods while they have scheduling gates, with certain constraints.
At a high level, you can only tighten the scheduling directives of a Pod. In other words, the updated
directives would cause the Pods to only be able to be scheduled on a subset of the nodes that it would
previously match. More concretely, the rules for updating a Pod's scheduling directives are as follows:</p><ol><li><p>For <code>.spec.nodeSelector</code>, only additions are allowed. If absent, it will be allowed to be set.</p></li><li><p>For <code>spec.affinity.nodeAffinity</code>, if nil, then setting anything is allowed.</p></li><li><p>If <code>NodeSelectorTerms</code> was empty, it will be allowed to be set.
If not empty, then only additions of <code>NodeSelectorRequirements</code> to <code>matchExpressions</code>
or <code>fieldExpressions</code> are allowed, and no changes to existing <code>matchExpressions</code>
and <code>fieldExpressions</code> will be allowed. This is because the terms in
<code>.requiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms</code>, are ORed
while the expressions in <code>nodeSelectorTerms[].matchExpressions</code> and
<code>nodeSelectorTerms[].fieldExpressions</code> are ANDed.</p></li><li><p>For <code>.preferredDuringSchedulingIgnoredDuringExecution</code>, all updates are allowed.
This is because preferred terms are not authoritative, and so policy controllers
don't validate those terms.</p></li></ol><h2 id="what-s-next">What's next</h2><ul><li>Read the <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/3521-pod-scheduling-readiness">PodSchedulingReadiness KEP</a> for more details</li></ul></div></div><div><div class="td-content"><h1>Pod Topology Spread Constraints</h1><p>You can use <em>topology spread constraints</em> to control how
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> are spread across your cluster
among failure-domains such as regions, zones, nodes, and other user-defined topology
domains. This can help to achieve high availability as well as efficient resource
utilization.</p><p>You can set <a href="#cluster-level-default-constraints">cluster-level constraints</a> as a default,
or configure topology spread constraints for individual workloads.</p><h2 id="motivation">Motivation</h2><p>Imagine that you have a cluster of up to twenty nodes, and you want to run a
<a class="glossary-tooltip" title="A workload is an application running on Kubernetes." href="/docs/concepts/workloads/" target="_blank">workload</a>
that automatically scales how many replicas it uses. There could be as few as
two Pods or as many as fifteen.
When there are only two Pods, you'd prefer not to have both of those Pods run on the
same node: you would run the risk that a single node failure takes your workload
offline.</p><p>In addition to this basic usage, there are some advanced usage examples that
enable your workloads to benefit on high availability and cluster utilization.</p><p>As you scale up and run more Pods, a different concern becomes important. Imagine
that you have three nodes running five Pods each. The nodes have enough capacity
to run that many replicas; however, the clients that interact with this workload
are split across three different datacenters (or infrastructure zones). Now you
have less concern about a single node failure, but you notice that latency is
higher than you'd like, and you are paying for network costs associated with
sending network traffic between the different zones.</p><p>You decide that under normal operation you'd prefer to have a similar number of replicas
<a href="/docs/concepts/scheduling-eviction/">scheduled</a> into each infrastructure zone,
and you'd like the cluster to self-heal in the case that there is a problem.</p><p>Pod topology spread constraints offer you a declarative way to configure that.</p><h2 id="topologyspreadconstraints-field"><code>topologySpreadConstraints</code> field</h2><p>The Pod API includes a field, <code>spec.topologySpreadConstraints</code>. The usage of this field looks like
the following:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span># Configure a topology spread constraint</span><span>
</span></span></span><span><span><span>  </span><span>topologySpreadConstraints</span>:<span>
</span></span></span><span><span><span>    </span>- <span>maxSkew</span>:<span> </span>&lt;integer&gt;<span>
</span></span></span><span><span><span>      </span><span>minDomains</span>:<span> </span>&lt;integer&gt;<span> </span><span># optional</span><span>
</span></span></span><span><span><span>      </span><span>topologyKey</span>:<span> </span>&lt;string&gt;<span>
</span></span></span><span><span><span>      </span><span>whenUnsatisfiable</span>:<span> </span>&lt;string&gt;<span>
</span></span></span><span><span><span>      </span><span>labelSelector</span>:<span> </span>&lt;object&gt;<span>
</span></span></span><span><span><span>      </span><span>matchLabelKeys</span>:<span> </span>&lt;list&gt;<span> </span><span># optional; beta since v1.27</span><span>
</span></span></span><span><span><span>      </span><span>nodeAffinityPolicy</span>:<span> </span>[Honor|Ignore]<span> </span><span># optional; beta since v1.26</span><span>
</span></span></span><span><span><span>      </span><span>nodeTaintsPolicy</span>:<span> </span>[Honor|Ignore]<span> </span><span># optional; beta since v1.26</span><span>
</span></span></span><span><span><span>  </span><span>### other Pod fields go here</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>There can only be one <code>topologySpreadConstraint</code> for a given <code>topologyKey</code> and <code>whenUnsatisfiable</code> value. For example, if you have defined a <code>topologySpreadConstraint</code> that uses the <code>topologyKey</code> "kubernetes.io/hostname" and <code>whenUnsatisfiable</code> value "DoNotSchedule", you can only add another <code>topologySpreadConstraint</code> for the <code>topologyKey</code> "kubernetes.io/hostname" if you use a different <code>whenUnsatisfiable</code> value.</div><p>You can read more about this field by running <code>kubectl explain Pod.spec.topologySpreadConstraints</code> or
refer to the <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling">scheduling</a> section of the API reference for Pod.</p><h3 id="spread-constraint-definition">Spread constraint definition</h3><p>You can define one or multiple <code>topologySpreadConstraints</code> entries to instruct the
kube-scheduler how to place each incoming Pod in relation to the existing Pods across
your cluster. Those fields are:</p><ul><li><p><strong>maxSkew</strong> describes the degree to which Pods may be unevenly distributed. You must
specify this field and the number must be greater than zero. Its semantics differ
according to the value of <code>whenUnsatisfiable</code>:</p><ul><li>if you select <code>whenUnsatisfiable: DoNotSchedule</code>, then <code>maxSkew</code> defines the
maximum permitted difference between the number of matching pods in the target
topology and the <em>global minimum</em>
(the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains).
For example, if you have 3 zones with 2, 2 and 1 matching pods respectively,
<code>MaxSkew</code> is set to 1 then the global minimum is 1.</li><li>if you select <code>whenUnsatisfiable: ScheduleAnyway</code>, the scheduler gives higher
precedence to topologies that would help reduce the skew.</li></ul></li><li><p><strong>minDomains</strong> indicates a minimum number of eligible domains. This field is optional.
A domain is a particular instance of a topology. An eligible domain is a domain whose
nodes match the node selector.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Before Kubernetes v1.30, the <code>minDomains</code> field was only available if the
<code>MinDomainsInPodTopologySpread</code> <a href="/docs/reference/command-line-tools-reference/feature-gates-removed/">feature gate</a>
was enabled (default since v1.28). In older Kubernetes clusters it might be explicitly
disabled or the field might not be available.</div><ul><li>The value of <code>minDomains</code> must be greater than 0, when specified.
You can only specify <code>minDomains</code> in conjunction with <code>whenUnsatisfiable: DoNotSchedule</code>.</li><li>When the number of eligible domains with match topology keys is less than <code>minDomains</code>,
Pod topology spread treats global minimum as 0, and then the calculation of <code>skew</code> is performed.
The global minimum is the minimum number of matching Pods in an eligible domain,
or zero if the number of eligible domains is less than <code>minDomains</code>.</li><li>When the number of eligible domains with matching topology keys equals or is greater than
<code>minDomains</code>, this value has no effect on scheduling.</li><li>If you do not specify <code>minDomains</code>, the constraint behaves as if <code>minDomains</code> is 1.</li></ul></li><li><p><strong>topologyKey</strong> is the key of <a href="#node-labels">node labels</a>. Nodes that have a label with this key
and identical values are considered to be in the same topology.
We call each instance of a topology (in other words, a &lt;key, value&gt; pair) a domain. The scheduler
will try to put a balanced number of pods into each domain.
Also, we define an eligible domain as a domain whose nodes meet the requirements of
nodeAffinityPolicy and nodeTaintsPolicy.</p></li><li><p><strong>whenUnsatisfiable</strong> indicates how to deal with a Pod if it doesn't satisfy the spread constraint:</p><ul><li><code>DoNotSchedule</code> (default) tells the scheduler not to schedule it.</li><li><code>ScheduleAnyway</code> tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.</li></ul></li><li><p><strong>labelSelector</strong> is used to find matching Pods. Pods
that match this label selector are counted to determine the
number of Pods in their corresponding topology domain.
See <a href="/docs/concepts/overview/working-with-objects/labels/#label-selectors">Label Selectors</a>
for more details.</p></li><li><p><strong>matchLabelKeys</strong> is a list of pod label keys to select the group of pods over which
the spreading skew will be calculated. At a pod creation,
the kube-apiserver uses those keys to lookup values from the incoming pod labels,
and those key-value labels will be merged with any existing <code>labelSelector</code>.
The same key is forbidden to exist in both <code>matchLabelKeys</code> and <code>labelSelector</code>.
<code>matchLabelKeys</code> cannot be set when <code>labelSelector</code> isn't set.
Keys that don't exist in the pod labels will be ignored.
A null or empty list means only match against the <code>labelSelector</code>.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>It's not recommended to use <code>matchLabelKeys</code> with labels that might be updated directly on pods.
Even if you edit the pod's label that is specified at <code>matchLabelKeys</code> <strong>directly</strong>,
(that is, you edit the Pod and not a Deployment),
kube-apiserver doesn't reflect the label update onto the merged <code>labelSelector</code>.</div><p>With <code>matchLabelKeys</code>, you don't need to update the <code>pod.spec</code> between different revisions.
The controller/operator just needs to set different values to the same label key for different
revisions. For example, if you are configuring a Deployment, you can use the label keyed with
<a href="/docs/concepts/workloads/controllers/deployment/#pod-template-hash-label">pod-template-hash</a>, which
is added automatically by the Deployment controller, to distinguish between different revisions
in a single Deployment.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>    </span><span>topologySpreadConstraints</span>:<span>
</span></span></span><span><span><span>        </span>- <span>maxSkew</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>          </span><span>topologyKey</span>:<span> </span>kubernetes.io/hostname<span>
</span></span></span><span><span><span>          </span><span>whenUnsatisfiable</span>:<span> </span>DoNotSchedule<span>
</span></span></span><span><span><span>          </span><span>labelSelector</span>:<span>
</span></span></span><span><span><span>            </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>              </span><span>app</span>:<span> </span>foo<span>
</span></span></span><span><span><span>          </span><span>matchLabelKeys</span>:<span>
</span></span></span><span><span><span>            </span>- pod-template-hash<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The <code>matchLabelKeys</code> field is a beta-level field and enabled by default in 1.27. You can disable it by disabling the
<code>MatchLabelKeysInPodTopologySpread</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</p><p>Before v1.34, <code>matchLabelKeys</code> was handled implicitly.
Since v1.34, key-value labels corresponding to <code>matchLabelKeys</code> are explicitly merged into <code>labelSelector</code>.
You can disable it and revert to the previous behavior by disabling the <code>MatchLabelKeysInPodTopologySpreadSelectorMerge</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> of kube-apiserver.</p></div></li><li><p><strong>nodeAffinityPolicy</strong> indicates how we will treat Pod's nodeAffinity/nodeSelector
when calculating pod topology spread skew. Options are:</p><ul><li>Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations.</li><li>Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations.</li></ul><p>If this value is null, the behavior is equivalent to the Honor policy.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>nodeAffinityPolicy</code> became beta in 1.26 and graduated to GA in 1.33.
It's enabled by default in beta, you can disable it by disabling the
<code>NodeInclusionPolicyInPodTopologySpread</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</div></li><li><p><strong>nodeTaintsPolicy</strong> indicates how we will treat node taints when calculating
pod topology spread skew. Options are:</p><ul><li>Honor: nodes without taints, along with tainted nodes for which the incoming pod
has a toleration, are included.</li><li>Ignore: node taints are ignored. All nodes are included.</li></ul><p>If this value is null, the behavior is equivalent to the Ignore policy.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>nodeTaintsPolicy</code> became beta in 1.26 and graduated to GA in 1.33.
It's enabled by default in beta, you can disable it by disabling the
<code>NodeInclusionPolicyInPodTopologySpread</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.</div></li></ul><p>When a Pod defines more than one <code>topologySpreadConstraint</code>, those constraints are
combined using a logical AND operation: the kube-scheduler looks for a node for the incoming Pod
that satisfies all the configured constraints.</p><h3 id="node-labels">Node labels</h3><p>Topology spread constraints rely on node labels to identify the topology
domain(s) that each <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a> is in.
For example, a node might have labels:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>  </span><span>region</span>:<span> </span>us-east-1<span>
</span></span></span><span><span><span>  </span><span>zone</span>:<span> </span>us-east-1a<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>For brevity, this example doesn't use the
<a href="/docs/reference/labels-annotations-taints/">well-known</a> label keys
<code>topology.kubernetes.io/zone</code> and <code>topology.kubernetes.io/region</code>. However,
those registered label keys are nonetheless recommended rather than the private
(unqualified) label keys <code>region</code> and <code>zone</code> that are used here.</p><p>You can't make a reliable assumption about the meaning of a private label key
between different contexts.</p></div><p>Suppose you have a 4-node cluster with the following labels:</p><pre tabindex="0"><code>NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre><p>Then the cluster is logically viewed as below:</p><figure><div class="mermaid">graph TB
subgraph "zoneB"
n3(Node3)
n4(Node4)
end
subgraph "zoneA"
n1(Node1)
n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><h2 id="consistency">Consistency</h2><p>You should set the same Pod topology spread constraints on all pods in a group.</p><p>Usually, if you are using a workload controller such as a Deployment, the pod template
takes care of this for you. If you mix different spread constraints then Kubernetes
follows the API definition of the field; however, the behavior is more likely to become
confusing and troubleshooting is less straightforward.</p><p>You need a mechanism to ensure that all the nodes in a topology domain (such as a
cloud provider region) are labeled consistently.
To avoid you needing to manually label nodes, most clusters automatically
populate well-known labels such as <code>kubernetes.io/hostname</code>. Check whether
your cluster supports this.</p><h2 id="topology-spread-constraint-examples">Topology spread constraint examples</h2><h3 id="example-one-topologyspreadconstraint">Example: one topology spread constraint</h3><p>Suppose you have a 4-node cluster where 3 Pods labeled <code>foo: bar</code> are located in
node1, node2 and node3 respectively:</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p3(Pod) --&gt; n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>If you want an incoming Pod to be evenly spread with existing Pods across zones, you
can use a manifest similar to:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/one-constraint.yaml"><code>pods/topology-spread-constraints/one-constraint.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard"></div><div class="includecode" id="pods-topology-spread-constraints-one-constraint-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span> </span>bar<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>topologySpreadConstraints</span>:<span>
</span></span></span><span><span><span>  </span>- <span>maxSkew</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>topologyKey</span>:<span> </span>zone<span>
</span></span></span><span><span><span>    </span><span>whenUnsatisfiable</span>:<span> </span>DoNotSchedule<span>
</span></span></span><span><span><span>    </span><span>labelSelector</span>:<span>
</span></span></span><span><span><span>      </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>        </span><span>foo</span>:<span> </span>bar<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pause<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><p>From that manifest, <code>topologyKey: zone</code> implies the even distribution will only be applied
to nodes that are labeled <code>zone: &lt;any value&gt;</code> (nodes that don't have a <code>zone</code> label
are skipped). The field <code>whenUnsatisfiable: DoNotSchedule</code> tells the scheduler to let the
incoming Pod stay pending if the scheduler can't find a way to satisfy the constraint.</p><p>If the scheduler placed this incoming Pod into zone <code>A</code>, the distribution of Pods would
become <code>[3, 1]</code>. That means the actual skew is then 2 (calculated as <code>3 - 1</code>), which
violates <code>maxSkew: 1</code>. To satisfy the constraints and context for this example, the
incoming Pod can only be placed onto a node in zone <code>B</code>:</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p3(Pod) --&gt; n3(Node3)
p4(mypod) --&gt; n4(Node4)
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>OR</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p3(Pod) --&gt; n3(Node3)
p4(mypod) --&gt; n3
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>You can tweak the Pod spec to meet various kinds of requirements:</p><ul><li>Change <code>maxSkew</code> to a bigger value - such as <code>2</code> - so that the incoming Pod can
be placed into zone <code>A</code> as well.</li><li>Change <code>topologyKey</code> to <code>node</code> so as to distribute the Pods evenly across nodes
instead of zones. In the above example, if <code>maxSkew</code> remains <code>1</code>, the incoming
Pod can only be placed onto the node <code>node4</code>.</li><li>Change <code>whenUnsatisfiable: DoNotSchedule</code> to <code>whenUnsatisfiable: ScheduleAnyway</code>
to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs
are satisfied). However, it's preferred to be placed into the topology domain which
has fewer matching Pods. (Be aware that this preference is jointly normalized
with other internal scheduling priorities such as resource usage ratio).</li></ul><h3 id="example-multiple-topologyspreadconstraints">Example: multiple topology spread constraints</h3><p>This builds upon the previous example. Suppose you have a 4-node cluster where 3
existing Pods labeled <code>foo: bar</code> are located on node1, node2 and node3 respectively:</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p3(Pod) --&gt; n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>You can combine two topology spread constraints to control the spread of Pods both
by node and by zone:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml"><code>pods/topology-spread-constraints/two-constraints.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard"></div><div class="includecode" id="pods-topology-spread-constraints-two-constraints-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span> </span>bar<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>topologySpreadConstraints</span>:<span>
</span></span></span><span><span><span>  </span>- <span>maxSkew</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>topologyKey</span>:<span> </span>zone<span>
</span></span></span><span><span><span>    </span><span>whenUnsatisfiable</span>:<span> </span>DoNotSchedule<span>
</span></span></span><span><span><span>    </span><span>labelSelector</span>:<span>
</span></span></span><span><span><span>      </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>        </span><span>foo</span>:<span> </span>bar<span>
</span></span></span><span><span><span>  </span>- <span>maxSkew</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>topologyKey</span>:<span> </span>node<span>
</span></span></span><span><span><span>    </span><span>whenUnsatisfiable</span>:<span> </span>DoNotSchedule<span>
</span></span></span><span><span><span>    </span><span>labelSelector</span>:<span>
</span></span></span><span><span><span>      </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>        </span><span>foo</span>:<span> </span>bar<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pause<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><p>In this case, to match the first constraint, the incoming Pod can only be placed onto
nodes in zone <code>B</code>; while in terms of the second constraint, the incoming Pod can only be
scheduled to the node <code>node4</code>. The scheduler only considers options that satisfy all
defined constraints, so the only valid placement is onto node <code>node4</code>.</p><h3 id="example-conflicting-topologyspreadconstraints">Example: conflicting topology spread constraints</h3><p>Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p4(Pod) --&gt; n3(Node3)
p5(Pod) --&gt; n3
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n1
p3(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>If you were to apply
<a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml"><code>two-constraints.yaml</code></a>
(the manifest from the previous example)
to <strong>this</strong> cluster, you would see that the Pod <code>mypod</code> stays in the <code>Pending</code> state.
This happens because: to satisfy the first constraint, the Pod <code>mypod</code> can only
be placed into zone <code>B</code>; while in terms of the second constraint, the Pod <code>mypod</code>
can only schedule to node <code>node2</code>. The intersection of the two constraints returns
an empty set, and the scheduler cannot place the Pod.</p><p>To overcome this situation, you can either increase the value of <code>maxSkew</code> or modify
one of the constraints to use <code>whenUnsatisfiable: ScheduleAnyway</code>. Depending on
circumstances, you might also decide to delete an existing Pod manually - for example,
if you are troubleshooting why a bug-fix rollout is not making progress.</p><h4 id="interaction-with-node-affinity-and-node-selectors">Interaction with node affinity and node selectors</h4><p>The scheduler will skip the non-matching nodes from the skew calculations if the
incoming Pod has <code>spec.nodeSelector</code> or <code>spec.affinity.nodeAffinity</code> defined.</p><h3 id="example-topologyspreadconstraints-with-nodeaffinity">Example: topology spread constraints with node affinity</h3><p>Suppose you have a 5-node cluster ranging across zones A to C:</p><figure><div class="mermaid">graph BT
subgraph "zoneB"
p3(Pod) --&gt; n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --&gt; n1(Node1)
p2(Pod) --&gt; n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><figure><div class="mermaid">graph BT
subgraph "zoneC"
n5(Node5)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n5 k8s;
class zoneC cluster;</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p>and you know that zone <code>C</code> must be excluded. In this case, you can compose a manifest
as below, so that Pod <code>mypod</code> will be placed into zone <code>B</code> instead of zone <code>C</code>.
Similarly, Kubernetes also respects <code>spec.nodeSelector</code>.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml"><code>pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard"></div><div class="includecode" id="pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>mypod<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>foo</span>:<span> </span>bar<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>topologySpreadConstraints</span>:<span>
</span></span></span><span><span><span>  </span>- <span>maxSkew</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>topologyKey</span>:<span> </span>zone<span>
</span></span></span><span><span><span>    </span><span>whenUnsatisfiable</span>:<span> </span>DoNotSchedule<span>
</span></span></span><span><span><span>    </span><span>labelSelector</span>:<span>
</span></span></span><span><span><span>      </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>        </span><span>foo</span>:<span> </span>bar<span>
</span></span></span><span><span><span>  </span><span>affinity</span>:<span>
</span></span></span><span><span><span>    </span><span>nodeAffinity</span>:<span>
</span></span></span><span><span><span>      </span><span>requiredDuringSchedulingIgnoredDuringExecution</span>:<span>
</span></span></span><span><span><span>        </span><span>nodeSelectorTerms</span>:<span>
</span></span></span><span><span><span>        </span>- <span>matchExpressions</span>:<span>
</span></span></span><span><span><span>          </span>- <span>key</span>:<span> </span>zone<span>
</span></span></span><span><span><span>            </span><span>operator</span>:<span> </span>NotIn<span>
</span></span></span><span><span><span>            </span><span>values</span>:<span>
</span></span></span><span><span><span>            </span>- zoneC<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>pause<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><h2 id="implicit-conventions">Implicit conventions</h2><p>There are some implicit conventions worth noting here:</p><ul><li><p>Only the Pods holding the same namespace as the incoming Pod can be matching candidates.</p></li><li><p>The scheduler only considers nodes that have all <code>topologySpreadConstraints[*].topologyKey</code> present at the same time.
Nodes missing any of these <code>topologyKeys</code> are bypassed. This implies that:</p><ol><li>any Pods located on those bypassed nodes do not impact <code>maxSkew</code> calculation - in the
above <a href="#example-conflicting-topologyspreadconstraints">example</a>, suppose the node <code>node1</code>
does not have a label "zone", then the 2 Pods will
be disregarded, hence the incoming Pod will be scheduled into zone <code>A</code>.</li><li>the incoming Pod has no chances to be scheduled onto this kind of nodes -
in the above example, suppose a node <code>node5</code> has the <strong>mistyped</strong> label <code>zone-typo: zoneC</code>
(and no <code>zone</code> label set). After node <code>node5</code> joins the cluster, it will be bypassed and
Pods for this workload aren't scheduled there.</li></ol></li><li><p>Be aware of what will happen if the incoming Pod's
<code>topologySpreadConstraints[*].labelSelector</code> doesn't match its own labels. In the
above example, if you remove the incoming Pod's labels, it can still be placed onto
nodes in zone <code>B</code>, since the constraints are still satisfied. However, after that
placement, the degree of imbalance of the cluster remains unchanged - it's still zone <code>A</code>
having 2 Pods labeled as <code>foo: bar</code>, and zone <code>B</code> having 1 Pod labeled as
<code>foo: bar</code>. If this is not what you expect, update the workload's
<code>topologySpreadConstraints[*].labelSelector</code> to match the labels in the pod template.</p></li></ul><h2 id="cluster-level-default-constraints">Cluster-level default constraints</h2><p>It is possible to set default topology spread constraints for a cluster. Default
topology spread constraints are applied to a Pod if, and only if:</p><ul><li>It doesn't define any constraints in its <code>.spec.topologySpreadConstraints</code>.</li><li>It belongs to a Service, ReplicaSet, StatefulSet or ReplicationController.</li></ul><p>Default constraints can be set as part of the <code>PodTopologySpread</code> plugin
arguments in a <a href="/docs/reference/scheduling/config/#profiles">scheduling profile</a>.
The constraints are specified with the same <a href="#topologyspreadconstraints-field">API above</a>, except that
<code>labelSelector</code> must be empty. The selectors are calculated from the Services,
ReplicaSets, StatefulSets or ReplicationControllers that the Pod belongs to.</p><p>An example configuration might look like follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubescheduler.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeSchedulerConfiguration<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>profiles</span>:<span>
</span></span></span><span><span><span>  </span>- <span>schedulerName</span>:<span> </span>default-scheduler<span>
</span></span></span><span><span><span>    </span><span>pluginConfig</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>PodTopologySpread<span>
</span></span></span><span><span><span>        </span><span>args</span>:<span>
</span></span></span><span><span><span>          </span><span>defaultConstraints</span>:<span>
</span></span></span><span><span><span>            </span>- <span>maxSkew</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>              </span><span>topologyKey</span>:<span> </span>topology.kubernetes.io/zone<span>
</span></span></span><span><span><span>              </span><span>whenUnsatisfiable</span>:<span> </span>ScheduleAnyway<span>
</span></span></span><span><span><span>          </span><span>defaultingType</span>:<span> </span>List<span>
</span></span></span></code></pre></div><h3 id="internal-default-constraints">Built-in default constraints</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>If you don't configure any cluster-level default constraints for pod topology spreading,
then kube-scheduler acts as if you specified the following default topology constraints:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>defaultConstraints</span>:<span>
</span></span></span><span><span><span>  </span>- <span>maxSkew</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>    </span><span>topologyKey</span>:<span> </span><span>"kubernetes.io/hostname"</span><span>
</span></span></span><span><span><span>    </span><span>whenUnsatisfiable</span>:<span> </span>ScheduleAnyway<span>
</span></span></span><span><span><span>  </span>- <span>maxSkew</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>    </span><span>topologyKey</span>:<span> </span><span>"topology.kubernetes.io/zone"</span><span>
</span></span></span><span><span><span>    </span><span>whenUnsatisfiable</span>:<span> </span>ScheduleAnyway<span>
</span></span></span></code></pre></div><p>Also, the legacy <code>SelectorSpread</code> plugin, which provides an equivalent behavior,
is disabled by default.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>The <code>PodTopologySpread</code> plugin does not score the nodes that don't have
the topology keys specified in the spreading constraints. This might result
in a different default behavior compared to the legacy <code>SelectorSpread</code> plugin when
using the default topology constraints.</p><p>If your nodes are not expected to have <strong>both</strong> <code>kubernetes.io/hostname</code> and
<code>topology.kubernetes.io/zone</code> labels set, define your own constraints
instead of using the Kubernetes defaults.</p></div><p>If you don't want to use the default Pod spreading constraints for your cluster,
you can disable those defaults by setting <code>defaultingType</code> to <code>List</code> and leaving
empty <code>defaultConstraints</code> in the <code>PodTopologySpread</code> plugin configuration:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubescheduler.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeSchedulerConfiguration<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>profiles</span>:<span>
</span></span></span><span><span><span>  </span>- <span>schedulerName</span>:<span> </span>default-scheduler<span>
</span></span></span><span><span><span>    </span><span>pluginConfig</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>PodTopologySpread<span>
</span></span></span><span><span><span>        </span><span>args</span>:<span>
</span></span></span><span><span><span>          </span><span>defaultConstraints</span>:<span> </span>[]<span>
</span></span></span><span><span><span>          </span><span>defaultingType</span>:<span> </span>List<span>
</span></span></span></code></pre></div><h2 id="comparison-with-podaffinity-podantiaffinity">Comparison with podAffinity and podAntiAffinity</h2><p>In Kubernetes, <a href="/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">inter-Pod affinity and anti-affinity</a>
control how Pods are scheduled in relation to one another - either more packed
or more scattered.</p><dl><dt><code>podAffinity</code></dt><dd>attracts Pods; you can try to pack any number of Pods into qualifying
topology domain(s).</dd><dt><code>podAntiAffinity</code></dt><dd>repels Pods. If you set this to <code>requiredDuringSchedulingIgnoredDuringExecution</code> mode then
only a single Pod can be scheduled into a single topology domain; if you choose
<code>preferredDuringSchedulingIgnoredDuringExecution</code> then you lose the ability to enforce the
constraint.</dd></dl><p>For finer control, you can specify topology spread constraints to distribute
Pods across different topology domains - to achieve either high availability or
cost-saving. This can also help on rolling update workloads and scaling out
replicas smoothly.</p><p>For more context, see the
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation">Motivation</a>
section of the enhancement proposal about Pod topology spread constraints.</p><h2 id="known-limitations">Known limitations</h2><ul><li><p>There's no guarantee that the constraints remain satisfied when Pods are removed. For
example, scaling down a Deployment may result in imbalanced Pods distribution.</p><p>You can use a tool such as the <a href="https://github.com/kubernetes-sigs/descheduler">Descheduler</a>
to rebalance the Pods distribution.</p></li><li><p>Pods matched on tainted nodes are respected.
See <a href="https://github.com/kubernetes/kubernetes/issues/80921">Issue 80921</a>.</p></li><li><p>The scheduler doesn't have prior knowledge of all the zones or other topology
domains that a cluster has. They are determined from the existing nodes in the
cluster. This could lead to a problem in autoscaled clusters, when a node pool (or
node group) is scaled to zero nodes, and you're expecting the cluster to scale up,
because, in this case, those topology domains won't be considered until there is
at least one node in them.</p><p>You can work around this by using a Node autoscaler that is aware of
Pod topology spread constraints and is also aware of the overall set of topology
domains.</p></li><li><p>Pods that don't match their own labelSelector create "ghost pods". If a pod's
labels don't match the <code>labelSelector</code> in its topology spread constraint, the pod
won't count itself in spread calculations. This means:</p><ul><li>Multiple such pods can just accumulate on the same topology (until matching pods are newly created/deleted) because those pod's schedule don't change a spreading calculation result.</li><li>The spreading constraint works in an unintended way, most likely not matching your expectations</li></ul><p>Ensure your pod's labels match the <code>labelSelector</code> in your spread constraints.
Typically, a pod should match its own topology spread constraint selector.</p></li></ul><h2 id="what-s-next">What's next</h2><ul><li>The blog article <a href="/blog/2020/05/introducing-podtopologyspread/">Introducing PodTopologySpread</a>
explains <code>maxSkew</code> in some detail, as well as covering some advanced usage examples.</li><li>Read the <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling">scheduling</a> section of
the API reference for Pod.</li></ul></div></div><div><div class="td-content"><h1>Taints and Tolerations</h1><p><a href="/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity"><em>Node affinity</em></a>
is a property of <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> that <em>attracts</em> them to
a set of <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">nodes</a> (either as a preference or a
hard requirement). <em>Taints</em> are the opposite -- they allow a node to repel a set of pods.</p><p><em>Tolerations</em> are applied to pods. Tolerations allow the scheduler to schedule pods with matching
taints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">evaluates other parameters</a>
as part of its function.</p><p>Taints and tolerations work together to ensure that pods are not scheduled
onto inappropriate nodes. One or more taints are applied to a node; this
marks that the node should not accept any pods that do not tolerate the taints.</p><h2 id="concepts">Concepts</h2><p>You add a taint to a node using <a href="/docs/reference/generated/kubectl/kubectl-commands#taint">kubectl taint</a>.
For example,</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl taint nodes node1 <span>key1</span><span>=</span>value1:NoSchedule
</span></span></code></pre></div><p>places a taint on node <code>node1</code>. The taint has key <code>key1</code>, value <code>value1</code>, and taint effect <code>NoSchedule</code>.
This means that no pod will be able to schedule onto <code>node1</code> unless it has a matching toleration.</p><p>To remove the taint added by the command above, you can run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl taint nodes node1 <span>key1</span><span>=</span>value1:NoSchedule-
</span></span></code></pre></div><p>You specify a toleration for a pod in the PodSpec. Both of the following tolerations "match" the
taint created by the <code>kubectl taint</code> line above, and thus a pod with either toleration would be able
to schedule onto <code>node1</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>tolerations</span>:<span>
</span></span></span><span><span><span></span>- <span>key</span>:<span> </span><span>"key1"</span><span>
</span></span></span><span><span><span>  </span><span>operator</span>:<span> </span><span>"Equal"</span><span>
</span></span></span><span><span><span>  </span><span>value</span>:<span> </span><span>"value1"</span><span>
</span></span></span><span><span><span>  </span><span>effect</span>:<span> </span><span>"NoSchedule"</span><span>
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>tolerations</span>:<span>
</span></span></span><span><span><span></span>- <span>key</span>:<span> </span><span>"key1"</span><span>
</span></span></span><span><span><span>  </span><span>operator</span>:<span> </span><span>"Exists"</span><span>
</span></span></span><span><span><span>  </span><span>effect</span>:<span> </span><span>"NoSchedule"</span><span>
</span></span></span></code></pre></div><p>The default Kubernetes scheduler takes taints and tolerations into account when
selecting a node to run a particular Pod. However, if you manually specify the
<code>.spec.nodeName</code> for a Pod, that action bypasses the scheduler; the Pod is then
bound onto the node where you assigned it, even if there are <code>NoSchedule</code>
taints on that node that you selected.
If this happens and the node also has a <code>NoExecute</code> taint set, the kubelet will
eject the Pod unless there is an appropriate tolerance set.</p><p>Here's an example of a pod that has some tolerations defined:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-toleration.yaml"><code>pods/pod-with-toleration.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy pods/pod-with-toleration.yaml to clipboard"></div><div class="includecode" id="pods-pod-with-toleration-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>env</span>:<span> </span>test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span><span><span><span>  </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>  </span>- <span>key</span>:<span> </span><span>"example-key"</span><span>
</span></span></span><span><span><span>    </span><span>operator</span>:<span> </span><span>"Exists"</span><span>
</span></span></span><span><span><span>    </span><span>effect</span>:<span> </span><span>"NoSchedule"</span><span>
</span></span></span></code></pre></div></div></div><p>The default value for <code>operator</code> is <code>Equal</code>.</p><p>A toleration "matches" a taint if the keys are the same and the effects are the same, and:</p><ul><li>the <code>operator</code> is <code>Exists</code> (in which case no <code>value</code> should be specified), or</li><li>the <code>operator</code> is <code>Equal</code> and the values should be equal.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>There are two special cases:</p><p>If the <code>key</code> is empty, then the <code>operator</code> must be <code>Exists</code>, which matches all keys and values. Note that the <code>effect</code> still needs to be matched at the same time.</p><p>An empty <code>effect</code> matches all effects with key <code>key1</code>.</p></div><p>The above example used the <code>effect</code> of <code>NoSchedule</code>. Alternatively, you can use the <code>effect</code> of <code>PreferNoSchedule</code>.</p><p>The allowed values for the <code>effect</code> field are:</p><dl><dt><code>NoExecute</code></dt><dd>This affects pods that are already running on the node as follows:<ul><li>Pods that do not tolerate the taint are evicted immediately</li><li>Pods that tolerate the taint without specifying <code>tolerationSeconds</code> in
their toleration specification remain bound forever</li><li>Pods that tolerate the taint with a specified <code>tolerationSeconds</code> remain
bound for the specified amount of time. After that time elapses, the node
lifecycle controller evicts the Pods from the node.</li></ul></dd><dt><code>NoSchedule</code></dt><dd>No new Pods will be scheduled on the tainted node unless they have a matching
toleration. Pods currently running on the node are <strong>not</strong> evicted.</dd><dt><code>PreferNoSchedule</code></dt><dd><code>PreferNoSchedule</code> is a "preference" or "soft" version of <code>NoSchedule</code>.
The control plane will <em>try</em> to avoid placing a Pod that does not tolerate
the taint on the node, but it is not guaranteed.</dd></dl><p>You can put multiple taints on the same node and multiple tolerations on the same pod.
The way Kubernetes processes multiple taints and tolerations is like a filter: start
with all of a node's taints, then ignore the ones for which the pod has a matching toleration; the
remaining un-ignored taints have the indicated effects on the pod. In particular,</p><ul><li>if there is at least one un-ignored taint with effect <code>NoSchedule</code> then Kubernetes will not schedule
the pod onto that node</li><li>if there is no un-ignored taint with effect <code>NoSchedule</code> but there is at least one un-ignored taint with
effect <code>PreferNoSchedule</code> then Kubernetes will <em>try</em> to not schedule the pod onto the node</li><li>if there is at least one un-ignored taint with effect <code>NoExecute</code> then the pod will be evicted from
the node (if it is already running on the node), and will not be
scheduled onto the node (if it is not yet running on the node).</li></ul><p>For example, imagine you taint a node like this</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl taint nodes node1 <span>key1</span><span>=</span>value1:NoSchedule
</span></span><span><span>kubectl taint nodes node1 <span>key1</span><span>=</span>value1:NoExecute
</span></span><span><span>kubectl taint nodes node1 <span>key2</span><span>=</span>value2:NoSchedule
</span></span></code></pre></div><p>And a pod has two tolerations:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>tolerations</span>:<span>
</span></span></span><span><span><span></span>- <span>key</span>:<span> </span><span>"key1"</span><span>
</span></span></span><span><span><span>  </span><span>operator</span>:<span> </span><span>"Equal"</span><span>
</span></span></span><span><span><span>  </span><span>value</span>:<span> </span><span>"value1"</span><span>
</span></span></span><span><span><span>  </span><span>effect</span>:<span> </span><span>"NoSchedule"</span><span>
</span></span></span><span><span><span></span>- <span>key</span>:<span> </span><span>"key1"</span><span>
</span></span></span><span><span><span>  </span><span>operator</span>:<span> </span><span>"Equal"</span><span>
</span></span></span><span><span><span>  </span><span>value</span>:<span> </span><span>"value1"</span><span>
</span></span></span><span><span><span>  </span><span>effect</span>:<span> </span><span>"NoExecute"</span><span>
</span></span></span></code></pre></div><p>In this case, the pod will not be able to schedule onto the node, because there is no
toleration matching the third taint. But it will be able to continue running if it is
already running on the node when the taint is added, because the third taint is the only
one of the three that is not tolerated by the pod.</p><p>Normally, if a taint with effect <code>NoExecute</code> is added to a node, then any pods that do
not tolerate the taint will be evicted immediately, and pods that do tolerate the
taint will never be evicted. However, a toleration with <code>NoExecute</code> effect can specify
an optional <code>tolerationSeconds</code> field that dictates how long the pod will stay bound
to the node after the taint is added. For example,</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>tolerations</span>:<span>
</span></span></span><span><span><span></span>- <span>key</span>:<span> </span><span>"key1"</span><span>
</span></span></span><span><span><span>  </span><span>operator</span>:<span> </span><span>"Equal"</span><span>
</span></span></span><span><span><span>  </span><span>value</span>:<span> </span><span>"value1"</span><span>
</span></span></span><span><span><span>  </span><span>effect</span>:<span> </span><span>"NoExecute"</span><span>
</span></span></span><span><span><span>  </span><span>tolerationSeconds</span>:<span> </span><span>3600</span><span>
</span></span></span></code></pre></div><p>means that if this pod is running and a matching taint is added to the node, then
the pod will stay bound to the node for 3600 seconds, and then be evicted. If the
taint is removed before that time, the pod will not be evicted.</p><h2 id="example-use-cases">Example Use Cases</h2><p>Taints and tolerations are a flexible way to steer pods <em>away</em> from nodes or evict
pods that shouldn't be running. A few of the use cases are</p><ul><li><p><strong>Dedicated Nodes</strong>: If you want to dedicate a set of nodes for exclusive use by
a particular set of users, you can add a taint to those nodes (say,
<code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>) and then add a corresponding
toleration to their pods (this would be done most easily by writing a custom
<a href="/docs/reference/access-authn-authz/admission-controllers/">admission controller</a>).
The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as
well as any other nodes in the cluster. If you want to dedicate the nodes to them <em>and</em>
ensure they <em>only</em> use the dedicated nodes, then you should additionally add a label similar
to the taint to the same set of nodes (e.g. <code>dedicated=groupName</code>), and the admission
controller should additionally add a node affinity to require that the pods can only schedule
onto nodes labeled with <code>dedicated=groupName</code>.</p></li><li><p><strong>Nodes with Special Hardware</strong>: In a cluster where a small subset of nodes have specialized
hardware (for example GPUs), it is desirable to keep pods that don't need the specialized
hardware off of those nodes, thus leaving room for later-arriving pods that do need the
specialized hardware. This can be done by tainting the nodes that have the specialized
hardware (e.g. <code>kubectl taint nodes nodename special=true:NoSchedule</code> or
<code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>) and adding a corresponding
toleration to pods that use the special hardware. As in the dedicated nodes use case,
it is probably easiest to apply the tolerations using a custom
<a href="/docs/reference/access-authn-authz/admission-controllers/">admission controller</a>.
For example, it is recommended to use <a href="/docs/concepts/configuration/manage-resources-containers/#extended-resources">Extended
Resources</a>
to represent the special hardware, taint your special hardware nodes with the
extended resource name and run the
<a href="/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration">ExtendedResourceToleration</a>
admission controller. Now, because the nodes are tainted, no pods without the
toleration will schedule on them. But when you submit a pod that requests the
extended resource, the <code>ExtendedResourceToleration</code> admission controller will
automatically add the correct toleration to the pod and that pod will schedule
on the special hardware nodes. This will make sure that these special hardware
nodes are dedicated for pods requesting such hardware and you don't have to
manually add tolerations to your pods.</p></li><li><p><strong>Taint based Evictions</strong>: A per-pod-configurable eviction behavior
when there are node problems, which is described in the next section.</p></li></ul><h2 id="taint-based-evictions">Taint based Evictions</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.18 [stable]</code></div><p>The node controller automatically taints a Node when certain conditions
are true. The following taints are built in:</p><ul><li><code>node.kubernetes.io/not-ready</code>: Node is not ready. This corresponds to
the NodeCondition <code>Ready</code> being "<code>False</code>".</li><li><code>node.kubernetes.io/unreachable</code>: Node is unreachable from the node
controller. This corresponds to the NodeCondition <code>Ready</code> being "<code>Unknown</code>".</li><li><code>node.kubernetes.io/memory-pressure</code>: Node has memory pressure.</li><li><code>node.kubernetes.io/disk-pressure</code>: Node has disk pressure.</li><li><code>node.kubernetes.io/pid-pressure</code>: Node has PID pressure.</li><li><code>node.kubernetes.io/network-unavailable</code>: Node's network is unavailable.</li><li><code>node.kubernetes.io/unschedulable</code>: Node is unschedulable.</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>: When the kubelet is started
with an "external" cloud provider, this taint is set on a node to mark it
as unusable. After a controller from the cloud-controller-manager initializes
this node, the kubelet removes this taint.</li></ul><p>In case a node is to be drained, the node controller or the kubelet adds relevant taints
with <code>NoExecute</code> effect. This effect is added by default for the
<code>node.kubernetes.io/not-ready</code> and <code>node.kubernetes.io/unreachable</code> taints.
If the fault condition returns to normal, the kubelet or node
controller can remove the relevant taint(s).</p><p>In some cases when the node is unreachable, the API server is unable to communicate
with the kubelet on the node. The decision to delete the pods cannot be communicated to
the kubelet until communication with the API server is re-established. In the meantime,
the pods that are scheduled for deletion may continue to run on the partitioned node.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The control plane limits the rate of adding new taints to nodes. This rate limiting
manages the number of evictions that are triggered when many nodes become unreachable at
once (for example: if there is a network disruption).</div><p>You can specify <code>tolerationSeconds</code> for a Pod to define how long that Pod stays bound
to a failing or unresponsive Node.</p><p>For example, you might want to keep an application with a lot of local state
bound to node for a long time in the event of network partition, hoping
that the partition will recover and thus the pod eviction can be avoided.
The toleration you set for that Pod might look like:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>tolerations</span>:<span>
</span></span></span><span><span><span></span>- <span>key</span>:<span> </span><span>"node.kubernetes.io/unreachable"</span><span>
</span></span></span><span><span><span>  </span><span>operator</span>:<span> </span><span>"Exists"</span><span>
</span></span></span><span><span><span>  </span><span>effect</span>:<span> </span><span>"NoExecute"</span><span>
</span></span></span><span><span><span>  </span><span>tolerationSeconds</span>:<span> </span><span>6000</span><span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Kubernetes automatically adds a toleration for
<code>node.kubernetes.io/not-ready</code> and <code>node.kubernetes.io/unreachable</code>
with <code>tolerationSeconds=300</code>,
unless you, or a controller, set those tolerations explicitly.</p><p>These automatically-added tolerations mean that Pods remain bound to
Nodes for 5 minutes after one of these problems is detected.</p></div><p><a href="/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> pods are created with
<code>NoExecute</code> tolerations for the following taints with no <code>tolerationSeconds</code>:</p><ul><li><code>node.kubernetes.io/unreachable</code></li><li><code>node.kubernetes.io/not-ready</code></li></ul><p>This ensures that DaemonSet pods are never evicted due to these problems.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The node controller was responsible for adding taints to nodes and evicting pods. But after 1.29,
the taint-based eviction implementation has been moved out of node controller into a separate,
and independent component called taint-eviction-controller. Users can optionally disable taint-based
eviction by setting <code>--controllers=-taint-eviction-controller</code> in kube-controller-manager.</div><h2 id="taint-nodes-by-condition">Taint Nodes by Condition</h2><p>The control plane, using the node <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>,
automatically creates taints with a <code>NoSchedule</code> effect for
<a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions">node conditions</a>.</p><p>The scheduler checks taints, not node conditions, when it makes scheduling
decisions. This ensures that node conditions don't directly affect scheduling.
For example, if the <code>DiskPressure</code> node condition is active, the control plane
adds the <code>node.kubernetes.io/disk-pressure</code> taint and does not schedule new pods
onto the affected node. If the <code>MemoryPressure</code> node condition is active, the
control plane adds the <code>node.kubernetes.io/memory-pressure</code> taint.</p><p>You can ignore node conditions for newly created pods by adding the corresponding
Pod tolerations. The control plane also adds the <code>node.kubernetes.io/memory-pressure</code>
toleration on pods that have a <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." href="/docs/concepts/workloads/pods/pod-qos/" target="_blank">QoS class</a>
other than <code>BestEffort</code>. This is because Kubernetes treats pods in the <code>Guaranteed</code>
or <code>Burstable</code> QoS classes (even pods with no memory request set) as if they are
able to cope with memory pressure, while new <code>BestEffort</code> pods are not scheduled
onto the affected node.</p><p>The DaemonSet controller automatically adds the following <code>NoSchedule</code>
tolerations to all daemons, to prevent DaemonSets from breaking.</p><ul><li><code>node.kubernetes.io/memory-pressure</code></li><li><code>node.kubernetes.io/disk-pressure</code></li><li><code>node.kubernetes.io/pid-pressure</code> (1.14 or later)</li><li><code>node.kubernetes.io/unschedulable</code> (1.10 or later)</li><li><code>node.kubernetes.io/network-unavailable</code> (<em>host network only</em>)</li></ul><p>Adding these tolerations ensures backward compatibility. You can also add
arbitrary tolerations to DaemonSets.</p><h2 id="device-taints-and-tolerations">Device taints and tolerations</h2><p>Instead of tainting entire nodes, administrators can also <a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#device-taints-and-tolerations">taint individual devices</a>
when the cluster uses <a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">dynamic resource allocation</a>
to manage special hardware. The advantage is that tainting can be targeted towards exactly the hardware that
is faulty or needs maintenance. Tolerations are also supported and can be specified when requesting
devices. Like taints they apply to all pods which share the same allocated device.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node-pressure Eviction</a>
and how you can configure it</li><li>Read about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod Priority</a></li><li>Read about <a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#device-taints-and-tolerations">device taints and tolerations</a></li></ul></div></div><div><div class="td-content"><h1>Scheduling Framework</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.19 [stable]</code></div><p>The <em>scheduling framework</em> is a pluggable architecture for the Kubernetes scheduler.
It consists of a set of "plugin" APIs that are compiled directly into the scheduler.
These APIs allow most scheduling features to be implemented as plugins,
while keeping the scheduling "core" lightweight and maintainable. Refer to the
<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md">design proposal of the scheduling framework</a> for more technical information on
the design of the framework.</p><h2 id="framework-workflow">Framework workflow</h2><p>The Scheduling Framework defines a few extension points. Scheduler plugins
register to be invoked at one or more extension points. Some of these plugins
can change the scheduling decisions and some are informational only.</p><p>Each attempt to schedule one Pod is split into two phases, the
<strong>scheduling cycle</strong> and the <strong>binding cycle</strong>.</p><h3 id="scheduling-cycle-binding-cycle">Scheduling cycle &amp; binding cycle</h3><p>The scheduling cycle selects a node for the Pod, and the binding cycle applies
that decision to the cluster. Together, a scheduling cycle and binding cycle are
referred to as a "scheduling context".</p><p>Scheduling cycles are run serially, while binding cycles may run concurrently.</p><p>A scheduling or binding cycle can be aborted if the Pod is determined to
be unschedulable or if there is an internal error. The Pod will be returned to
the queue and retried.</p><h2 id="interfaces">Interfaces</h2><p>The following picture shows the scheduling context of a Pod and the interfaces
that the scheduling framework exposes.</p><p>One plugin may implement multiple interfaces to perform more complex or
stateful tasks.</p><p>Some interfaces match the scheduler extension points which can be configured through
<a href="/docs/reference/scheduling/config/#extension-points">Scheduler Configuration</a>.</p><figure class="diagram-large"><img src="/images/docs/scheduling-framework-extensions.png"><figcaption><h4>Scheduling framework extension points</h4></figcaption></figure><h3 id="pre-enqueue">PreEnqueue</h3><p>These plugins are called prior to adding Pods to the internal active queue, where Pods are marked as
ready for scheduling.</p><p>Only when all PreEnqueue plugins return <code>Success</code>, the Pod is allowed to enter the active queue.
Otherwise, it's placed in the internal unschedulable Pods list, and doesn't get an <code>Unschedulable</code> condition.</p><p>For more details about how internal scheduler queues work, read
<a href="https://github.com/kubernetes/community/blob/f03b6d5692bd979f07dd472e7b6836b2dad0fd9b/contributors/devel/sig-scheduling/scheduler_queues.md">Scheduling queue in kube-scheduler</a>.</p><h3 id="enqueueextension">EnqueueExtension</h3><p>EnqueueExtension is the interface where the plugin can control
whether to retry scheduling of Pods rejected by this plugin, based on changes in the cluster.
Plugins that implement PreEnqueue, PreFilter, Filter, Reserve or Permit should implement this interface.</p><h3 id="queueinghint">QueueingHint</h3><div class="feature-state-notice feature-stable" title="Feature Gate: SchedulerQueueingHints"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>QueueingHint is a callback function for deciding whether a Pod can be requeued to the active queue or backoff queue.
It's executed every time a certain kind of event or change happens in the cluster.
When the QueueingHint finds that the event might make the Pod schedulable,
the Pod is put into the active queue or the backoff queue
so that the scheduler will retry the scheduling of the Pod.</p><h3 id="queue-sort">QueueSort</h3><p>These plugins are used to sort Pods in the scheduling queue. A queue sort plugin
essentially provides a <code>Less(Pod1, Pod2)</code> function. Only one queue sort
plugin may be enabled at a time.</p><h3 id="pre-filter">PreFilter</h3><p>These plugins are used to pre-process info about the Pod, or to check certain
conditions that the cluster or the Pod must meet. If a PreFilter plugin returns
an error, the scheduling cycle is aborted.</p><h3 id="filter">Filter</h3><p>These plugins are used to filter out nodes that cannot run the Pod. For each
node, the scheduler will call filter plugins in their configured order. If any
filter plugin marks the node as infeasible, the remaining plugins will not be
called for that node. Nodes may be evaluated concurrently.</p><h3 id="post-filter">PostFilter</h3><p>These plugins are called after the Filter phase, but only when no feasible nodes
were found for the pod. Plugins are called in their configured order. If
any postFilter plugin marks the node as <code>Schedulable</code>, the remaining plugins
will not be called. A typical PostFilter implementation is preemption, which
tries to make the pod schedulable by preempting other Pods.</p><h3 id="pre-score">PreScore</h3><p>These plugins are used to perform "pre-scoring" work, which generates a sharable
state for Score plugins to use. If a PreScore plugin returns an error, the
scheduling cycle is aborted.</p><h3 id="scoring">Score</h3><p>These plugins are used to rank nodes that have passed the filtering phase. The
scheduler will call each scoring plugin for each node. There will be a well
defined range of integers representing the minimum and maximum scores. After the
<a href="#normalize-scoring">NormalizeScore</a> phase, the scheduler will combine node
scores from all plugins according to the configured plugin weights.</p><h4 id="scoring-capacity">Capacity scoring</h4><div class="feature-state-notice feature-alpha" title="Feature Gate: StorageCapacityScoring"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>The feature gate <code>VolumeCapacityPriority</code> was used in v1.32 to support storage that are
statically provisioned. Starting from v1.33, the new feature gate <code>StorageCapacityScoring</code>
replaces the old <code>VolumeCapacityPriority</code> gate with added support to dynamically provisioned storage.
When <code>StorageCapacityScoring</code> is enabled, the VolumeBinding plugin in the kube-scheduler is extended
to score Nodes based on the storage capacity on each of them.
This feature is applicable to CSI volumes that supported <a href="/docs/concepts/storage/storage-capacity/">Storage Capacity</a>,
including local storage backed by a CSI driver.</p><h3 id="normalize-scoring">NormalizeScore</h3><p>These plugins are used to modify scores before the scheduler computes a final
ranking of Nodes. A plugin that registers for this extension point will be
called with the <a href="#scoring">Score</a> results from the same plugin. This is called
once per plugin per scheduling cycle.</p><p>For example, suppose a plugin <code>BlinkingLightScorer</code> ranks Nodes based on how
many blinking lights they have.</p><div class="highlight"><pre tabindex="0"><code class="language-go"><span><span><span>func</span> <span>ScoreNode</span>(_ <span>*</span>v1.pod, n <span>*</span>v1.Node) (<span>int</span>, <span>error</span>) {
</span></span><span><span>    <span>return</span> <span>getBlinkingLightCount</span>(n)
</span></span><span><span>}
</span></span></code></pre></div><p>However, the maximum count of blinking lights may be small compared to
<code>NodeScoreMax</code>. To fix this, <code>BlinkingLightScorer</code> should also register for this
extension point.</p><div class="highlight"><pre tabindex="0"><code class="language-go"><span><span><span>func</span> <span>NormalizeScores</span>(scores <span>map</span>[<span>string</span>]<span>int</span>) {
</span></span><span><span>    highest <span>:=</span> <span>0</span>
</span></span><span><span>    <span>for</span> _, score <span>:=</span> <span>range</span> scores {
</span></span><span><span>        highest = <span>max</span>(highest, score)
</span></span><span><span>    }
</span></span><span><span>    <span>for</span> node, score <span>:=</span> <span>range</span> scores {
</span></span><span><span>        scores[node] = score<span>*</span>NodeScoreMax<span>/</span>highest
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>If any NormalizeScore plugin returns an error, the scheduling cycle is
aborted.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Plugins wishing to perform "pre-reserve" work should use the
NormalizeScore extension point.</div><h3 id="reserve">Reserve</h3><p>A plugin that implements the Reserve interface has two methods, namely <code>Reserve</code>
and <code>Unreserve</code>, that back two informational scheduling phases called Reserve
and Unreserve, respectively. Plugins which maintain runtime state (aka "stateful
plugins") should use these phases to be notified by the scheduler when resources
on a node are being reserved and unreserved for a given Pod.</p><p>The Reserve phase happens before the scheduler actually binds a Pod to its
designated node. It exists to prevent race conditions while the scheduler waits
for the bind to succeed. The <code>Reserve</code> method of each Reserve plugin may succeed
or fail; if one <code>Reserve</code> method call fails, subsequent plugins are not executed
and the Reserve phase is considered to have failed. If the <code>Reserve</code> method of
all plugins succeed, the Reserve phase is considered to be successful and the
rest of the scheduling cycle and the binding cycle are executed.</p><p>The Unreserve phase is triggered if the Reserve phase or a later phase fails.
When this happens, the <code>Unreserve</code> method of <strong>all</strong> Reserve plugins will be
executed in the reverse order of <code>Reserve</code> method calls. This phase exists to
clean up the state associated with the reserved Pod.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>The implementation of the <code>Unreserve</code> method in Reserve plugins must be
idempotent and may not fail.</div><h3 id="permit">Permit</h3><p><em>Permit</em> plugins are invoked at the end of the scheduling cycle for each Pod, to
prevent or delay the binding to the candidate node. A permit plugin can do one of
the three things:</p><ol><li><p><strong>approve</strong><br>Once all Permit plugins approve a Pod, it is sent for binding.</p></li><li><p><strong>deny</strong><br>If any Permit plugin denies a Pod, it is returned to the scheduling queue.
This will trigger the Unreserve phase in <a href="#reserve">Reserve plugins</a>.</p></li><li><p><strong>wait</strong> (with a timeout)<br>If a Permit plugin returns "wait", then the Pod is kept in an internal "waiting"
Pods list, and the binding cycle of this Pod starts but directly blocks until it
gets approved. If a timeout occurs, <strong>wait</strong> becomes <strong>deny</strong>
and the Pod is returned to the scheduling queue, triggering the
Unreserve phase in <a href="#reserve">Reserve plugins</a>.</p></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>While any plugin can access the list of "waiting" Pods and approve them
(see <a href="https://git.k8s.io/enhancements/keps/sig-scheduling/624-scheduling-framework#frameworkhandle"><code>FrameworkHandle</code></a>),
we expect only the permit plugins to approve binding of reserved Pods that are in "waiting" state.
Once a Pod is approved, it is sent to the <a href="#pre-bind">PreBind</a> phase.</div><h3 id="pre-bind">PreBind</h3><p>These plugins are used to perform any work required before a Pod is bound. For
example, a pre-bind plugin may provision a network volume and mount it on the
target node before allowing the Pod to run there.</p><p>If any PreBind plugin returns an error, the Pod is <a href="#reserve">rejected</a> and
returned to the scheduling queue.</p><h3 id="bind">Bind</h3><p>These plugins are used to bind a Pod to a Node. Bind plugins will not be called
until all PreBind plugins have completed. Each bind plugin is called in the
configured order. A bind plugin may choose whether or not to handle the given
Pod. If a bind plugin chooses to handle a Pod, <strong>the remaining bind plugins are
skipped</strong>.</p><h3 id="post-bind">PostBind</h3><p>This is an informational interface. Post-bind plugins are called after a
Pod is successfully bound. This is the end of a binding cycle, and can be used
to clean up associated resources.</p><h2 id="plugin-api">Plugin API</h2><p>There are two steps to the plugin API. First, plugins must register and get
configured, then they use the extension point interfaces. Extension point
interfaces have the following form.</p><div class="highlight"><pre tabindex="0"><code class="language-go"><span><span><span>type</span> Plugin <span>interface</span> {
</span></span><span><span>    <span>Name</span>() <span>string</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>type</span> QueueSortPlugin <span>interface</span> {
</span></span><span><span>    Plugin
</span></span><span><span>    <span>Less</span>(<span>*</span>v1.pod, <span>*</span>v1.pod) <span>bool</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>type</span> PreFilterPlugin <span>interface</span> {
</span></span><span><span>    Plugin
</span></span><span><span>    <span>PreFilter</span>(context.Context, <span>*</span>framework.CycleState, <span>*</span>v1.pod) <span>error</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// ...
</span></span></span></code></pre></div><h2 id="plugin-configuration">Plugin configuration</h2><p>You can enable or disable plugins in the scheduler configuration. If you are using
Kubernetes v1.18 or later, most scheduling
<a href="/docs/reference/scheduling/config/#scheduling-plugins">plugins</a> are in use and
enabled by default.</p><p>In addition to default plugins, you can also implement your own scheduling
plugins and get them configured along with default plugins. You can visit
<a href="https://github.com/kubernetes-sigs/scheduler-plugins">scheduler-plugins</a> for more details.</p><p>If you are using Kubernetes v1.18 or later, you can configure a set of plugins as
a scheduler profile and then define multiple profiles to fit various kinds of workload.
Learn more at <a href="/docs/reference/scheduling/config/#multiple-profiles">multiple profiles</a>.</p></div></div><div><div class="td-content"><h1>Dynamic Resource Allocation</h1><div class="feature-state-notice feature-stable" title="Feature Gate: DynamicResourceAllocation"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>This page describes <em>dynamic resource allocation (DRA)</em> in Kubernetes.</p><h2 id="about-dra">About DRA</h2><p><p>DRA is a Kubernetes feature that lets you request and share resources among Pods.
These resources are often attached
<a class="glossary-tooltip" title="Any resource that's directly or indirectly attached your cluster's nodes, like GPUs or circuit boards." href="/docs/reference/glossary/?all=true#term-device" target="_blank">devices</a> like hardware
accelerators.</p></p><p>With DRA, device drivers and cluster admins define device <em>classes</em> that are
available to <em>claim</em> in workloads. Kubernetes allocates matching devices to
specific claims and places the corresponding Pods on nodes that can access the
allocated devices.</p><p>Allocating resources with DRA is a similar experience to
<a href="/docs/concepts/storage/dynamic-provisioning/">dynamic volume provisioning</a>, in
which you use PersistentVolumeClaims to claim storage capacity from storage
classes and request the claimed capacity in your Pods.</p><h3 id="dra-benefits">Benefits of DRA</h3><p>DRA provides a flexible way to categorize, request, and use devices in your
cluster. Using DRA provides benefits like the following:</p><ul><li><strong>Flexible device filtering</strong>: use common expression language (CEL) to perform
fine-grained filtering for specific device attributes.</li><li><strong>Device sharing</strong>: share the same resource with multiple containers or Pods
by referencing the corresponding resource claim.</li><li><strong>Centralized device categorization</strong>: device drivers and cluster admins can
use device classes to provide app operators with hardware categories that are
optimized for various use cases. For example, you can create a cost-optimized
device class for general-purpose workloads, and a high-performance device
class for critical jobs.</li><li><strong>Simplified Pod requests</strong>: with DRA, app operators don't need to specify
device quantities in Pod resource requests. Instead, the Pod references a
resource claim, and the device configuration in that claim applies to the Pod.</li></ul><p>These benefits provide significant improvements in the device allocation
workflow when compared to
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">device plugins</a>,
which require per-container device requests, don't support device sharing, and
don't support expression-based device filtering.</p><h3 id="dra-user-types">Types of DRA users</h3><p>The workflow of using DRA to allocate devices involves the following types of
users:</p><ul><li><p><strong>Device owner</strong>: responsible for devices. Device owners might be commercial
vendors, the cluster operator, or another entity. To use DRA, devices must
have DRA-compatible drivers that do the following:</p><ul><li>Create ResourceSlices that provide Kubernetes with information about
nodes and resources.</li><li>Update ResourceSlices when resource capacity in the cluster changes.</li><li>Optionally, create DeviceClasses that workload operators can use to
claim devices.</li></ul></li><li><p><strong>Cluster admin</strong>: responsible for configuring clusters and nodes,
attaching devices, installing drivers, and similar tasks. To use DRA,
cluster admins do the following:</p><ul><li>Attach devices to nodes.</li><li>Install device drivers that support DRA.</li><li>Optionally, create DeviceClasses that workload operators can use to claim
devices.</li></ul></li><li><p><strong>Workload operator</strong>: responsible for deploying and managing workloads in the
cluster. To use DRA to allocate devices to Pods, workload operators do the
following:</p><ul><li>Create ResourceClaims or ResourceClaimTemplates to request specific
configurations within DeviceClasses.</li><li>Deploy workloads that use specific ResourceClaims or ResourceClaimTemplates.</li></ul></li></ul><h2 id="terminology">DRA terminology</h2><p>DRA uses the following Kubernetes API kinds to provide the core allocation
functionality. All of these API kinds are included in the
<code>resource.k8s.io/v1</code>
<a class="glossary-tooltip" title="A set of related paths in the Kubernetes API." href="/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning" target="_blank">API group</a>.</p><dl><dt>DeviceClass</dt><dd>Defines a category of devices that can be claimed and how to select specific
device attributes in claims. The DeviceClass parameters can match zero or
more devices in ResourceSlices. To claim devices from a DeviceClass,
ResourceClaims select specific device attributes.</dd><dt>ResourceClaim</dt><dd>Describes a request for access to attached resources, such as
devices, in the cluster. ResourceClaims provide Pods with access to
a specific resource. ResourceClaims can be created by workload operators
or generated by Kubernetes based on a ResourceClaimTemplate.</dd><dt>ResourceClaimTemplate</dt><dd>Defines a template that Kubernetes uses to create per-Pod
ResourceClaims for a workload. ResourceClaimTemplates provide Pods with
access to separate, similar resources. Each ResourceClaim that Kubernetes
generates from the template is bound to a specific Pod. When the Pod
terminates, Kubernetes deletes the corresponding ResourceClaim.</dd><dt>ResourceSlice</dt><dd>Represents one or more resources that are attached to nodes, such as devices.
Drivers create and manage ResourceSlices in the cluster. When a ResourceClaim
is created and used in a Pod, Kubernetes uses ResourceSlices to find nodes
that have access to the claimed resources. Kubernetes allocates resources to
the ResourceClaim and schedules the Pod onto a node that can access the
resources.</dd></dl><h3 id="deviceclass">DeviceClass</h3><p>A DeviceClass lets cluster admins or device drivers define categories of devices
in the cluster. DeviceClasses tell operators what devices they can request and
how they can request those devices. You can use
<a href="https://cel.dev">common expression language (CEL)</a> to select devices based on
specific attributes. A ResourceClaim that references the DeviceClass can then
request specific configurations within the DeviceClass.</p><p>To create a DeviceClass, see
<a href="/docs/tasks/configure-pod-container/assign-resources/set-up-dra-cluster/">Set Up DRA in a Cluster</a>.</p><h3 id="resourceclaims-templates">ResourceClaims and ResourceClaimTemplates</h3><p>A ResourceClaim defines the resources that a workload needs. Every ResourceClaim
has <em>requests</em> that reference a DeviceClass and select devices from that
DeviceClass. ResourceClaims can also use <em>selectors</em> to filter for devices that
meet specific requirements, and can use <em>constraints</em> to limit the devices that
can satisfy a request. ResourceClaims can be created by workload operators or
can be generated by Kubernetes based on a ResourceClaimTemplate. A
ResourceClaimTemplate defines a template that Kubernetes can use to
auto-generate ResourceClaims for Pods.</p><h4 id="when-to-use-rc-rct">Use cases for ResourceClaims and ResourceClaimTemplates</h4><p>The method that you use depends on your requirements, as follows:</p><ul><li><strong>ResourceClaim</strong>: you want multiple Pods to share access to specific
devices. You manually manage the lifecycle of ResourceClaims that you create.</li><li><strong>ResourceClaimTemplate</strong>: you want Pods to have independent access to
separate, similarly-configured devices. Kubernetes generates ResourceClaims
from the specification in the ResourceClaimTemplate. The lifetime of each
generated ResourceClaim is bound to the lifetime of the corresponding Pod.</li></ul><p>When you define a workload, you can use
<a class="glossary-tooltip" title="An expression language that's designed to be safe for executing user code." href="https://cel.dev" target="_blank">Common Expression Language (CEL)</a>
to filter for specific device attributes or capacity. The available parameters
for filtering depend on the device and the drivers.</p><p>If you directly reference a specific ResourceClaim in a Pod, that ResourceClaim
must already exist in the same namespace as the Pod. If the ResourceClaim
doesn't exist in the namespace, the Pod won't schedule. This behavior is similar
to how a PersistentVolumeClaim must exist in the same namespace as a Pod that
references it.</p><p>You can reference an auto-generated ResourceClaim in a Pod, but this isn't
recommended because auto-generated ResourceClaims are bound to the lifetime of
the Pod that triggered the generation.</p><p>To learn how to claim resources using one of these methods, see
<a href="/docs/tasks/configure-pod-container/assign-resources/allocate-devices-dra/">Allocate Devices to Workloads with DRA</a>.</p><h4 id="prioritized-list">Prioritized list</h4><div class="feature-state-notice feature-beta" title="Feature Gate: DRAPrioritizedList"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>You can provide a prioritized list of subrequests for requests in a ResourceClaim or
ResourceClaimTemplate. The scheduler will then select the first subrequest that can be allocated.
This allows users to specify alternative devices that can be used by the workload if the primary
choice is not available.</p><p>In the example below, the ResourceClaimTemplate requested a device with the color black
and the size large. If a device with those attributes is not available, the pod cannot
be scheduled. With the prioritized list feature, a second alternative can be specified, which
requests two devices with the color white and size small. The large black device will be
allocated if it is available. If it is not, but two small white devices are available,
the pod will still be able to run.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceClaimTemplate<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>prioritized-list-claim-template<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span>devices</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>req-0<span>
</span></span></span><span><span><span>        </span><span>firstAvailable</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>large-black<span>
</span></span></span><span><span><span>          </span><span>deviceClassName</span>:<span> </span>resource.example.com<span>
</span></span></span><span><span><span>          </span><span>selectors</span>:<span>
</span></span></span><span><span><span>          </span>- <span>cel</span>:<span>
</span></span></span><span><span><span>              </span><span>expression</span>:<span> </span>|-<span>
</span></span></span><span><span><span>                device.attributes["resource-driver.example.com"].color == "black" &amp;&amp;
</span></span></span><span><span><span>                device.attributes["resource-driver.example.com"].size == "large"</span><span>                
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>small-white<span>
</span></span></span><span><span><span>          </span><span>deviceClassName</span>:<span> </span>resource.example.com<span>
</span></span></span><span><span><span>          </span><span>selectors</span>:<span>
</span></span></span><span><span><span>          </span>- <span>cel</span>:<span>
</span></span></span><span><span><span>              </span><span>expression</span>:<span> </span>|-<span>
</span></span></span><span><span><span>                device.attributes["resource-driver.example.com"].color == "white" &amp;&amp;
</span></span></span><span><span><span>                device.attributes["resource-driver.example.com"].size == "small"</span><span>                
</span></span></span><span><span><span>          </span><span>count</span>:<span> </span><span>2</span><span>
</span></span></span></code></pre></div><p>The decision is made on a per-Pod basis, so if the Pod is a member of a ReplicaSet or
similar grouping, you cannot rely on all the members of the group having the same subrequest
chosen. Your workload must be able to accommodate this.</p><p>Prioritized lists is a <em>beta feature</em> and is enabled by default with the
<code>DRAPrioritizedList</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> in
the kube-apiserver and kube-scheduler.</p><h3 id="resourceslice">ResourceSlice</h3><p>Each ResourceSlice represents one or more
<a class="glossary-tooltip" title="Any resource that's directly or indirectly attached your cluster's nodes, like GPUs or circuit boards." href="/docs/reference/glossary/?all=true#term-device" target="_blank">devices</a> in a pool. The pool is
managed by a device driver, which creates and manages ResourceSlices. The
resources in a pool might be represented by a single ResourceSlice or span
multiple ResourceSlices.</p><p>ResourceSlices provide useful information to device users and to the scheduler,
and are crucial for dynamic resource allocation. Every ResourceSlice must include
the following information:</p><ul><li><strong>Resource pool</strong>: a group of one or more resources that the driver manages.
The pool can span more than one ResourceSlice. Changes to the resources in a
pool must be propagated across all of the ResourceSlices in that pool. The
device driver that manages the pool is responsible for ensuring that this
propagation happens.</li><li><strong>Devices</strong>: devices in the managed pool. A ResourceSlice can list every
device in a pool or a subset of the devices in a pool. The ResourceSlice
defines device information like attributes, versions, and capacity. Device
users can select devices for allocation by filtering for device information
in ResourceClaims or in DeviceClasses.</li><li><strong>Nodes</strong>: the nodes that can access the resources. Drivers can choose which
nodes can access the resources, whether that's all of the nodes in the
cluster, a single named node, or nodes that have specific node labels.</li></ul><p>Drivers use a <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> to
reconcile ResourceSlices in the cluster with the information that the driver has
to publish. This controller overwrites any manual changes, such as cluster users
creating or modifying ResourceSlices.</p><p>Consider the following example ResourceSlice:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceSlice<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>cat-slice<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>driver</span>:<span> </span><span>"resource-driver.example.com"</span><span>
</span></span></span><span><span><span>  </span><span>pool</span>:<span>
</span></span></span><span><span><span>    </span><span>generation</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span><span>"black-cat-pool"</span><span>
</span></span></span><span><span><span>    </span><span>resourceSliceCount</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span># The allNodes field defines whether any node in the cluster can access the device.</span><span>
</span></span></span><span><span><span>  </span><span>allNodes</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>  </span><span>devices</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span><span>"large-black-cat"</span><span>
</span></span></span><span><span><span>    </span><span>attributes</span>:<span>
</span></span></span><span><span><span>      </span><span>color</span>:<span>
</span></span></span><span><span><span>        </span><span>string</span>:<span> </span><span>"black"</span><span>
</span></span></span><span><span><span>      </span><span>size</span>:<span>
</span></span></span><span><span><span>        </span><span>string</span>:<span> </span><span>"large"</span><span>
</span></span></span><span><span><span>      </span><span>cat</span>:<span>
</span></span></span><span><span><span>        </span><span>bool</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><p>This ResourceSlice is managed by the <code>resource-driver.example.com</code> driver in the
<code>black-cat-pool</code> pool. The <code>allNodes: true</code> field indicates that any node in the
cluster can access the devices. There's one device in the ResourceSlice, named
<code>large-black-cat</code>, with the following attributes:</p><ul><li><code>color</code>: <code>black</code></li><li><code>size</code>: <code>large</code></li><li><code>cat</code>: <code>true</code></li></ul><p>A DeviceClass could select this ResourceSlice by using these attributes, and a
ResourceClaim could filter for specific devices in that DeviceClass.</p><h2 id="how-it-works">How resource allocation with DRA works</h2><p>The following sections describe the workflow for the various
<a href="#dra-user-types">types of DRA users</a> and for the Kubernetes system during
dynamic resource allocation.</p><h3 id="user-workflow">Workflow for users</h3><ol><li><strong>Driver creation</strong>: device owners or third-party entities create drivers
that can create and manage ResourceSlices in the cluster. These drivers
optionally also create DeviceClasses that define a category of devices and
how to request them.</li><li><strong>Cluster configuration</strong>: cluster admins create clusters, attach devices to
nodes, and install the DRA device drivers. Cluster admins optionally create
DeviceClasses that define categories of devices and how to request them.</li><li><strong>Resource claims</strong>: workload operators create ResourceClaimTemplates or
ResourceClaims that request specific device configurations within a
DeviceClass. In the same step, workload operators modify their Kubernetes
manifests to request those ResourceClaimTemplates or ResourceClaims.</li></ol><h3 id="kubernetes-workflow">Workflow for Kubernetes</h3><ol><li><p><strong>ResourceSlice creation</strong>: drivers in the cluster create ResourceSlices that
represent one or more devices in a managed pool of similar devices.</p></li><li><p><strong>Workload creation</strong>: the cluster control plane checks new workloads for
references to ResourceClaimTemplates or to specific ResourceClaims.</p><ul><li>If the workload uses a ResourceClaimTemplate, a controller named the
<code>resourceclaim-controller</code> generates ResourceClaims for every Pod in the
workload.</li><li>If the workload uses a specific ResourceClaim, Kubernetes checks whether
that ResourceClaim exists in the cluster. If the ResourceClaim doesn't
exist, the Pods won't deploy.</li></ul></li><li><p><strong>ResourceSlice filtering</strong>: for every Pod, Kubernetes checks the
ResourceSlices in the cluster to find a device that satisfies all of the
following criteria:</p><ul><li>The nodes that can access the resources are eligible to run the Pod.</li><li>The ResourceSlice has unallocated resources that match the requirements of
the Pod's ResourceClaim.</li></ul></li><li><p><strong>Resource allocation</strong>: after finding an eligible ResourceSlice for a
Pod's ResourceClaim, the Kubernetes scheduler updates the ResourceClaim
with the allocation details.</p></li><li><p><strong>Pod scheduling</strong>: when resource allocation is complete, the scheduler
places the Pod on a node that can access the allocated resource. The device
driver and the kubelet on that node configure the device and the Pod's access
to the device.</p></li></ol><h2 id="observability-dynamic-resources">Observability of dynamic resources</h2><p>You can check the status of dynamically allocated resources by using any of the
following methods:</p><ul><li><a href="#monitoring-resources">kubelet device metrics</a></li><li><a href="#resourceclaim-device-status">ResourceClaim status</a></li><li><a href="#device-health-monitoring">Device health monitoring</a></li></ul><h3 id="monitoring-resources">kubelet device metrics</h3><p>The <code>PodResourcesLister</code> kubelet gRPC service lets you monitor in-use devices.
The <code>DynamicResource</code> message provides information that's specific to dynamic
resource allocation, such as the device name and the claim name. For details,
see
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#monitoring-device-plugin-resources">Monitoring device plugin resources</a>.</p><h3 id="resourceclaim-device-status">ResourceClaim device status</h3><div class="feature-state-notice feature-beta" title="Feature Gate: DRAResourceClaimDeviceStatus"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: true)</div><p>DRA drivers can report driver-specific
<a href="/docs/concepts/overview/working-with-objects/#object-spec-and-status">device status</a>
data for each allocated device in the <code>status.devices</code> field of a ResourceClaim.
For example, the driver might list the IP addresses that are assigned to a
network interface device.</p><p>The accuracy of the information that a driver adds to a ResourceClaim
<code>status.devices</code> field depends on the driver. Evaluate drivers to decide whether
you can rely on this field as the only source of device information.</p><p>If you disable the <code>DRAResourceClaimDeviceStatus</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>, the
<code>status.devices</code> field automatically gets cleared when storing the ResourceClaim.
A ResourceClaim device status is supported when it is possible, from a DRA
driver, to update an existing ResourceClaim where the <code>status.devices</code> field is
set.</p><p>For details about the <code>status.devices</code> field, see the
<a href="#ResourceClaimStatus">ResourceClaim</a> API reference.</p><h3 id="device-health-monitoring">Device Health Monitoring</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: ResourceHealthStatus"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [alpha]</code> (enabled by default: false)</div><p>As an alpha feature, Kubernetes provides a mechanism for monitoring and reporting the health of dynamically allocated infrastructure resources.
For stateful applications running on specialized hardware, it is critical to know when a device has failed or become unhealthy. It is also helpful to find out if the device recovers.</p><p>To enable this functionality, the <code>ResourceHealthStatus</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/resource-health-status/">feature gate</a> must be enabled, and the DRA driver must implement the <code>DRAResourceHealth</code> gRPC service.</p><p>When a DRA driver detects that an allocated device has become unhealthy, it reports this status back to the kubelet. This health information is then exposed directly in the Pod's status. The kubelet populates the <code>allocatedResourcesStatus</code> field in the status of each container, detailing the health of each device assigned to that container.</p><p>This provides crucial visibility for users and controllers to react to hardware failures. For a Pod that is failing, you can inspect this status to determine if the failure was related to an unhealthy device.</p><h2 id="pre-scheduled-pods">Pre-scheduled Pods</h2><p>When you - or another API client - create a Pod with <code>spec.nodeName</code> already set, the scheduler gets bypassed.
If some ResourceClaim needed by that Pod does not exist yet, is not allocated
or not reserved for the Pod, then the kubelet will fail to run the Pod and
re-check periodically because those requirements might still get fulfilled
later.</p><p>Such a situation can also arise when support for dynamic resource allocation
was not enabled in the scheduler at the time when the Pod got scheduled
(version skew, configuration, feature gate, etc.). kube-controller-manager
detects this and tries to make the Pod runnable by reserving the required
ResourceClaims. However, this only works if those were allocated by
the scheduler for some other pod.</p><p>It is better to avoid bypassing the scheduler because a Pod that is assigned to a node
blocks normal resources (RAM, CPU) that then cannot be used for other Pods
while the Pod is stuck. To make a Pod run on a specific node while still going
through the normal scheduling flow, create the Pod with a node selector that
exactly matches the desired node:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>pod-with-cats<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/hostname</span>:<span> </span>name-of-the-intended-node<span>
</span></span></span><span><span><span>  </span>...<span>
</span></span></span></code></pre></div><p>You may also be able to mutate the incoming Pod, at admission time, to unset
the <code>.spec.nodeName</code> field and to use a node selector instead.</p><h2 id="beta-features">DRA beta features</h2><p>The following sections describe DRA features that are available in the Beta
<a href="/docs/reference/command-line-tools-reference/feature-gates/#feature-stages">feature stage</a>.
For more information, see
<a href="/docs/tasks/configure-pod-container/assign-resources/set-up-dra-cluster/">Set up DRA in the cluster</a>.</p><h3 id="admin-access">Admin access</h3><div class="feature-state-notice feature-beta" title="Feature Gate: DRAAdminAccess"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>You can mark a request in a ResourceClaim or ResourceClaimTemplate as having
privileged features for maintenance and troubleshooting tasks. A request with
admin access grants access to in-use devices and may enable additional
permissions when making the device available in a container:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceClaimTemplate<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>large-black-cat-claim-template<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span>devices</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>req-0<span>
</span></span></span><span><span><span>        </span><span>exactly</span>:<span>
</span></span></span><span><span><span>          </span><span>deviceClassName</span>:<span> </span>resource.example.com<span>
</span></span></span><span><span><span>          </span><span>allocationMode</span>:<span> </span>All<span>
</span></span></span><span><span><span>          </span><span>adminAccess</span>:<span> </span><span>true</span><span>
</span></span></span></code></pre></div><p>If this feature is disabled, the <code>adminAccess</code> field will be removed
automatically when creating such a ResourceClaim.</p><p>Admin access is a privileged mode and should not be granted to regular users in
multi-tenant clusters. Starting with Kubernetes v1.33, only users authorized to
create ResourceClaim or ResourceClaimTemplate objects in namespaces labeled with
<code>resource.k8s.io/admin-access: "true"</code> (case-sensitive) can use the
<code>adminAccess</code> field. This ensures that non-admin users cannot misuse the
feature. Starting with Kubernetes v1.34, this label has been updated to <code>resource.kubernetes.io/admin-access: "true"</code>.</p><h2 id="alpha-features">DRA alpha features</h2><p>The following sections describe DRA features that are available in the Alpha
<a href="/docs/reference/command-line-tools-reference/feature-gates/#feature-stages">feature stage</a>.
To use any of these features, you must also set up DRA in your clusters by
enabling the DynamicResourceAllocation feature gate and the DRA
<a class="glossary-tooltip" title="A set of related paths in the Kubernetes API." href="/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning" target="_blank">API groups</a>. For more
information, see
<a href="/docs/tasks/configure-pod-container/assign-resources/set-up-dra-cluster/">Set up DRA in the cluster</a>.</p><h3 id="extended-resource">Extended resource allocation by DRA</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DRAExtendedResource"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>You can provide an extended resource name for a DeviceClass. The scheduler will then
select the devices matching the class for the extended resource requests. This allows
users to continue using extended resource requests in a pod to request either
extended resources provided by device plugin, or DRA devices. The same extended
resource can be provided either by device plugin, or DRA on one single cluster node.
The same extended resource can be provided by device plugin on some nodes, and
DRA on other nodes in the same cluster.</p><p>In the example below, the DeviceClass is given an extendedResourceName <code>example.com/gpu</code>.
If a pod requested for the extended resource <code>example.com/gpu: 2</code>, it can be scheduled to
a node with two or more devices matching the DeviceClass.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DeviceClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>gpu.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>selectors</span>:<span>
</span></span></span><span><span><span>  </span>- <span>cel</span>:<span>
</span></span></span><span><span><span>      </span><span>expression</span>:<span> </span>device.driver == 'gpu.example.com' &amp;&amp; device.attributes['gpu.example.com'].type<span>
</span></span></span><span><span><span>        </span>== 'gpu'<span>
</span></span></span><span><span><span>  </span><span>extendedResourceName</span>:<span> </span>example.com/gpu<span>
</span></span></span></code></pre></div><p>In addition, users can use a special extended resource to allocate devices without
having to explicitly create a ResourceClaim. Using the extended resource name
prefix <code>deviceclass.resource.kubernetes.io/</code> and the DeviceClass name. This works
for any DeviceClass, even if it does not specify the an extended resource name.
The resulting ResourceClaim will contain a request for an <code>ExactCount</code> of the
specified number of devices of that DeviceClass.</p><p>Extended resource allocation by DRA is an <em>alpha feature</em> and only enabled when the
<code>DRAExtendedResource</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled in the kube-apiserver, kube-scheduler, and kubelet.</p><h3 id="partitionable-devices">Partitionable devices</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DRAPartitionableDevices"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>Devices represented in DRA don't necessarily have to be a single unit connected to a single machine,
but can also be a logical device comprised of multiple devices connected to multiple machines. These
devices might consume overlapping resources of the underlying phyical devices, meaning that when one
logical device is allocated other devices will no longer be available.</p><p>In the ResourceSlice API, this is represented as a list of named CounterSets, each of which
contains a set of named counters. The counters represent the resources available on the physical
device that are used by the logical devices advertised through DRA.</p><p>Logical devices can specify the ConsumesCounters list. Each entry contains a reference to a CounterSet
and a set of named counters with the amounts they will consume. So for a device to be allocatable,
the referenced counter sets must have sufficient quantity for the counters referenced by the device.</p><p>Here is an example of two devices, each consuming 6Gi of memory from the a shared counter with
8Gi of memory. Thus, only one of the devices can be allocated at any point in time. The scheduler
handles this and it is transparent to the consumer as the ResourceClaim API is not affected.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>ResourceSlice<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>resourceslice<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>nodeName</span>:<span> </span>worker-1<span>
</span></span></span><span><span><span>  </span><span>pool</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>pool<span>
</span></span></span><span><span><span>    </span><span>generation</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>resourceSliceCount</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>driver</span>:<span> </span>dra.example.com<span>
</span></span></span><span><span><span>  </span><span>sharedCounters</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>gpu-1-counters<span>
</span></span></span><span><span><span>    </span><span>counters</span>:<span>
</span></span></span><span><span><span>      </span><span>memory</span>:<span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span>8Gi<span>
</span></span></span><span><span><span>  </span><span>devices</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>device-1<span>
</span></span></span><span><span><span>    </span><span>consumesCounters</span>:<span>
</span></span></span><span><span><span>    </span>- <span>counterSet</span>:<span> </span>gpu-1-counters<span>
</span></span></span><span><span><span>      </span><span>counters</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span>6Gi<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>device-2<span>
</span></span></span><span><span><span>    </span><span>consumesCounters</span>:<span>
</span></span></span><span><span><span>    </span>- <span>counterSet</span>:<span> </span>gpu-1-counters<span>
</span></span></span><span><span><span>      </span><span>counters</span>:<span>
</span></span></span><span><span><span>        </span><span>memory</span>:<span>
</span></span></span><span><span><span>          </span><span>value</span>:<span> </span>6Gi<span>
</span></span></span></code></pre></div><p>Partitionable devices is an <em>alpha feature</em> and only enabled when the
<code>DRAPartitionableDevices</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled in the kube-apiserver and kube-scheduler.</p><h2 id="consumable-capacity">Consumable capacity</h2><div class="feature-state-notice feature-alpha" title="Feature Gate: DRAConsumableCapacity"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>The consumable capacity feature allows the same devices to be consumed by multiple independent ResourceClaims, with the Kubernetes scheduler
managing how much of the device's capacity is used up by each claim. This is analogous to how Pods can share
the resources on a Node; ResourceClaims can share the resources on a Device.</p><p>The device driver can set <code>allowMultipleAllocations</code> field added in <code>.spec.devices</code> of <code>ResourceSlice</code> to allow allocating that device to multiple independent ResourceClaims or to multiple requests within a ResourceClaim.</p><p>Users can set <code>capacity</code> field added in <code>spec.devices.requests</code> of <code>ResourceClaim</code> to specify the device resource requirements for each allocation.</p><p>For the device that allows multiple allocations, the requested capacity is drawn from &#8212; or consumed from &#8212; its total capacity, a concept known as <strong>consumable capacity</strong>.
Then, the scheduler ensures that the aggregate consumed capacity across all claims does not exceed the device&#8217;s overall capacity. Furthermore, driver authors can use the <code>requestPolicy</code> constraints on individual device capacities to control how those capacities are consumed. For example, the driver author can specify that a given capacity is only consumed in increments of 1Gi.</p><p>Here is an example of a network device which allows multiple allocations and contains
a consumable bandwidth capacity.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>kind</span>:<span> </span>ResourceSlice<span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>resourceslice<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>nodeName</span>:<span> </span>worker-1<span>
</span></span></span><span><span><span>  </span><span>pool</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>pool<span>
</span></span></span><span><span><span>    </span><span>generation</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>resourceSliceCount</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>driver</span>:<span> </span>dra.example.com<span>
</span></span></span><span><span><span>  </span><span>devices</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>eth1<span>
</span></span></span><span><span><span>    </span><span>allowMultipleAllocations</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>attributes</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span>
</span></span></span><span><span><span>        </span><span>string</span>:<span> </span><span>"eth1"</span><span>
</span></span></span><span><span><span>    </span><span>capacity</span>:<span>
</span></span></span><span><span><span>      </span><span>bandwidth</span>:<span>
</span></span></span><span><span><span>        </span><span>requestPolicy</span>:<span>
</span></span></span><span><span><span>          </span><span>default</span>:<span> </span><span>"1M"</span><span>
</span></span></span><span><span><span>          </span><span>validRange</span>:<span>
</span></span></span><span><span><span>            </span><span>min</span>:<span> </span><span>"1M"</span><span>
</span></span></span><span><span><span>            </span><span>step</span>:<span> </span><span>"8"</span><span>
</span></span></span><span><span><span>        </span><span>value</span>:<span> </span><span>"10G"</span><span>
</span></span></span></code></pre></div><p>The consumable capacity can be requested as shown in the below example.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceClaimTemplate<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>bandwidth-claim-template<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>spec</span>:<span>
</span></span></span><span><span><span>    </span><span>devices</span>:<span>
</span></span></span><span><span><span>      </span><span>requests</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>req-0<span>
</span></span></span><span><span><span>        </span><span>exactly</span>:<span>
</span></span></span><span><span><span>          </span><span>deviceClassName</span>:<span> </span>resource.example.com<span>
</span></span></span><span><span><span>          </span><span>capacity</span>:<span>
</span></span></span><span><span><span>            </span><span>requests</span>:<span>
</span></span></span><span><span><span>              </span><span>bandwidth</span>:<span> </span>1G<span>
</span></span></span></code></pre></div><p>The allocation result will include the consumed capacity and the identifier of the share.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceClaim<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span></span><span>status</span>:<span>
</span></span></span><span><span><span>  </span><span>allocation</span>:<span>
</span></span></span><span><span><span>    </span><span>devices</span>:<span>
</span></span></span><span><span><span>      </span><span>results</span>:<span>
</span></span></span><span><span><span>      </span>- <span>consumedCapacity</span>:<span>
</span></span></span><span><span><span>          </span><span>bandwidth</span>:<span> </span>1G<span>
</span></span></span><span><span><span>        </span><span>device</span>:<span> </span>eth1<span>
</span></span></span><span><span><span>        </span><span>shareID</span>:<span> </span><span>"a671734a-e8e5-11e4-8fde-42010af09327"</span><span>
</span></span></span></code></pre></div><p>In this example, a multiply-allocatable device was chosen. However, any <code>resource.example.com</code> device with at least the requested 1G bandwidth could have met the requirement. If a non-multiply-allocatable device were chosen, the allocation would have resulted in the entire device. To force the use of a only multiply-allocatable devices, you can use the CEL criteria <code>device.allowMultipleAllocations == true</code>.</p><h3 id="device-taints-and-tolerations">Device taints and tolerations</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DRADeviceTaints"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [alpha]</code> (enabled by default: false)</div><p>Device taints are similar to node taints: a taint has a string key, a string
value, and an effect. The effect is applied to the ResourceClaim which is
using a tainted device and to all Pods referencing that ResourceClaim.
The "NoSchedule" effect prevents scheduling those Pods.
Tainted devices are ignored when trying to allocate a ResourceClaim
because using them would prevent scheduling of Pods.</p><p>The "NoExecute" effect implies "NoSchedule" and in addition causes eviction
of all Pods which have been scheduled already. This eviction is implemented
in the device taint eviction controller in kube-controller-manager by
deleting affected Pods.</p><p>ResourceClaims can tolerate taints. If a taint is tolerated, its effect does
not apply. An empty toleration matches all taints. A toleration can be limited to
certain effects and/or match certain key/value pairs. A toleration can check
that a certain key exists, regardless which value it has, or it can check
for specific values of a key.
For more information on this matching see the
<a href="/docs/concepts/scheduling-eviction/taint-and-toleration/#concepts">node taint concepts</a>.</p><p>Eviction can be delayed by tolerating a taint for a certain duration.
That delay starts at the time when a taint gets added to a device, which is recorded in a field
of the taint.</p><p>Taints apply as described above also to ResourceClaims allocating "all" devices on a node.
All devices must be untainted or all of their taints must be tolerated.
Allocating a device with admin access (described <a href="#admin-access">above</a>)
is not exempt either. An admin using that mode must explicitly tolerate all taints
to access tainted devices.</p><p>Device taints and tolerations is an <em>alpha feature</em> and only enabled when the
<code>DRADeviceTaints</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled in the kube-apiserver, kube-controller-manager and kube-scheduler.
To use DeviceTaintRules, the <code>resource.k8s.io/v1alpha3</code> API version must be
enabled.</p><p>You can add taints to devices in the following ways, by using the
DeviceTaintRule API kind.</p><h4 id="taints-set-by-the-driver">Taints set by the driver</h4><p>A DRA driver can add taints to the device information that it publishes in ResourceSlices.
Consult the documentation of a DRA driver to learn whether the driver uses taints and what
their keys and values are.</p><h4 id="taints-set-by-an-admin">Taints set by an admin</h4><p>An admin or a control plane component can taint devices without having to tell
the DRA driver to include taints in its device information in ResourceSlices. They do that by
creating DeviceTaintRules. Each DeviceTaintRule adds one taint to devices which
match the device selector. Without such a selector, no devices are tainted. This
makes it harder to accidentally evict all pods using ResourceClaims when leaving out
the selector by mistake.</p><p>Devices can be selected by giving the name of a DeviceClass, driver, pool,
and/or device. The DeviceClass selects all devices that are selected by the
selectors in that DeviceClass. With just the driver name, an admin can taint
all devices managed by that driver, for example while doing some kind of
maintenance of that driver across the entire cluster. Adding a pool name can
limit the taint to a single node, if the driver manages node-local devices.</p><p>Finally, adding the device name can select one specific device. The device name
and pool name can also be used alone, if desired. For example, drivers for node-local
devices are encouraged to use the node name as their pool name. Then tainting with
that pool name automatically taints all devices on a node.</p><p>Drivers might use stable names like "gpu-0" that hide which specific device is
currently assigned to that name. To support tainting a specific hardware
instance, CEL selectors can be used in a DeviceTaintRule to match a vendor-specific
unique ID attribute, if the driver supports one for its hardware.</p><p>The taint applies as long as the DeviceTaintRule exists. It can be modified and
and removed at any time. Here is one example of a DeviceTaintRule for a fictional
DRA driver:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1alpha3<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>DeviceTaintRule<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>example<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span># The entire hardware installation for this</span><span>
</span></span></span><span><span><span>  </span><span># particular driver is broken.</span><span>
</span></span></span><span><span><span>  </span><span># Evict all pods and don't schedule new ones.</span><span>
</span></span></span><span><span><span>  </span><span>deviceSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>driver</span>:<span> </span>dra.example.com<span>
</span></span></span><span><span><span>  </span><span>taint</span>:<span>
</span></span></span><span><span><span>    </span><span>key</span>:<span> </span>dra.example.com/unhealthy<span>
</span></span></span><span><span><span>    </span><span>value</span>:<span> </span>Broken<span>
</span></span></span><span><span><span>    </span><span>effect</span>:<span> </span>NoExecute<span>
</span></span></span></code></pre></div><h3 id="device-binding-conditions">Device Binding Conditions</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: DRADeviceBindingConditions"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [alpha]</code> (enabled by default: false)</div><p>Device Binding Conditions allow the Kubernetes scheduler to delay Pod binding until
external resources, such as fabric-attached GPUs or reprogrammable FPGAs, are confirmed
to be ready.</p><p>This waiting behavior is implemented in the
<a href="/docs/concepts/scheduling-eviction/scheduling-framework/#pre-bind">PreBind phase</a>
of the scheduling framework.
During this phase, the scheduler checks whether all required device conditions are
satisfied before proceeding with binding.</p><p>This improves scheduling reliability by avoiding premature binding and enables coordination
with external device controllers.</p><p>To use this feature, device drivers (typically managed by driver owners) must publish the
following fields in the <code>Device</code> section of a <code>ResourceSlice</code>. Cluster administrators
must enable the <code>DRADeviceBindingConditions</code> and <code>DRAResourceClaimDeviceStatus</code> feature
gates for the scheduler to honor these fields.</p><ul><li><code>bindingConditions</code>: A list of condition types that must be set to True in the
status.conditions field of the associated ResourceClaim before the Pod can be bound.
These typically represent readiness signals such as "DeviceAttached" or "DeviceInitialized".</li><li><code>bindingFailureConditions</code>: A list of condition types that, if set to True in
status.conditions field of the associated ResourceClaim, indicate a failure state.
If any of these conditions are True, the scheduler will abort binding and reschedule the Pod.</li><li><code>bindsToNode</code>: if set to <code>true</code>, the scheduler records the selected node name in the
<code>status.allocation.nodeSelector</code> field of the ResourceClaim.
This does not affect the Pod's <code>spec.nodeSelector</code>. Instead, it sets a node selector
inside the ResourceClaim, which external controllers can use to perform node-specific
operations such as device attachment or preparation.</li></ul><p>All condition types listed in bindingConditions and bindingFailureConditions are evaluated
from the <code>status.conditions</code> field of the ResourceClaim.
External controllers are responsible for updating these conditions using standard Kubernetes
condition semantics (<code>type</code>, <code>status</code>, <code>reason</code>, <code>message</code>, <code>lastTransitionTime</code>).</p><p>The scheduler waits up to <strong>600 seconds</strong> for all <code>bindingConditions</code> to become <code>True</code>.
If the timeout is reached or any <code>bindingFailureConditions</code> are <code>True</code>, the scheduler
clears the allocation and reschedules the Pod.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>resource.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ResourceSlice<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>gpu-slice<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>driver</span>:<span> </span>dra.example.com<span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>nodeSelectorTerms</span>:<span>
</span></span></span><span><span><span>    </span>- <span>matchExpressions</span>:<span>
</span></span></span><span><span><span>      </span>- <span>key</span>:<span> </span>accelerator-type<span>
</span></span></span><span><span><span>        </span><span>operator</span>:<span> </span>In<span>
</span></span></span><span><span><span>        </span><span>values</span>:<span>
</span></span></span><span><span><span>        </span>- <span>"high-performance"</span><span>
</span></span></span><span><span><span>  </span><span>pool</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>gpu-pool<span>
</span></span></span><span><span><span>    </span><span>generation</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>    </span><span>resourceSliceCount</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>devices</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>gpu-1<span>
</span></span></span><span><span><span>      </span><span>attributes</span>:<span>
</span></span></span><span><span><span>        </span><span>vendor</span>:<span>
</span></span></span><span><span><span>          </span><span>string</span>:<span> </span><span>"example"</span><span>
</span></span></span><span><span><span>        </span><span>model</span>:<span>
</span></span></span><span><span><span>          </span><span>string</span>:<span> </span><span>"example-gpu"</span><span>
</span></span></span><span><span><span>      </span><span>bindsToNode</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>      </span><span>bindingConditions</span>:<span>
</span></span></span><span><span><span>        </span>- dra.example.com/is-prepared<span>
</span></span></span><span><span><span>      </span><span>bindingFailureConditions</span>:<span>
</span></span></span><span><span><span>        </span>- dra.example.com/preparing-failed<span>
</span></span></span></code></pre></div><p>This example ResourceSlice has the following properties:</p><ul><li>The ResourceSlice targets nodes labeled with <code>accelerator-type=high-performance</code>,
so that the scheduler uses only a specific set of eligible nodes.</li><li>The scheduler selects one node from the selected group (for example, <code>node-3</code>) and sets
the <code>status.allocation.nodeSelector</code> field in the ResourceClaim to that node name.</li><li>The <code>dra.example.com/is-prepared</code> binding condition indicates that the device <code>gpu-1</code>
must be prepared (the <code>is-prepared</code> condition has a status of <code>True</code>) before binding.</li><li>If the <code>gpu-1</code> device preparation fails (the <code>preparing-failed</code> condition has a status of <code>True</code>), the scheduler aborts binding.</li><li>The scheduler waits up to 600 seconds for the device to become ready.</li><li>External controllers can use the node selector in the ResourceClaim to perform
node-specific setup on the selected node.</li></ul><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/tasks/configure-pod-container/assign-resources/set-up-dra-cluster/">Set Up DRA in a Cluster</a></li><li><a href="/docs/tasks/configure-pod-container/assign-resources/allocate-devices-dra/">Allocate devices to workloads using DRA</a></li><li>For more information on the design, see the
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/4381-dra-structured-parameters">Dynamic Resource Allocation with Structured Parameters</a>
KEP.</li></ul></div></div><div><div class="td-content"><h1>Scheduler Performance Tuning</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.14 [beta]</code></div><p><a href="/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler">kube-scheduler</a>
is the Kubernetes default scheduler. It is responsible for placement of Pods
on Nodes in a cluster.</p><p>Nodes in a cluster that meet the scheduling requirements of a Pod are
called <em>feasible</em> Nodes for the Pod. The scheduler finds feasible Nodes
for a Pod and then runs a set of functions to score the feasible Nodes,
picking a Node with the highest score among the feasible ones to run
the Pod. The scheduler then notifies the API server about this decision
in a process called <em>Binding</em>.</p><p>This page explains performance tuning optimizations that are relevant for
large Kubernetes clusters.</p><p>In large clusters, you can tune the scheduler's behaviour balancing
scheduling outcomes between latency (new Pods are placed quickly) and
accuracy (the scheduler rarely makes poor placement decisions).</p><p>You configure this tuning setting via kube-scheduler setting
<code>percentageOfNodesToScore</code>. This KubeSchedulerConfiguration setting determines
a threshold for scheduling nodes in your cluster.</p><h3 id="setting-the-threshold">Setting the threshold</h3><p>The <code>percentageOfNodesToScore</code> option accepts whole numeric values between 0
and 100. The value 0 is a special number which indicates that the kube-scheduler
should use its compiled-in default.
If you set <code>percentageOfNodesToScore</code> above 100, kube-scheduler acts as if you
had set a value of 100.</p><p>To change the value, edit the
<a href="/docs/reference/config-api/kube-scheduler-config.v1/">kube-scheduler configuration file</a>
and then restart the scheduler.
In many cases, the configuration file can be found at <code>/etc/kubernetes/config/kube-scheduler.yaml</code>.</p><p>After you have made this change, you can run</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get pods -n kube-system | grep kube-scheduler
</span></span></code></pre></div><p>to verify that the kube-scheduler component is healthy.</p><h2 id="percentage-of-nodes-to-score">Node scoring threshold</h2><p>To improve scheduling performance, the kube-scheduler can stop looking for
feasible nodes once it has found enough of them. In large clusters, this saves
time compared to a naive approach that would consider every node.</p><p>You specify a threshold for how many nodes are enough, as a whole number percentage
of all the nodes in your cluster. The kube-scheduler converts this into an
integer number of nodes. During scheduling, if the kube-scheduler has identified
enough feasible nodes to exceed the configured percentage, the kube-scheduler
stops searching for more feasible nodes and moves on to the
<a href="/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation">scoring phase</a>.</p><p><a href="#how-the-scheduler-iterates-over-nodes">How the scheduler iterates over Nodes</a>
describes the process in detail.</p><h3 id="default-threshold">Default threshold</h3><p>If you don't specify a threshold, Kubernetes calculates a figure using a
linear formula that yields 50% for a 100-node cluster and yields 10%
for a 5000-node cluster. The lower bound for the automatic value is 5%.</p><p>This means that the kube-scheduler always scores at least 5% of your cluster no
matter how large the cluster is, unless you have explicitly set
<code>percentageOfNodesToScore</code> to be smaller than 5.</p><p>If you want the scheduler to score all nodes in your cluster, set
<code>percentageOfNodesToScore</code> to 100.</p><h2 id="example">Example</h2><p>Below is an example configuration that sets <code>percentageOfNodesToScore</code> to 50%.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubescheduler.config.k8s.io/v1alpha1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeSchedulerConfiguration<span>
</span></span></span><span><span><span></span><span>algorithmSource</span>:<span>
</span></span></span><span><span><span>  </span><span>provider</span>:<span> </span>DefaultProvider<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>percentageOfNodesToScore</span>:<span> </span><span>50</span><span>
</span></span></span></code></pre></div><h2 id="tuning-percentageofnodestoscore">Tuning percentageOfNodesToScore</h2><p><code>percentageOfNodesToScore</code> must be a value between 1 and 100 with the default
value being calculated based on the cluster size. There is also a hardcoded
minimum value of 100 nodes.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>In clusters with less than 100 feasible nodes, the scheduler still
checks all the nodes because there are not enough feasible nodes to stop
the scheduler's search early.</p><p>In a small cluster, if you set a low value for <code>percentageOfNodesToScore</code>, your
change will have no or little effect, for a similar reason.</p><p>If your cluster has several hundred Nodes or fewer, leave this configuration option
at its default value. Making changes is unlikely to improve the
scheduler's performance significantly.</p></div><p>An important detail to consider when setting this value is that when a smaller
number of nodes in a cluster are checked for feasibility, some nodes are not
sent to be scored for a given Pod. As a result, a Node which could possibly
score a higher value for running the given Pod might not even be passed to the
scoring phase. This would result in a less than ideal placement of the Pod.</p><p>You should avoid setting <code>percentageOfNodesToScore</code> very low so that kube-scheduler
does not make frequent, poor Pod placement decisions. Avoid setting the
percentage to anything below 10%, unless the scheduler's throughput is critical
for your application and the score of nodes is not important. In other words, you
prefer to run the Pod on any Node as long as it is feasible.</p><h2 id="how-the-scheduler-iterates-over-nodes">How the scheduler iterates over Nodes</h2><p>This section is intended for those who want to understand the internal details
of this feature.</p><p>In order to give all the Nodes in a cluster a fair chance of being considered
for running Pods, the scheduler iterates over the nodes in a round robin
fashion. You can imagine that Nodes are in an array. The scheduler starts from
the start of the array and checks feasibility of the nodes until it finds enough
Nodes as specified by <code>percentageOfNodesToScore</code>. For the next Pod, the
scheduler continues from the point in the Node array that it stopped at when
checking feasibility of Nodes for the previous Pod.</p><p>If Nodes are in multiple zones, the scheduler iterates over Nodes in various
zones to ensure that Nodes from different zones are considered in the
feasibility checks. As an example, consider six nodes in two zones:</p><pre tabindex="0"><code>Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
</code></pre><p>The Scheduler evaluates feasibility of the nodes in this order:</p><pre tabindex="0"><code>Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
</code></pre><p>After going over all the Nodes, it goes back to Node 1.</p><h2 id="what-s-next">What's next</h2><ul><li>Check the <a href="/docs/reference/config-api/kube-scheduler-config.v1/">kube-scheduler configuration reference (v1)</a></li></ul></div></div><div><div class="td-content"><h1>Resource Bin Packing</h1><p>In the <a href="/docs/reference/scheduling/config/#scheduling-plugins">scheduling-plugin</a> <code>NodeResourcesFit</code> of kube-scheduler, there are two
scoring strategies that support the bin packing of resources: <code>MostAllocated</code> and <code>RequestedToCapacityRatio</code>.</p><h2 id="enabling-bin-packing-using-mostallocated-strategy">Enabling bin packing using MostAllocated strategy</h2><p>The <code>MostAllocated</code> strategy scores the nodes based on the utilization of resources, favoring the ones with higher allocation.
For each resource type, you can set a weight to modify its influence in the node score.</p><p>To set the <code>MostAllocated</code> strategy for the <code>NodeResourcesFit</code> plugin, use a
<a href="/docs/reference/scheduling/config/">scheduler configuration</a> similar to the following:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubescheduler.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeSchedulerConfiguration<span>
</span></span></span><span><span><span></span><span>profiles</span>:<span>
</span></span></span><span><span><span></span>- <span>pluginConfig</span>:<span>
</span></span></span><span><span><span>  </span>- <span>args</span>:<span>
</span></span></span><span><span><span>      </span><span>scoringStrategy</span>:<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>          </span><span>weight</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>memory<span>
</span></span></span><span><span><span>          </span><span>weight</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>intel.com/foo<span>
</span></span></span><span><span><span>          </span><span>weight</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>intel.com/bar<span>
</span></span></span><span><span><span>          </span><span>weight</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>MostAllocated<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>NodeResourcesFit<span>
</span></span></span></code></pre></div><p>To learn more about other parameters and their default configuration, see the API documentation for
<a href="/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-NodeResourcesFitArgs"><code>NodeResourcesFitArgs</code></a>.</p><h2 id="enabling-bin-packing-using-requestedtocapacityratio">Enabling bin packing using RequestedToCapacityRatio</h2><p>The <code>RequestedToCapacityRatio</code> strategy allows the users to specify the resources along with weights for
each resource to score nodes based on the request to capacity ratio. This
allows users to bin pack extended resources by using appropriate parameters
to improve the utilization of scarce resources in large clusters. It favors nodes according to a
configured function of the allocated resources. The behavior of the <code>RequestedToCapacityRatio</code> in
the <code>NodeResourcesFit</code> score function can be controlled by the
<a href="/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-ScoringStrategy">scoringStrategy</a> field.
Within the <code>scoringStrategy</code> field, you can configure two parameters: <code>requestedToCapacityRatio</code> and
<code>resources</code>. The <code>shape</code> in the <code>requestedToCapacityRatio</code>
parameter allows the user to tune the function as least requested or most
requested based on <code>utilization</code> and <code>score</code> values. The <code>resources</code> parameter
comprises both the <code>name</code> of the resource to be considered during scoring and
its corresponding <code>weight</code>, which specifies the weight of each resource.</p><p>Below is an example configuration that sets
the bin packing behavior for extended resources <code>intel.com/foo</code> and <code>intel.com/bar</code>
using the <code>requestedToCapacityRatio</code> field.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubescheduler.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeSchedulerConfiguration<span>
</span></span></span><span><span><span></span><span>profiles</span>:<span>
</span></span></span><span><span><span></span>- <span>pluginConfig</span>:<span>
</span></span></span><span><span><span>  </span>- <span>args</span>:<span>
</span></span></span><span><span><span>      </span><span>scoringStrategy</span>:<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>intel.com/foo<span>
</span></span></span><span><span><span>          </span><span>weight</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>        </span>- <span>name</span>:<span> </span>intel.com/bar<span>
</span></span></span><span><span><span>          </span><span>weight</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>        </span><span>requestedToCapacityRatio</span>:<span>
</span></span></span><span><span><span>          </span><span>shape</span>:<span>
</span></span></span><span><span><span>          </span>- <span>utilization</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>            </span><span>score</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>          </span>- <span>utilization</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>            </span><span>score</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>RequestedToCapacityRatio<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>NodeResourcesFit<span>
</span></span></span></code></pre></div><p>Referencing the <code>KubeSchedulerConfiguration</code> file with the kube-scheduler
flag <code>--config=/path/to/config/file</code> will pass the configuration to the
scheduler.</p><p>To learn more about other parameters and their default configuration, see the API documentation for
<a href="/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-NodeResourcesFitArgs"><code>NodeResourcesFitArgs</code></a>.</p><h3 id="tuning-the-score-function">Tuning the score function</h3><p><code>shape</code> is used to specify the behavior of the <code>RequestedToCapacityRatio</code> function.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>shape</span>:<span>
</span></span></span><span><span><span>  </span>- <span>utilization</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>    </span><span>score</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>  </span>- <span>utilization</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>    </span><span>score</span>:<span> </span><span>10</span><span>
</span></span></span></code></pre></div><p>The above arguments give the node a <code>score</code> of 0 if <code>utilization</code> is 0% and 10 for
<code>utilization</code> 100%, thus enabling bin packing behavior. To enable least
requested the score value must be reversed as follows.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>shape</span>:<span>
</span></span></span><span><span><span>  </span>- <span>utilization</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>    </span><span>score</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span>- <span>utilization</span>:<span> </span><span>100</span><span>
</span></span></span><span><span><span>    </span><span>score</span>:<span> </span><span>0</span><span>
</span></span></span></code></pre></div><p><code>resources</code> is an optional parameter which defaults to:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>    </span><span>weight</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>memory<span>
</span></span></span><span><span><span>    </span><span>weight</span>:<span> </span><span>1</span><span>
</span></span></span></code></pre></div><p>It can be used to add extended resources as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>resources</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>intel.com/foo<span>
</span></span></span><span><span><span>    </span><span>weight</span>:<span> </span><span>5</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>cpu<span>
</span></span></span><span><span><span>    </span><span>weight</span>:<span> </span><span>3</span><span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>memory<span>
</span></span></span><span><span><span>    </span><span>weight</span>:<span> </span><span>1</span><span>
</span></span></span></code></pre></div><p>The <code>weight</code> parameter is optional and is set to 1 if not specified. Also, the
<code>weight</code> cannot be set to a negative value.</p><h3 id="node-scoring-for-capacity-allocation">Node scoring for capacity allocation</h3><p>This section is intended for those who want to understand the internal details
of this feature.
Below is an example of how the node score is calculated for a given set of values.</p><p>Requested resources:</p><pre tabindex="0"><code>intel.com/foo : 2
memory: 256MB
cpu: 2
</code></pre><p>Resource weights:</p><pre tabindex="0"><code>intel.com/foo : 5
memory: 1
cpu: 3
</code></pre><p>FunctionShapePoint {{0, 0}, {100, 10}}</p><p>Node 1 spec:</p><pre tabindex="0"><code>Available:
  intel.com/foo: 4
  memory: 1 GB
  cpu: 8

Used:
  intel.com/foo: 1
  memory: 256MB
  cpu: 1
</code></pre><p>Node score:</p><pre tabindex="0"><code>intel.com/foo  = resourceScoringFunction((2+1),4)
               = (100 - ((4-3)*100/4))
               = (100 - 25)
               = 75                       # requested + used = 75% * available
               = rawScoringFunction(75)
               = 7                        # floor(75/10)

memory         = resourceScoringFunction((256+256),1024)
               = (100 -((1024-512)*100/1024))
               = 50                       # requested + used = 50% * available
               = rawScoringFunction(50)
               = 5                        # floor(50/10)

cpu            = resourceScoringFunction((2+1),8)
               = (100 -((8-3)*100/8))
               = 37.5                     # requested + used = 37.5% * available
               = rawScoringFunction(37.5)
               = 3                        # floor(37.5/10)

NodeScore   =  ((7 * 5) + (5 * 1) + (3 * 3)) / (5 + 1 + 3)
            =  5
</code></pre><p>Node 2 spec:</p><pre tabindex="0"><code>Available:
  intel.com/foo: 8
  memory: 1GB
  cpu: 8
Used:
  intel.com/foo: 2
  memory: 512MB
  cpu: 6
</code></pre><p>Node score:</p><pre tabindex="0"><code>intel.com/foo  = resourceScoringFunction((2+2),8)
               =  (100 - ((8-4)*100/8)
               =  (100 - 50)
               =  50
               =  rawScoringFunction(50)
               = 5

memory         = resourceScoringFunction((256+512),1024)
               = (100 -((1024-768)*100/1024))
               = 75
               = rawScoringFunction(75)
               = 7

cpu            = resourceScoringFunction((2+6),8)
               = (100 -((8-8)*100/8))
               = 100
               = rawScoringFunction(100)
               = 10

NodeScore   =  ((5 * 5) + (7 * 1) + (10 * 3)) / (5 + 1 + 3)
            =  7
</code></pre><h2 id="what-s-next">What's next</h2><ul><li>Read more about the <a href="/docs/concepts/scheduling-eviction/scheduling-framework/">scheduling framework</a></li><li>Read more about <a href="/docs/reference/scheduling/config/">scheduler configuration</a></li></ul></div></div><div><div class="td-content"><h1>Pod Priority and Preemption</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.14 [stable]</code></div><p><a href="/docs/concepts/workloads/pods/">Pods</a> can have <em>priority</em>. Priority indicates the
importance of a Pod relative to other Pods. If a Pod cannot be scheduled, the
scheduler tries to preempt (evict) lower priority Pods to make scheduling of the
pending Pod possible.</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4><p>In a cluster where not all users are trusted, a malicious user could create Pods
at the highest possible priorities, causing other Pods to be evicted/not get
scheduled.
An administrator can use ResourceQuota to prevent users from creating pods at
high priorities.</p><p>See <a href="/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default">limit Priority Class consumption by default</a>
for details.</p></div><h2 id="how-to-use-priority-and-preemption">How to use priority and preemption</h2><p>To use priority and preemption:</p><ol><li><p>Add one or more <a href="#priorityclass">PriorityClasses</a>.</p></li><li><p>Create Pods with<a href="#pod-priority"><code>priorityClassName</code></a> set to one of the added
PriorityClasses. Of course you do not need to create the Pods directly;
normally you would add <code>priorityClassName</code> to the Pod template of a
collection object like a Deployment.</p></li></ol><p>Keep reading for more information about these steps.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Kubernetes already ships with two PriorityClasses:
<code>system-cluster-critical</code> and <code>system-node-critical</code>.
These are common classes and are used to <a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">ensure that critical components are always scheduled first</a>.</div><h2 id="priorityclass">PriorityClass</h2><p>A PriorityClass is a non-namespaced object that defines a mapping from a
priority class name to the integer value of the priority. The name is specified
in the <code>name</code> field of the PriorityClass object's metadata. The value is
specified in the required <code>value</code> field. The higher the value, the higher the
priority.
The name of a PriorityClass object must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a>,
and it cannot be prefixed with <code>system-</code>.</p><p>A PriorityClass object can have any 32-bit integer value smaller than or equal
to 1 billion. This means that the range of values for a PriorityClass object is
from -2147483648 to 1000000000 inclusive. Larger numbers are reserved for
built-in PriorityClasses that represent critical system Pods. A cluster
admin should create one PriorityClass object for each such mapping that they want.</p><p>PriorityClass also has two optional fields: <code>globalDefault</code> and <code>description</code>.
The <code>globalDefault</code> field indicates that the value of this PriorityClass should
be used for Pods without a <code>priorityClassName</code>. Only one PriorityClass with
<code>globalDefault</code> set to true can exist in the system. If there is no
PriorityClass with <code>globalDefault</code> set, the priority of Pods with no
<code>priorityClassName</code> is zero.</p><p>The <code>description</code> field is an arbitrary string. It is meant to tell users of the
cluster when they should use this PriorityClass.</p><h3 id="notes-about-podpriority-and-existing-clusters">Notes about PodPriority and existing clusters</h3><ul><li><p>If you upgrade an existing cluster without this feature, the priority
of your existing Pods is effectively zero.</p></li><li><p>Addition of a PriorityClass with <code>globalDefault</code> set to <code>true</code> does not
change the priorities of existing Pods. The value of such a PriorityClass is
used only for Pods created after the PriorityClass is added.</p></li><li><p>If you delete a PriorityClass, existing Pods that use the name of the
deleted PriorityClass remain unchanged, but you cannot create more Pods that
use the name of the deleted PriorityClass.</p></li></ul><h3 id="example-priorityclass">Example PriorityClass</h3><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>scheduling.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PriorityClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>high-priority<span>
</span></span></span><span><span><span></span><span>value</span>:<span> </span><span>1000000</span><span>
</span></span></span><span><span><span></span><span>globalDefault</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span></span><span>description</span>:<span> </span><span>"This priority class should be used for XYZ service pods only."</span><span>
</span></span></span></code></pre></div><h2 id="non-preempting-priority-class">Non-preempting PriorityClass</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [stable]</code></div><p>Pods with <code>preemptionPolicy: Never</code> will be placed in the scheduling queue
ahead of lower-priority pods,
but they cannot preempt other pods.
A non-preempting pod waiting to be scheduled will stay in the scheduling queue,
until sufficient resources are free,
and it can be scheduled.
Non-preempting pods,
like other pods,
are subject to scheduler back-off.
This means that if the scheduler tries these pods and they cannot be scheduled,
they will be retried with lower frequency,
allowing other pods with lower priority to be scheduled before them.</p><p>Non-preempting pods may still be preempted by other,
high-priority pods.</p><p><code>preemptionPolicy</code> defaults to <code>PreemptLowerPriority</code>,
which will allow pods of that PriorityClass to preempt lower-priority pods
(as is existing default behavior).
If <code>preemptionPolicy</code> is set to <code>Never</code>,
pods in that PriorityClass will be non-preempting.</p><p>An example use case is for data science workloads.
A user may submit a job that they want to be prioritized above other workloads,
but do not wish to discard existing work by preempting running pods.
The high priority job with <code>preemptionPolicy: Never</code> will be scheduled
ahead of other queued pods,
as soon as sufficient cluster resources "naturally" become free.</p><h3 id="example-non-preempting-priorityclass">Example Non-preempting PriorityClass</h3><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>scheduling.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>PriorityClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>high-priority-nonpreempting<span>
</span></span></span><span><span><span></span><span>value</span>:<span> </span><span>1000000</span><span>
</span></span></span><span><span><span></span><span>preemptionPolicy</span>:<span> </span>Never<span>
</span></span></span><span><span><span></span><span>globalDefault</span>:<span> </span><span>false</span><span>
</span></span></span><span><span><span></span><span>description</span>:<span> </span><span>"This priority class will not cause other pods to be preempted."</span><span>
</span></span></span></code></pre></div><h2 id="pod-priority">Pod priority</h2><p>After you have one or more PriorityClasses, you can create Pods that specify one
of those PriorityClass names in their specifications. The priority admission
controller uses the <code>priorityClassName</code> field and populates the integer value of
the priority. If the priority class is not found, the Pod is rejected.</p><p>The following YAML is an example of a Pod configuration that uses the
PriorityClass created in the preceding example. The priority admission
controller checks the specification and resolves the priority of the Pod to
1000000.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>env</span>:<span> </span>test<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>nginx<span>
</span></span></span><span><span><span>    </span><span>imagePullPolicy</span>:<span> </span>IfNotPresent<span>
</span></span></span><span><span><span>  </span><span>priorityClassName</span>:<span> </span>high-priority<span>
</span></span></span></code></pre></div><h3 id="effect-of-pod-priority-on-scheduling-order">Effect of Pod priority on scheduling order</h3><p>When Pod priority is enabled, the scheduler orders pending Pods by
their priority and a pending Pod is placed ahead of other pending Pods
with lower priority in the scheduling queue. As a result, the higher
priority Pod may be scheduled sooner than Pods with lower priority if
its scheduling requirements are met. If such Pod cannot be scheduled, the
scheduler will continue and try to schedule other lower priority Pods.</p><h2 id="preemption">Preemption</h2><p>When Pods are created, they go to a queue and wait to be scheduled. The
scheduler picks a Pod from the queue and tries to schedule it on a Node. If no
Node is found that satisfies all the specified requirements of the Pod,
preemption logic is triggered for the pending Pod. Let's call the pending Pod P.
Preemption logic tries to find a Node where removal of one or more Pods with
lower priority than P would enable P to be scheduled on that Node. If such a
Node is found, one or more lower priority Pods get evicted from the Node. After
the Pods are gone, P can be scheduled on the Node.</p><h3 id="user-exposed-information">User exposed information</h3><p>When Pod P preempts one or more Pods on Node N, <code>nominatedNodeName</code> field of Pod
P's status is set to the name of Node N. This field helps the scheduler track
resources reserved for Pod P and also gives users information about preemptions
in their clusters.</p><p>Please note that Pod P is not necessarily scheduled to the "nominated Node".
The scheduler always tries the "nominated Node" before iterating over any other nodes.
After victim Pods are preempted, they get their graceful termination period. If
another node becomes available while scheduler is waiting for the victim Pods to
terminate, scheduler may use the other node to schedule Pod P. As a result
<code>nominatedNodeName</code> and <code>nodeName</code> of Pod spec are not always the same. Also, if
the scheduler preempts Pods on Node N, but then a higher priority Pod than Pod P
arrives, the scheduler may give Node N to the new higher priority Pod. In such a
case, scheduler clears <code>nominatedNodeName</code> of Pod P. By doing this, scheduler
makes Pod P eligible to preempt Pods on another Node.</p><h3 id="limitations-of-preemption">Limitations of preemption</h3><h4 id="graceful-termination-of-preemption-victims">Graceful termination of preemption victims</h4><p>When Pods are preempted, the victims get their
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">graceful termination period</a>.
They have that much time to finish their work and exit. If they don't, they are
killed. This graceful termination period creates a time gap between the point
that the scheduler preempts Pods and the time when the pending Pod (P) can be
scheduled on the Node (N). In the meantime, the scheduler keeps scheduling other
pending Pods. As victims exit or get terminated, the scheduler tries to schedule
Pods in the pending queue. Therefore, there is usually a time gap between the
point that scheduler preempts victims and the time that Pod P is scheduled. In
order to minimize this gap, one can set graceful termination period of lower
priority Pods to zero or a small number.</p><h4 id="poddisruptionbudget-is-supported-but-not-guaranteed">PodDisruptionBudget is supported, but not guaranteed</h4><p>A <a href="/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a> (PDB)
allows application owners to limit the number of Pods of a replicated application
that are down simultaneously from voluntary disruptions. Kubernetes supports
PDB when preempting Pods, but respecting PDB is best effort. The scheduler tries
to find victims whose PDB are not violated by preemption, but if no such victims
are found, preemption will still happen, and lower priority Pods will be removed
despite their PDBs being violated.</p><h4 id="inter-pod-affinity-on-lower-priority-pods">Inter-Pod affinity on lower-priority Pods</h4><p>A Node is considered for preemption only when the answer to this question is
yes: "If all the Pods with lower priority than the pending Pod are removed from
the Node, can the pending Pod be scheduled on the Node?"</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Preemption does not necessarily remove all lower-priority
Pods. If the pending Pod can be scheduled by removing fewer than all
lower-priority Pods, then only a portion of the lower-priority Pods are removed.
Even so, the answer to the preceding question must be yes. If the answer is no,
the Node is not considered for preemption.</div><p>If a pending Pod has inter-pod <a class="glossary-tooltip" title="Rules used by the scheduler to determine where to place pods" href="/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity" target="_blank">affinity</a>
to one or more of the lower-priority Pods on the Node, the inter-Pod affinity
rule cannot be satisfied in the absence of those lower-priority Pods. In this case,
the scheduler does not preempt any Pods on the Node. Instead, it looks for another
Node. The scheduler might find a suitable Node or it might not. There is no
guarantee that the pending Pod can be scheduled.</p><p>Our recommended solution for this problem is to create inter-Pod affinity only
towards equal or higher priority Pods.</p><h4 id="cross-node-preemption">Cross node preemption</h4><p>Suppose a Node N is being considered for preemption so that a pending Pod P can
be scheduled on N. P might become feasible on N only if a Pod on another Node is
preempted. Here's an example:</p><ul><li>Pod P is being considered for Node N.</li><li>Pod Q is running on another Node in the same Zone as Node N.</li><li>Pod P has Zone-wide anti-affinity with Pod Q (<code>topologyKey: topology.kubernetes.io/zone</code>).</li><li>There are no other cases of anti-affinity between Pod P and other Pods in
the Zone.</li><li>In order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler
does not perform cross-node preemption. So, Pod P will be deemed
unschedulable on Node N.</li></ul><p>If Pod Q were removed from its Node, the Pod anti-affinity violation would be
gone, and Pod P could possibly be scheduled on Node N.</p><p>We may consider adding cross Node preemption in future versions if there is
enough demand and if we find an algorithm with reasonable performance.</p><h2 id="troubleshooting">Troubleshooting</h2><p>Pod priority and preemption can have unwanted side effects. Here are some
examples of potential problems and ways to deal with them.</p><h3 id="pods-are-preempted-unnecessarily">Pods are preempted unnecessarily</h3><p>Preemption removes existing Pods from a cluster under resource pressure to make
room for higher priority pending Pods. If you give high priorities to
certain Pods by mistake, these unintentionally high priority Pods may cause
preemption in your cluster. Pod priority is specified by setting the
<code>priorityClassName</code> field in the Pod's specification. The integer value for
priority is then resolved and populated to the <code>priority</code> field of <code>podSpec</code>.</p><p>To address the problem, you can change the <code>priorityClassName</code> for those Pods
to use lower priority classes, or leave that field empty. An empty
<code>priorityClassName</code> is resolved to zero by default.</p><p>When a Pod is preempted, there will be events recorded for the preempted Pod.
Preemption should happen only when a cluster does not have enough resources for
a Pod. In such cases, preemption happens only when the priority of the pending
Pod (preemptor) is higher than the victim Pods. Preemption must not happen when
there is no pending Pod, or when the pending Pods have equal or lower priority
than the victims. If preemption happens in such scenarios, please file an issue.</p><h3 id="pods-are-preempted-but-the-preemptor-is-not-scheduled">Pods are preempted, but the preemptor is not scheduled</h3><p>When pods are preempted, they receive their requested graceful termination
period, which is by default 30 seconds. If the victim Pods do not terminate within
this period, they are forcibly terminated. Once all the victims go away, the
preemptor Pod can be scheduled.</p><p>While the preemptor Pod is waiting for the victims to go away, a higher priority
Pod may be created that fits on the same Node. In this case, the scheduler will
schedule the higher priority Pod instead of the preemptor.</p><p>This is expected behavior: the Pod with the higher priority should take the place
of a Pod with a lower priority.</p><h3 id="higher-priority-pods-are-preempted-before-lower-priority-pods">Higher priority Pods are preempted before lower priority pods</h3><p>The scheduler tries to find nodes that can run a pending Pod. If no node is
found, the scheduler tries to remove Pods with lower priority from an arbitrary
node in order to make room for the pending pod.
If a node with low priority Pods is not feasible to run the pending Pod, the scheduler
may choose another node with higher priority Pods (compared to the Pods on the
other node) for preemption. The victims must still have lower priority than the
preemptor Pod.</p><p>When there are multiple nodes available for preemption, the scheduler tries to
choose the node with a set of Pods with lowest priority. However, if such Pods
have PodDisruptionBudget that would be violated if they are preempted then the
scheduler may choose another node with higher priority Pods.</p><p>When multiple nodes exist for preemption and none of the above scenarios apply,
the scheduler chooses a node with the lowest priority.</p><h2 id="interactions-of-pod-priority-and-qos">Interactions between Pod priority and quality of service</h2><p>Pod priority and <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." href="/docs/concepts/workloads/pods/pod-qos/" target="_blank">QoS class</a>
are two orthogonal features with few interactions and no default restrictions on
setting the priority of a Pod based on its QoS classes. The scheduler's
preemption logic does not consider QoS when choosing preemption targets.
Preemption considers Pod priority and attempts to choose a set of targets with
the lowest priority. Higher-priority Pods are considered for preemption only if
the removal of the lowest priority Pods is not sufficient to allow the scheduler
to schedule the preemptor Pod, or if the lowest priority Pods are protected by
<code>PodDisruptionBudget</code>.</p><p>The kubelet uses Priority to determine pod order for <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">node-pressure eviction</a>.
You can use the QoS class to estimate the order in which pods are most likely
to get evicted. The kubelet ranks pods for eviction based on the following factors:</p><ol><li>Whether the starved resource usage exceeds requests</li><li>Pod Priority</li><li>Amount of resource usage relative to requests</li></ol><p>See <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction">Pod selection for kubelet eviction</a>
for more details.</p><p>kubelet node-pressure eviction does not evict Pods when their
usage does not exceed their requests. If a Pod with lower priority is not
exceeding its requests, it won't be evicted. Another Pod with higher priority
that exceeds its requests may be evicted.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about using ResourceQuotas in connection with PriorityClasses: <a href="/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default">limit Priority Class consumption by default</a></li><li>Learn about <a href="/docs/concepts/workloads/pods/disruptions/">Pod Disruption</a></li><li>Learn about <a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated Eviction</a></li><li>Learn about <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node-pressure Eviction</a></li></ul></div></div><div><div class="td-content"><h1>Node-pressure Eviction</h1><p>Node-pressure eviction is the process by which the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> proactively terminates
pods to reclaim <a class="glossary-tooltip" title="A defined amount of infrastructure available for consumption (CPU, memory, etc)." href="/docs/reference/glossary/?all=true#term-infrastructure-resource" target="_blank">resource</a>
on nodes.</p><p>The <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> monitors resources
like memory, disk space, and filesystem inodes on your cluster's nodes.
When one or more of these resources reach specific consumption levels, the
kubelet can proactively fail one or more pods on the node to reclaim resources
and prevent starvation.</p><p>During a node-pressure eviction, the kubelet sets the <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase">phase</a> for the
selected pods to <code>Failed</code>, and terminates the Pod.</p><p>Node-pressure eviction is not the same as
<a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated eviction</a>.</p><p>The kubelet does not respect your configured <a class="glossary-tooltip" title="An object that limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions." href="/docs/reference/glossary/?all=true#term-pod-disruption-budget" target="_blank">PodDisruptionBudget</a>
or the pod's
<code>terminationGracePeriodSeconds</code>. If you use <a href="#soft-eviction-thresholds">soft eviction thresholds</a>,
the kubelet respects your configured <code>eviction-max-pod-grace-period</code>. If you use
<a href="#hard-eviction-thresholds">hard eviction thresholds</a>, the kubelet uses a <code>0s</code> grace period (immediate shutdown) for termination.</p><h2 id="self-healing-behavior">Self healing behavior</h2><p>The kubelet attempts to <a href="#reclaim-node-resources">reclaim node-level resources</a>
before it terminates end-user pods. For example, it removes unused container
images when disk resources are starved.</p><p>If the pods are managed by a <a class="glossary-tooltip" title="A workload is an application running on Kubernetes." href="/docs/concepts/workloads/" target="_blank">workload</a>
management object (such as <a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSet</a>
or <a class="glossary-tooltip" title="Manages a replicated application on your cluster." href="/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a>) that
replaces failed pods, the control plane (<code>kube-controller-manager</code>) creates new
pods in place of the evicted pods.</p><h3 id="self-healing-for-static-pods">Self healing for static pods</h3><p>If you are running a <a href="/docs/concepts/workloads/pods/#static-pods">static pod</a>
on a node that is under resource pressure, the kubelet may evict that static
Pod. The kubelet then tries to create a replacement, because static Pods always
represent an intent to run a Pod on that node.</p><p>The kubelet takes the <em>priority</em> of the static pod into account when creating
a replacement. If the static pod manifest specifies a low priority, and there
are higher-priority Pods defined within the cluster's control plane, and the
node is under resource pressure, the kubelet may not be able to make room for
that static pod. The kubelet continues to attempt to run all static pods even
when there is resource pressure on a node.</p><h2 id="eviction-signals-and-thresholds">Eviction signals and thresholds</h2><p>The kubelet uses various parameters to make eviction decisions, like the following:</p><ul><li>Eviction signals</li><li>Eviction thresholds</li><li>Monitoring intervals</li></ul><h3 id="eviction-signals">Eviction signals</h3><p>Eviction signals are the current state of a particular resource at a specific
point in time. The kubelet uses eviction signals to make eviction decisions by
comparing the signals to eviction thresholds, which are the minimum amount of
the resource that should be available on the node.</p><p>The kubelet uses the following eviction signals:</p><table><thead><tr><th>Eviction Signal</th><th>Description</th><th>Linux Only</th></tr></thead><tbody><tr><td><code>memory.available</code></td><td><code>memory.available</code> := <code>node.status.capacity[memory]</code> - <code>node.stats.memory.workingSet</code></td><td></td></tr><tr><td><code>nodefs.available</code></td><td><code>nodefs.available</code> := <code>node.stats.fs.available</code></td><td></td></tr><tr><td><code>nodefs.inodesFree</code></td><td><code>nodefs.inodesFree</code> := <code>node.stats.fs.inodesFree</code></td><td>&#8226;</td></tr><tr><td><code>imagefs.available</code></td><td><code>imagefs.available</code> := <code>node.stats.runtime.imagefs.available</code></td><td></td></tr><tr><td><code>imagefs.inodesFree</code></td><td><code>imagefs.inodesFree</code> := <code>node.stats.runtime.imagefs.inodesFree</code></td><td>&#8226;</td></tr><tr><td><code>containerfs.available</code></td><td><code>containerfs.available</code> := <code>node.stats.runtime.containerfs.available</code></td><td></td></tr><tr><td><code>containerfs.inodesFree</code></td><td><code>containerfs.inodesFree</code> := <code>node.stats.runtime.containerfs.inodesFree</code></td><td>&#8226;</td></tr><tr><td><code>pid.available</code></td><td><code>pid.available</code> := <code>node.stats.rlimit.maxpid</code> - <code>node.stats.rlimit.curproc</code></td><td>&#8226;</td></tr></tbody></table><p>In this table, the <strong>Description</strong> column shows how kubelet gets the value of the
signal. Each signal supports either a percentage or a literal value. The kubelet
calculates the percentage value relative to the total capacity associated with
the signal.</p><h4 id="memory-signals">Memory signals</h4><p>On Linux nodes, the value for <code>memory.available</code> is derived from the cgroupfs instead of tools
like <code>free -m</code>. This is important because <code>free -m</code> does not work in a
container, and if users use the <a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">node allocatable</a>
feature, out of resource decisions
are made local to the end user Pod part of the cgroup hierarchy as well as the
root node. This <a href="/examples/admin/resource/memory-available.sh">script</a> or
<a href="/examples/admin/resource/memory-available-cgroupv2.sh">cgroupv2 script</a>
reproduces the same set of steps that the kubelet performs to calculate
<code>memory.available</code>. The kubelet excludes inactive_file (the number of bytes of
file-backed memory on the inactive LRU list) from its calculation, as it assumes that
memory is reclaimable under pressure.</p><p>On Windows nodes, the value for <code>memory.available</code> is derived from the node's global
memory commit levels (queried through the <a href="https://learn.microsoft.com/windows/win32/api/psapi/nf-psapi-getperformanceinfo"><code>GetPerformanceInfo()</code></a>
system call) by subtracting the node's global <a href="https://learn.microsoft.com/windows/win32/api/psapi/ns-psapi-performance_information"><code>CommitTotal</code></a> from the node's <a href="https://learn.microsoft.com/windows/win32/api/psapi/ns-psapi-performance_information"><code>CommitLimit</code></a>. Please note that <code>CommitLimit</code> can change if the node's page-file size changes!</p><h4 id="filesystem-signals">Filesystem signals</h4><p>The kubelet recognizes three specific filesystem identifiers that can be used with
eviction signals (<code>&lt;identifier&gt;.inodesFree</code> or <code>&lt;identifier&gt;.available</code>):</p><ol><li><p><code>nodefs</code>: The node's main filesystem, used for local disk volumes,
emptyDir volumes not backed by memory, log storage, ephemeral storage,
and more. For example, <code>nodefs</code> contains <code>/var/lib/kubelet</code>.</p></li><li><p><code>imagefs</code>: An optional filesystem that container runtimes can use to store
container images (which are the read-only layers) and container writable
layers.</p></li><li><p><code>containerfs</code>: An optional filesystem that container runtime can use to
store the writeable layers. Similar to the main filesystem (see <code>nodefs</code>),
it's used to store local disk volumes, emptyDir volumes not backed by memory,
log storage, and ephemeral storage, except for the container images. When
<code>containerfs</code> is used, the <code>imagefs</code> filesystem can be split to only store
images (read-only layers) and nothing else.</p></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><div class="feature-state-notice feature-beta" title="Feature Gate: KubeletSeparateDiskGC"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [beta]</code> (enabled by default: true)</div><p>The <em>split image filesystem</em> feature, which enables support for the <code>containerfs</code>
filesystem, adds several new eviction signals, thresholds and metrics. To use
<code>containerfs</code>, the Kubernetes release v1.34 requires the
<code>KubeletSeparateDiskGC</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
to be enabled. Currently, only CRI-O (v1.29 or higher) offers the <code>containerfs</code>
filesystem support.</p></div><p>As such, kubelet generally allows three options for container filesystems:</p><ul><li><p>Everything is on the single <code>nodefs</code>, also referred to as "rootfs" or
simply "root", and there is no dedicated image filesystem.</p></li><li><p>Container storage (see <code>nodefs</code>) is on a dedicated disk, and <code>imagefs</code>
(writable and read-only layers) is separate from the root filesystem.
This is often referred to as "split disk" (or "separate disk") filesystem.</p></li><li><p>Container filesystem <code>containerfs</code> (same as <code>nodefs</code> plus writable
layers) is on root and the container images (read-only layers) are
stored on separate <code>imagefs</code>. This is often referred to as "split image"
filesystem.</p></li></ul><p>The kubelet will attempt to auto-discover these filesystems with their current
configuration directly from the underlying container runtime and will ignore
other local node filesystems.</p><p>The kubelet does not support other container filesystems or storage configurations,
and it does not currently support multiple filesystems for images and containers.</p><h3 id="deprecated-kubelet-garbage-collection-features">Deprecated kubelet garbage collection features</h3><p>Some kubelet garbage collection features are deprecated in favor of eviction:</p><table><thead><tr><th>Existing Flag</th><th>Rationale</th></tr></thead><tbody><tr><td><code>--maximum-dead-containers</code></td><td>deprecated once old logs are stored outside of container's context</td></tr><tr><td><code>--maximum-dead-containers-per-container</code></td><td>deprecated once old logs are stored outside of container's context</td></tr><tr><td><code>--minimum-container-ttl-duration</code></td><td>deprecated once old logs are stored outside of container's context</td></tr></tbody></table><h3 id="eviction-thresholds">Eviction thresholds</h3><p>You can specify custom eviction thresholds for the kubelet to use when it makes
eviction decisions. You can configure <a href="#soft-eviction-thresholds">soft</a> and
<a href="#hard-eviction-thresholds">hard</a> eviction thresholds.</p><p>Eviction thresholds have the form <code>[eviction-signal][operator][quantity]</code>, where:</p><ul><li><code>eviction-signal</code> is the <a href="#eviction-signals">eviction signal</a> to use.</li><li><code>operator</code> is the <a href="https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators">relational operator</a>
you want, such as <code>&lt;</code> (less than).</li><li><code>quantity</code> is the eviction threshold amount, such as <code>1Gi</code>. The value of <code>quantity</code>
must match the quantity representation used by Kubernetes. You can use either
literal values or percentages (<code>%</code>).</li></ul><p>For example, if a node has 10GiB of total memory and you want trigger eviction if
the available memory falls below 1GiB, you can define the eviction threshold as
either <code>memory.available&lt;10%</code> or <code>memory.available&lt;1Gi</code> (you cannot use both).</p><h4 id="soft-eviction-thresholds">Soft eviction thresholds</h4><p>A soft eviction threshold pairs an eviction threshold with a required
administrator-specified grace period. The kubelet does not evict pods until the
grace period is exceeded. The kubelet returns an error on startup if you do
not specify a grace period.</p><p>You can specify both a soft eviction threshold grace period and a maximum
allowed pod termination grace period for kubelet to use during evictions. If you
specify a maximum allowed grace period and the soft eviction threshold is met,
the kubelet uses the lesser of the two grace periods. If you do not specify a
maximum allowed grace period, the kubelet kills evicted pods immediately without
graceful termination.</p><p>You can use the following flags to configure soft eviction thresholds:</p><ul><li><code>eviction-soft</code>: A set of eviction thresholds like <code>memory.available&lt;1.5Gi</code>
that can trigger pod eviction if held over the specified grace period.</li><li><code>eviction-soft-grace-period</code>: A set of eviction grace periods like <code>memory.available=1m30s</code>
that define how long a soft eviction threshold must hold before triggering a Pod eviction.</li><li><code>eviction-max-pod-grace-period</code>: The maximum allowed grace period (in seconds)
to use when terminating pods in response to a soft eviction threshold being met.</li></ul><h4 id="hard-eviction-thresholds">Hard eviction thresholds</h4><p>A hard eviction threshold has no grace period. When a hard eviction threshold is
met, the kubelet kills pods immediately without graceful termination to reclaim
the starved resource.</p><p>You can use the <code>eviction-hard</code> flag to configure a set of hard eviction
thresholds like <code>memory.available&lt;1Gi</code>.</p><p>The kubelet has the following default hard eviction thresholds:</p><ul><li><code>memory.available&lt;100Mi</code> (Linux nodes)</li><li><code>memory.available&lt;500Mi</code> (Windows nodes)</li><li><code>nodefs.available&lt;10%</code></li><li><code>imagefs.available&lt;15%</code></li><li><code>nodefs.inodesFree&lt;5%</code> (Linux nodes)</li><li><code>imagefs.inodesFree&lt;5%</code> (Linux nodes)</li></ul><p>These default values of hard eviction thresholds will only be set if none
of the parameters is changed. If you change the value of any parameter,
then the values of other parameters will not be inherited as the default
values and will be set to zero. In order to provide custom values, you
should provide all the thresholds respectively. You can also set the kubelet config
MergeDefaultEvictionSettings to true in the kubelet configuration file.
If set to true and any parameter is changed, then the other parameters will
inherit their default values instead of 0.</p><p>The <code>containerfs.available</code> and <code>containerfs.inodesFree</code> (Linux nodes) default
eviction thresholds will be set as follows:</p><ul><li><p>If a single filesystem is used for everything, then <code>containerfs</code> thresholds
are set the same as <code>nodefs</code>.</p></li><li><p>If separate filesystems are configured for both images and containers,
then <code>containerfs</code> thresholds are set the same as <code>imagefs</code>.</p></li></ul><p>Setting custom overrides for thresholds related to <code>containersfs</code> is currently
not supported, and a warning will be issued if an attempt to do so is made; any
provided custom values will, as such, be ignored.</p><h2 id="eviction-monitoring-interval">Eviction monitoring interval</h2><p>The kubelet evaluates eviction thresholds based on its configured <code>housekeeping-interval</code>,
which defaults to <code>10s</code>.</p><h2 id="node-conditions">Node conditions</h2><p>The kubelet reports <a href="/docs/concepts/architecture/nodes/#condition">node conditions</a>
to reflect that the node is under pressure because hard or soft eviction
threshold is met, independent of configured grace periods.</p><p>The kubelet maps eviction signals to node conditions as follows:</p><table><thead><tr><th>Node Condition</th><th>Eviction Signal</th><th>Description</th></tr></thead><tbody><tr><td><code>MemoryPressure</code></td><td><code>memory.available</code></td><td>Available memory on the node has satisfied an eviction threshold</td></tr><tr><td><code>DiskPressure</code></td><td><code>nodefs.available</code>, <code>nodefs.inodesFree</code>, <code>imagefs.available</code>, <code>imagefs.inodesFree</code>, <code>containerfs.available</code>, or <code>containerfs.inodesFree</code></td><td>Available disk space and inodes on either the node's root filesystem, image filesystem, or container filesystem has satisfied an eviction threshold</td></tr><tr><td><code>PIDPressure</code></td><td><code>pid.available</code></td><td>Available processes identifiers on the (Linux) node has fallen below an eviction threshold</td></tr></tbody></table><p>The control plane also <a href="/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-nodes-by-condition">maps</a>
these node conditions to taints.</p><p>The kubelet updates the node conditions based on the configured
<code>--node-status-update-frequency</code>, which defaults to <code>10s</code>.</p><h3 id="node-condition-oscillation">Node condition oscillation</h3><p>In some cases, nodes oscillate above and below soft eviction thresholds without
holding for the defined grace periods. This causes the reported node condition
to constantly switch between <code>true</code> and <code>false</code>, leading to bad eviction decisions.</p><p>To protect against oscillation, you can use the <code>eviction-pressure-transition-period</code>
flag, which controls how long the kubelet must wait before transitioning a node
condition to a different state. The transition period has a default value of <code>5m</code>.</p><h3 id="reclaim-node-resources">Reclaiming node level resources</h3><p>The kubelet tries to reclaim node-level resources before it evicts end-user pods.</p><p>When a <code>DiskPressure</code> node condition is reported, the kubelet reclaims node-level
resources based on the filesystems on the node.</p><h4 id="without-imagefs-or-containerfs">Without <code>imagefs</code> or <code>containerfs</code></h4><p>If the node only has a <code>nodefs</code> filesystem that meets eviction thresholds,
the kubelet frees up disk space in the following order:</p><ol><li>Garbage collect dead pods and containers.</li><li>Delete unused images.</li></ol><h4 id="with-imagefs">With <code>imagefs</code></h4><p>If the node has a dedicated <code>imagefs</code> filesystem for container runtimes to use,
the kubelet does the following:</p><ul><li><p>If the <code>nodefs</code> filesystem meets the eviction thresholds, the kubelet garbage
collects dead pods and containers.</p></li><li><p>If the <code>imagefs</code> filesystem meets the eviction thresholds, the kubelet
deletes all unused images.</p></li></ul><h4 id="with-imagefs-and-containerfs">With <code>imagefs</code> and <code>containerfs</code></h4><p>If the node has a dedicated <code>containerfs</code> alongside the <code>imagefs</code> filesystem
configured for the container runtimes to use, then kubelet will attempt to
reclaim resources as follows:</p><ul><li><p>If the <code>containerfs</code> filesystem meets the eviction thresholds, the kubelet
garbage collects dead pods and containers.</p></li><li><p>If the <code>imagefs</code> filesystem meets the eviction thresholds, the kubelet
deletes all unused images.</p></li></ul><h3 id="pod-selection-for-kubelet-eviction">Pod selection for kubelet eviction</h3><p>If the kubelet's attempts to reclaim node-level resources don't bring the eviction
signal below the threshold, the kubelet begins to evict end-user pods.</p><p>The kubelet uses the following parameters to determine the pod eviction order:</p><ol><li>Whether the pod's resource usage exceeds requests</li><li><a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod Priority</a></li><li>The pod's resource usage relative to requests</li></ol><p>As a result, kubelet ranks and evicts pods in the following order:</p><ol><li><p><code>BestEffort</code> or <code>Burstable</code> pods where the usage exceeds requests. These pods
are evicted based on their Priority and then by how much their usage level
exceeds the request.</p></li><li><p><code>Guaranteed</code> pods and <code>Burstable</code> pods where the usage is less than requests
are evicted last, based on their Priority.</p></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The kubelet does not use the pod's <a href="/docs/concepts/workloads/pods/pod-qos/">QoS class</a> to determine the eviction order.
You can use the QoS class to estimate the most likely pod eviction order when
reclaiming resources like memory. QoS classification does not apply to EphemeralStorage requests,
so the above scenario will not apply if the node is, for example, under <code>DiskPressure</code>.</div><p><code>Guaranteed</code> pods are guaranteed only when requests and limits are specified for
all the containers and they are equal. These pods will never be evicted because
of another pod's resource consumption. If a system daemon (such as <code>kubelet</code>
and <code>journald</code>) is consuming more resources than were reserved via
<code>system-reserved</code> or <code>kube-reserved</code> allocations, and the node only has
<code>Guaranteed</code> or <code>Burstable</code> pods using less resources than requests left on it,
then the kubelet must choose to evict one of these pods to preserve node stability
and to limit the impact of resource starvation on other pods. In this case, it
will choose to evict pods of lowest Priority first.</p><p>If you are running a <a href="/docs/concepts/workloads/pods/#static-pods">static pod</a>
and want to avoid having it evicted under resource pressure, set the
<code>priority</code> field for that Pod directly. Static pods do not support the
<code>priorityClassName</code> field.</p><p>When the kubelet evicts pods in response to inode or process ID starvation, it uses
the Pods' relative priority to determine the eviction order, because inodes and PIDs have no
requests.</p><p>The kubelet sorts pods differently based on whether the node has a dedicated
<code>imagefs</code> or <code>containerfs</code> filesystem:</p><h4 id="without-imagefs">Without <code>imagefs</code> or <code>containerfs</code> (<code>nodefs</code> and <code>imagefs</code> use the same filesystem)</h4><ul><li>If <code>nodefs</code> triggers evictions, the kubelet sorts pods based on their
total disk usage (<code>local volumes + logs and a writable layer of all containers</code>).</li></ul><h4 id="with-imagefs">With <code>imagefs</code> (<code>nodefs</code> and <code>imagefs</code> filesystems are separate)</h4><ul><li><p>If <code>nodefs</code> triggers evictions, the kubelet sorts pods based on <code>nodefs</code>
usage (<code>local volumes + logs of all containers</code>).</p></li><li><p>If <code>imagefs</code> triggers evictions, the kubelet sorts pods based on the
writable layer usage of all containers.</p></li></ul><h4 id="with-containersfs">With <code>imagesfs</code> and <code>containerfs</code> (<code>imagefs</code> and <code>containerfs</code> have been split)</h4><ul><li><p>If <code>containerfs</code> triggers evictions, the kubelet sorts pods based on
<code>containerfs</code> usage (<code>local volumes + logs and a writable layer of all containers</code>).</p></li><li><p>If <code>imagefs</code> triggers evictions, the kubelet sorts pods based on the
<code>storage of images</code> rank, which represents the disk usage of a given image.</p></li></ul><h3 id="minimum-eviction-reclaim">Minimum eviction reclaim</h3><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>As of Kubernetes v1.34, you cannot set a custom value
for the <code>containerfs.available</code> metric. The configuration for this specific
metric will be set automatically to reflect values set for either the <code>nodefs</code>
or <code>imagefs</code>, depending on the configuration.</div><p>In some cases, pod eviction only reclaims a small amount of the starved resource.
This can lead to the kubelet repeatedly hitting the configured eviction thresholds
and triggering multiple evictions.</p><p>You can use the <code>--eviction-minimum-reclaim</code> flag or a <a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet config file</a>
to configure a minimum reclaim amount for each resource. When the kubelet notices
that a resource is starved, it continues to reclaim that resource until it
reclaims the quantity you specify.</p><p>For example, the following configuration sets minimum reclaim amounts:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubelet.config.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeletConfiguration<span>
</span></span></span><span><span><span></span><span>evictionHard</span>:<span>
</span></span></span><span><span><span>  </span><span>memory.available</span>:<span> </span><span>"500Mi"</span><span>
</span></span></span><span><span><span>  </span><span>nodefs.available</span>:<span> </span><span>"1Gi"</span><span>
</span></span></span><span><span><span>  </span><span>imagefs.available</span>:<span> </span><span>"100Gi"</span><span>
</span></span></span><span><span><span></span><span>evictionMinimumReclaim</span>:<span>
</span></span></span><span><span><span>  </span><span>memory.available</span>:<span> </span><span>"0Mi"</span><span>
</span></span></span><span><span><span>  </span><span>nodefs.available</span>:<span> </span><span>"500Mi"</span><span>
</span></span></span><span><span><span>  </span><span>imagefs.available</span>:<span> </span><span>"2Gi"</span><span>
</span></span></span></code></pre></div><p>In this example, if the <code>nodefs.available</code> signal meets the eviction threshold,
the kubelet reclaims the resource until the signal reaches the threshold of 1GiB,
and then continues to reclaim the minimum amount of 500MiB, until the available
nodefs storage value reaches 1.5GiB.</p><p>Similarly, the kubelet tries to reclaim the <code>imagefs</code> resource until the <code>imagefs.available</code>
value reaches <code>102Gi</code>, representing 102 GiB of available container image storage. If the amount
of storage that the kubelet could reclaim is less than 2GiB, the kubelet doesn't reclaim anything.</p><p>The default <code>eviction-minimum-reclaim</code> is <code>0</code> for all resources.</p><h2 id="node-out-of-memory-behavior">Node out of memory behavior</h2><p>If the node experiences an <em>out of memory</em> (OOM) event prior to the kubelet
being able to reclaim memory, the node depends on the <a href="https://lwn.net/Articles/391222/">oom_killer</a>
to respond.</p><p>The kubelet sets an <code>oom_score_adj</code> value for each container based on the QoS for the pod.</p><table><thead><tr><th>Quality of Service</th><th><code>oom_score_adj</code></th></tr></thead><tbody><tr><td><code>Guaranteed</code></td><td>-997</td></tr><tr><td><code>BestEffort</code></td><td>1000</td></tr><tr><td><code>Burstable</code></td><td><em>min(max(2, 1000 - (1000 &#215; memoryRequestBytes) / machineMemoryCapacityBytes), 999)</em></td></tr></tbody></table><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The kubelet also sets an <code>oom_score_adj</code> value of <code>-997</code> for any containers in Pods that have
<code>system-node-critical</code> <a class="glossary-tooltip" title="Pod Priority indicates the importance of a Pod relative to other Pods." href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority" target="_blank">Priority</a>.</div><p>If the kubelet can't reclaim memory before a node experiences OOM, the
<code>oom_killer</code> calculates an <code>oom_score</code> based on the percentage of memory it's
using on the node, and then adds the <code>oom_score_adj</code> to get an effective <code>oom_score</code>
for each container. It then kills the container with the highest score.</p><p>This means that containers in low QoS pods that consume a large amount of memory
relative to their scheduling requests are killed first.</p><p>Unlike pod eviction, if a container is OOM killed, the kubelet can restart it
based on its <code>restartPolicy</code>.</p><h2 id="node-pressure-eviction-good-practices">Good practices</h2><p>The following sections describe good practice for eviction configuration.</p><h3 id="schedulable-resources-and-eviction-policies">Schedulable resources and eviction policies</h3><p>When you configure the kubelet with an eviction policy, you should make sure that
the scheduler will not schedule pods if they will trigger eviction because they
immediately induce memory pressure.</p><p>Consider the following scenario:</p><ul><li>Node memory capacity: 10GiB</li><li>Operator wants to reserve 10% of memory capacity for system daemons (kernel, <code>kubelet</code>, etc.)</li><li>Operator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM.</li></ul><p>For this to work, the kubelet is launched as follows:</p><pre tabindex="0"><code class="language-none">--eviction-hard=memory.available&lt;500Mi
--system-reserved=memory=1.5Gi
</code></pre><p>In this configuration, the <code>--system-reserved</code> flag reserves 1.5GiB of memory
for the system, which is <code>10% of the total memory + the eviction threshold amount</code>.</p><p>The node can reach the eviction threshold if a pod is using more than its request,
or if the system is using more than 1GiB of memory, which makes the <code>memory.available</code>
signal fall below 500MiB and triggers the threshold.</p><h3 id="daemonset">DaemonSets and node-pressure eviction</h3><p>Pod priority is a major factor in making eviction decisions. If you do not want
the kubelet to evict pods that belong to a DaemonSet, give those pods a high
enough priority by specifying a suitable <code>priorityClassName</code> in the pod spec.
You can also use a lower priority, or the default, to only allow pods from that
DaemonSet to run when there are enough resources.</p><h2 id="known-issues">Known issues</h2><p>The following sections describe known issues related to out of resource handling.</p><h3 id="kubelet-may-not-observe-memory-pressure-right-away">kubelet may not observe memory pressure right away</h3><p>By default, the kubelet polls cAdvisor to collect memory usage stats at a
regular interval. If memory usage increases within that window rapidly, the
kubelet may not observe <code>MemoryPressure</code> fast enough, and the OOM killer
will still be invoked.</p><p>You can use the <code>--kernel-memcg-notification</code> flag to enable the <code>memcg</code>
notification API on the kubelet to get notified immediately when a threshold
is crossed.</p><p>If you are not trying to achieve extreme utilization, but a sensible measure of
overcommit, a viable workaround for this issue is to use the <code>--kube-reserved</code>
and <code>--system-reserved</code> flags to allocate memory for the system.</p><h3 id="active-file-memory-is-not-considered-as-available-memory">active_file memory is not considered as available memory</h3><p>On Linux, the kernel tracks the number of bytes of file-backed memory on active
least recently used (LRU) list as the <code>active_file</code> statistic. The kubelet treats <code>active_file</code> memory
areas as not reclaimable. For workloads that make intensive use of block-backed
local storage, including ephemeral local storage, kernel-level caches of file
and block data means that many recently accessed cache pages are likely to be
counted as <code>active_file</code>. If enough of these kernel block buffers are on the
active LRU list, the kubelet is liable to observe this as high resource use and
taint the node as experiencing memory pressure - triggering pod eviction.</p><p>For more details, see <a href="https://github.com/kubernetes/kubernetes/issues/43916">https://github.com/kubernetes/kubernetes/issues/43916</a></p><p>You can work around that behavior by setting the memory limit and memory request
the same for containers likely to perform intensive I/O activity. You will need
to estimate or measure an optimal memory limit value for that container.</p><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/concepts/scheduling-eviction/api-eviction/">API-initiated Eviction</a></li><li>Learn about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod Priority and Preemption</a></li><li>Learn about <a href="/docs/tasks/run-application/configure-pdb/">PodDisruptionBudgets</a></li><li>Learn about <a href="/docs/tasks/configure-pod-container/quality-service-pod/">Quality of Service</a> (QoS)</li><li>Check out the <a href="/docs/reference/generated/kubernetes-api/v1.34/#create-eviction-pod-v1-core">Eviction API</a></li></ul></div></div><div><div class="td-content"><h1>API-initiated Eviction</h1><p>API-initiated eviction is the process by which you use the <a href="/docs/reference/generated/kubernetes-api/v1.34/#create-eviction-pod-v1-core">Eviction API</a>
to create an <code>Eviction</code> object that triggers graceful pod termination.</p><p>You can request eviction by calling the Eviction API directly, or programmatically
using a client of the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API server</a>, like the <code>kubectl drain</code> command. This
creates an <code>Eviction</code> object, which causes the API server to terminate the Pod.</p><p>API-initiated evictions respect your configured <a href="/docs/tasks/run-application/configure-pdb/"><code>PodDisruptionBudgets</code></a>
and <a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination"><code>terminationGracePeriodSeconds</code></a>.</p><p>Using the API to create an Eviction object for a Pod is like performing a
policy-controlled <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#delete-delete-a-pod"><code>DELETE</code> operation</a>
on the Pod.</p><h2 id="calling-the-eviction-api">Calling the Eviction API</h2><p>You can use a <a href="/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api">Kubernetes language client</a>
to access the Kubernetes API and create an <code>Eviction</code> object. To do this, you
POST the attempted operation, similar to the following example:</p><ul class="nav nav-tabs" id="eviction-example"><li class="nav-item"><a class="nav-link active" href="#eviction-example-0">policy/v1</a></li><li class="nav-item"><a class="nav-link" href="#eviction-example-1">policy/v1beta1</a></li></ul><div class="tab-content" id="eviction-example"><div id="eviction-example-0" class="tab-pane show active"><p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><code>policy/v1</code> Eviction is available in v1.22+. Use <code>policy/v1beta1</code> with prior releases.</div><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"apiVersion"</span>: <span>"policy/v1"</span>,
</span></span><span><span>  <span>"kind"</span>: <span>"Eviction"</span>,
</span></span><span><span>  <span>"metadata"</span>: {
</span></span><span><span>    <span>"name"</span>: <span>"quux"</span>,
</span></span><span><span>    <span>"namespace"</span>: <span>"default"</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div></p></div><div id="eviction-example-1" class="tab-pane"><p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Deprecated in v1.22 in favor of <code>policy/v1</code></div><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"apiVersion"</span>: <span>"policy/v1beta1"</span>,
</span></span><span><span>  <span>"kind"</span>: <span>"Eviction"</span>,
</span></span><span><span>  <span>"metadata"</span>: {
</span></span><span><span>    <span>"name"</span>: <span>"quux"</span>,
</span></span><span><span>    <span>"namespace"</span>: <span>"default"</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div></p></div></div><p>Alternatively, you can attempt an eviction operation by accessing the API using
<code>curl</code> or <code>wget</code>, similar to the following example:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>curl -v -H <span>'Content-type: application/json'</span> https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json
</span></span></code></pre></div><h2 id="how-api-initiated-eviction-works">How API-initiated eviction works</h2><p>When you request an eviction using the API, the API server performs admission
checks and responds in one of the following ways:</p><ul><li><code>200 OK</code>: the eviction is allowed, the <code>Eviction</code> subresource is created, and
the Pod is deleted, similar to sending a <code>DELETE</code> request to the Pod URL.</li><li><code>429 Too Many Requests</code>: the eviction is not currently allowed because of the
configured <a class="glossary-tooltip" title="An object that limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions." href="/docs/reference/glossary/?all=true#term-pod-disruption-budget" target="_blank">PodDisruptionBudget</a>.
You may be able to attempt the eviction again later. You might also see this
response because of API rate limiting.</li><li><code>500 Internal Server Error</code>: the eviction is not allowed because there is a
misconfiguration, like if multiple PodDisruptionBudgets reference the same Pod.</li></ul><p>If the Pod you want to evict isn't part of a workload that has a
PodDisruptionBudget, the API server always returns <code>200 OK</code> and allows the
eviction.</p><p>If the API server allows the eviction, the Pod is deleted as follows:</p><ol><li>The <code>Pod</code> resource in the API server is updated with a deletion timestamp,
after which the API server considers the <code>Pod</code> resource to be terminated. The
<code>Pod</code> resource is also marked with the configured grace period.</li><li>The <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> on the node where the local Pod is running notices that the <code>Pod</code>
resource is marked for termination and starts to gracefully shut down the
local Pod.</li><li>While the kubelet is shutting the Pod down, the control plane removes the Pod
from <a class="glossary-tooltip" title="EndpointSlices track the IP addresses of Pods for Services." href="/docs/concepts/services-networking/endpoint-slices/" target="_blank">EndpointSlice</a>
objects. As a result, controllers no longer consider the Pod as a valid object.</li><li>After the grace period for the Pod expires, the kubelet forcefully terminates
the local Pod.</li><li>The kubelet tells the API server to remove the <code>Pod</code> resource.</li><li>The API server deletes the <code>Pod</code> resource.</li></ol><h2 id="troubleshooting-stuck-evictions">Troubleshooting stuck evictions</h2><p>In some cases, your applications may enter a broken state, where the Eviction
API will only return <code>429</code> or <code>500</code> responses until you intervene. This can
happen if, for example, a ReplicaSet creates pods for your application but new
pods do not enter a <code>Ready</code> state. You may also notice this behavior in cases
where the last evicted Pod had a long termination grace period.</p><p>If you notice stuck evictions, try one of the following solutions:</p><ul><li>Abort or pause the automated operation causing the issue. Investigate the stuck
application before you restart the operation.</li><li>Wait a while, then directly delete the Pod from your cluster control plane
instead of using the Eviction API.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn how to protect your applications with a <a href="/docs/tasks/run-application/configure-pdb/">Pod Disruption Budget</a>.</li><li>Learn about <a href="/docs/concepts/scheduling-eviction/node-pressure-eviction/">Node-pressure Eviction</a>.</li><li>Learn about <a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod Priority and Preemption</a>.</li></ul></div></div><div><div class="td-content"><h1>Cluster Administration</h1><div class="lead">Lower-level detail relevant to creating or administering a Kubernetes cluster.</div><p>The cluster administration overview is for anyone creating or administering a Kubernetes cluster.
It assumes some familiarity with core Kubernetes <a href="/docs/concepts/">concepts</a>.</p><h2 id="planning-a-cluster">Planning a cluster</h2><p>See the guides in <a href="/docs/setup/">Setup</a> for examples of how to plan, set up, and configure
Kubernetes clusters. The solutions listed in this article are called <em>distros</em>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Not all distros are actively maintained. Choose distros which have been tested with a recent
version of Kubernetes.</div><p>Before choosing a guide, here are some considerations:</p><ul><li>Do you want to try out Kubernetes on your computer, or do you want to build a high-availability,
multi-node cluster? Choose distros best suited for your needs.</li><li>Will you be using <strong>a hosted Kubernetes cluster</strong>, such as
<a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a>, or <strong>hosting your own cluster</strong>?</li><li>Will your cluster be <strong>on-premises</strong>, or <strong>in the cloud (IaaS)</strong>? Kubernetes does not directly
support hybrid clusters. Instead, you can set up multiple clusters.</li><li><strong>If you are configuring Kubernetes on-premises</strong>, consider which
<a href="/docs/concepts/cluster-administration/networking/">networking model</a> fits best.</li><li>Will you be running Kubernetes on <strong>"bare metal" hardware</strong> or on <strong>virtual machines (VMs)</strong>?</li><li>Do you <strong>want to run a cluster</strong>, or do you expect to do <strong>active development of Kubernetes project code</strong>?
If the latter, choose an actively-developed distro. Some distros only use binary releases, but
offer a greater variety of choices.</li><li>Familiarize yourself with the <a href="/docs/concepts/overview/components/">components</a> needed to run a cluster.</li></ul><h2 id="managing-a-cluster">Managing a cluster</h2><ul><li><p>Learn how to <a href="/docs/concepts/architecture/nodes/">manage nodes</a>.</p><ul><li>Read about <a href="/docs/concepts/cluster-administration/node-autoscaling/">Node autoscaling</a>.</li></ul></li><li><p>Learn how to set up and manage the <a href="/docs/concepts/policy/resource-quotas/">resource quota</a> for shared clusters.</p></li></ul><h2 id="securing-a-cluster">Securing a cluster</h2><ul><li><p><a href="/docs/tasks/administer-cluster/certificates/">Generate Certificates</a> describes the steps to
generate certificates using different tool chains.</p></li><li><p><a href="/docs/concepts/containers/container-environment/">Kubernetes Container Environment</a> describes
the environment for Kubelet managed containers on a Kubernetes node.</p></li><li><p><a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a> describes
how Kubernetes implements access control for its own API.</p></li><li><p><a href="/docs/reference/access-authn-authz/authentication/">Authenticating</a> explains authentication in
Kubernetes, including the various authentication options.</p></li><li><p><a href="/docs/reference/access-authn-authz/authorization/">Authorization</a> is separate from
authentication, and controls how HTTP calls are handled.</p></li><li><p><a href="/docs/reference/access-authn-authz/admission-controllers/">Using Admission Controllers</a>
explains plug-ins which intercepts requests to the Kubernetes API server after authentication
and authorization.</p></li><li><p><a href="/docs/concepts/cluster-administration/admission-webhooks-good-practices/">Admission Webhook Good Practices</a>
provides good practices and considerations when designing mutating admission
webhooks and validating admission webhooks.</p></li><li><p><a href="/docs/tasks/administer-cluster/sysctl-cluster/">Using Sysctls in a Kubernetes Cluster</a>
describes to an administrator how to use the <code>sysctl</code> command-line tool to set kernel parameters
.</p></li><li><p><a href="/docs/tasks/debug/debug-cluster/audit/">Auditing</a> describes how to interact with Kubernetes'
audit logs.</p></li></ul><h3 id="securing-the-kubelet">Securing the kubelet</h3><ul><li><a href="/docs/concepts/architecture/control-plane-node-communication/">Control Plane-Node communication</a></li><li><a href="/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/">TLS bootstrapping</a></li><li><a href="/docs/reference/access-authn-authz/kubelet-authn-authz/">Kubelet authentication/authorization</a></li></ul><h2 id="optional-cluster-services">Optional Cluster Services</h2><ul><li><p><a href="/docs/concepts/services-networking/dns-pod-service/">DNS Integration</a> describes how to resolve
a DNS name directly to a Kubernetes service.</p></li><li><p><a href="/docs/concepts/cluster-administration/logging/">Logging and Monitoring Cluster Activity</a>
explains how logging in Kubernetes works and how to implement it.</p></li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Node Shutdowns</h1><p>In a Kubernetes cluster, a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a>
can be shut down in a planned graceful way or unexpectedly because of reasons such
as a power outage or something else external. A node shutdown could lead to workload
failure if the node is not drained before the shutdown. A node shutdown can be
either <strong>graceful</strong> or <strong>non-graceful</strong>.</p><h2 id="graceful-node-shutdown">Graceful node shutdown</h2><p>The kubelet attempts to detect node system shutdown and terminates pods running on the node.</p><p>Kubelet ensures that pods follow the normal
<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">pod termination process</a>
during the node shutdown. During node shutdown, the kubelet does not accept new
Pods (even if those Pods are already bound to the node).</p><h3 id="enabling-graceful-node-shutdown">Enabling graceful node shutdown</h3><ul class="nav nav-tabs" id="graceful-shutdown-os"><li class="nav-item"><a class="nav-link active" href="#graceful-shutdown-os-0">Linux</a></li><li class="nav-item"><a class="nav-link" href="#graceful-shutdown-os-1">Windows</a></li></ul><div class="tab-content" id="graceful-shutdown-os"><div id="graceful-shutdown-os-0" class="tab-pane show active"><p><div class="feature-state-notice feature-beta" title="Feature Gate: GracefulNodeShutdown"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [beta]</code> (enabled by default: true)</div><p>On Linux, the graceful node shutdown feature is controlled with the <code>GracefulNodeShutdown</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> which is
enabled by default in 1.21.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The graceful node shutdown feature depends on systemd since it takes advantage of
<a href="https://www.freedesktop.org/wiki/Software/systemd/inhibit/">systemd inhibitor locks</a> to
delay the node shutdown with a given duration.</div></p></div><div id="graceful-shutdown-os-1" class="tab-pane"><p><div class="feature-state-notice feature-beta" title="Feature Gate: WindowsGracefulNodeShutdown"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code> (enabled by default: true)</div><p>On Windows, the graceful node shutdown feature is controlled with the <code>WindowsGracefulNodeShutdown</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
which is introduced in 1.32 as an alpha feature. In Kubernetes 1.34 the feature is Beta
and is enabled by default.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The Windows graceful node shutdown feature depends on kubelet running as a Windows service,
it will then have a registered <a href="https://learn.microsoft.com/en-us/windows/win32/services/service-control-handler-function">service control handler</a>
to delay the preshutdown event with a given duration.</div><p>Windows graceful node shutdown can not be cancelled.</p><p>If kubelet is not running as a Windows service, it will not be able to set and monitor
the <a href="https://learn.microsoft.com/en-us/windows/win32/api/winsvc/ns-winsvc-service_preshutdown_info">Preshutdown</a> event,
the node will have to go through the <a href="#non-graceful-node-shutdown">Non-Graceful Node Shutdown</a> procedure mentioned above.</p><p>In the case where the Windows graceful node shutdown feature is enabled, but the kubelet is not
running as a Windows service, the kubelet will continue running instead of failing. However,
it will log an error indicating that it needs to be run as a Windows service.</p></p></div></div><h3 id="configuring-graceful-node-shutdown">Configuring graceful node shutdown</h3><p>Note that by default, both configuration options described below,
<code>shutdownGracePeriod</code> and <code>shutdownGracePeriodCriticalPods</code>, are set to zero,
thus not activating the graceful node shutdown functionality.
To activate the feature, both options should be configured appropriately and
set to non-zero values.</p><p>Once the kubelet is notified of a node shutdown, it sets a <code>NotReady</code> condition on
the Node, with the <code>reason</code> set to <code>"node is shutting down"</code>. The kube-scheduler honors this condition
and does not schedule any Pods onto the affected node; other third-party schedulers are
expected to follow the same logic. This means that new Pods won't be scheduled onto that node
and therefore none will start.</p><p>The kubelet <strong>also</strong> rejects Pods during the <code>PodAdmission</code> phase if an ongoing
node shutdown has been detected, so that even Pods with a
<a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint." href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">toleration</a> for
<code>node.kubernetes.io/not-ready:NoSchedule</code> do not start there.</p><p>When kubelet is setting that condition on its Node via the API,
the kubelet also begins terminating any Pods that are running locally.</p><p>During a graceful shutdown, kubelet terminates pods in two phases:</p><ol><li>Terminate regular pods running on the node.</li><li>Terminate <a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">critical pods</a>
running on the node.</li></ol><p>The graceful node shutdown feature is configured with two
<a href="/docs/tasks/administer-cluster/kubelet-config-file/"><code>KubeletConfiguration</code></a> options:</p><ul><li><p><code>shutdownGracePeriod</code>:</p><p>Specifies the total duration that the node should delay the shutdown by. This is the total
grace period for pod termination for both regular and
<a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">critical pods</a>.</p></li><li><p><code>shutdownGracePeriodCriticalPods</code>:</p><p>Specifies the duration used to terminate
<a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">critical pods</a>
during a node shutdown. This value should be less than <code>shutdownGracePeriod</code>.</p></li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>There are cases when Node termination was cancelled by the system (or perhaps manually
by an administrator). In either of those situations the Node will return to the <code>Ready</code> state.
However, Pods which already started the process of termination will not be restored by kubelet
and will need to be re-scheduled.</div><p>For example, if <code>shutdownGracePeriod=30s</code>, and
<code>shutdownGracePeriodCriticalPods=10s</code>, kubelet will delay the node shutdown by
30 seconds. During the shutdown, the first 20 (30-10) seconds would be reserved
for gracefully terminating normal pods, and the last 10 seconds would be
reserved for terminating <a href="/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">critical pods</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>When pods were evicted during the graceful node shutdown, they are marked as shutdown.
Running <code>kubectl get pods</code> shows the status of the evicted pods as <code>Terminated</code>.
And <code>kubectl describe pod</code> indicates that the pod was evicted because of node shutdown:</p><pre tabindex="0"><code>Reason:         Terminated
Message:        Pod was terminated in response to imminent node shutdown.
</code></pre></div><h3 id="pod-priority-graceful-node-shutdown">Pod Priority based graceful node shutdown</h3><div class="feature-state-notice feature-beta" title="Feature Gate: GracefulNodeShutdownBasedOnPodPriority"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.24 [beta]</code> (enabled by default: true)</div><p>To provide more flexibility during graceful node shutdown around the ordering
of pods during shutdown, graceful node shutdown honors the PriorityClass for
Pods, provided that you enabled this feature in your cluster. The feature
allows cluster administrators to explicitly define the ordering of pods
during graceful node shutdown based on
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">priority classes</a>.</p><p>The <a href="#graceful-node-shutdown">Graceful Node Shutdown</a> feature, as described
above, shuts down pods in two phases, non-critical pods, followed by critical
pods. If additional flexibility is needed to explicitly define the ordering of
pods during shutdown in a more granular way, pod priority based graceful
shutdown can be used.</p><p>When graceful node shutdown honors pod priorities, this makes it possible to do
graceful node shutdown in multiple phases, each phase shutting down a
particular priority class of pods. The kubelet can be configured with the exact
phases and shutdown time per phase.</p><p>Assuming the following custom pod
<a href="/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">priority classes</a>
in a cluster,</p><table><thead><tr><th>Pod priority class name</th><th>Pod priority class value</th></tr></thead><tbody><tr><td><code>custom-class-a</code></td><td>100000</td></tr><tr><td><code>custom-class-b</code></td><td>10000</td></tr><tr><td><code>custom-class-c</code></td><td>1000</td></tr><tr><td><code>regular/unset</code></td><td>0</td></tr></tbody></table><p>Within the <a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet configuration</a>
the settings for <code>shutdownGracePeriodByPodPriority</code> could look like:</p><table><thead><tr><th>Pod priority class value</th><th>Shutdown period</th></tr></thead><tbody><tr><td>100000</td><td>10 seconds</td></tr><tr><td>10000</td><td>180 seconds</td></tr><tr><td>1000</td><td>120 seconds</td></tr><tr><td>0</td><td>60 seconds</td></tr></tbody></table><p>The corresponding kubelet config YAML configuration would be:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>shutdownGracePeriodByPodPriority</span>:<span>
</span></span></span><span><span><span>  </span>- <span>priority</span>:<span> </span><span>100000</span><span>
</span></span></span><span><span><span>    </span><span>shutdownGracePeriodSeconds</span>:<span> </span><span>10</span><span>
</span></span></span><span><span><span>  </span>- <span>priority</span>:<span> </span><span>10000</span><span>
</span></span></span><span><span><span>    </span><span>shutdownGracePeriodSeconds</span>:<span> </span><span>180</span><span>
</span></span></span><span><span><span>  </span>- <span>priority</span>:<span> </span><span>1000</span><span>
</span></span></span><span><span><span>    </span><span>shutdownGracePeriodSeconds</span>:<span> </span><span>120</span><span>
</span></span></span><span><span><span>  </span>- <span>priority</span>:<span> </span><span>0</span><span>
</span></span></span><span><span><span>    </span><span>shutdownGracePeriodSeconds</span>:<span> </span><span>60</span><span>
</span></span></span></code></pre></div><p>The above table implies that any pod with <code>priority</code> value &gt;= 100000 will get
just 10 seconds to shut down, any pod with value &gt;= 10000 and &lt; 100000 will get 180
seconds to shut down, any pod with value &gt;= 1000 and &lt; 10000 will get 120 seconds to shut down.
Finally, all other pods will get 60 seconds to shut down.</p><p>One doesn't have to specify values corresponding to all of the classes. For
example, you could instead use these settings:</p><table><thead><tr><th>Pod priority class value</th><th>Shutdown period</th></tr></thead><tbody><tr><td>100000</td><td>300 seconds</td></tr><tr><td>1000</td><td>120 seconds</td></tr><tr><td>0</td><td>60 seconds</td></tr></tbody></table><p>In the above case, the pods with <code>custom-class-b</code> will go into the same bucket
as <code>custom-class-c</code> for shutdown.</p><p>If there are no pods in a particular range, then the kubelet does not wait
for pods in that priority range. Instead, the kubelet immediately skips to the
next priority class value range.</p><p>If this feature is enabled and no configuration is provided, then no ordering
action will be taken.</p><p>Using this feature requires enabling the <code>GracefulNodeShutdownBasedOnPodPriority</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>,
and setting <code>ShutdownGracePeriodByPodPriority</code> in the
<a href="/docs/reference/config-api/kubelet-config.v1beta1/">kubelet config</a>
to the desired configuration containing the pod priority class values and
their respective shutdown periods.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The ability to take Pod priority into account during graceful node shutdown was introduced
as an Alpha feature in Kubernetes v1.23. In Kubernetes 1.34
the feature is Beta and is enabled by default.</div><p>Metrics <code>graceful_shutdown_start_time_seconds</code> and <code>graceful_shutdown_end_time_seconds</code>
are emitted under the kubelet subsystem to monitor node shutdowns.</p><h2 id="non-graceful-node-shutdown">Non-graceful node shutdown handling</h2><div class="feature-state-notice feature-stable" title="Feature Gate: NodeOutOfServiceVolumeDetach"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [stable]</code> (enabled by default: true)</div><p>A node shutdown action may not be detected by kubelet's Node Shutdown Manager,
either because the command does not trigger the inhibitor locks mechanism used by
kubelet or because of a user error, i.e., the ShutdownGracePeriod and
ShutdownGracePeriodCriticalPods are not configured properly. Please refer to above
section <a href="#graceful-node-shutdown">Graceful Node Shutdown</a> for more details.</p><p>When a node is shutdown but not detected by kubelet's Node Shutdown Manager, the pods
that are part of a <a class="glossary-tooltip" title="A StatefulSet manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod." href="/docs/concepts/workloads/controllers/statefulset/" target="_blank">StatefulSet</a>
will be stuck in terminating status on the shutdown node and cannot move to a new running node.
This is because kubelet on the shutdown node is not available to delete the pods so
the StatefulSet cannot create a new pod with the same name. If there are volumes used by the pods,
the VolumeAttachments will not be deleted from the original shutdown node so the volumes
used by these pods cannot be attached to a new running node. As a result, the
application running on the StatefulSet cannot function properly. If the original
shutdown node comes up, the pods will be deleted by kubelet and new pods will be
created on a different running node. If the original shutdown node does not come up,
these pods will be stuck in terminating status on the shutdown node forever.</p><p>To mitigate the above situation, a user can manually add the taint <code>node.kubernetes.io/out-of-service</code>
with either <code>NoExecute</code> or <code>NoSchedule</code> effect to a Node marking it out-of-service.
If a Node is marked out-of-service with this taint, the pods on the node will be forcefully deleted
if there are no matching tolerations on it and volume detach operations for the pods terminating on
the node will happen immediately. This allows the Pods on the out-of-service node to recover quickly
on a different node.</p><p>During a non-graceful shutdown, Pods are terminated in the two phases:</p><ol><li>Force delete the Pods that do not have matching <code>out-of-service</code> tolerations.</li><li>Immediately perform detach volume operation for such pods.</li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li>Before adding the taint <code>node.kubernetes.io/out-of-service</code>, it should be verified
that the node is already in shutdown or power off state (not in the middle of restarting).</li><li>The user is required to manually remove the out-of-service taint after the pods are
moved to a new node and the user has checked that the shutdown node has been
recovered since the user was the one who originally added the taint.</li></ul></div><h3 id="storage-force-detach-on-timeout">Forced storage detach on timeout</h3><p>In any situation where a pod deletion has not succeeded for 6 minutes, kubernetes will
force detach volumes being unmounted if the node is unhealthy at that instant. Any
workload still running on the node that uses a force-detached volume will cause a
violation of the
<a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#controllerunpublishvolume">CSI specification</a>,
which states that <code>ControllerUnpublishVolume</code> "<strong>must</strong> be called after all
<code>NodeUnstageVolume</code> and <code>NodeUnpublishVolume</code> on the volume are called and succeed".
In such circumstances, volumes on the node in question might encounter data corruption.</p><p>The forced storage detach behaviour is optional; users might opt to use the "Non-graceful
node shutdown" feature instead.</p><p>Force storage detach on timeout can be disabled by setting the <code>disable-force-detach-on-timeout</code>
config field in <code>kube-controller-manager</code>. Disabling the force detach on timeout feature means
that a volume that is hosted on a node that is unhealthy for more than 6 minutes will not have
its associated
<a href="/docs/reference/kubernetes-api/config-and-storage-resources/volume-attachment-v1/">VolumeAttachment</a>
deleted.</p><p>After this setting has been applied, unhealthy pods still attached to volumes must be recovered
via the <a href="#non-graceful-node-shutdown">Non-Graceful Node Shutdown</a> procedure mentioned above.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><ul><li>Caution must be taken while using the <a href="#non-graceful-node-shutdown">Non-Graceful Node Shutdown</a> procedure.</li><li>Deviation from the steps documented above can result in data corruption.</li></ul></div><h2 id="what-s-next">What's next</h2><p>Learn more about the following:</p><ul><li>Blog: <a href="/blog/2023/08/16/kubernetes-1-28-non-graceful-node-shutdown-ga/">Non-Graceful Node Shutdown</a>.</li><li>Cluster Architecture: <a href="/docs/concepts/architecture/nodes/">Nodes</a>.</li></ul></div></div><div><div class="td-content"><h1>Swap memory management</h1><p>Kubernetes can be configured to use swap memory on a <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">node</a>,
allowing the kernel to free up physical memory by swapping out pages to backing storage.
This is useful for multiple use-cases.
For example, nodes running workloads that can benefit from using swap,
such as those that have large memory footprints but only access a portion of that memory at any given time.
It also helps prevent Pods from being terminated during memory pressure spikes,
shields nodes from system-level memory spikes that might compromise its stability,
allows for more flexible memory management on the node, and much more.</p><p>To learn about configuring swap in your cluster, read
<a href="/docs/tutorials/cluster-management/provision-swap-memory/">Configuring swap memory on Kubernetes nodes</a>.</p><h2 id="operating-system-support">Operating system support</h2><ul><li>Linux nodes support swap; you need to configure each node to enable it.
By default, the kubelet will <strong>not</strong> start on a Linux node that has swap enabled.</li><li>Windows nodes require swap space.
By default, the kubelet does <strong>not</strong> start on a Windows node that has swap disabled.</li></ul><h2 id="how-does-it-work">How does it work?</h2><p>There are a number of possible ways that one could envision swap use on a node.
If kubelet is already running on a node, it would need to be restarted after swap is provisioned in order to identify it.</p><p>When kubelet starts on a node in which swap is provisioned and available
(with the <code>failSwapOn: false</code> configuration), kubelet will:</p><ul><li>Be able to start on this swap-enabled node.</li><li>Direct the Container Runtime Interface (CRI) implementation, often referred to as the container runtime,
to allocate zero swap memory to Kubernetes workloads by default.</li></ul><p>Swap configuration on a node is exposed to a cluster admin via the
<a href="/docs/reference/config-api/kubelet-config.v1/"><code>memorySwap</code> in the KubeletConfiguration</a>.
As a cluster administrator, you can specify the node's behaviour in the
presence of swap memory by setting <code>memorySwap.swapBehavior</code>.</p><h3 id="swap-behaviors">Swap behaviors</h3><p>You need to pick a <a href="/docs/reference/node/swap-behavior/">swap behavior</a> to
use. Different nodes in your cluster can use different swap behaviors.</p><p>The swap behaviors you can choose for Linux nodes are:</p><dl><dt><code>NoSwap</code> (default)</dt><dd>Workloads running as Pods on this node do not and cannot use swap.</dd><dt><code>LimitedSwap</code></dt><dd>Kubernetes workloads can utilize swap memory.</dd></dl><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If you choose the NoSwap behavior, and you configure the kubelet to tolerate
swap space (<code>failSwapOn: false</code>), then your workloads don't use any swap.</p><p>However, processes outside of Kubernetes-managed containers, such as systemi
services (and even the kubelet itself!) <strong>can</strong> utilize swap.</p></div><p>You can read <a href="/docs/tutorials/cluster-management/provision-swap-memory/">configuring swap memory on Kubernetes nodes</a> to learn about enabling swap for your cluster.</p><h3 id="container-runtime-integration">Container runtime integration</h3><p>The kubelet uses the container runtime API, and directs the container runtime to
apply specific configuration (for example, in the cgroup v2 case, <code>memory.swap.max</code>) in a manner that will
enable the desired swap configuration for a container. For runtimes that use control groups, or cgroups,
the container runtime is then responsible for writing these settings to the container-level cgroup.</p><h2 id="observability-for-swap-use">Observability for swap use</h2><h3 id="node-and-container-level-metric-statistics">Node and container level metric statistics</h3><p>Kubelet now collects node and container level metric statistics,
which can be accessed at the <code>/metrics/resource</code> (which is used mainly by monitoring
tools like Prometheus) and <code>/stats/summary</code> (which is used mainly by Autoscalers) kubelet HTTP endpoints.
This allows clients who can directly request the kubelet to
monitor swap usage and remaining swap memory when using <code>LimitedSwap</code>.
Additionally, a <code>machine_swap_bytes</code> metric has been added to cadvisor to show
the total physical swap capacity of the machine.
See <a href="/docs/reference/instrumentation/node-metrics/">this page</a> for more info.</p><p>For example, these <code>/metrics/resource</code> are supported:</p><ul><li><code>node_swap_usage_bytes</code>: Current swap usage of the node in bytes.</li><li><code>container_swap_usage_bytes</code>: Current amount of the container swap usage in bytes.</li><li><code>container_swap_limit_bytes</code>: Current amount of the container swap limit in bytes.</li></ul><h3 id="using-kubectl-top-show-swap">Using <code>kubectl top --show-swap</code></h3><p>Querying metrics is valuable, but somewhat cumbersome, as these metrics
are designed to be used by software rather than humans.
In order to consume this data in a more user-friendly way,
the <code>kubectl top</code> command has been extended to support swap metrics, using the <code>--show-swap</code> flag.</p><p>In order to receive information about swap usage on nodes, <code>kubectl top nodes --show-swap</code> can be used:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl top nodes --show-swap
</span></span></code></pre></div><p>This will result in an output similar to:</p><pre tabindex="0"><code>NAME    CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   SWAP(bytes)    SWAP(%)       
node1   1m           10%      2Mi             10%         1Mi            0%   
node2   5m           10%      6Mi             10%         2Mi            0%   
node3   3m           10%      4Mi             10%         &lt;unknown&gt;      &lt;unknown&gt;   
</code></pre><p>In order to receive information about swap usage by pods, <code>kubectl top nodes --show-swap</code> can be used:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl top pod -n kube-system --show-swap
</span></span></code></pre></div><p>This will result in an output similar to:</p><pre tabindex="0"><code>NAME                                      CPU(cores)   MEMORY(bytes)   SWAP(bytes)
coredns-58d5bc5cdb-5nbk4                  2m           19Mi            0Mi
coredns-58d5bc5cdb-jsh26                  3m           37Mi            0Mi
etcd-node01                               51m          143Mi           5Mi
kube-apiserver-node01                     98m          824Mi           16Mi
kube-controller-manager-node01            20m          135Mi           9Mi
kube-proxy-ffgs2                          1m           24Mi            0Mi
kube-proxy-fhvwx                          1m           39Mi            0Mi
kube-scheduler-node01                     13m          69Mi            0Mi
metrics-server-8598789fdb-d2kcj           5m           26Mi            0Mi   
</code></pre><h3 id="nodes-to-report-swap-capacity-as-part-of-node-status">Nodes to report swap capacity as part of node status</h3><p>A new node status field is now added, <code>node.status.nodeInfo.swap.capacity</code>, to report the swap capacity of a node.</p><p>As an example, the following command can be used to retrieve the swap capacity of the nodes in a cluster:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes -o go-template<span>=</span><span>'{{range .items}}{{.metadata.name}}: {{if .status.nodeInfo.swap.capacity}}{{.status.nodeInfo.swap.capacity}}{{else}}&lt;unknown&gt;{{end}}{{"\n"}}{{end}}'</span>
</span></span></code></pre></div><p>This will result in an output similar to:</p><pre tabindex="0"><code>node1: 21474836480
node2: 42949664768
node3: &lt;unknown&gt;
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The <code>&lt;unknown&gt;</code> value indicates that the <code>.status.nodeInfo.swap.capacity</code> field is not set for that Node.
This probably means that the node does not have swap provisioned, or less likely,
that the kubelet is not able to determine the swap capacity of the node.</div><h3 id="node-feature-discovery">Swap discovery using Node Feature Discovery (NFD)</h3><p><a href="https://github.com/kubernetes-sigs/node-feature-discovery">Node Feature Discovery</a>
is a Kubernetes addon for detecting hardware features and configuration.
It can be utilized to discover which nodes are provisioned with swap.</p><p>As an example, to figure out which nodes are provisioned with swap,
use the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get nodes -o <span>jsonpath</span><span>=</span><span>'{range .items[?(@.metadata.labels.feature\.node\.kubernetes\.io/memory-swap)]}{.metadata.name}{"\t"}{.metadata.labels.feature\.node\.kubernetes\.io/memory-swap}{"\n"}{end}'</span>
</span></span></code></pre></div><p>This will result in an output similar to:</p><pre tabindex="0"><code>k8s-worker1: true
k8s-worker2: true
k8s-worker3: false
</code></pre><p>In this example, swap is provisioned on nodes <code>k8s-worker1</code> and <code>k8s-worker2</code>, but not on <code>k8s-worker3</code>.</p><h2 id="risks-and-caveats">Risks and caveats</h2><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>It is deeply encouraged to encrypt the swap space.
See Memory-backed volumes <a href="#memory-backed-volumes">memory-backed volumes</a> for more info.</div><p>Having swap available on a system reduces predictability.
While swap can enhance performance by making more RAM available, swapping data
back to memory is a heavy operation, sometimes slower by many orders of magnitude,
which can cause unexpected performance regressions.
Furthermore, swap changes a system's behaviour under memory pressure.
Enabling swap increases the risk of noisy neighbors,
where Pods that frequently use their RAM may cause other Pods to swap.
In addition, since swap allows for greater memory usage for workloads in Kubernetes that cannot be predictably accounted for,
and due to unexpected packing configurations,
the scheduler currently does not account for swap memory usage.
This heightens the risk of noisy neighbors.</p><p>The performance of a node with swap memory enabled depends on the underlying physical storage.
When swap memory is in use, performance will be significantly worse in an I/O
operations per second (IOPS) constrained environment, such as a cloud VM with
I/O throttling, when compared to faster storage mediums like solid-state drives
or NVMe.
As swap might cause IO pressure, it is recommended to give a higher IO latency
priority to system critical daemons. See the relevant section in the
<a href="#good-practice-for-using-swap-in-a-kubernetes-cluster">recommended practices</a> section below.</p><h3 id="memory-backed-volumes">Memory-backed volumes</h3><p>On Linux nodes, memory-backed volumes (such as <a href="/docs/concepts/configuration/secret/"><code>secret</code></a>
volume mounts, or <a href="/docs/concepts/storage/volumes/#emptydir"><code>emptyDir</code></a> with <code>medium: Memory</code>)
are implemented with a <code>tmpfs</code> filesystem.
The contents of such volumes should remain in memory at all times, hence should
not be swapped to disk.
To ensure the contents of such volumes remain in memory, the <code>noswap</code> tmpfs option
is being used.</p><p>The Linux kernel officially supports the <code>noswap</code> option from version 6.3 (more info
can be found in <a href="/docs/reference/node/kernel-version-requirements/#requirements-other">Linux Kernel Version Requirements</a>).
However, the different distributions often choose to backport this mount option to older
Linux versions as well.</p><p>In order to verify whether the node supports the <code>noswap</code> option, the kubelet will do the following:</p><ul><li>If the kernel's version is above 6.3 then the <code>noswap</code> option will be assumed to be supported.</li><li>Otherwise, kubelet would try to mount a dummy tmpfs with the <code>noswap</code> option at startup.
If kubelet fails with an error indicating of an unknown option, <code>noswap</code> will be assumed
to not be supported, hence will not be used.
A kubelet log entry will be emitted to warn the user about memory-backed volumes might swap to disk.
If kubelet succeeds, the dummy tmpfs will be deleted and the <code>noswap</code> option will be used.<ul><li>If the <code>noswap</code> option is not supported, kubelet will emit a warning log entry,
then continue its execution.</li></ul></li></ul><p>See the <a href="#setting-up-encrypted-swap">section above</a> with an example for setting unencrypted swap.
However, handling encrypted swap is not within the scope of kubelet;
rather, it is a general OS configuration concern and should be addressed at that level.
It is the administrator's responsibility to provision encrypted swap to mitigate this risk.</p><h3 id="evictions">Evictions</h3><p>Configuring memory eviction thresholds for swap-enabled nodes can be tricky.</p><p>With swap being disabled, it is reasonable to configure kubelet's eviction thresholds
to be a bit lower than the node's memory capacity.
The rationale is that we want Kubernetes to start evicting Pods before the node runs out of memory
and invokes the Out Of Memory (OOM) killer, since the OOM killer is not Kubernetes-aware,
therefore does not consider things like QoS, pod priority, or other Kubernetes-specific factors.</p><p>With swap enabled, the situation is more complex.
In Linux, the <code>vm.min_free_kbytes</code> parameter defines the memory threshold for the kernel
to start aggressively reclaiming memory, which includes swapping out pages.
If the kubelet's eviction thresholds are set in a way that eviction would take place
before the kernel starts reclaiming memory, it could lead to workloads never
being able to swap out during node memory pressure.
However, setting the eviction thresholds too high could result in the node running out of memory
and invoking the OOM killer, which is not ideal either.</p><p>To address this, it is recommended to set the kubelet's eviction thresholds
to be slightly lower than the <code>vm.min_free_kbytes</code> value.
This way, the node can start swapping before kubelet would start evicting Pods,
allowing workloads to swap out unused data and preventing evictions from happening.
On the other hand, since it is just slightly lower, kubelet is likely to start evicting Pods
before the node runs out of memory, thus avoiding the OOM killer.</p><p>The value of <code>vm.min_free_kbytes</code> can be determined by running the following command on the node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>cat /proc/sys/vm/min_free_kbytes
</span></span></code></pre></div><h3 id="unutilized-swap-space">Unutilized swap space</h3><p>Under the <code>LimitedSwap</code> behavior, the amount of swap available to a Pod is determined automatically,
based on the proportion of the memory requested relative to the node's total memory
(For more details, see the <a href="#how-is-the-swap-limit-being-determined-with-limitedswap">section below</a>).</p><p>This design means that usually there would be some portion of swap that will remain
restricted for Kubernetes workloads.
For example, since Kubernetes 1.34 does not permit swap use for
Pods in the Guaranteed <a class="glossary-tooltip" title="QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction." href="/docs/concepts/workloads/pods/pod-qos/" target="_blank">QoS class</a>,
the amount of swap that's proportional to the memory request for Guaranteed pods would
remain unused by Kubernetes workloads.</p><p>This behavior carries some risk in a situation where many pods are not eligible for swapping.
On the other hand, it effectively keeps some system-reserved amount of swap memory that can be used by processes
outside of Kubernetes' scope, such as system daemons and even kubelet itself.</p><h2 id="good-practice-for-using-swap-in-a-kubernetes-cluster">Good practice for using swap in a Kubernetes cluster</h2><h3 id="disable-swap-for-system-critical-daemons">Disable swap for system-critical daemons</h3><p>During the testing phase and based on user feedback, it was observed that the performance
of system-critical daemons and services might degrade.
This implies that system daemons, including the kubelet, could operate slower than usual.
If this issue is encountered, it is advisable to configure the cgroup of the system slice
to prevent swapping (i.e., set <code>memory.swap.max=0</code>).</p><h3 id="protect-system-critical-daemons-for-i-o-latency">Protect system-critical daemons for I/O latency</h3><p>Swap can increase the I/O load on a node.
When memory pressure causes the kernel to rapidly swap pages in and out,
system-critical daemons and services that rely on I/O operations may
experience performance degradation.</p><p>To mitigate this, it is recommended for systemd users to prioritize the system slice in terms of I/O latency.
For non-systemd users,
setting up a dedicated cgroup for system daemons and processes and prioritizing I/O latency in the same way is advised.
This can be achieved by setting <code>io.latency</code> for the system slice,
thereby granting it higher I/O priority.
See <a href="https://www.kernel.org/doc/Documentation/admin-guide/cgroup-v2.rst">cgroup's documentation</a> for more info.</p><h3 id="swap-and-control-plane-nodes">Swap and control plane nodes</h3><p>The Kubernetes project recommends running control plane nodes without any swap space configured.
The control plane primarily hosts Guaranteed QoS Pods, so swap can generally be disabled.
The main concern is that swapping critical services on the control plane could negatively impact performance.</p><h3 id="use-of-a-dedicated-disk-for-swap">Use of a dedicated disk for swap</h3><p>The Kubernetes project recommends using encrypted swap, whenever you run nodes with swap enabled.
If swap resides on a partition or the root filesystem, workloads may interfere
with system processes that need to write to disk.
When they share the same disk, processes can overwhelm swap,
disrupting the I/O of kubelet, container runtime, and systemd, which would impact other workloads.
Since swap space is located on a disk, it is crucial to ensure the disk is fast enough for the intended use cases.
Alternatively, one can configure I/O priorities between different mapped areas of a single backing device.</p><h3 id="swap-aware-scheduling">Swap-aware scheduling</h3><p>Kubernetes 1.34 does not support allocating Pods to nodes in a way that accounts
for swap memory usage. The scheduler typically uses <em>requests</em> for infrastructure resources
to guide Pod placement, and Pods do not request swap space; they just request <code>memory</code>.
This means that the scheduler does not consider swap memory when making scheduling decisions.
While this is something we are actively working on, it is not yet implemented.</p><p>In order for administrators to ensure that Pods are not scheduled on nodes
with swap memory unless they are specifically intended to use it,
Administrators can taint nodes with swap available to protect against this problem.
Taints will ensure that workloads which tolerate swap will not spill onto nodes without swap under load.</p><h3 id="selecting-storage-for-optimal-performance">Selecting storage for optimal performance</h3><p>The storage device designated for swap space is critical to maintaining system responsiveness
during high memory usage.
Rotational hard disk drives (HDDs) are ill-suited for this task as their mechanical nature introduces significant latency,
leading to severe performance degradation and system thrashing.
For modern performance needs, a device such as a Solid State Drive (SSD) is probably the appropriate choice for swap,
as its low-latency electronic access minimizes the slowdown.</p><h2 id="swap-behavior-details">Swap behavior details</h2><h3 id="how-is-the-swap-limit-being-determined-with-limitedswap">How is the swap limit being determined with LimitedSwap?</h3><p>The configuration of swap memory, including its limitations, presents a significant
challenge. Not only is it prone to misconfiguration, but as a system-level property, any
misconfiguration could potentially compromise the entire node rather than just a specific
workload. To mitigate this risk and ensure the health of the node, we have implemented
Swap with automatic configuration of limitations.</p><p>With <code>LimitedSwap</code>, Pods that do not fall under the Burstable QoS classification (i.e.
<code>BestEffort</code>/<code>Guaranteed</code> QoS Pods) are prohibited from utilizing swap memory.
<code>BestEffort</code> QoS Pods exhibit unpredictable memory consumption patterns and lack
information regarding their memory usage, making it difficult to determine a safe
allocation of swap memory.
Conversely, <code>Guaranteed</code> QoS Pods are typically employed for applications that rely on the
precise allocation of resources specified by the workload, with memory being immediately available.
To maintain the aforementioned security and node health guarantees,
these Pods are not permitted to use swap memory when <code>LimitedSwap</code> is in effect.
In addition, high-priority pods are not permitted to use swap in order to ensure the memory
they consume always residents on disk, hence ready to use.</p><p>Prior to detailing the calculation of the swap limit, it is necessary to define the following terms:</p><ul><li><code>nodeTotalMemory</code>: The total amount of physical memory available on the node.</li><li><code>totalPodsSwapAvailable</code>: The total amount of swap memory on the node that is available for use by Pods (some swap memory may be reserved for system use).</li><li><code>containerMemoryRequest</code>: The container's memory request.</li></ul><p>Swap limitation is configured as:<br>( <code>containerMemoryRequest</code> / <code>nodeTotalMemory</code> ) &#215; <code>totalPodsSwapAvailable</code></p><p>In other words, the amount of swap that a container is able to use is proportionate to its
memory request, the node's total physical memory and the total amount of swap memory on
the node that is available for use by Pods.</p><p>It is important to note that, for containers within Burstable QoS Pods, it is possible to
opt-out of swap usage by specifying memory requests that are equal to memory limits.
Containers configured in this manner will not have access to swap memory.</p><h2 id="what-s-next">What's next</h2><ul><li>To learn about managing swap on Linux nodes, read
<a href="/docs/tutorials/cluster-management/provision-swap-memory/">configuring swap memory on Kubernetes nodes</a>.</li><li>You can check out a <a href="/blog/2025/03/25/swap-linux-improvements/">blog post about Kubernetes and swap</a></li><li>For background information, please see the original KEP, <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2400-node-swap">KEP-2400</a>,
and its <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md">design</a>.</li></ul></div></div><div><div class="td-content"><h1>Node Autoscaling</h1><div class="lead">Automatically provision and consolidate the Nodes in your cluster to adapt to demand and optimize cost.</div><p>In order to run workloads in your cluster, you need
<a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">Nodes</a>. Nodes in your cluster can be <em>autoscaled</em> -
dynamically <a href="#provisioning"><em>provisioned</em></a>, or <a href="#consolidation"><em>consolidated</em></a> to provide needed
capacity while optimizing cost. Autoscaling is performed by Node <a href="#autoscalers"><em>autoscalers</em></a>.</p><h2 id="provisioning">Node provisioning</h2><p>If there are Pods in a cluster that can't be scheduled on existing Nodes, new Nodes can be
automatically added to the cluster&#8212;<em>provisioned</em>&#8212;to accommodate the Pods. This is
especially useful if the number of Pods changes over time, for example as a result of
<a href="#horizontal-workload-autoscaling">combining horizontal workload with Node autoscaling</a>.</p><p>Autoscalers provision the Nodes by creating and deleting cloud provider resources backing them. Most
commonly, the resources backing the Nodes are Virtual Machines.</p><p>The main goal of provisioning is to make all Pods schedulable. This goal is not always attainable
because of various limitations, including reaching configured provisioning limits, provisioning
configuration not being compatible with a particular set of pods, or the lack of cloud provider
capacity. While provisioning, Node autoscalers often try to achieve additional goals (for example
minimizing the cost of the provisioned Nodes or balancing the number of Nodes between failure
domains).</p><p>There are two main inputs to a Node autoscaler when determining Nodes to
provision&#8212;<a href="#provisioning-pod-constraints">Pod scheduling constraints</a>,
and <a href="#provisioning-node-constraints">Node constraints imposed by autoscaler configuration</a>.</p><p>Autoscaler configuration may also include other Node provisioning triggers (for example the number
of Nodes falling below a configured minimum limit).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Provisioning was formerly known as <em>scale-up</em> in Cluster Autoscaler.</div><h3 id="provisioning-pod-constraints">Pod scheduling constraints</h3><p>Pods can express <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">scheduling constraints</a> to
impose limitations on the kind of Nodes they can be scheduled on. Node autoscalers take these
constraints into account to ensure that the pending Pods can be scheduled on the provisioned Nodes.</p><p>The most common kind of scheduling constraints are the resource requests specified by Pod
containers. Autoscalers will make sure that the provisioned Nodes have enough resources to satisfy
the requests. However, they don't directly take into account the real resource usage of the Pods
after they start running. In order to autoscale Nodes based on actual workload resource usage, you
can combine <a href="#horizontal-workload-autoscaling">horizontal workload autoscaling</a> with Node
autoscaling.</p><p>Other common Pod scheduling constraints include
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity">Node affinity</a>,
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">inter-Pod affinity</a>,
or a requirement for a particular <a href="/docs/concepts/storage/volumes/">storage volume</a>.</p><h3 id="provisioning-node-constraints">Node constraints imposed by autoscaler configuration</h3><p>The specifics of the provisioned Nodes (for example the amount of resources, the presence of a given
label) depend on autoscaler configuration. Autoscalers can either choose them from a pre-defined set
of Node configurations, or use <a href="#autoprovisioning">auto-provisioning</a>.</p><h3 id="autoprovisioning">Auto-provisioning</h3><p>Node auto-provisioning is a mode of provisioning in which a user doesn't have to fully configure the
specifics of the Nodes that can be provisioned. Instead, the autoscaler dynamically chooses the Node
configuration based on the pending Pods it's reacting to, as well as pre-configured constraints (for
example, the minimum amount of resources or the need for a given label).</p><h2 id="consolidation">Node consolidation</h2><p>The main consideration when running a cluster is ensuring that all schedulable pods are running,
whilst keeping the cost of the cluster as low as possible. To achieve this, the Pods' resource
requests should utilize as much of the Nodes' resources as possible. From this perspective, the
overall Node utilization in a cluster can be used as a proxy for how cost-effective the cluster is.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Correctly setting the resource requests of your Pods is as important to the overall
cost-effectiveness of a cluster as optimizing Node utilization.
Combining Node autoscaling with <a href="#vertical-workload-autoscaling">vertical workload autoscaling</a> can
help you achieve this.</div><p>Nodes in your cluster can be automatically <em>consolidated</em> in order to improve the overall Node
utilization, and in turn the cost-effectiveness of the cluster. Consolidation happens through
removing a set of underutilized Nodes from the cluster. Optionally, a different set of Nodes can
be <a href="#provisioning">provisioned</a> to replace them.</p><p>Consolidation, like provisioning, only considers Pod resource requests and not real resource usage
when making decisions.</p><p>For the purpose of consolidation, a Node is considered <em>empty</em> if it only has DaemonSet and static
Pods running on it. Removing empty Nodes during consolidation is more straightforward than non-empty
ones, and autoscalers often have optimizations designed specifically for consolidating empty Nodes.</p><p>Removing non-empty Nodes during consolidation is disruptive&#8212;the Pods running on them are
terminated, and possibly have to be recreated (for example by a Deployment). However, all such
recreated Pods should be able to schedule on existing Nodes in the cluster, or the replacement Nodes
provisioned as part of consolidation. <strong>No Pods should normally become pending as a result of
consolidation.</strong></p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Autoscalers predict how a recreated Pod will likely be scheduled after a Node is provisioned or
consolidated, but they don't control the actual scheduling. Because of this, some Pods might
become pending as a result of consolidation - if for example a completely new Pod appears while
consolidation is being performed.</div><p>Autoscaler configuration may also enable triggering consolidation by other conditions (for example,
the time elapsed since a Node was created), in order to optimize different properties (for example,
the maximum lifespan of Nodes in a cluster).</p><p>The details of how consolidation is performed depend on the configuration of a given autoscaler.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Consolidation was formerly known as <em>scale-down</em> in Cluster Autoscaler.</div><h2 id="autoscalers">Autoscalers</h2><p>The functionalities described in previous sections are provided by Node <em>autoscalers</em>. In addition
to the Kubernetes API, autoscalers also need to interact with cloud provider APIs to provision and
consolidate Nodes. This means that they need to be explicitly integrated with each supported cloud
provider. The performance and feature set of a given autoscaler can differ between cloud provider
integrations.</p><figure><div class="mermaid">graph TD
na[Node autoscaler]
k8s[Kubernetes]
cp[Cloud Provider]
k8s --&gt; |get Pods/Nodes|na
na --&gt; |drain Nodes|k8s
na --&gt; |create/remove resources backing Nodes|cp
cp --&gt; |get resources backing Nodes|na
classDef white_on_blue fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef blue_on_white fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class na blue_on_white;
class k8s,cp white_on_blue;</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><h3 id="autoscaler-implementations">Autoscaler implementations</h3><p><a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">Cluster Autoscaler</a>
and <a href="https://github.com/kubernetes-sigs/karpenter">Karpenter</a> are the two Node autoscalers currently
sponsored by <a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling">SIG Autoscaling</a>.</p><p>From the perspective of a cluster user, both autoscalers should provide a similar Node autoscaling
experience. Both will provision new Nodes for unschedulable Pods, and both will consolidate the
Nodes that are no longer optimally utilized.</p><p>Different autoscalers may also provide features outside the Node autoscaling scope described on this
page, and those additional features may differ between them.</p><p>Consult the sections below, and the linked documentation for the individual autoscalers to decide
which autoscaler fits your use case better.</p><h4 id="cluster-autoscaler">Cluster Autoscaler</h4><p>Cluster Autoscaler adds or removes Nodes to pre-configured <em>Node groups</em>. Node groups generally map
to some sort of cloud provider resource group (most commonly a Virtual Machine group). A single
instance of Cluster Autoscaler can simultaneously manage multiple Node groups. When provisioning,
Cluster Autoscaler will add Nodes to the group that best fits the requests of pending Pods. When
consolidating, Cluster Autoscaler always selects specific Nodes to remove, as opposed to just
resizing the underlying cloud provider resource group.</p><p>Additional context:</p><ul><li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/README.md">Documentation overview</a></li><li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/README.md#faqdocumentation">Cloud provider integrations</a></li><li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md">Cluster Autoscaler FAQ</a></li><li><a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling#contact">Contact</a></li></ul><h4 id="karpenter">Karpenter</h4><p>Karpenter auto-provisions Nodes based on <a href="https://karpenter.sh/docs/concepts/nodepools/">NodePool</a>
configurations provided by the cluster operator. Karpenter handles all aspects of node lifecycle,
not just autoscaling. This includes automatically refreshing Nodes once they reach a certain
lifetime, and auto-upgrading Nodes when new worker Node images are released. It works directly with
individual cloud provider resources (most commonly individual Virtual Machines), and doesn't rely on
cloud provider resource groups.</p><p>Additional context:</p><ul><li><a href="https://karpenter.sh/">Documentation</a></li><li><a href="https://github.com/kubernetes-sigs/karpenter?tab=readme-ov-file#karpenter-implementations">Cloud provider integrations</a></li><li><a href="https://karpenter.sh/docs/faq/">Karpenter FAQ</a></li><li><a href="https://github.com/kubernetes-sigs/karpenter#community-discussion-contribution-and-support">Contact</a></li></ul><h4 id="implementation-comparison">Implementation comparison</h4><p>Main differences between Cluster Autoscaler and Karpenter:</p><ul><li>Cluster Autoscaler provides features related to just Node autoscaling. Karpenter has a wider
scope, and also provides features intended for managing Node lifecycle altogether (for example,
utilizing disruption to auto-recreate Nodes once they reach a certain lifetime, or auto-upgrade
them to new versions).</li><li>Cluster Autoscaler doesn't support auto-provisioning, the Node groups it can provision from have
to be pre-configured. Karpenter supports auto-provisioning, so the user only has to configure a
set of constraints for the provisioned Nodes, instead of fully configuring homogenous groups.</li><li>Cluster Autoscaler provides cloud provider integrations directly, which means that they're a part
of the Kubernetes project. For Karpenter, the Kubernetes project publishes Karpenter as a library
that cloud providers can integrate with to build a Node autoscaler.</li><li>Cluster Autoscaler provides integrations with numerous cloud providers, including smaller and less
popular providers. There are fewer cloud providers that integrate with Karpenter, including
<a href="https://github.com/aws/karpenter-provider-aws">AWS</a>, and
<a href="https://github.com/Azure/karpenter-provider-azure">Azure</a>.</li></ul><h2 id="combine-workload-and-node-autoscaling">Combine workload and Node autoscaling</h2><h3 id="horizontal-workload-autoscaling">Horizontal workload autoscaling</h3><p>Node autoscaling usually works in response to Pods&#8212;it provisions new Nodes to accommodate
unschedulable Pods, and then consolidates the Nodes once they're no longer needed.</p><p><a href="/docs/concepts/workloads/autoscaling/#scaling-workloads-horizontally">Horizontal workload autoscaling</a>
automatically scales the number of workload replicas to maintain a desired average resource
utilization across the replicas. In other words, it automatically creates new Pods in response to
application load, and then removes the Pods once the load decreases.</p><p>You can use Node autoscaling together with horizontal workload autoscaling to autoscale the Nodes in
your cluster based on the average real resource utilization of your Pods.</p><p>If the application load increases, the average utilization of its Pods should also increase,
prompting workload autoscaling to create new Pods. Node autoscaling should then provision new Nodes
to accommodate the new Pods.</p><p>Once the application load decreases, workload autoscaling should remove unnecessary Pods. Node
autoscaling should, in turn, consolidate the Nodes that are no longer needed.</p><p>If configured correctly, this pattern ensures that your application always has the Node capacity to
handle load spikes if needed, but you don't have to pay for the capacity when it's not needed.</p><h3 id="vertical-workload-autoscaling">Vertical workload autoscaling</h3><p>When using Node autoscaling, it's important to set Pod resource requests correctly. If the requests
of a given Pod are too low, provisioning a new Node for it might not help the Pod actually run.
If the requests of a given Pod are too high, it might incorrectly prevent consolidating its Node.</p><p><a href="/docs/concepts/workloads/autoscaling/#scaling-workloads-vertically">Vertical workload autoscaling</a>
automatically adjusts the resource requests of your Pods based on their historical resource usage.</p><p>You can use Node autoscaling together with vertical workload autoscaling in order to adjust the
resource requests of your Pods while preserving Node autoscaling capabilities in your cluster.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>When using Node autoscaling, it's not recommended to set up vertical workload autoscaling for
DaemonSet Pods. Autoscalers have to predict what DaemonSet Pods on a new Node will look like in
order to predict available Node resources. Vertical workload autoscaling might make these
predictions unreliable, leading to incorrect scaling decisions.</div><h2 id="related-components">Related components</h2><p>This section describes components providing functionality related to Node autoscaling.</p><h3 id="descheduler">Descheduler</h3><p>The <a href="https://github.com/kubernetes-sigs/descheduler">descheduler</a> is a component providing Node
consolidation functionality based on custom policies, as well as other features related to
optimizing Nodes and Pods (for example deleting frequently restarting Pods).</p><h3 id="workload-autoscalers-based-on-cluster-size">Workload autoscalers based on cluster size</h3><p><a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">Cluster Proportional Autoscaler</a>
and <a href="https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler">Cluster Proportional Vertical
Autoscaler</a> provide
horizontal, and vertical workload autoscaling based on the number of Nodes in the cluster. You can
read more in
<a href="/docs/concepts/workloads/autoscaling/#autoscaling-based-on-cluster-size">autoscaling based on cluster size</a>.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/workloads/autoscaling/">workload-level autoscaling</a></li></ul></div></div><div><div class="td-content"><h1>Certificates</h1><p>To learn how to generate certificates for your cluster, see <a href="/docs/tasks/administer-cluster/certificates/">Certificates</a>.</p></div></div><div><div class="td-content"><h1>Cluster Networking</h1><p>Networking is a central part of Kubernetes, but it can be challenging to
understand exactly how it is expected to work. There are 4 distinct networking
problems to address:</p><ol><li>Highly-coupled container-to-container communications: this is solved by
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> and <code>localhost</code> communications.</li><li>Pod-to-Pod communications: this is the primary focus of this document.</li><li>Pod-to-Service communications: this is covered by <a href="/docs/concepts/services-networking/service/">Services</a>.</li><li>External-to-Service communications: this is also covered by Services.</li></ol><p>Kubernetes is all about sharing machines among applications. Typically,
sharing machines requires ensuring that two applications do not try to use the
same ports. Coordinating ports across multiple developers is very difficult to
do at scale and exposes users to cluster-level issues outside of their control.</p><p>Dynamic port allocation brings a lot of complications to the system - every
application has to take ports as flags, the API servers have to know how to
insert dynamic port numbers into configuration blocks, services have to know
how to find each other, etc. Rather than deal with this, Kubernetes takes a
different approach.</p><p>To learn about the Kubernetes networking model, see <a href="/docs/concepts/services-networking/">here</a>.</p><h2 id="kubernetes-ip-address-ranges">Kubernetes IP address ranges</h2><p>Kubernetes clusters require to allocate non-overlapping IP addresses for Pods, Services and Nodes,
from a range of available addresses configured in the following components:</p><ul><li>The network plugin is configured to assign IP addresses to Pods.</li><li>The kube-apiserver is configured to assign IP addresses to Services.</li><li>The kubelet or the cloud-controller-manager is configured to assign IP addresses to Nodes.</li></ul><figure class="diagram-medium"><img src="/docs/images/kubernetes-cluster-network.svg" alt="A figure illustrating the different network ranges in a kubernetes cluster"></figure><h2 id="cluster-network-ipfamilies">Cluster networking types</h2><p>Kubernetes clusters, attending to the IP families configured, can be categorized into:</p><ul><li>IPv4 only: The network plugin, kube-apiserver and kubelet/cloud-controller-manager are configured to assign only IPv4 addresses.</li><li>IPv6 only: The network plugin, kube-apiserver and kubelet/cloud-controller-manager are configured to assign only IPv6 addresses.</li><li>IPv4/IPv6 or IPv6/IPv4 <a href="/docs/concepts/services-networking/dual-stack/">dual-stack</a>:<ul><li>The network plugin is configured to assign IPv4 and IPv6 addresses.</li><li>The kube-apiserver is configured to assign IPv4 and IPv6 addresses.</li><li>The kubelet or cloud-controller-manager is configured to assign IPv4 and IPv6 address.</li><li>All components must agree on the configured primary IP family.</li></ul></li></ul><p>Kubernetes clusters only consider the IP families present on the Pods, Services and Nodes objects,
independently of the existing IPs of the represented objects. Per example, a server or a pod can have multiple
IP addresses on its interfaces, but only the IP addresses in <code>node.status.addresses</code> or <code>pod.status.ips</code> are
considered for implementing the Kubernetes network model and defining the type of the cluster.</p><h2 id="how-to-implement-the-kubernetes-network-model">How to implement the Kubernetes network model</h2><p>The network model is implemented by the container runtime on each node. The most common container
runtimes use <a href="https://github.com/containernetworking/cni">Container Network Interface</a> (CNI)
plugins to manage their network and security capabilities. Many different CNI plugins exist from
many different vendors. Some of these provide only basic features of adding and removing network
interfaces, while others provide more sophisticated solutions, such as integration with other
container orchestration systems, running multiple CNI plugins, advanced IPAM features etc.</p><p>See <a href="/docs/concepts/cluster-administration/addons/#networking-and-network-policy">this page</a>
for a non-exhaustive list of networking addons supported by Kubernetes.</p><h2 id="what-s-next">What's next</h2><p>The early design of the networking model and its rationale are described in more detail in the
<a href="https://git.k8s.io/design-proposals-archive/network/networking.md">networking design document</a>.
For future plans and some on-going efforts that aim to improve Kubernetes networking, please
refer to the SIG-Network
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network">KEPs</a>.</p></div></div><div><div class="td-content"><h1>Observability</h1><div class="lead">Understand how to gain end-to-end visibility of a Kubernetes cluster through the collection of metrics, logs, and traces.</div><p>In Kubernetes, observability is the process of collecting and analyzing metrics, logs, and traces&#8212;often referred to as the three pillars of observability&#8212;in order to obtain a better understanding of the internal state, performance, and health of the cluster.</p><p>Kubernetes control plane components, as well as many add-ons, generate and emit these signals. By aggregating and correlating them, you can gain a unified picture of the control plane, add-ons, and applications across the cluster.</p><p>Figure 1 outlines how cluster components emit the three primary signal types.</p><figure><div class="mermaid">flowchart LR
A[Cluster components] --&gt; M[Metrics pipeline]
A --&gt; L[Log pipeline]
A --&gt; T[Trace pipeline]
M --&gt; S[(Storage and analysis)]
L --&gt; S
T --&gt; S
S --&gt; O[Operators and automation]</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p><em>Figure 1. High-level signals emitted by cluster components and their consumers.</em></p><h2 id="metrics">Metrics</h2><p>Kubernetes components emit metrics in <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">Prometheus format</a> from their <code>/metrics</code> endpoints, including:</p><ul><li>kube-controller-manager</li><li>kube-proxy</li><li>kube-apiserver</li><li>kube-scheduler</li><li>kubelet</li></ul><p>The kubelet also exposes metrics at <code>/metrics/cadvisor</code>, <code>/metrics/resource</code>, and <code>/metrics/probes</code>, and add-ons such as <a href="/docs/concepts/cluster-administration/kube-state-metrics/">kube-state-metrics</a> enrich those control plane signals with Kubernetes object status.</p><p>A typical Kubernetes metrics pipeline periodically scrapes these endpoints and stores the samples in a time series database (for example with Prometheus).</p><p>See the <a href="/docs/concepts/cluster-administration/system-metrics/">system metrics guide</a> for details and configuration options.</p><p>Figure 2 outlines a common Kubernetes metrics pipeline.</p><figure><div class="mermaid">flowchart LR
C[Cluster components] --&gt; P[Prometheus scraper]
P --&gt; TS[(Time series storage)]
TS --&gt; D[Dashboards and alerts]
TS --&gt; A[Automated actions]</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p><em>Figure 2. Components of a typical Kubernetes metrics pipeline.</em></p><p>For multi-cluster or multi-cloud visibility, distributed time series databases (for example Thanos or Cortex) can complement Prometheus.</p><p>See <a href="#metrics-tools">Common observability tools - metrics tools</a> for metrics scrapers and time series databases.</p><h4 id="see-also">See Also</h4><ul><li><a href="/docs/concepts/cluster-administration/system-metrics/">System metrics for Kubernetes components</a></li><li><a href="/docs/tasks/debug/debug-cluster/resource-usage-monitoring/">Resource usage monitoring with metrics-server</a></li><li><a href="/docs/concepts/cluster-administration/kube-state-metrics/">kube-state-metrics concept</a></li><li><a href="/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/">Resource metrics pipeline overview</a></li></ul><h2 id="logs">Logs</h2><p>Logs provide a chronological record of events inside applications, Kubernetes system components, and security-related activities such as audit logging.</p><p>Container runtimes capture a containerized application&#8217;s output from standard output (<code>stdout</code>) and standard error (<code>stderr</code>) streams. While runtimes implement this differently, the integration with the kubelet is standardized through the <em>CRI logging format</em>, and the kubelet makes these logs available through <code>kubectl logs</code>.</p><p><img alt="Node-level logging" src="/images/docs/user-guide/logging/logging-node-level.png"></p><p><em>Figure 3a. Node-level logging architecture.</em></p><p>System component logs capture events from the cluster and are often useful for debugging and troubleshooting. These components are classified in two different ways: those that run in a container and those that do not. For example, the <code>kube-scheduler</code> and <code>kube-proxy</code> usually run in containers, whereas the <code>kubelet</code> and the container runtime run directly on the host.</p><ul><li>On machines with <code>systemd</code>, the kubelet and container runtime write to journald. Otherwise, they write to <code>.log</code> files in the <code>/var/log</code> directory.</li><li>System components that run inside containers always write to <code>.log</code> files in <code>/var/log</code>, bypassing the default container logging mechanism.</li></ul><p>System component and container logs stored under <code>/var/log</code> require log rotation to prevent uncontrolled growth. Some cluster provisioning scripts install log rotation by default; verify your environment and adjust as needed. See the <a href="/docs/concepts/cluster-administration/system-logs/">system logs reference</a> for details on locations, formats, and configuration options.</p><p>Most clusters run a node-level logging agent (for example, Fluent Bit or Fluentd) that tails these files and forwards entries to a central log store. The <a href="/docs/concepts/cluster-administration/logging/">logging architecture guidance</a> explains how to design such pipelines, apply retention, and log flows to backends.</p><p>Figure 3 outlines a common log aggregation pipeline.</p><figure><div class="mermaid">flowchart LR
subgraph Sources
A[Application stdout / stderr]
B[Control plane logs]
C[Audit records]
end
A --&gt; N[Node log agent]
B --&gt; N
C --&gt; N
N --&gt; L[Central log store]
L --&gt; Q[Dashboards, alerting, SIEM]</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p><em>Figure 3. Components of a typical Kubernetes logs pipeline.</em></p><p>See <a href="#logging-tools">Common observability tools - logging tools</a> for logging agents and central log stores.</p><h4 id="see-also-1">See Also</h4><ul><li><a href="/docs/concepts/cluster-administration/logging/">Logging architecture</a></li><li><a href="/docs/concepts/cluster-administration/system-logs/">System logs</a></li><li><a href="/docs/tasks/debug/logging/">Logging tasks and tutorials</a></li><li><a href="/docs/tasks/debug/debug-cluster/audit/">Configure audit logging</a></li></ul><h2 id="traces">Traces</h2><p>Traces capture how requests moves across Kubernetes components and applications, linking latency, timing and relationships between operations.By collecting traces, you can visualize end-to-end request flow, diagnose performance issues, and identify bottlenecks or unexpected interactions in the control plane, add-ons, or applications.</p><p>Kubernetes 1.34 can export spans over the <a href="/docs/concepts/cluster-administration/system-traces/">OpenTelemetry Protocol</a> (OTLP), either directly via built-in gRPC exporters or by forwarding them through an OpenTelemetry Collector.</p><p>The OpenTelemetry Collector receives spans from components and applications, processes them (for example by applying sampling or redaction), and forwards them to a tracing backend for storage and analysis.</p><p>Figure 4 outlines a typical distributed tracing pipeline.</p><figure><div class="mermaid">flowchart LR
subgraph Sources
A[Control plane spans]
B[Application spans]
end
A --&gt; X[OTLP exporter]
B --&gt; X
X --&gt; COL[OpenTelemetry Collector]
COL --&gt; TS[(Tracing backend)]
TS --&gt; V[Visualization and analysis]</div></figure><noscript><div class="alert alert-secondary callout"><em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em></div></noscript><p><em>Figure 4. Components of a typical Kubernetes traces pipeline.</em></p><p>See <a href="#tracing-tools">Common observability tools - tracing tools</a> for tracing collectors and backends.</p><h4 id="see-also-2">See Also</h4><ul><li><a href="/docs/concepts/cluster-administration/system-traces/">System traces for Kubernetes components</a></li><li><a href="https://opentelemetry.io/docs/collector/getting-started/">OpenTelemetry Collector getting started guide</a></li><li><a href="/docs/tasks/debug/monitoring/">Monitoring and tracing tasks</a></li></ul><h2 id="common-observability-tools">Common observability tools</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Note: This section links to third-party projects that provide observability capabilities required by Kubernetes.
The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a
project to this list, read the <a href="/docs/contribute/style/content-guide/">content guide</a> before submitting a change.</p><h3 id="metrics-tools">Metrics tools</h3><ul><li><a href="https://cortexmetrics.io/">Cortex</a> offers horizontally scalable, long-term Prometheus storage.</li><li><a href="https://grafana.com/oss/mimir/">Grafana Mimir</a> is a Grafana Labs project that provides multi-tenant, horizontally scalable Prometheus-compatible storage.</li><li><a href="https://prometheus.io/">Prometheus</a> is the monitoring system that scrapes and stores metrics from Kubernetes components.</li><li><a href="https://thanos.io/">Thanos</a> extends Prometheus with global querying, downsampling, and object storage support.</li></ul><h3 id="logging-tools">Logging tools</h3><ul><li><a href="https://www.elastic.co/elasticsearch/">Elasticsearch</a> delivers distributed log indexing and search.</li><li><a href="https://fluentbit.io/">Fluent Bit</a> collects and forwards container and node logs with a low resource footprint.</li><li><a href="https://www.fluentd.org/">Fluentd</a> routes and transforms logs to multiple destinations.</li><li><a href="https://grafana.com/oss/loki/">Grafana Loki</a> stores logs in a Prometheus-inspired, label-based format.</li><li><a href="https://opensearch.org/">OpenSearch</a> provides open source log indexing and search compatible with Elasticsearch APIs.</li></ul><h3 id="tracing-tools">Tracing tools</h3><ul><li><a href="https://grafana.com/oss/tempo/">Grafana Tempo</a> offers scalable, low-cost distributed tracing storage.</li><li><a href="https://www.jaegertracing.io/">Jaeger</a> captures and visualizes distributed traces for microservices.</li><li><a href="https://opentelemetry.io/docs/collector/">OpenTelemetry Collector</a> receives, processes, and exports telemetry data including traces.</li><li><a href="https://zipkin.io/">Zipkin</a> provides distributed tracing collection and visualization.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn how to <a href="/docs/tasks/debug/debug-cluster/resource-usage-monitoring/">collect resource usage metrics with metrics-server</a></li><li>Explore <a href="/docs/tasks/debug/logging/">logging tasks and tutorials</a></li><li>Follow the <a href="/docs/tasks/debug/monitoring/">monitoring and tracing task guides</a></li><li>Review the <a href="/docs/concepts/cluster-administration/system-metrics/">system metrics guide</a> for component endpoints and stability</li><li>Review the <a href="#common-observability-tools">common observability tools</a> section for vetted third-party options</li></ul></div></div><div><div class="td-content"><h1>Admission Webhook Good Practices</h1><div class="lead">Recommendations for designing and deploying admission webhooks in Kubernetes.</div><p>This page provides good practices and considerations when designing
<em>admission webhooks</em> in Kubernetes. This information is intended for
cluster operators who run admission webhook servers or third-party applications
that modify or validate your API requests.</p><p>Before reading this page, ensure that you're familiar with the following
concepts:</p><ul><li><a href="/docs/reference/access-authn-authz/admission-controllers/">Admission controllers</a></li><li><a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">Admission webhooks</a></li></ul><h2 id="why-good-webhook-design-matters">Importance of good webhook design</h2><p>Admission control occurs when any create, update, or delete request
is sent to the Kubernetes API. Admission controllers intercept requests that
match specific criteria that you define. These requests are then sent to
mutating admission webhooks or validating admission webhooks. These webhooks are
often written to ensure that specific fields in object specifications exist or
have specific allowed values.</p><p>Webhooks are a powerful mechanism to extend the Kubernetes API. Badly-designed
webhooks often result in workload disruptions because of how much control
the webhooks have over objects in the cluster. Like other API extension
mechanisms, webhooks are challenging to test at scale for compatibility with
all of your workloads, other webhooks, add-ons, and plugins.</p><p>Additionally, with every release, Kubernetes adds or modifies the API with new
features, feature promotions to beta or stable status, and deprecations. Even
stable Kubernetes APIs are likely to change. For example, the <code>Pod</code> API changed
in v1.29 to add the
<a href="/docs/concepts/workloads/pods/sidecar-containers/">Sidecar containers</a> feature.
While it's rare for a Kubernetes object to enter a broken state because of a new
Kubernetes API, webhooks that worked as expected with earlier versions of an API
might not be able to reconcile more recent changes to that API. This can result
in unexpected behavior after you upgrade your clusters to newer versions.</p><p>This page describes common webhook failure scenarios and how to avoid them by
cautiously and thoughtfully designing and implementing your webhooks.</p><h2 id="identify-admission-webhooks">Identify whether you use admission webhooks</h2><p>Even if you don't run your own admission webhooks, some third-party applications
that you run in your clusters might use mutating or validating admission
webhooks.</p><p>To check whether your cluster has any mutating admission webhooks, run the
following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get mutatingwebhookconfigurations
</span></span></code></pre></div><p>The output lists any mutating admission controllers in the cluster.</p><p>To check whether your cluster has any validating admission webhooks, run the
following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get validatingwebhookconfigurations
</span></span></code></pre></div><p>The output lists any validating admission controllers in the cluster.</p><h2 id="choose-admission-mechanism">Choose an admission control mechanism</h2><p>Kubernetes includes multiple admission control and policy enforcement options.
Knowing when to use a specific option can help you to improve latency and
performance, reduce management overhead, and avoid issues during version
upgrades. The following table describes the mechanisms that let you mutate or
validate resources during admission:</p><table><caption>Mutating and validating admission control in Kubernetes</caption><thead><tr><th>Mechanism</th><th>Description</th><th>Use cases</th></tr></thead><tbody><tr><td><a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">Mutating admission webhook</a></td><td>Intercept API requests before admission and modify as needed using
custom logic.</td><td><ul><li>Make critical modifications that must happen before resource
admission.</li><li>Make complex modifications that require advanced logic, like calling
external APIs.</li></ul></td></tr><tr><td><a href="/docs/reference/access-authn-authz/mutating-admission-policy/">Mutating admission policy</a></td><td>Intercept API requests before admission and modify as needed using
Common Expression Language (CEL) expressions.</td><td><ul><li>Make critical modifications that must happen before resource
admission.</li><li>Make simple modifications, such as adjusting labels or replica
counts.</li></ul></td></tr><tr><td><a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">Validating admission webhook</a></td><td>Intercept API requests before admission and validate against complex
policy declarations.</td><td><ul><li>Validate critical configurations before resource admission.</li><li>Enforce complex policy logic before admission.</li></ul></td></tr><tr><td><a href="/docs/reference/access-authn-authz/validating-admission-policy/">Validating admission policy</a></td><td>Intercept API requests before admission and validate against CEL
expressions.</td><td><ul><li>Validate critical configurations before resource admission.</li><li>Enforce policy logic using CEL expressions.</li></ul></td></tr></tbody></table><p>In general, use <em>webhook</em> admission control when you want an extensible way to
declare or configure the logic. Use built-in CEL-based admission control when
you want to declare simpler logic without the overhead of running a webhook
server. The Kubernetes project recommends that you use CEL-based admission
control when possible.</p><h3 id="no-crd-validation-defaulting">Use built-in validation and defaulting for CustomResourceDefinitions</h3><p>If you use
<a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinitions</a>,
don't use admission webhooks to validate values in CustomResource specifications
or to set default values for fields. Kubernetes lets you define validation rules
and default field values when you create CustomResourceDefinitions.</p><p>To learn more, see the following resources:</p><ul><li><a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">Validation rules</a></li><li><a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting">Defaulting</a></li></ul><h2 id="performance-latency">Performance and latency</h2><p>This section describes recommendations for improving performance and reducing
latency. In summary, these are as follows:</p><ul><li>Consolidate webhooks and limit the number of API calls per webhook.</li><li>Use audit logs to check for webhooks that repeatedly do the same action.</li><li>Use load balancing for webhook availability.</li><li>Set a small timeout value for each webhook.</li><li>Consider cluster availability needs during webhook design.</li></ul><h3 id="design-admission-webhooks-low-latency">Design admission webhooks for low latency</h3><p>Mutating admission webhooks are called in sequence. Depending on the mutating
webhook setup, some webhooks might be called multiple times. Every mutating
webhook call adds latency to the admission process. This is unlike validating
webhooks, which get called in parallel.</p><p>When designing your mutating webhooks, consider your latency requirements and
tolerance. The more mutating webhooks there are in your cluster, the greater the
chance of latency increases.</p><p>Consider the following to reduce latency:</p><ul><li>Consolidate webhooks that perform a similar mutation on different objects.</li><li>Reduce the number of API calls made in the mutating webhook server logic.</li><li>Limit the match conditions of each mutating webhook to reduce how many
webhooks are triggered by a specific API request.</li><li>Consolidate small webhooks into one server and configuration to help with
ordering and organization.</li></ul><h3 id="prevent-loops-competing-controllers">Prevent loops caused by competing controllers</h3><p>Consider any other components that run in your cluster that might conflict with
the mutations that your webhook makes. For example, if your webhook adds a label
that a different controller removes, your webhook gets called again. This leads
to a loop.</p><p>To detect these loops, try the following:</p><ol><li><p>Update your cluster audit policy to log audit events. Use the following
parameters:</p><ul><li><code>level</code>: <code>RequestResponse</code></li><li><code>verbs</code>: <code>["patch"]</code></li><li><code>omitStages</code>: <code>RequestReceived</code></li></ul><p>Set the audit rule to create events for the specific resources that your
webhook mutates.</p></li><li><p>Check your audit events for webhooks being reinvoked multiple times with the
same patch being applied to the same object, or for an object having
a field updated and reverted multiple times.</p></li></ol><h3 id="small-timeout">Set a small timeout value</h3><p>Admission webhooks should evaluate as quickly as possible (typically in
milliseconds), since they add to API request latency. Use a small timeout for
webhooks.</p><p>For details, see
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#timeouts">Timeouts</a>.</p><h3 id="load-balancer-webhook">Use a load balancer to ensure webhook availability</h3><p>Admission webhooks should leverage some form of load-balancing to provide high
availability and performance benefits. If a webhook is running within the
cluster, you can run multiple webhook backends behind a Service of type
<code>ClusterIP</code>.</p><h3 id="ha-deployment">Use a high-availability deployment model</h3><p>Consider your cluster's availability requirements when designing your webhook.
For example, during node downtime or zonal outages, Kubernetes marks Pods as
<code>NotReady</code> to allow load balancers to reroute traffic to available zones and
nodes. These updates to Pods might trigger your mutating webhooks. Depending on
the number of affected Pods, the mutating webhook server has a risk of timing
out or causing delays in Pod processing. As a result, traffic won't get
rerouted as quickly as you need.</p><p>Consider situations like the preceding example when writing your webhooks.
Exclude operations that are a result of Kubernetes responding to unavoidable
incidents.</p><h2 id="request-filtering">Request filtering</h2><p>This section provides recommendations for filtering which requests trigger
specific webhooks. In summary, these are as follows:</p><ul><li>Limit the webhook scope to avoid system components and read-only requests.</li><li>Limit webhooks to specific namespaces.</li><li>Use match conditions to perform fine-grained request filtering.</li><li>Match all versions of an object.</li></ul><h3 id="webhook-limit-scope">Limit the scope of each webhook</h3><p>Admission webhooks are only called when an API request matches the corresponding
webhook configuration. Limit the scope of each webhook to reduce unnecessary
calls to the webhook server. Consider the following scope limitations:</p><ul><li>Avoid matching objects in the <code>kube-system</code> namespace. If you run your own
Pods in the <code>kube-system</code> namespace, use an
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-objectselector"><code>objectSelector</code></a>
to avoid mutating a critical workload.</li><li>Don't mutate node leases, which exist as Lease objects in the
<code>kube-node-lease</code> system namespace. Mutating node leases might result in
failed node upgrades. Only apply validation controls to Lease objects in this
namespace if you're confident that the controls won't put your cluster at
risk.</li><li>Don't mutate TokenReview or SubjectAccessReview objects. These are always
read-only requests. Modifying these objects might break your cluster.</li><li>Limit each webhook to a specific namespace by using a
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-namespaceselector"><code>namespaceSelector</code></a>.</li></ul><h3 id="filter-match-conditions">Filter for specific requests by using match conditions</h3><p>Admission controllers support multiple fields that you can use to match requests
that meet specific criteria. For example, you can use a <code>namespaceSelector</code> to
filter for requests that target a specific namespace.</p><p>For more fine-grained request filtering, use the <code>matchConditions</code> field in your
webhook configuration. This field lets you write multiple CEL expressions that
must evaluate to <code>true</code> for a request to trigger your admission webhook. Using
<code>matchConditions</code> might significantly reduce the number of calls to your webhook
server.</p><p>For details, see
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-matchconditions">Matching requests: <code>matchConditions</code></a>.</p><h3 id="match-all-versions">Match all versions of an API</h3><p>By default, admission webhooks run on any API versions that affect a specified
resource. The <code>matchPolicy</code> field in the webhook configuration controls this
behavior. Specify a value of <code>Equivalent</code> in the <code>matchPolicy</code> field or omit
the field to allow the webhook to run on any API version.</p><p>For details, see
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-matchpolicy">Matching requests: <code>matchPolicy</code></a>.</p><h2 id="mutation-scope-considerations">Mutation scope and field considerations</h2><p>This section provides recommendations for the scope of mutations and any special
considerations for object fields. In summary, these are as follows:</p><ul><li>Patch only the fields that you need to patch.</li><li>Don't overwrite array values.</li><li>Avoid side effects in mutations when possible.</li><li>Avoid self-mutations.</li><li>Fail open and validate the final state.</li><li>Plan for future field updates in later versions.</li><li>Prevent webhooks from self-triggering.</li><li>Don't change immutable objects.</li></ul><h3 id="patch-required-fields">Patch only required fields</h3><p>Admission webhook servers send HTTP responses to indicate what to do with a
specific Kubernetes API request. This response is an AdmissionReview object.
A mutating webhook can add specific fields to mutate before allowing admission
by using the <code>patchType</code> field and the <code>patch</code> field in the response. Ensure
that you only modify the fields that require a change.</p><p>For example, consider a mutating webhook that's configured to ensure that
<code>web-server</code> Deployments have at least three replicas. When a request to
create a Deployment object matches your webhook configuration, the webhook
should only update the value in the <code>spec.replicas</code> field.</p><h3 id="dont-overwrite-arrays">Don't overwrite array values</h3><p>Fields in Kubernetes object specifications might include arrays. Some arrays
contain key:value pairs (like the <code>envVar</code> field in a container specification),
while other arrays are unkeyed (like the <code>readinessGates</code> field in a Pod
specification). The order of values in an array field might matter in some
situations. For example, the order of arguments in the <code>args</code> field of a
container specification might affect the container.</p><p>Consider the following when modifying arrays:</p><ul><li>Whenever possible, use the <code>add</code> JSONPatch operation instead of <code>replace</code> to
avoid accidentally replacing a required value.</li><li>Treat arrays that don't use key:value pairs as sets.</li><li>Ensure that the values in the field that you modify aren't required to be
in a specific order.</li><li>Don't overwrite existing key:value pairs unless absolutely necessary.</li><li>Use caution when modifying label fields. An accidental modification might
cause label selectors to break, resulting in unintended behavior.</li></ul><h3 id="avoid-side-effects">Avoid side effects</h3><p>Ensure that your webhooks operate only on the content of the AdmissionReview
that's sent to them, and do not make out-of-band changes. These additional
changes, called <em>side effects</em>, might cause conflicts during admission if they
aren't reconciled properly. The <code>.webhooks[].sideEffects</code> field should
be set to <code>None</code> if a webhook doesn't have any side effect.</p><p>If side effects are required during the admission evaluation, they must be
suppressed when processing an AdmissionReview object with <code>dryRun</code> set to
<code>true</code>, and the <code>.webhooks[].sideEffects</code> field should be set to <code>NoneOnDryRun</code>.</p><p>For details, see
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#side-effects">Side effects</a>.</p><h3 id="avoid-self-mutation">Avoid self-mutations</h3><p>A webhook running inside the cluster might cause deadlocks for its own
deployment if it is configured to intercept resources required to start its own
Pods.</p><p>For example, a mutating admission webhook is configured to admit <strong>create</strong> Pod
requests only if a certain label is set in the Pod (such as <code>env: prod</code>).
The webhook server runs in a Deployment that doesn't set the <code>env</code> label.</p><p>When a node that runs the webhook server Pods becomes unhealthy, the webhook
Deployment tries to reschedule the Pods to another node. However, the existing
webhook server rejects the requests since the <code>env</code> label is unset. As a
result, the migration cannot happen.</p><p>Exclude the namespace where your webhook is running with a
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-namespaceselector"><code>namespaceSelector</code></a>.</p><h3 id="avoid-dependency-loops">Avoid dependency loops</h3><p>Dependency loops can occur in scenarios like the following:</p><ul><li>Two webhooks check each other's Pods. If both webhooks become unavailable
at the same time, neither webhook can start.</li><li>Your webhook intercepts cluster add-on components, such as networking plugins
or storage plugins, that your webhook depends on. If both the webhook and the
dependent add-on become unavailable, neither component can function.</li></ul><p>To avoid these dependency loops, try the following:</p><ul><li>Use
<a href="/docs/reference/access-authn-authz/validating-admission-policy/">ValidatingAdmissionPolicies</a>
to avoid introducing dependencies.</li><li>Prevent webhooks from validating or mutating other webhooks. Consider
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-namespaceselector">excluding specific namespaces</a>
from triggering your webhook.</li><li>Prevent your webhooks from acting on dependent add-ons by using an
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-objectselector"><code>objectSelector</code></a>.</li></ul><h3 id="fail-open-validate-final-state">Fail open and validate the final state</h3><p>Mutating admission webhooks support the <code>failurePolicy</code> configuration field.
This field indicates whether the API server should admit or reject the request
if the webhook fails. Webhook failures might occur because of timeouts or errors
in the server logic.</p><p>By default, admission webhooks set the <code>failurePolicy</code> field to Fail. The API
server rejects a request if the webhook fails. However, rejecting requests by
default might result in compliant requests being rejected during webhook
downtime.</p><p>Let your mutating webhooks "fail open" by setting the <code>failurePolicy</code> field to
Ignore. Use a validating controller to check the state of requests to ensure
that they comply with your policies.</p><p>This approach has the following benefits:</p><ul><li>Mutating webhook downtime doesn't affect compliant resources from deploying.</li><li>Policy enforcement occurs during validating admission control.</li><li>Mutating webhooks don't interfere with other controllers in the cluster.</li></ul><h3 id="plan-future-field-updates">Plan for future updates to fields</h3><p>In general, design your webhooks under the assumption that Kubernetes APIs might
change in a later version. Don't write a server that takes the stability of an
API for granted. For example, the release of sidecar containers in Kubernetes
added a <code>restartPolicy</code> field to the Pod API.</p><h3 id="prevent-webhook-self-trigger">Prevent your webhook from triggering itself</h3><p>Mutating webhooks that respond to a broad range of API requests might
unintentionally trigger themselves. For example, consider a webhook that
responds to all requests in the cluster. If you configure the webhook to create
Event objects for every mutation, it'll respond to its own Event object
creation requests.</p><p>To avoid this, consider setting a unique label in any resources that your
webhook creates. Exclude this label from your webhook match conditions.</p><h3 id="dont-change-immutable-objects">Don't change immutable objects</h3><p>Some Kubernetes objects in the API server can't change. For example, when you
deploy a <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static Pod</a>, the
kubelet on the node creates a
<a class="glossary-tooltip" title="An object in the API server that tracks a static pod on a kubelet." href="/docs/reference/glossary/?all=true#term-mirror-pod" target="_blank">mirror Pod</a> in the API
server to track the static Pod. However, changes to the mirror Pod don't
propagate to the static Pod.</p><p>Don't attempt to mutate these objects during admission. All mirror Pods have the
<code>kubernetes.io/config.mirror</code> annotation. To exclude mirror Pods while reducing
the security risk of ignoring an annotation, allow static Pods to only run in
specific namespaces.</p><h2 id="ordering-idempotence">Mutating webhook ordering and idempotence</h2><p>This section provides recommendations for webhook order and designing idempotent
webhooks. In summary, these are as follows:</p><ul><li>Don't rely on a specific order of execution.</li><li>Validate mutations before admission.</li><li>Check for mutations being overwritten by other controllers.</li><li>Ensure that the set of mutating webhooks is idempotent, not just the
individual webhooks.</li></ul><h3 id="dont-rely-webhook-order">Don't rely on mutating webhook invocation order</h3><p>Mutating admission webhooks don't run in a consistent order. Various factors
might change when a specific webhook is called. Don't rely on your webhook
running at a specific point in the admission process. Other webhooks could still
mutate your modified object.</p><p>The following recommendations might help to minimize the risk of unintended
changes:</p><ul><li><a href="#validate-mutations">Validate mutations before admission</a></li><li>Use a reinvocation policy to observe changes to an object by other plugins
and re-run the webhook as needed. For details, see
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#reinvocation-policy">Reinvocation policy</a>.</li></ul><h3 id="ensure-mutating-webhook-idempotent">Ensure that the mutating webhooks in your cluster are idempotent</h3><p>Every mutating admission webhook should be <em>idempotent</em>. The webhook should be
able to run on an object that it already modified without making additional
changes beyond the original change.</p><p>Additionally, all of the mutating webhooks in your cluster should, as a
collection, be idempotent. After the mutation phase of admission control ends,
every individual mutating webhook should be able to run on an object without
making additional changes to the object.</p><p>Depending on your environment, ensuring idempotence at scale might be
challenging. The following recommendations might help:</p><ul><li>Use validating admission controllers to verify the final state of
critical workloads.</li><li>Test your deployments in a staging cluster to see if any objects get modified
multiple times by the same webhook.</li><li>Ensure that the scope of each mutating webhook is specific and limited.</li></ul><p>The following examples show idempotent mutation logic:</p><ol><li><p>For a <strong>create</strong> Pod request, set the field
<code>.spec.securityContext.runAsNonRoot</code> of the Pod to true.</p></li><li><p>For a <strong>create</strong> Pod request, if the field
<code>.spec.containers[].resources.limits</code> of a container is not set, set default
resource limits.</p></li><li><p>For a <strong>create</strong> Pod request, inject a sidecar container with name
<code>foo-sidecar</code> if no container with the name <code>foo-sidecar</code> already exists.</p></li></ol><p>In these cases, the webhook can be safely reinvoked, or admit an object that
already has the fields set.</p><p>The following examples show non-idempotent mutation logic:</p><ol><li><p>For a <strong>create</strong> Pod request, inject a sidecar container with name
<code>foo-sidecar</code> suffixed with the current timestamp (such as
<code>foo-sidecar-19700101-000000</code>).</p><p>Reinvoking the webhook can result in the same sidecar being injected multiple
times to a Pod, each time with a different container name. Similarly, the
webhook can inject duplicated containers if the sidecar already exists in
a user-provided pod.</p></li><li><p>For a <strong>create</strong>/<strong>update</strong> Pod request, reject if the Pod has label <code>env</code>
set, otherwise add an <code>env: prod</code> label to the Pod.</p><p>Reinvoking the webhook will result in the webhook failing on its own output.</p></li><li><p>For a <strong>create</strong> Pod request, append a sidecar container named <code>foo-sidecar</code>
without checking whether a <code>foo-sidecar</code> container exists.</p><p>Reinvoking the webhook will result in duplicated containers in the Pod, which
makes the request invalid and rejected by the API server.</p></li></ol><h2 id="mutation-testing-validation">Mutation testing and validation</h2><p>This section provides recommendations for testing your mutating webhooks and
validating mutated objects. In summary, these are as follows:</p><ul><li>Test webhooks in staging environments.</li><li>Avoid mutations that violate validations.</li><li>Test minor version upgrades for regressions and conflicts.</li><li>Validate mutated objects before admission.</li></ul><h3 id="test-in-staging-environments">Test webhooks in staging environments</h3><p>Robust testing should be a core part of your release cycle for new or updated
webhooks. If possible, test any changes to your cluster webhooks in a staging
environment that closely resembles your production clusters. At the very least,
consider using a tool like <a href="https://minikube.sigs.k8s.io/docs/">minikube</a> or
<a href="https://kind.sigs.k8s.io/">kind</a> to create a small test cluster for webhook
changes.</p><h3 id="ensure-mutations-dont-violate-validations">Ensure that mutations don't violate validations</h3><p>Your mutating webhooks shouldn't break any of the validations that apply to an
object before admission. For example, consider a mutating webhook that sets the
default CPU request of a Pod to a specific value. If the CPU limit of that Pod
is set to a lower value than the mutated request, the Pod fails admission.</p><p>Test every mutating webhook against the validations that run in your cluster.</p><h3 id="test-minor-version-upgrades">Test minor version upgrades to ensure consistent behavior</h3><p>Before upgrading your production clusters to a new minor version, test your
webhooks and workloads in a staging environment. Compare the results to ensure
that your webhooks continue to function as expected after the upgrade.</p><p>Additionally, use the following resources to stay informed about API changes:</p><ul><li><a href="/releases/">Kubernetes release notes</a></li><li><a href="/blog/">Kubernetes blog</a></li></ul><h3 id="validate-mutations">Validate mutations before admission</h3><p>Mutating webhooks run to completion before any validating webhooks run. There is
no stable order in which mutations are applied to objects. As a result, your
mutations could get overwritten by a mutating webhook that runs at a later time.</p><p>Add a validating admission controller like a ValidatingAdmissionWebhook or a
ValidatingAdmissionPolicy to your cluster to ensure that your mutations
are still present. For example, consider a mutating webhook that inserts the
<code>restartPolicy: Always</code> field to specific init containers to make them run as
sidecar containers. You could run a validating webhook to ensure that those
init containers retained the <code>restartPolicy: Always</code> configuration after all
mutations were completed.</p><p>For details, see the following resources:</p><ul><li><a href="/docs/reference/access-authn-authz/validating-admission-policy/">Validating Admission Policy</a></li><li><a href="/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook">ValidatingAdmissionWebhooks</a></li></ul><h2 id="mutating-webhook-deployment">Mutating webhook deployment</h2><p>This section provides recommendations for deploying your mutating admission
webhooks. In summary, these are as follows:</p><ul><li>Gradually roll out the webhook configuration and monitor for issues by
namespace.</li><li>Limit access to edit the webhook configuration resources.</li><li>Limit access to the namespace that runs the webhook server, if the server is
in-cluster.</li></ul><h3 id="install-enable-mutating-webhook">Install and enable a mutating webhook</h3><p>When you're ready to deploy your mutating webhook to a cluster, use the
following order of operations:</p><ol><li>Install the webhook server and start it.</li><li>Set the <code>failurePolicy</code> field in the MutatingWebhookConfiguration manifest
to Ignore. This lets you avoid disruptions caused by misconfigured webhooks.</li><li>Set the <code>namespaceSelector</code> field in the MutatingWebhookConfiguration
manifest to a test namespace.</li><li>Deploy the MutatingWebhookConfiguration to your cluster.</li></ol><p>Monitor the webhook in the test namespace to check for any issues, then roll the
webhook out to other namespaces. If the webhook intercepts an API request that
it wasn't meant to intercept, pause the rollout and adjust the scope of the
webhook configuration.</p><h3 id="limit-edit-access">Limit edit access to mutating webhooks</h3><p>Mutating webhooks are powerful Kubernetes controllers. Use RBAC or another
authorization mechanism to limit access to your webhook configurations and
servers. For RBAC, ensure that the following access is only available to trusted
entities:</p><ul><li>Verbs: <strong>create</strong>, <strong>update</strong>, <strong>patch</strong>, <strong>delete</strong>, <strong>deletecollection</strong></li><li>API group: <code>admissionregistration.k8s.io/v1</code></li><li>API kind: MutatingWebhookConfigurations</li></ul><p>If your mutating webhook server runs in the cluster, limit access to create or
modify any resources in that namespace.</p><h2 id="example-good-implementations">Examples of good implementations</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>The following projects are examples of "good" custom webhook server
implementations. You can use them as a starting point when designing your own
webhooks. Don't use these examples as-is; use them as a starting point and
design your webhooks to run well in your specific environment.</p><ul><li><a href="https://github.com/cert-manager/cert-manager/tree/master/internal/webhook"><code>cert-manager</code></a></li><li><a href="https://open-policy-agent.github.io/gatekeeper/website/docs/mutation">Gatekeeper Open Policy Agent (OPA)</a></li></ul><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/reference/access-authn-authz/webhook/">Use webhooks for authentication and authorization</a></li><li><a href="/docs/reference/access-authn-authz/mutating-admission-policy/">Learn about MutatingAdmissionPolicies</a></li><li><a href="/docs/reference/access-authn-authz/validating-admission-policy/">Learn about ValidatingAdmissionPolicies</a></li></ul></div></div><div><div class="td-content"><h1>Good practices for Dynamic Resource Allocation as a Cluster Admin</h1><p>This page describes good practices when configuring a Kubernetes cluster
utilizing Dynamic Resource Allocation (DRA). These instructions are for cluster
administrators.</p><h2 id="separate-permissions-to-dra-related-apis">Separate permissions to DRA related APIs</h2><p>DRA is orchestrated through a number of different APIs. Use authorization tools
(like RBAC, or another solution) to control access to the right APIs depending
on the persona of your user.</p><p>In general, DeviceClasses and ResourceSlices should be restricted to admins and
the DRA drivers. Cluster operators that will be deploying Pods with claims will
need access to ResourceClaim and ResourceClaimTemplate APIs; both of these APIs
are namespace scoped.</p><h2 id="dra-driver-deployment-and-maintenance">DRA driver deployment and maintenance</h2><p>DRA drivers are third-party applications that run on each node of your cluster
to interface with the hardware of that node and Kubernetes' native DRA
components. The installation procedure depends on the driver you choose, but is
likely deployed as a DaemonSet to all or a selection of the nodes (using node
selectors or similar mechanisms) in your cluster.</p><h3 id="use-drivers-with-seamless-upgrade-if-available">Use drivers with seamless upgrade if available</h3><p>DRA drivers implement the <a href="https://pkg.go.dev/k8s.io/dynamic-resource-allocation/kubeletplugin"><code>kubeletplugin</code> package
interface</a>.
Your driver may support <em>seamless upgrades</em> by implementing a property of this
interface that allows two versions of the same DRA driver to coexist for a short
time. This is only available for kubelet versions 1.33 and above and may not be
supported by your driver for heterogeneous clusters with attached nodes running
older versions of Kubernetes - check your driver's documentation to be sure.</p><p>If seamless upgrades are available for your situation, consider using it to
minimize scheduling delays when your driver updates.</p><p>If you cannot use seamless upgrades, during driver downtime for upgrades you may
observe that:</p><ul><li>Pods cannot start unless the claims they depend on were already prepared for
use.</li><li>Cleanup after the last pod which used a claim gets delayed until the driver is
available again. The pod is not marked as terminated. This prevents reusing
the resources used by the pod for other pods.</li><li>Running pods will continue to run.</li></ul><h3 id="confirm-your-dra-driver-exposes-a-liveness-probe-and-utilize-it">Confirm your DRA driver exposes a liveness probe and utilize it</h3><p>Your DRA driver likely implements a gRPC socket for healthchecks as part of DRA
driver good practices. The easiest way to utilize this grpc socket is to
configure it as a liveness probe for the DaemonSet deploying your DRA driver.
Your driver's documentation or deployment tooling may already include this, but
if you are building your configuration separately or not running your DRA driver
as a Kubernetes pod, be sure that your orchestration tooling restarts the DRA
driver on failed healthchecks to this grpc socket. Doing so will minimize any
accidental downtime of the DRA driver and give it more opportunities to self
heal, reducing scheduling delays or troubleshooting time.</p><h3 id="when-draining-a-node-drain-the-dra-driver-as-late-as-possible">When draining a node, drain the DRA driver as late as possible</h3><p>The DRA driver is responsible for unpreparing any devices that were allocated to
Pods, and if the DRA driver is <a class="glossary-tooltip" title="Safely evicts Pods from a Node to prepare for maintenance or removal." href="/docs/reference/glossary/?all=true#term-drain" target="_blank">drained</a> before Pods with claims have been deleted, it will not be
able to finalize its cleanup. If you implement custom drain logic for nodes,
consider checking that there are no allocated/reserved ResourceClaim or
ResourceClaimTemplates before terminating the DRA driver itself.</p><h2 id="monitor-and-tune-components-for-higher-load-especially-in-high-scale-environments">Monitor and tune components for higher load, especially in high scale environments</h2><p>Control plane component <a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">kube-scheduler</a> and the internal ResourceClaim controller
orchestrated by the component <a class="glossary-tooltip" title="Control Plane component that runs controller processes." href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank">kube-controller-manager</a> do the
heavy lifting during scheduling of Pods with claims based on metadata stored in
the DRA APIs. Compared to non-DRA scheduled Pods, the number of API server
calls, memory, and CPU utilization needed by these components is increased for
Pods using DRA claims. In addition, node local components like the DRA driver
and kubelet utilize DRA APIs to allocated the hardware request at Pod sandbox
creation time. Especially in high scale environments where clusters have many
nodes, and/or deploy many workloads that heavily utilize DRA defined resource
claims, the cluster administrator should configure the relevant components to
anticipate the increased load.</p><p>The effects of mistuned components can have direct or snowballing affects
causing different symptoms during the Pod lifecycle. If the <code>kube-scheduler</code>
component's QPS and burst configurations are too low, the scheduler might
quickly identify a suitable node for a Pod but take longer to bind the Pod to
that node. With DRA, during Pod scheduling, the QPS and Burst parameters in the
client-go configuration within <code>kube-controller-manager</code> are critical.</p><p>The specific values to tune your cluster to depend on a variety of factors like
number of nodes/pods, rate of pod creation, churn, even in non-DRA environments;
see the <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md">SIG Scalability README on Kubernetes scalability
thresholds</a>
for more information. In scale tests performed against a DRA enabled cluster
with 100 nodes, involving 720 long-lived pods (90% saturation) and 80 churn pods
(10% churn, 10 times), with a job creation QPS of 10, <code>kube-controller-manager</code>
QPS could be set to as low as 75 and Burst to 150 to meet equivalent metric
targets for non-DRA deployments. At this lower bound, it was observed that the
client side rate limiter was triggered enough to protect the API server from
explosive burst but was high enough that pod startup SLOs were not impacted.
While this is a good starting point, you can get a better idea of how to tune
the different components that have the biggest effect on DRA performance for
your deployment by monitoring the following metrics. For more information on all
the stable metrics in Kubernetes, see the <a href="/docs/reference/generated/metrics/">Kubernetes Metrics
Reference</a>.</p><h3 id="kube-controller-manager-metrics"><code>kube-controller-manager</code> metrics</h3><p>The following metrics look closely at the internal ResourceClaim controller
managed by the <code>kube-controller-manager</code> component.</p><ul><li>Workqueue Add Rate: Monitor <code class="code-inline language-promql"><span></span><span>sum</span><span>(</span><span>rate</span><span>(</span><span>workqueue_adds_total</span>{<span>name</span><span>=</span>"<span>resource_claim</span>"}[<span>5m</span>]<span>))</span><span> </span></code>to gauge how quickly items are added to the ResourceClaim controller.</li><li>Workqueue Depth: Track
<code class="code-inline language-promql"><span>sum</span><span>(</span><span>workqueue_depth</span>{<span>endpoint</span><span>=</span>"<span>kube-controller-manager</span>",<span>
</span><span></span><span>name</span><span>=</span>"<span>resource_claim</span>"}<span>)</span></code> to identify any backlogs in the ResourceClaim
controller.</li><li>Workqueue Work Duration: Observe <code class="code-inline language-promql"><span>histogram_quantile</span><span>(</span><span>0.99</span>,<span>
</span><span></span><span>sum</span><span>(</span><span>rate</span><span>(</span><span>workqueue_work_duration_seconds_bucket</span>{<span>name</span><span>=</span>"<span>resource_claim</span>"}[<span>5m</span>]<span>))</span><span>
</span><span></span><span>by</span><span> </span><span>(</span><span>le</span><span>))</span></code> to understand the speed at which the ResourceClaim controller
processes work.</li></ul><p>If you are experiencing low Workqueue Add Rate, high Workqueue Depth, and/or
high Workqueue Work Duration, this suggests the controller isn't performing
optimally. Consider tuning parameters like QPS, burst, and CPU/memory
configurations.</p><p>If you are experiencing high Workequeue Add Rate, high Workqueue Depth, but
reasonable Workqueue Work Duration, this indicates the controller is processing
work, but concurrency might be insufficient. Concurrency is hardcoded in the
controller, so as a cluster administrator, you can tune for this by reducing the
pod creation QPS, so the add rate to the resource claim workqueue is more
manageable.</p><h3 id="kube-scheduler-metrics"><code>kube-scheduler</code> metrics</h3><p>The following scheduler metrics are high level metrics aggregating performance
across all Pods scheduled, not just those using DRA. It is important to note
that the end-to-end metrics are ultimately influenced by the
<code>kube-controller-manager</code>'s performance in creating ResourceClaims from
ResourceClainTemplates in deployments that heavily use ResourceClainTemplates.</p><ul><li>Scheduler End-to-End Duration: Monitor <code class="code-inline language-promql"><span>histogram_quantile</span><span>(</span><span>0.99</span>,<span>
</span><span></span><span>sum</span><span>(</span><span>increase</span><span>(</span><span>scheduler_pod_scheduling_sli_duration_seconds_bucket</span>[<span>5m</span>]<span>))</span><span> </span><span>by</span><span>
</span><span></span><span>(</span><span>le</span><span>))</span></code>.</li><li>Scheduler Algorithm Latency: Track <code class="code-inline language-promql"><span>histogram_quantile</span><span>(</span><span>0.99</span>,<span>
</span><span></span><span>sum</span><span>(</span><span>increase</span><span>(</span><span>scheduler_scheduling_algorithm_duration_seconds_bucket</span>[<span>5m</span>]<span>))</span><span> </span><span>by</span><span>
</span><span></span><span>(</span><span>le</span><span>))</span></code>.</li></ul><h3 id="kubelet-metrics"><code>kubelet</code> metrics</h3><p>When a Pod bound to a node must have a ResourceClaim satisfied, kubelet calls
the <code>NodePrepareResources</code> and <code>NodeUnprepareResources</code> methods of the DRA
driver. You can observe this behavior from the kubelet's point of view with the
following metrics.</p><ul><li>Kubelet NodePrepareResources: Monitor <code class="code-inline language-promql"><span>histogram_quantile</span><span>(</span><span>0.99</span>,<span>
</span><span></span><span>sum</span><span>(</span><span>rate</span><span>(</span><span>dra_operations_duration_seconds_bucket</span>{<span>operation_name</span><span>=</span>"<span>PrepareResources</span>"}[<span>5m</span>]<span>))</span><span>
</span><span></span><span>by</span><span> </span><span>(</span><span>le</span><span>))</span></code>.</li><li>Kubelet NodeUnprepareResources: Track <code class="code-inline language-promql"><span>histogram_quantile</span><span>(</span><span>0.99</span>,<span>
</span><span></span><span>sum</span><span>(</span><span>rate</span><span>(</span><span>dra_operations_duration_seconds_bucket</span>{<span>operation_name</span><span>=</span>"<span>UnprepareResources</span>"}[<span>5m</span>]<span>))</span><span>
</span><span></span><span>by</span><span> </span><span>(</span><span>le</span><span>))</span></code>.</li></ul><h3 id="dra-kubeletplugin-operations">DRA kubeletplugin operations</h3><p>DRA drivers implement the <a href="https://pkg.go.dev/k8s.io/dynamic-resource-allocation/kubeletplugin"><code>kubeletplugin</code> package
interface</a>
which surfaces its own metric for the underlying gRPC operation
<code>NodePrepareResources</code> and <code>NodeUnprepareResources</code>. You can observe this
behavior from the point of view of the internal kubeletplugin with the following
metrics.</p><ul><li>DRA kubeletplugin gRPC NodePrepareResources operation: Observe <code class="code-inline language-promql"><span>histogram_quantile</span><span>(</span><span>0.99</span>,<span>
</span><span></span><span>sum</span><span>(</span><span>rate</span><span>(</span><span>dra_grpc_operations_duration_seconds_bucket</span>{<span>method_name</span><span>=~</span>"<span>.*NodePrepareResources</span>"}[<span>5m</span>]<span>))</span><span>
</span><span></span><span>by</span><span> </span><span>(</span><span>le</span><span>))</span></code>.</li><li>DRA kubeletplugin gRPC NodeUnprepareResources operation: Observe <code class="code-inline language-promql"><span></span><span>histogram_quantile</span><span>(</span><span>0.99</span>,<span>
</span><span></span><span>sum</span><span>(</span><span>rate</span><span>(</span><span>dra_grpc_operations_duration_seconds_bucket</span>{<span>method_name</span><span>=~</span>"<span>.*NodeUnprepareResources</span>"}[<span>5m</span>]<span>))</span><span>
</span><span></span><span>by</span><span> </span><span>(</span><span>le</span><span>))</span></code>.</li></ul><h2 id="what-s-next">What's next</h2><ul><li><a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">Learn more about
DRA</a></li><li>Read the <a href="/docs/reference/generated/metrics/">Kubernetes Metrics
Reference</a></li></ul></div></div><div><div class="td-content"><h1>Logging Architecture</h1><p>Application logs can help you understand what is happening inside your application. The
logs are particularly useful for debugging problems and monitoring cluster activity. Most
modern applications have some kind of logging mechanism. Likewise, container engines
are designed to support logging. The easiest and most adopted logging method for
containerized applications is writing to standard output and standard error streams.</p><p>However, the native functionality provided by a container engine or runtime is usually
not enough for a complete logging solution.</p><p>For example, you may want to access your application's logs if a container crashes,
a pod gets evicted, or a node dies.</p><p>In a cluster, logs should have a separate storage and lifecycle independent of nodes,
pods, or containers. This concept is called
<a href="#cluster-level-logging-architectures">cluster-level logging</a>.</p><p>Cluster-level logging architectures require a separate backend to store, analyze, and
query logs. Kubernetes does not provide a native storage solution for log data. Instead,
there are many logging solutions that integrate with Kubernetes. The following sections
describe how to handle and store logs on nodes.</p><h2 id="basic-logging-in-kubernetes">Pod and container logs</h2><p>Kubernetes captures logs from each container in a running Pod.</p><p>This example uses a manifest for a <code>Pod</code> with a container
that writes text to the standard output stream, once per second.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/debug/counter-pod.yaml"><code>debug/counter-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy debug/counter-pod.yaml to clipboard"></div><div class="includecode" id="debug-counter-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>counter<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>count<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[/bin/sh, -c,<span>
</span></span></span><span><span><span>            </span><span>'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done'</span>]<span>
</span></span></span></code></pre></div></div></div><p>To run this pod, use the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml
</span></span></code></pre></div><p>The output is:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>pod/counter created
</span></span></span></code></pre></div><p>To fetch the logs, use the <code>kubectl logs</code> command, as follows:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs counter
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>0: Fri Apr  1 11:42:23 UTC 2022
</span></span></span><span><span><span>1: Fri Apr  1 11:42:24 UTC 2022
</span></span></span><span><span><span>2: Fri Apr  1 11:42:25 UTC 2022
</span></span></span></code></pre></div><p>You can use <code>kubectl logs --previous</code> to retrieve logs from a previous instantiation of a container.
If your pod has multiple containers, specify which container's logs you want to access by
appending a container name to the command, with a <code>-c</code> flag, like so:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs counter -c count
</span></span></code></pre></div><h3 id="container-log-streams">Container log streams</h3><div class="feature-state-notice feature-alpha" title="Feature Gate: PodLogsQuerySplitStreams"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [alpha]</code> (enabled by default: false)</div><p>As an alpha feature, the kubelet can split out the logs from the two standard streams produced
by a container: <a href="https://en.wikipedia.org/wiki/Standard_streams#Standard_output_(stdout)">standard output</a>
and <a href="https://en.wikipedia.org/wiki/Standard_streams#Standard_error_(stderr)">standard error</a>.
To use this behavior, you must enable the <code>PodLogsQuerySplitStreams</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>.
With that feature gate enabled, Kubernetes 1.34 allows access to these
log streams directly via the Pod API. You can fetch a specific stream by specifying the stream name (either <code>Stdout</code> or <code>Stderr</code>),
using the <code>stream</code> query string. You must have access to read the <code>log</code> subresource of that Pod.</p><p>To demonstrate this feature, you can create a Pod that periodically writes text to both the standard output and error stream.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/debug/counter-pod-err.yaml"><code>debug/counter-pod-err.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy debug/counter-pod-err.yaml to clipboard"></div><div class="includecode" id="debug-counter-pod-err-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>counter-err<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>count<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[/bin/sh, -c,<span>
</span></span></span><span><span><span>            </span><span>'i=0; while true; do echo "$i: $(date)"; echo "$i: err" &gt;&amp;2 ; i=$((i+1)); sleep 1; done'</span>]<span>
</span></span></span></code></pre></div></div></div><p>To run this pod, use the following command:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl apply -f https://k8s.io/examples/debug/counter-pod-err.yaml
</span></span></code></pre></div><p>To fetch only the stderr log stream, you can run:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get --raw <span>"/api/v1/namespaces/default/pods/counter-err/log?stream=Stderr"</span>
</span></span></code></pre></div><p>See the <a href="/docs/reference/generated/kubectl/kubectl-commands#logs"><code>kubectl logs</code> documentation</a>
for more details.</p><h3 id="how-nodes-handle-container-logs">How nodes handle container logs</h3><p><img alt="Node level logging" src="/images/docs/user-guide/logging/logging-node-level.png"></p><p>A container runtime handles and redirects any output generated to a containerized
application's <code>stdout</code> and <code>stderr</code> streams.
Different container runtimes implement this in different ways; however, the integration
with the kubelet is standardized as the <em>CRI logging format</em>.</p><p>By default, if a container restarts, the kubelet keeps one terminated container with its logs.
If a pod is evicted from the node, all corresponding containers are also evicted, along with their logs.</p><p>The kubelet makes logs available to clients via a special feature of the Kubernetes API.
The usual way to access this is by running <code>kubectl logs</code>.</p><h3 id="log-rotation">Log rotation</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [stable]</code></div><p>The kubelet is responsible for rotating container logs and managing the
logging directory structure.
The kubelet sends this information to the container runtime (using CRI),
and the runtime writes the container logs to the given location.</p><p>You can configure two kubelet <a href="/docs/reference/config-api/kubelet-config.v1beta1/">configuration settings</a>,
<code>containerLogMaxSize</code> (default 10Mi) and <code>containerLogMaxFiles</code> (default 5),
using the <a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a>.
These settings let you configure the maximum size for each log file and the maximum number of
files allowed for each container respectively.</p><p>In order to perform an efficient log rotation in clusters where the volume of the logs generated by
the workload is large, kubelet also provides a mechanism to tune how the logs are rotated in
terms of how many concurrent log rotations can be performed and the interval at which the logs are
monitored and rotated as required.
You can configure two kubelet <a href="/docs/reference/config-api/kubelet-config.v1beta1/">configuration settings</a>,
<code>containerLogMaxWorkers</code> and <code>containerLogMonitorInterval</code> using the
<a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet configuration file</a>.</p><p>When you run <a href="/docs/reference/generated/kubectl/kubectl-commands#logs"><code>kubectl logs</code></a> as in
the basic logging example, the kubelet on the node handles the request and
reads directly from the log file. The kubelet returns the content of the log file.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Only the contents of the latest log file are available through <code>kubectl logs</code>.</p><p>For example, if a Pod writes 40 MiB of logs and the kubelet rotates logs
after 10 MiB, running <code>kubectl logs</code> returns at most 10MiB of data.</p></div><h2 id="system-component-logs">System component logs</h2><p>There are two types of system components: those that typically run in a container,
and those components directly involved in running containers. For example:</p><ul><li>The kubelet and container runtime do not run in containers. The kubelet runs
your containers (grouped together in <a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">pods</a>)</li><li>The Kubernetes scheduler, controller manager, and API server run within pods
(usually <a class="glossary-tooltip" title="A pod managed directly by the kubelet daemon on a specific node." href="/docs/tasks/configure-pod-container/static-pod/" target="_blank">static Pods</a>).
The etcd component runs in the control plane, and most commonly also as a static pod.
If your cluster uses kube-proxy, you typically run this as a <code>DaemonSet</code>.</li></ul><h3 id="log-location-node">Log locations</h3><p>The way that the kubelet and container runtime write logs depends on the operating
system that the node uses:</p><ul class="nav nav-tabs" id="log-location-node-tabs"><li class="nav-item"><a class="nav-link active" href="#log-location-node-tabs-0">Linux</a></li><li class="nav-item"><a class="nav-link" href="#log-location-node-tabs-1">Windows</a></li></ul><div class="tab-content" id="log-location-node-tabs"><div id="log-location-node-tabs-0" class="tab-pane show active"><p><p>On Linux nodes that use systemd, the kubelet and container runtime write to journald
by default. You use <code>journalctl</code> to read the systemd journal; for example:
<code>journalctl -u kubelet</code>.</p><p>If systemd is not present, the kubelet and container runtime write to <code>.log</code> files in the
<code>/var/log</code> directory. If you want to have logs written elsewhere, you can indirectly
run the kubelet via a helper tool, <code>kube-log-runner</code>, and use that tool to redirect
kubelet logs to a directory that you choose.</p><p>By default, kubelet directs your container runtime to write logs into directories within
<code>/var/log/pods</code>.</p><p>For more information on <code>kube-log-runner</code>, read <a href="/docs/concepts/cluster-administration/system-logs/#klog">System Logs</a>.</p></p></div><div id="log-location-node-tabs-1" class="tab-pane"><p><p>By default, the kubelet writes logs to files within the directory <code>C:\var\logs</code>
(notice that this is not <code>C:\var\log</code>).</p><p>Although <code>C:\var\log</code> is the Kubernetes default location for these logs, several
cluster deployment tools set up Windows nodes to log to <code>C:\var\log\kubelet</code> instead.</p><p>If you want to have logs written elsewhere, you can indirectly
run the kubelet via a helper tool, <code>kube-log-runner</code>, and use that tool to redirect
kubelet logs to a directory that you choose.</p><p>However, by default, kubelet directs your container runtime to write logs within the
directory <code>C:\var\log\pods</code>.</p><p>For more information on <code>kube-log-runner</code>, read <a href="/docs/concepts/cluster-administration/system-logs/#klog">System Logs</a>.</p></p></div></div><p><br></p><p>For Kubernetes cluster components that run in pods, these write to files inside
the <code>/var/log</code> directory, bypassing the default logging mechanism (the components
do not write to the systemd journal). You can use Kubernetes' storage mechanisms
to map persistent storage into the container that runs the component.</p><p>Kubelet allows changing the pod logs directory from default <code>/var/log/pods</code>
to a custom path. This adjustment can be made by configuring the <code>podLogsDir</code>
parameter in the kubelet's configuration file.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4><p>It's important to note that the default location <code>/var/log/pods</code> has been in use for
an extended period and certain processes might implicitly assume this path.
Therefore, altering this parameter must be approached with caution and at your own risk.</p><p>Another caveat to keep in mind is that the kubelet supports the location being on the same
disk as <code>/var</code>. Otherwise, if the logs are on a separate filesystem from <code>/var</code>,
then the kubelet will not track that filesystem's usage, potentially leading to issues if
it fills up.</p></div><p>For details about etcd and its logs, view the <a href="https://etcd.io/docs/">etcd documentation</a>.
Again, you can use Kubernetes' storage mechanisms to map persistent storage into
the container that runs the component.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>If you deploy Kubernetes cluster components (such as the scheduler) to log to
a volume shared from the parent node, you need to consider and ensure that those
logs are rotated. <strong>Kubernetes does not manage that log rotation</strong>.</p><p>Your operating system may automatically implement some log rotation - for example,
if you share the directory <code>/var/log</code> into a static Pod for a component, node-level
log rotation treats a file in that directory the same as a file written by any component
outside Kubernetes.</p><p>Some deploy tools account for that log rotation and automate it; others leave this
as your responsibility.</p></div><h2 id="cluster-level-logging-architectures">Cluster-level logging architectures</h2><p>While Kubernetes does not provide a native solution for cluster-level logging, there are
several common approaches you can consider. Here are some options:</p><ul><li>Use a node-level logging agent that runs on every node.</li><li>Include a dedicated sidecar container for logging in an application pod.</li><li>Push logs directly to a backend from within an application.</li></ul><h3 id="using-a-node-logging-agent">Using a node logging agent</h3><p><img alt="Using a node level logging agent" src="/images/docs/user-guide/logging/logging-with-node-agent.png"></p><p>You can implement cluster-level logging by including a <em>node-level logging agent</em> on each node.
The logging agent is a dedicated tool that exposes logs or pushes logs to a backend.
Commonly, the logging agent is a container that has access to a directory with log files from all of the
application containers on that node.</p><p>Because the logging agent must run on every node, it is recommended to run the agent
as a <code>DaemonSet</code>.</p><p>Node-level logging creates only one agent per node and doesn't require any changes to the
applications running on the node.</p><p>Containers write to stdout and stderr, but with no agreed format. A node-level agent collects
these logs and forwards them for aggregation.</p><h3 id="sidecar-container-with-logging-agent">Using a sidecar container with the logging agent</h3><p>You can use a sidecar container in one of the following ways:</p><ul><li>The sidecar container streams application logs to its own <code>stdout</code>.</li><li>The sidecar container runs a logging agent, which is configured to pick up logs
from an application container.</li></ul><h4 id="streaming-sidecar-container">Streaming sidecar container</h4><p><img alt="Sidecar container with a streaming container" src="/images/docs/user-guide/logging/logging-with-streaming-sidecar.png"></p><p>By having your sidecar containers write to their own <code>stdout</code> and <code>stderr</code>
streams, you can take advantage of the kubelet and the logging agent that
already run on each node. The sidecar containers read logs from a file, a socket,
or journald. Each sidecar container prints a log to its own <code>stdout</code> or <code>stderr</code> stream.</p><p>This approach allows you to separate several log streams from different
parts of your application, some of which can lack support
for writing to <code>stdout</code> or <code>stderr</code>. The logic behind redirecting logs
is minimal, so it's not a significant overhead. Additionally, because
<code>stdout</code> and <code>stderr</code> are handled by the kubelet, you can use built-in tools
like <code>kubectl logs</code>.</p><p>For example, a pod runs a single container, and the container
writes to two different log files using two different formats. Here's a
manifest for the Pod:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod.yaml"><code>admin/logging/two-files-counter-pod.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/logging/two-files-counter-pod.yaml to clipboard"></div><div class="includecode" id="admin-logging-two-files-counter-pod-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>counter<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>count<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span>
</span></span></span><span><span><span>    </span>- /bin/sh<span>
</span></span></span><span><span><span>    </span>- -c<span>
</span></span></span><span><span><span>    </span>- &gt;<span>
</span></span></span><span><span><span>      i=0;
</span></span></span><span><span><span>      while true;
</span></span></span><span><span><span>      do
</span></span></span><span><span><span>        echo "$i: $(date)" &gt;&gt; /var/log/1.log;
</span></span></span><span><span><span>        echo "$(date) INFO $i" &gt;&gt; /var/log/2.log;
</span></span></span><span><span><span>        i=$((i+1));
</span></span></span><span><span><span>        sleep 1;
</span></span></span><span><span><span>      done</span><span>      
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span> </span>{}<span>
</span></span></span></code></pre></div></div></div><p>It is not recommended to write log entries with different formats to the same log
stream, even if you managed to redirect both components to the <code>stdout</code> stream of
the container. Instead, you can create two sidecar containers. Each sidecar
container could tail a particular log file from a shared volume and then redirect
the logs to its own <code>stdout</code> stream.</p><p>Here's a manifest for a pod that has two sidecar containers:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml"><code>admin/logging/two-files-counter-pod-streaming-sidecar.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/logging/two-files-counter-pod-streaming-sidecar.yaml to clipboard"></div><div class="includecode" id="admin-logging-two-files-counter-pod-streaming-sidecar-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>counter<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>count<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span>
</span></span></span><span><span><span>    </span>- /bin/sh<span>
</span></span></span><span><span><span>    </span>- -c<span>
</span></span></span><span><span><span>    </span>- &gt;<span>
</span></span></span><span><span><span>      i=0;
</span></span></span><span><span><span>      while true;
</span></span></span><span><span><span>      do
</span></span></span><span><span><span>        echo "$i: $(date)" &gt;&gt; /var/log/1.log;
</span></span></span><span><span><span>        echo "$(date) INFO $i" &gt;&gt; /var/log/2.log;
</span></span></span><span><span><span>        i=$((i+1));
</span></span></span><span><span><span>        sleep 1;
</span></span></span><span><span><span>      done</span><span>      
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>count-log-1<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[/bin/sh, -c, 'tail -n+1 -F /var/log/1.log']<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>count-log-2<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span> </span>[/bin/sh, -c, 'tail -n+1 -F /var/log/2.log']<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span> </span>{}<span>
</span></span></span></code></pre></div></div></div><p>Now when you run this pod, you can access each log stream separately by
running the following commands:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs counter count-log-1
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>0: Fri Apr  1 11:42:26 UTC 2022
</span></span></span><span><span><span>1: Fri Apr  1 11:42:27 UTC 2022
</span></span></span><span><span><span>2: Fri Apr  1 11:42:28 UTC 2022
</span></span></span><span><span><span>...
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl logs counter count-log-2
</span></span></code></pre></div><p>The output is similar to:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>Fri Apr  1 11:42:29 UTC 2022 INFO 0
</span></span></span><span><span><span>Fri Apr  1 11:42:30 UTC 2022 INFO 0
</span></span></span><span><span><span>Fri Apr  1 11:42:31 UTC 2022 INFO 0
</span></span></span><span><span><span>...
</span></span></span></code></pre></div><p>If you installed a node-level agent in your cluster, that agent picks up those log
streams automatically without any further configuration. If you like, you can configure
the agent to parse log lines depending on the source container.</p><p>Even for Pods that only have low CPU and memory usage (order of a couple of millicores
for cpu and order of several megabytes for memory), writing logs to a file and
then streaming them to <code>stdout</code> can double how much storage you need on the node.
If you have an application that writes to a single file, it's recommended to set
<code>/dev/stdout</code> as the destination rather than implement the streaming sidecar
container approach.</p><p>Sidecar containers can also be used to rotate log files that cannot be rotated by
the application itself. An example of this approach is a small container running
<code>logrotate</code> periodically.
However, it's more straightforward to use <code>stdout</code> and <code>stderr</code> directly, and
leave rotation and retention policies to the kubelet.</p><h4 id="sidecar-container-with-a-logging-agent">Sidecar container with a logging agent</h4><p><img alt="Sidecar container with a logging agent" src="/images/docs/user-guide/logging/logging-with-sidecar-agent.png"></p><p>If the node-level logging agent is not flexible enough for your situation, you
can create a sidecar container with a separate logging agent that you have
configured specifically to run with your application.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Using a logging agent in a sidecar container can lead
to significant resource consumption. Moreover, you won't be able to access
those logs using <code>kubectl logs</code> because they are not controlled
by the kubelet.</div><p>Here are two example manifests that you can use to implement a sidecar container with a logging agent.
The first manifest contains a <a href="/docs/tasks/configure-pod-container/configure-pod-configmap/"><code>ConfigMap</code></a>
to configure fluentd.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/fluentd-sidecar-config.yaml"><code>admin/logging/fluentd-sidecar-config.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/logging/fluentd-sidecar-config.yaml to clipboard"></div><div class="includecode" id="admin-logging-fluentd-sidecar-config-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ConfigMap<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>fluentd-config<span>
</span></span></span><span><span><span></span><span>data</span>:<span>
</span></span></span><span><span><span>  </span><span>fluentd.conf</span>:<span> </span>|<span>
</span></span></span><span><span><span>    &lt;source&gt;
</span></span></span><span><span><span>      type tail
</span></span></span><span><span><span>      format none
</span></span></span><span><span><span>      path /var/log/1.log
</span></span></span><span><span><span>      pos_file /var/log/1.log.pos
</span></span></span><span><span><span>      tag count.format1
</span></span></span><span><span><span>    &lt;/source&gt;
</span></span></span><span><span><span>
</span></span></span><span><span><span>    &lt;source&gt;
</span></span></span><span><span><span>      type tail
</span></span></span><span><span><span>      format none
</span></span></span><span><span><span>      path /var/log/2.log
</span></span></span><span><span><span>      pos_file /var/log/2.log.pos
</span></span></span><span><span><span>      tag count.format2
</span></span></span><span><span><span>    &lt;/source&gt;
</span></span></span><span><span><span>
</span></span></span><span><span><span>    &lt;match **&gt;
</span></span></span><span><span><span>      type google_cloud
</span></span></span><span><span><span>    &lt;/match&gt;</span><span>    
</span></span></span></code></pre></div></div></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In the sample configurations, you can replace fluentd with any logging agent, reading
from any source inside an application container.</div><p>The second manifest describes a pod that has a sidecar container running fluentd.
The pod mounts a volume where fluentd can pick up its configuration data.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod-agent-sidecar.yaml"><code>admin/logging/two-files-counter-pod-agent-sidecar.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy admin/logging/two-files-counter-pod-agent-sidecar.yaml to clipboard"></div><div class="includecode" id="admin-logging-two-files-counter-pod-agent-sidecar-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>counter<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>count<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>busybox:1.28<span>
</span></span></span><span><span><span>    </span><span>args</span>:<span>
</span></span></span><span><span><span>    </span>- /bin/sh<span>
</span></span></span><span><span><span>    </span>- -c<span>
</span></span></span><span><span><span>    </span>- &gt;<span>
</span></span></span><span><span><span>      i=0;
</span></span></span><span><span><span>      while true;
</span></span></span><span><span><span>      do
</span></span></span><span><span><span>        echo "$i: $(date)" &gt;&gt; /var/log/1.log;
</span></span></span><span><span><span>        echo "$(date) INFO $i" &gt;&gt; /var/log/2.log;
</span></span></span><span><span><span>        i=$((i+1));
</span></span></span><span><span><span>        sleep 1;
</span></span></span><span><span><span>      done</span><span>      
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>count-agent<span>
</span></span></span><span><span><span>    </span><span>image</span>:<span> </span>registry.k8s.io/fluentd-gcp:1.30<span>
</span></span></span><span><span><span>    </span><span>env</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>FLUENTD_ARGS<span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span>-c /etc/fluentd-config/fluentd.conf<span>
</span></span></span><span><span><span>    </span><span>volumeMounts</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/var/log<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>config-volume<span>
</span></span></span><span><span><span>      </span><span>mountPath</span>:<span> </span>/etc/fluentd-config<span>
</span></span></span><span><span><span>  </span><span>volumes</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>varlog<span>
</span></span></span><span><span><span>    </span><span>emptyDir</span>:<span> </span>{}<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>config-volume<span>
</span></span></span><span><span><span>    </span><span>configMap</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>fluentd-config<span>
</span></span></span></code></pre></div></div></div><h3 id="exposing-logs-directly-from-the-application">Exposing logs directly from the application</h3><p><img alt="Exposing logs directly from the application" src="/images/docs/user-guide/logging/logging-from-application.png"></p><p>Cluster-logging that exposes or pushes logs directly from every application is outside the scope
of Kubernetes.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="/docs/concepts/cluster-administration/system-logs/">Kubernetes system logs</a></li><li>Learn about <a href="/docs/concepts/cluster-administration/system-traces/">Traces For Kubernetes System Components</a></li><li>Learn how to <a href="/docs/tasks/debug/debug-application/determine-reason-pod-failure/#customizing-the-termination-message">customise the termination message</a>
that Kubernetes records when a Pod fails</li></ul></div></div><div><div class="td-content"><h1>Compatibility Version For Kubernetes Control Plane Components</h1><p>Since release v1.32, we introduced configurable version compatibility and emulation options to Kubernetes control plane components to make upgrades safer by providing more control and increasing the granularity of steps available to cluster administrators.</p><h2 id="emulated-version">Emulated Version</h2><p>The emulation option is set by the <code>--emulated-version</code> flag of control plane components. It allows the component to emulate the behavior (APIs, features, ...) of an earlier version of Kubernetes.</p><p>When used, the capabilities available will match the emulated version:</p><ul><li>Any capabilities present in the binary version that were introduced after the emulation version will be unavailable.</li><li>Any capabilities removed after the emulation version will be available.</li></ul><p>This enables a binary from a particular Kubernetes release to emulate the behavior of a previous version with sufficient fidelity that interoperability with other system components can be defined in terms of the emulated version.</p><p>The <code>--emulated-version</code> must be &lt;= <code>binaryVersion</code>. See the help message of the <code>--emulated-version</code> flag for supported range of emulated versions.</p></div></div><div><div class="td-content"><h1>Metrics For Kubernetes System Components</h1><p>System component metrics can give a better look into what is happening inside them. Metrics are
particularly useful for building dashboards and alerts.</p><p>Kubernetes components emit metrics in <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">Prometheus format</a>.
This format is structured plain text, designed so that people and machines can both read it.</p><h2 id="metrics-in-kubernetes">Metrics in Kubernetes</h2><p>In most cases metrics are available on <code>/metrics</code> endpoint of the HTTP server. For components that
don't expose endpoint by default, it can be enabled using <code>--bind-address</code> flag.</p><p>Examples of those components:</p><ul><li><a class="glossary-tooltip" title="Control Plane component that runs controller processes." href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank">kube-controller-manager</a></li><li><a class="glossary-tooltip" title="kube-proxy is a network proxy that runs on each node in the cluster." href="/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank">kube-proxy</a></li><li><a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">kube-apiserver</a></li><li><a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">kube-scheduler</a></li><li><a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a></li></ul><p>In a production environment you may want to configure <a href="https://prometheus.io/">Prometheus Server</a>
or some other metrics scraper to periodically gather these metrics and make them available in some
kind of time series database.</p><p>Note that <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a> also exposes metrics in
<code>/metrics/cadvisor</code>, <code>/metrics/resource</code> and <code>/metrics/probes</code> endpoints. Those metrics do not
have the same lifecycle.</p><p>If your cluster uses <a class="glossary-tooltip" title="Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API." href="/docs/reference/access-authn-authz/rbac/" target="_blank">RBAC</a>, reading metrics requires
authorization via a user, group or ServiceAccount with a ClusterRole that allows accessing
<code>/metrics</code>. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>rbac.authorization.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>ClusterRole<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>prometheus<span>
</span></span></span><span><span><span></span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>nonResourceURLs</span>:<span>
</span></span></span><span><span><span>      </span>- <span>"/metrics"</span><span>
</span></span></span><span><span><span>    </span><span>verbs</span>:<span>
</span></span></span><span><span><span>      </span>- get<span>
</span></span></span></code></pre></div><h2 id="metric-lifecycle">Metric lifecycle</h2><p>Alpha metric &#8594; Beta metric &#8594; Stable metric &#8594; Deprecated metric &#8594; Hidden metric &#8594; Deleted metric</p><p>Alpha metrics have no stability guarantees. These metrics can be modified or deleted at any time.</p><p>Beta metrics observe a looser API contract than its stable counterparts. No labels can be removed from beta metrics during their lifetime, however, labels can be added while the metric is in the beta stage.</p><p>Stable metrics are guaranteed to not change. This means:</p><ul><li>A stable metric without a deprecated signature will not be deleted or renamed</li><li>A stable metric's type will not be modified</li></ul><p>Deprecated metrics are slated for deletion, but are still available for use.
These metrics include an annotation about the version in which they became deprecated.</p><p>For example:</p><ul><li><p>Before deprecation</p><pre tabindex="0"><code># HELP some_counter this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li><li><p>After deprecation</p><pre tabindex="0"><code># HELP some_counter (Deprecated since 1.15.0) this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li></ul><p>Hidden metrics are no longer published for scraping, but are still available for use.
A deprecated metric becomes a hidden metric after a period of time, based on its stability level:</p><ul><li><strong>STABLE</strong> metrics become hidden after a minimum of 3 releases or 9 months, whichever is longer.</li><li><strong>BETA</strong> metrics become hidden after a minimum of 1 release or 4 months, whichever is longer.</li><li><strong>ALPHA</strong> metrics can be hidden or removed in the same release in which they are deprecated.</li></ul><p>To use a hidden metric, you must enable it. For more details, refer to the <a href="#show-hidden-metrics">Show hidden metrics</a> section.</p><p>Deleted metrics are no longer published and cannot be used.</p><h2 id="show-hidden-metrics">Show hidden metrics</h2><p>As described above, admins can enable hidden metrics through a command-line flag on a specific
binary. This intends to be used as an escape hatch for admins if they missed the migration of the
metrics deprecated in the last release.</p><p>The flag <code>show-hidden-metrics-for-version</code> takes a version for which you want to show metrics
deprecated in that release. The version is expressed as x.y, where x is the major version, y is
the minor version. The patch version is not needed even though a metrics can be deprecated in a
patch release, the reason for that is the metrics deprecation policy runs against the minor release.</p><p>The flag can only take the previous minor version as its value. If you want to show all metrics hidden in the previous release, you can set the <code>show-hidden-metrics-for-version</code> flag to the previous version. Using a version that is too old is not allowed because it violates the metrics deprecation policy.</p><p>For example, let's assume metric <code>A</code> is deprecated in <code>1.29</code>. The version in which metric <code>A</code> becomes hidden depends on its stability level:</p><ul><li>If metric <code>A</code> is <strong>ALPHA</strong>, it could be hidden in <code>1.29</code>.</li><li>If metric <code>A</code> is <strong>BETA</strong>, it will be hidden in <code>1.30</code> at the earliest. If you are upgrading to <code>1.30</code> and still need <code>A</code>, you must use the command-line flag <code>--show-hidden-metrics-for-version=1.29</code>.</li><li>If metric <code>A</code> is <strong>STABLE</strong>, it will be hidden in <code>1.32</code> at the earliest. If you are upgrading to <code>1.32</code> and still need <code>A</code>, you must use the command-line flag <code>--show-hidden-metrics-for-version=1.31</code>.</li></ul><h2 id="component-metrics">Component metrics</h2><h3 id="kube-controller-manager-metrics">kube-controller-manager metrics</h3><p>Controller manager metrics provide important insight into the performance and health of the
controller manager. These metrics include common Go language runtime metrics such as go_routine
count and controller specific metrics such as etcd request latencies or Cloudprovider (AWS, GCE,
OpenStack) API latencies that can be used to gauge the health of a cluster.</p><p>Starting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage operations
for GCE, AWS, Vsphere and OpenStack.
These metrics can be used to monitor health of persistent volume operations.</p><p>For example, for GCE these metrics are called:</p><pre tabindex="0"><code>cloudprovider_gce_api_request_duration_seconds { request = "instance_list"}
cloudprovider_gce_api_request_duration_seconds { request = "disk_insert"}
cloudprovider_gce_api_request_duration_seconds { request = "disk_delete"}
cloudprovider_gce_api_request_duration_seconds { request = "attach_disk"}
cloudprovider_gce_api_request_duration_seconds { request = "detach_disk"}
cloudprovider_gce_api_request_duration_seconds { request = "list_disk"}
</code></pre><h3 id="kube-scheduler-metrics">kube-scheduler metrics</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.21 [beta]</code></div><p>The scheduler exposes optional metrics that reports the requested resources and the desired limits
of all running pods. These metrics can be used to build capacity planning dashboards, assess
current or historical scheduling limits, quickly identify workloads that cannot schedule due to
lack of resources, and compare actual usage to the pod's request.</p><p>The kube-scheduler identifies the resource <a href="/docs/concepts/configuration/manage-resources-containers/">requests and limits</a>
configured for each Pod; when either a request or limit is non-zero, the kube-scheduler reports a
metrics timeseries. The time series is labelled by:</p><ul><li>namespace</li><li>pod name</li><li>the node where the pod is scheduled or an empty string if not yet scheduled</li><li>priority</li><li>the assigned scheduler for that pod</li><li>the name of the resource (for example, <code>cpu</code>)</li><li>the unit of the resource if known (for example, <code>cores</code>)</li></ul><p>Once a pod reaches completion (has a <code>restartPolicy</code> of <code>Never</code> or <code>OnFailure</code> and is in the
<code>Succeeded</code> or <code>Failed</code> pod phase, or has been deleted and all containers have a terminated state)
the series is no longer reported since the scheduler is now free to schedule other pods to run.
The two metrics are called <code>kube_pod_resource_request</code> and <code>kube_pod_resource_limit</code>.</p><p>The metrics are exposed at the HTTP endpoint <code>/metrics/resources</code>. They require
authorization for the <code>/metrics/resources</code> endpoint, usually granted by a
ClusterRole with the <code>get</code> verb for the <code>/metrics/resources</code> non-resource URL.</p><p>On Kubernetes 1.21 you must use the <code>--show-hidden-metrics-for-version=1.20</code>
flag to expose these alpha stability metrics.</p><h3 id="kubelet-pressure-stall-information-psi-metrics">kubelet Pressure Stall Information (PSI) metrics</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code></div><p>As a beta feature, Kubernetes lets you configure kubelet to collect Linux kernel
<a href="https://docs.kernel.org/accounting/psi.html">Pressure Stall Information</a>
(PSI) for CPU, memory and I/O usage.
The information is collected at node, pod and container level.
The metrics are exposed at the <code>/metrics/cadvisor</code> endpoint with the following names:</p><pre tabindex="0"><code>container_pressure_cpu_stalled_seconds_total
container_pressure_cpu_waiting_seconds_total
container_pressure_memory_stalled_seconds_total
container_pressure_memory_waiting_seconds_total
container_pressure_io_stalled_seconds_total
container_pressure_io_waiting_seconds_total
</code></pre><p>This feature is enabled by default, by setting the <code>KubeletPSI</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>. The information is also exposed in the
<a href="/docs/reference/instrumentation/node-metrics/#psi">Summary API</a>.</p><p>You can learn how to interpret the PSI metrics in <a href="/docs/reference/instrumentation/understand-psi-metrics/">Understand PSI Metrics</a>.</p><h4 id="requirements">Requirements</h4><p>Pressure Stall Information requires:</p><ul><li><a href="/docs/reference/node/kernel-version-requirements/#requirements-psi">Linux kernel versions 4.20 or later</a>.</li><li><a href="/docs/concepts/architecture/cgroups/">cgroup v2</a></li></ul><h2 id="disabling-metrics">Disabling metrics</h2><p>You can explicitly turn off metrics via command line flag <code>--disabled-metrics</code>. This may be
desired if, for example, a metric is causing a performance problem. The input is a list of
disabled metrics (i.e. <code>--disabled-metrics=metric1,metric2</code>).</p><h2 id="metric-cardinality-enforcement">Metric cardinality enforcement</h2><p>Metrics with unbounded dimensions could cause memory issues in the components they instrument. To
limit resource use, you can use the <code>--allow-metric-labels</code> command line option to dynamically
configure an allow-list of label values for a metric.</p><p>In alpha stage, the flag can only take in a series of mappings as metric label allow-list.
Each mapping is of the format <code>&lt;metric_name&gt;,&lt;label_name&gt;=&lt;allowed_labels&gt;</code> where
<code>&lt;allowed_labels&gt;</code> is a comma-separated list of acceptable label names.</p><p>The overall format looks like:</p><pre tabindex="0"><code>--allow-metric-labels &lt;metric_name&gt;,&lt;label_name&gt;='&lt;allow_value1&gt;, &lt;allow_value2&gt;...', &lt;metric_name2&gt;,&lt;label_name&gt;='&lt;allow_value1&gt;, &lt;allow_value2&gt;...', ...
</code></pre><p>Here is an example:</p><pre tabindex="0"><code class="language-none">--allow-metric-labels number_count_metric,odd_number='1,3,5', number_count_metric,even_number='2,4,6', date_gauge_metric,weekend='Saturday,Sunday'
</code></pre><p>In addition to specifying this from the CLI, this can also be done within a configuration file. You
can specify the path to that configuration file using the <code>--allow-metric-labels-manifest</code> command
line argument to a component. Here's an example of the contents of that configuration file:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>"metric1,label2": </span><span>"v1,v2,v3"</span><span>
</span></span></span><span><span><span></span><span>"metric2,label1": </span><span>"v1,v2,v3"</span><span>
</span></span></span></code></pre></div><p>Additionally, the <code>cardinality_enforcement_unexpected_categorizations_total</code> meta-metric records the
count of unexpected categorizations during cardinality enforcement, that is, whenever a label value
is encountered that is not allowed with respect to the allow-list constraints.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about the <a href="https://github.com/prometheus/docs/blob/main/docs/instrumenting/exposition_formats.md#text-based-format">Prometheus text format</a>
for metrics</li><li>See the list of <a href="https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml">stable Kubernetes metrics</a></li><li>Read about the <a href="/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior">Kubernetes deprecation policy</a></li></ul></div></div><div><div class="td-content"><h1>Metrics for Kubernetes Object States</h1><div class="lead">kube-state-metrics, an add-on agent to generate and expose cluster-level metrics.</div><p>The state of Kubernetes objects in the Kubernetes API can be exposed as metrics.
An add-on agent called <a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a> can connect to the Kubernetes API server and expose a HTTP endpoint with metrics generated from the state of individual objects in the cluster.
It exposes various information about the state of objects like labels and annotations, startup and termination times, status or the phase the object currently is in.
For example, containers running in pods create a <code>kube_pod_container_info</code> metric.
This includes the name of the container, the name of the pod it is part of, the <a class="glossary-tooltip" title="An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster." href="/docs/concepts/overview/working-with-objects/namespaces" target="_blank">namespace</a> the pod is running in, the name of the container image, the ID of the image, the image name from the spec of the container, the ID of the running container and the ID of the pod as labels.</p><div class="alert alert-secondary callout third-party-content">&#128711; This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><p>An external component that is able and capable to scrape the endpoint of kube-state-metrics (for example via Prometheus) can now be used to enable the following use cases.</p><h2 id="example-kube-state-metrics-query-1">Example: using metrics from kube-state-metrics to query the cluster state</h2><p>Metric series generated by kube-state-metrics are helpful to gather further insights into the cluster, as they can be used for querying.</p><p>If you use Prometheus or another tool that uses the same query language, the following PromQL query returns the number of pods that are not ready:</p><pre tabindex="0"><code>count(kube_pod_status_ready{condition="false"}) by (namespace, pod)
</code></pre><h2 id="example-kube-state-metrics-alert-1">Example: alerting based on from kube-state-metrics</h2><p>Metrics generated from kube-state-metrics also allow for alerting on issues in the cluster.</p><p>If you use Prometheus or a similar tool that uses the same alert rule language, the following alert will fire if there are pods that have been in a <code>Terminating</code> state for more than 5 minutes:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>groups</span>:<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>Pod state<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>  </span>- <span>alert</span>:<span> </span>PodsBlockedInTerminatingState<span>
</span></span></span><span><span><span>    </span><span>expr</span>:<span> </span>count(kube_pod_deletion_timestamp) by (namespace, pod) * count(kube_pod_status_reason{reason="NodeLost"} == 0) by (namespace, pod) &gt; 0<span>
</span></span></span><span><span><span>    </span><span>for</span>:<span> </span>5m<span>
</span></span></span><span><span><span>    </span><span>labels</span>:<span>
</span></span></span><span><span><span>      </span><span>severity</span>:<span> </span>page<span>
</span></span></span><span><span><span>    </span><span>annotations</span>:<span>
</span></span></span><span><span><span>      </span><span>summary</span>:<span> </span>Pod {{$labels.namespace}}/{{$labels.pod}} blocked in Terminating state.<span>
</span></span></span></code></pre></div></div></div><div><div class="td-content"><h1>System Logs</h1><p>System component logs record events happening in cluster, which can be very useful for debugging.
You can configure log verbosity to see more or less detail.
Logs can be as coarse-grained as showing errors within a component, or as fine-grained as showing
step-by-step traces of events (like HTTP access logs, pod state changes, controller actions, or
scheduler decisions).</p><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4>In contrast to the command line flags described here, the <em>log
output</em> itself does <em>not</em> fall under the Kubernetes API stability guarantees:
individual log entries and their formatting may change from one release
to the next!</div><h2 id="klog">Klog</h2><p>klog is the Kubernetes logging library. <a href="https://github.com/kubernetes/klog">klog</a>
generates log messages for the Kubernetes system components.</p><p>Kubernetes is in the process of simplifying logging in its components.
The following klog command line flags
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components">are deprecated</a>
starting with Kubernetes v1.23 and removed in Kubernetes v1.26:</p><ul><li><code>--add-dir-header</code></li><li><code>--alsologtostderr</code></li><li><code>--log-backtrace-at</code></li><li><code>--log-dir</code></li><li><code>--log-file</code></li><li><code>--log-file-max-size</code></li><li><code>--logtostderr</code></li><li><code>--one-output</code></li><li><code>--skip-headers</code></li><li><code>--skip-log-headers</code></li><li><code>--stderrthreshold</code></li></ul><p>Output will always be written to stderr, regardless of the output format. Output redirection is
expected to be handled by the component which invokes a Kubernetes component. This can be a POSIX
shell or a tool like systemd.</p><p>In some cases, for example a distroless container or a Windows system service, those options are
not available. Then the
<a href="https://github.com/kubernetes/kubernetes/blob/d2a8a81639fcff8d1221b900f66d28361a170654/staging/src/k8s.io/component-base/logs/kube-log-runner/README.md"><code>kube-log-runner</code></a>
binary can be used as wrapper around a Kubernetes component to redirect
output. A prebuilt binary is included in several Kubernetes base images under
its traditional name as <code>/go-runner</code> and as <code>kube-log-runner</code> in server and
node release archives.</p><p>This table shows how <code>kube-log-runner</code> invocations correspond to shell redirection:</p><table><thead><tr><th>Usage</th><th>POSIX shell (such as bash)</th><th><code>kube-log-runner &lt;options&gt; &lt;cmd&gt;</code></th></tr></thead><tbody><tr><td>Merge stderr and stdout, write to stdout</td><td><code>2&gt;&amp;1</code></td><td><code>kube-log-runner</code> (default behavior)</td></tr><tr><td>Redirect both into log file</td><td><code>1&gt;&gt;/tmp/log 2&gt;&amp;1</code></td><td><code>kube-log-runner -log-file=/tmp/log</code></td></tr><tr><td>Copy into log file and to stdout</td><td><code>2&gt;&amp;1 | tee -a /tmp/log</code></td><td><code>kube-log-runner -log-file=/tmp/log -also-stdout</code></td></tr><tr><td>Redirect only stdout into log file</td><td><code>&gt;/tmp/log</code></td><td><code>kube-log-runner -log-file=/tmp/log -redirect-stderr=false</code></td></tr></tbody></table><h3 id="klog-output">Klog output</h3><p>An example of the traditional klog native format:</p><pre tabindex="0"><code>I1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756]
</code></pre><p>The message string may contain line breaks:</p><pre tabindex="0"><code>I1025 00:15:15.525108       1 example.go:79] This is a message
which has a line break.
</code></pre><h3 id="structured-logging">Structured Logging</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.23 [beta]</code></div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4><p>Migration to structured log messages is an ongoing process. Not all log messages are structured in
this version. When parsing log files, you must also handle unstructured log messages.</p><p>Log formatting and value serialization are subject to change.</p></div><p>Structured logging introduces a uniform structure in log messages allowing for programmatic
extraction of information. You can store and process structured logs with less effort and cost.
The code which generates a log message determines whether it uses the traditional unstructured
klog output or structured logging.</p><p>The default formatting of structured log messages is as text, with a format that is backward
compatible with traditional klog:</p><pre tabindex="0"><code>&lt;klog header&gt; "&lt;message&gt;" &lt;key1&gt;="&lt;value1&gt;" &lt;key2&gt;="&lt;value2&gt;" ...
</code></pre><p>Example:</p><pre tabindex="0"><code>I1025 00:15:15.525108       1 controller_utils.go:116] "Pod status updated" pod="kube-system/kubedns" status="ready"
</code></pre><p>Strings are quoted. Other values are formatted with
<a href="https://pkg.go.dev/fmt#hdr-Printing"><code>%+v</code></a>, which may cause log messages to
continue on the next line <a href="https://github.com/kubernetes/kubernetes/issues/106428">depending on the data</a>.</p><pre tabindex="0"><code>I1025 00:15:15.525108       1 example.go:116] "Example" data="This is text with a line break\nand \"quotation marks\"." someInt=1 someFloat=0.1 someStruct={StringField: First line,
second line.}
</code></pre><h3 id="contextual-logging">Contextual Logging</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [beta]</code></div><p>Contextual logging builds on top of structured logging. It is primarily about
how developers use logging calls: code based on that concept is more flexible
and supports additional use cases as described in the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging">Contextual Logging
KEP</a>.</p><p>If developers use additional functions like <code>WithValues</code> or <code>WithName</code> in
their components, then log entries contain additional information that gets
passed into functions by their caller.</p><p>For Kubernetes 1.34, this is gated behind the <code>ContextualLogging</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> and is
enabled by default. The infrastructure for this was added in 1.24 without
modifying components. The
<a href="https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go"><code>component-base/logs/example</code></a>
command demonstrates how to use the new logging calls and how a component
behaves that supports contextual logging.</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>$</span> <span>cd</span> <span>$GOPATH</span>/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/
</span></span><span><span><span>$</span> go run . --help
</span></span><span><span><span>...
</span></span></span><span><span><span>      --feature-gates mapStringBool  A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:
</span></span></span><span><span><span>                                     AllAlpha=true|false (ALPHA - default=false)
</span></span></span><span><span><span>                                     AllBeta=true|false (BETA - default=false)
</span></span></span><span><span><span>                                     ContextualLogging=true|false (BETA - default=true)
</span></span></span><span><span><span></span><span>$</span> go run . --feature-gates <span>ContextualLogging</span><span>=</span><span>true</span>
</span></span><span><span><span>...
</span></span></span><span><span><span>I0222 15:13:31.645988  197901 example.go:54] "runtime" logger="example.myname" foo="bar" duration="1m0s"
</span></span></span><span><span><span>I0222 15:13:31.646007  197901 example.go:55] "another runtime" logger="example" foo="bar" duration="1h0m0s" duration="1m0s"
</span></span></span></code></pre></div><p>The <code>logger</code> key and <code>foo="bar"</code> were added by the caller of the function
which logs the <code>runtime</code> message and <code>duration="1m0s"</code> value, without having to
modify that function.</p><p>With contextual logging disable, <code>WithValues</code> and <code>WithName</code> do nothing and log
calls go through the global klog logger. Therefore this additional information
is not in the log output anymore:</p><div class="highlight"><pre tabindex="0"><code class="language-console"><span><span><span>$</span> go run . --feature-gates <span>ContextualLogging</span><span>=</span><span>false</span>
</span></span><span><span><span>...
</span></span></span><span><span><span>I0222 15:14:40.497333  198174 example.go:54] "runtime" duration="1m0s"
</span></span></span><span><span><span>I0222 15:14:40.497346  198174 example.go:55] "another runtime" duration="1h0m0s" duration="1m0s"
</span></span></span></code></pre></div><h3 id="json-log-format">JSON log format</h3><div class="feature-state-notice feature-alpha"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.19 [alpha]</code></div><div class="alert alert-danger"><h4 class="alert-heading">Warning:</h4><p>JSON output does not support many standard klog flags. For list of unsupported klog flags, see the
<a href="/docs/reference/command-line-tools-reference/">Command line tool reference</a>.</p><p>Not all logs are guaranteed to be written in JSON format (for example, during process start).
If you intend to parse logs, make sure you can handle log lines that are not JSON as well.</p><p>Field names and JSON serialization are subject to change.</p></div><p>The <code>--logging-format=json</code> flag changes the format of logs from klog native format to JSON format.
Example of JSON log format (pretty printed):</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>   <span>"ts"</span>: <span>1580306777.04728</span>,
</span></span><span><span>   <span>"v"</span>: <span>4</span>,
</span></span><span><span>   <span>"msg"</span>: <span>"Pod status updated"</span>,
</span></span><span><span>   <span>"pod"</span>:{
</span></span><span><span>      <span>"name"</span>: <span>"nginx-1"</span>,
</span></span><span><span>      <span>"namespace"</span>: <span>"default"</span>
</span></span><span><span>   },
</span></span><span><span>   <span>"status"</span>: <span>"ready"</span>
</span></span><span><span>}
</span></span></code></pre></div><p>Keys with special meaning:</p><ul><li><code>ts</code> - timestamp as Unix time (required, float)</li><li><code>v</code> - verbosity (only for info and not for error messages, int)</li><li><code>err</code> - error string (optional, string)</li><li><code>msg</code> - message (required, string)</li></ul><p>List of components currently supporting JSON format:</p><ul><li><a class="glossary-tooltip" title="Control Plane component that runs controller processes." href="/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank">kube-controller-manager</a></li><li><a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">kube-apiserver</a></li><li><a class="glossary-tooltip" title="Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on." href="/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank">kube-scheduler</a></li><li><a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">kubelet</a></li></ul><h3 id="log-verbosity-level">Log verbosity level</h3><p>The <code>-v</code> flag controls log verbosity. Increasing the value increases the number of logged events.
Decreasing the value decreases the number of logged events. Increasing verbosity settings logs
increasingly less severe events. A verbosity setting of 0 logs only critical events.</p><h3 id="log-location">Log location</h3><p>There are two types of system components: those that run in a container and those
that do not run in a container. For example:</p><ul><li>The Kubernetes scheduler and kube-proxy run in a container.</li><li>The kubelet and <a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>
do not run in containers.</li></ul><p>On machines with systemd, the kubelet and container runtime write to journald.
Otherwise, they write to <code>.log</code> files in the <code>/var/log</code> directory.
System components inside containers always write to <code>.log</code> files in the <code>/var/log</code> directory,
bypassing the default logging mechanism.
Similar to the container logs, you should rotate system component logs in the <code>/var/log</code> directory.
In Kubernetes clusters created by the <code>kube-up.sh</code> script, log rotation is configured by the <code>logrotate</code> tool.
The <code>logrotate</code> tool rotates logs daily, or once the log size is greater than 100MB.</p><h2 id="log-query">Log query</h2><div class="feature-state-notice feature-beta" title="Feature Gate: NodeLogQuery"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.30 [beta]</code> (enabled by default: false)</div><p>To help with debugging issues on nodes, Kubernetes v1.27 introduced a feature that allows viewing logs of services
running on the node. To use the feature, ensure that the <code>NodeLogQuery</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> is enabled for that node, and that the
kubelet configuration options <code>enableSystemLogHandler</code> and <code>enableSystemLogQuery</code> are both set to true. On Linux
the assumption is that service logs are available via journald. On Windows the assumption is that service logs are
available in the application log provider. On both operating systems, logs are also available by reading files within
<code>/var/log/</code>.</p><p>Provided you are authorized to interact with node objects, you can try out this feature on all your nodes or
just a subset. Here is an example to retrieve the kubelet service logs from a node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Fetch kubelet logs from a node named node-1.example</span>
</span></span><span><span>kubectl get --raw <span>"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet"</span>
</span></span></code></pre></div><p>You can also fetch files, provided that the files are in a directory that the kubelet allows for log
fetches. For example, you can fetch a log from <code>/var/log</code> on a Linux node:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get --raw <span>"/api/v1/nodes/&lt;insert-node-name-here&gt;/proxy/logs/?query=/&lt;insert-log-file-name-here&gt;"</span>
</span></span></code></pre></div><p>The kubelet uses heuristics to retrieve logs. This helps if you are not aware whether a given system service is
writing logs to the operating system's native logger like journald or to a log file in <code>/var/log/</code>. The heuristics
first checks the native logger and if that is not available attempts to retrieve the first logs from
<code>/var/log/&lt;servicename&gt;</code> or <code>/var/log/&lt;servicename&gt;.log</code> or <code>/var/log/&lt;servicename&gt;/&lt;servicename&gt;.log</code>.</p><p>The complete list of options that can be used are:</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>boot</code></td><td>boot show messages from a specific system boot</td></tr><tr><td><code>pattern</code></td><td>pattern filters log entries by the provided PERL-compatible regular expression</td></tr><tr><td><code>query</code></td><td>query specifies services(s) or files from which to return logs (required)</td></tr><tr><td><code>sinceTime</code></td><td>an <a href="https://www.rfc-editor.org/rfc/rfc3339">RFC3339</a> timestamp from which to show logs (inclusive)</td></tr><tr><td><code>untilTime</code></td><td>an <a href="https://www.rfc-editor.org/rfc/rfc3339">RFC3339</a> timestamp until which to show logs (inclusive)</td></tr><tr><td><code>tailLines</code></td><td>specify how many lines from the end of the log to retrieve; the default is to fetch the whole log</td></tr></tbody></table><p>Example of a more complex query:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span><span># Fetch kubelet logs from a node named node-1.example that have the word "error"</span>
</span></span><span><span>kubectl get --raw <span>"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet&amp;pattern=error"</span>
</span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Read about the <a href="/docs/concepts/cluster-administration/logging/">Kubernetes Logging Architecture</a></li><li>Read about <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging">Structured Logging</a></li><li>Read about <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging">Contextual Logging</a></li><li>Read about <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components">deprecation of klog flags</a></li><li>Read about the <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md">Conventions for logging severity</a></li><li>Read about <a href="https://kep.k8s.io/2258">Log Query</a></li></ul></div></div><div><div class="td-content"><h1>Traces For Kubernetes System Components</h1><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [beta]</code></div><p>System component traces record the latency of and relationships between operations in the cluster.</p><p>Kubernetes components emit traces using the
<a href="https://opentelemetry.io/docs/specs/otlp/">OpenTelemetry Protocol</a>
with the gRPC exporter and can be collected and routed to tracing backends using an
<a href="https://github.com/open-telemetry/opentelemetry-collector#-opentelemetry-collector">OpenTelemetry Collector</a>.</p><h2 id="trace-collection">Trace Collection</h2><p>Kubernetes components have built-in gRPC exporters for OTLP to export traces, either with an OpenTelemetry Collector,
or without an OpenTelemetry Collector.</p><p>For a complete guide to collecting traces and using the collector, see
<a href="https://opentelemetry.io/docs/collector/getting-started/">Getting Started with the OpenTelemetry Collector</a>.
However, there are a few things to note that are specific to Kubernetes components.</p><p>By default, Kubernetes components export traces using the grpc exporter for OTLP on the
<a href="https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=opentelemetry">IANA OpenTelemetry port</a>, 4317.
As an example, if the collector is running as a sidecar to a Kubernetes component,
the following receiver configuration will collect spans and log them to standard output:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>receivers</span>:<span>
</span></span></span><span><span><span>  </span><span>otlp</span>:<span>
</span></span></span><span><span><span>    </span><span>protocols</span>:<span>
</span></span></span><span><span><span>      </span><span>grpc</span>:<span>
</span></span></span><span><span><span></span><span>exporters</span>:<span>
</span></span></span><span><span><span>  </span><span># Replace this exporter with the exporter for your backend</span><span>
</span></span></span><span><span><span>  </span><span>exporters</span>:<span>
</span></span></span><span><span><span>    </span><span>debug</span>:<span>
</span></span></span><span><span><span>      </span><span>verbosity</span>:<span> </span>detailed<span>
</span></span></span><span><span><span></span><span>service</span>:<span>
</span></span></span><span><span><span>  </span><span>pipelines</span>:<span>
</span></span></span><span><span><span>    </span><span>traces</span>:<span>
</span></span></span><span><span><span>      </span><span>receivers</span>:<span> </span>[otlp]<span>
</span></span></span><span><span><span>      </span><span>exporters</span>:<span> </span>[debug]<span>
</span></span></span></code></pre></div><p>To directly emit traces to a backend without utilizing a collector,
specify the endpoint field in the Kubernetes tracing configuration file with the desired trace backend address.
This method negates the need for a collector and simplifies the overall structure.</p><p>For trace backend header configuration, including authentication details, environment variables can be used with <code>OTEL_EXPORTER_OTLP_HEADERS</code>,
see <a href="https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/">OTLP Exporter Configuration</a>.</p><p>Additionally, for trace resource attribute configuration such as Kubernetes cluster name, namespace, Pod name, etc.,
environment variables can also be used with <code>OTEL_RESOURCE_ATTRIBUTES</code>, see <a href="https://opentelemetry.io/docs/specs/semconv/resource/k8s/">OTLP Kubernetes Resource</a>.</p><h2 id="component-traces">Component traces</h2><h3 id="kube-apiserver-traces">kube-apiserver traces</h3><p>The kube-apiserver generates spans for incoming HTTP requests, and for outgoing requests
to webhooks, etcd, and re-entrant requests. It propagates the
<a href="https://www.w3.org/TR/trace-context/">W3C Trace Context</a> with outgoing requests
but does not make use of the trace context attached to incoming requests,
as the kube-apiserver is often a public endpoint.</p><h4 id="enabling-tracing-in-the-kube-apiserver">Enabling tracing in the kube-apiserver</h4><p>To enable tracing, provide the kube-apiserver with a tracing configuration file
with <code>--tracing-config-file=&lt;path-to-config&gt;</code>. This is an example config that records
spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiserver.config.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>TracingConfiguration<span>
</span></span></span><span><span><span></span><span># default value</span><span>
</span></span></span><span><span><span></span><span>#endpoint: localhost:4317</span><span>
</span></span></span><span><span><span></span><span>samplingRatePerMillion</span>:<span> </span><span>100</span><span>
</span></span></span></code></pre></div><p>For more information about the <code>TracingConfiguration</code> struct, see
<a href="/docs/reference/config-api/apiserver-config.v1/#apiserver-k8s-io-v1-TracingConfiguration">API server config API (v1)</a>.</p><h3 id="kubelet-traces">kubelet traces</h3><div class="feature-state-notice feature-stable" title="Feature Gate: KubeletTracing"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [stable]</code> (enabled by default: true)</div><p>The kubelet CRI interface and authenticated http servers are instrumented to generate
trace spans. As with the apiserver, the endpoint and sampling rate are configurable.
Trace context propagation is also configured. A parent span's sampling decision is always respected.
A provided tracing configuration sampling rate will apply to spans without a parent.
Enabled without a configured endpoint, the default OpenTelemetry Collector receiver address of "localhost:4317" is set.</p><h4 id="enabling-tracing-in-the-kubelet">Enabling tracing in the kubelet</h4><p>To enable tracing, apply the <a href="https://github.com/kubernetes/component-base/blob/release-1.27/tracing/api/v1/types.go">tracing configuration</a>.
This is an example snippet of a kubelet config that records spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>kubelet.config.k8s.io/v1beta1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>KubeletConfiguration<span>
</span></span></span><span><span><span></span><span>tracing</span>:<span>
</span></span></span><span><span><span>  </span><span># default value</span><span>
</span></span></span><span><span><span>  </span><span>#endpoint: localhost:4317</span><span>
</span></span></span><span><span><span>  </span><span>samplingRatePerMillion</span>:<span> </span><span>100</span><span>
</span></span></span></code></pre></div><p>If the <code>samplingRatePerMillion</code> is set to one million (<code>1000000</code>), then every
span will be sent to the exporter.</p><p>The kubelet in Kubernetes v1.34 collects spans from
the garbage collection, pod synchronization routine as well as every gRPC
method. The kubelet propagates trace context with gRPC requests so that
container runtimes with trace instrumentation, such as CRI-O and containerd,
can associate their exported spans with the trace context from the kubelet.
The resulting traces will have parent-child links between kubelet and
container runtime spans, providing helpful context when debugging node
issues.</p><p>Please note that exporting spans always comes with a small performance overhead
on the networking and CPU side, depending on the overall configuration of the
system. If there is any issue like that in a cluster which is running with
tracing enabled, then mitigate the problem by either reducing the
<code>samplingRatePerMillion</code> or disabling tracing completely by removing the
configuration.</p><h2 id="stability">Stability</h2><p>Tracing instrumentation is still under active development, and may change
in a variety of ways. This includes span names, attached attributes,
instrumented endpoints, etc. Until this feature graduates to stable,
there are no guarantees of backwards compatibility for tracing instrumentation.</p><h2 id="what-s-next">What's next</h2><ul><li>Read about <a href="https://opentelemetry.io/docs/collector/getting-started/">Getting Started with the OpenTelemetry Collector</a></li><li>Read about <a href="https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/">OTLP Exporter Configuration</a></li></ul></div></div><div><div class="td-content"><h1>Proxies in Kubernetes</h1><p>This page explains proxies used with Kubernetes.</p><h2 id="proxies">Proxies</h2><p>There are several different proxies you may encounter when using Kubernetes:</p><ol><li><p>The <a href="/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api">kubectl proxy</a>:</p><ul><li>runs on a user's desktop or in a pod</li><li>proxies from a localhost address to the Kubernetes apiserver</li><li>client to proxy uses HTTP</li><li>proxy to apiserver uses HTTPS</li><li>locates apiserver</li><li>adds authentication headers</li></ul></li><li><p>The <a href="/docs/tasks/access-application-cluster/access-cluster-services/#discovering-builtin-services">apiserver proxy</a>:</p><ul><li>is a bastion built into the apiserver</li><li>connects a user outside of the cluster to cluster IPs which otherwise might not be reachable</li><li>runs in the apiserver processes</li><li>client to proxy uses HTTPS (or http if apiserver so configured)</li><li>proxy to target may use HTTP or HTTPS as chosen by proxy using available information</li><li>can be used to reach a Node, Pod, or Service</li><li>does load balancing when used to reach a Service</li></ul></li><li><p>The <a href="/docs/concepts/services-networking/service/#ips-and-vips">kube proxy</a>:</p><ul><li>runs on each node</li><li>proxies UDP, TCP and SCTP</li><li>does not understand HTTP</li><li>provides load balancing</li><li>is only used to reach services</li></ul></li><li><p>A Proxy/Load-balancer in front of apiserver(s):</p><ul><li>existence and implementation varies from cluster to cluster (e.g. nginx)</li><li>sits between all clients and one or more apiservers</li><li>acts as load balancer if there are several apiservers.</li></ul></li><li><p>Cloud Load Balancers on external services:</p><ul><li>are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)</li><li>are created automatically when the Kubernetes service has type <code>LoadBalancer</code></li><li>usually supports UDP/TCP only</li><li>SCTP support is up to the load balancer implementation of the cloud provider</li><li>implementation varies by cloud provider.</li></ul></li></ol><p>Kubernetes users will typically not need to worry about anything other than the first two types. The cluster admin
will typically ensure that the latter types are set up correctly.</p><h2 id="requesting-redirects">Requesting redirects</h2><p>Proxies have replaced redirect capabilities. Redirects have been deprecated.</p></div></div><div><div class="td-content"><h1>API Priority and Fairness</h1><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.29 [stable]</code></div><p>Controlling the behavior of the Kubernetes API server in an overload situation
is a key task for cluster administrators. The <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">kube-apiserver</a> has some controls available
(i.e. the <code>--max-requests-inflight</code> and <code>--max-mutating-requests-inflight</code>
command-line flags) to limit the amount of outstanding work that will be
accepted, preventing a flood of inbound requests from overloading and
potentially crashing the API server, but these flags are not enough to ensure
that the most important requests get through in a period of high traffic.</p><p>The API Priority and Fairness feature (APF) is an alternative that improves upon
aforementioned max-inflight limitations. APF classifies
and isolates requests in a more fine-grained way. It also introduces
a limited amount of queuing, so that no requests are rejected in cases
of very brief bursts. Requests are dispatched from queues using a
fair queuing technique so that, for example, a poorly-behaved
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a> need not
starve others (even at the same priority level).</p><p>This feature is designed to work well with standard controllers, which
use informers and react to failures of API requests with exponential
back-off, and other clients that also work this way.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Some requests classified as "long-running"&#8212;such as remote
command execution or log tailing&#8212;are not subject to the API
Priority and Fairness filter. This is also true for the
<code>--max-requests-inflight</code> flag without the API Priority and Fairness
feature enabled. API Priority and Fairness <em>does</em> apply to <strong>watch</strong>
requests. When API Priority and Fairness is disabled, <strong>watch</strong> requests
are not subject to the <code>--max-requests-inflight</code> limit.</div><h2 id="enabling-disabling-api-priority-and-fairness">Enabling/Disabling API Priority and Fairness</h2><p>The API Priority and Fairness feature is controlled by a command-line flag
and is enabled by default. See
<a href="/docs/reference/command-line-tools-reference/kube-apiserver/#options">Options</a>
for a general explanation of the available kube-apiserver command-line
options and how to enable and disable them. The name of the
command-line option for APF is "--enable-priority-and-fairness". This feature
also involves an <a class="glossary-tooltip" title="A set of related paths in the Kubernetes API." href="/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning" target="_blank">API Group</a>
with: (a) a stable <code>v1</code> version, introduced in 1.29, and
enabled by default (b) a <code>v1beta3</code> version, enabled by default, and
deprecated in v1.29. You can
disable the API group beta version <code>v1beta3</code> by adding the
following command-line flags to your <code>kube-apiserver</code> invocation:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kube-apiserver <span>\
</span></span></span><span><span><span></span>--runtime-config<span>=</span>flowcontrol.apiserver.k8s.io/v1beta3<span>=</span><span>false</span> <span>\
</span></span></span><span><span><span></span> <span># &#8230;and other flags as usual</span>
</span></span></code></pre></div><p>The command-line flag <code>--enable-priority-and-fairness=false</code> will disable the
API Priority and Fairness feature.</p><h2 id="recursive-server-scenarios">Recursive server scenarios</h2><p>API Priority and Fairness must be used carefully in recursive server
scenarios. These are scenarios in which some server A, while serving
a request, issues a subsidiary request to some server B. Perhaps
server B might even make a further subsidiary call back to server
A. In situations where Priority and Fairness control is applied to
both the original request and some subsidiary ones(s), no matter how
deep in the recursion, there is a danger of priority inversions and/or
deadlocks.</p><p>One example of recursion is when the <code>kube-apiserver</code> issues an
admission webhook call to server B, and while serving that call,
server B makes a further subsidiary request back to the
<code>kube-apiserver</code>. Another example of recursion is when an <code>APIService</code>
object directs the <code>kube-apiserver</code> to delegate requests about a
certain API group to a custom external server B (this is one of the
things called "aggregation").</p><p>When the original request is known to belong to a certain priority
level, and the subsidiary controlled requests are classified to higher
priority levels, this is one possible solution. When the original
requests can belong to any priority level, the subsidiary controlled
requests have to be exempt from Priority and Fairness limitation. One
way to do that is with the objects that configure classification and
handling, discussed below. Another way is to disable Priority and
Fairness on server B entirely, using the techniques discussed above. A
third way, which is the simplest to use when server B is not
<code>kube-apiserver</code>, is to build server B with Priority and Fairness
disabled in the code.</p><h2 id="concepts">Concepts</h2><p>There are several distinct features involved in the API Priority and Fairness
feature. Incoming requests are classified by attributes of the request using
<em>FlowSchemas</em>, and assigned to priority levels. Priority levels add a degree of
isolation by maintaining separate concurrency limits, so that requests assigned
to different priority levels cannot starve each other. Within a priority level,
a fair-queuing algorithm prevents requests from different <em>flows</em> from starving
each other, and allows for requests to be queued to prevent bursty traffic from
causing failed requests when the average load is acceptably low.</p><h3 id="priority-levels">Priority Levels</h3><p>Without APF enabled, overall concurrency in the API server is limited by the
<code>kube-apiserver</code> flags <code>--max-requests-inflight</code> and
<code>--max-mutating-requests-inflight</code>. With APF enabled, the concurrency limits
defined by these flags are summed and then the sum is divided up among a
configurable set of <em>priority levels</em>. Each incoming request is assigned to a
single priority level, and each priority level will only dispatch as many
concurrent requests as its particular limit allows.</p><p>The default configuration, for example, includes separate priority levels for
leader-election requests, requests from built-in controllers, and requests from
Pods. This means that an ill-behaved Pod that floods the API server with
requests cannot prevent leader election or actions by the built-in controllers
from succeeding.</p><p>The concurrency limits of the priority levels are periodically
adjusted, allowing under-utilized priority levels to temporarily lend
concurrency to heavily-utilized levels. These limits are based on
nominal limits and bounds on how much concurrency a priority level may
lend and how much it may borrow, all derived from the configuration
objects mentioned below.</p><h3 id="seats-occupied-by-a-request">Seats Occupied by a Request</h3><p>The above description of concurrency management is the baseline story.
Requests have different durations but are counted equally at any given
moment when comparing against a priority level's concurrency limit. In
the baseline story, each request occupies one unit of concurrency. The
word "seat" is used to mean one unit of concurrency, inspired by the
way each passenger on a train or aircraft takes up one of the fixed
supply of seats.</p><p>But some requests take up more than one seat. Some of these are <strong>list</strong>
requests that the server estimates will return a large number of
objects. These have been found to put an exceptionally heavy burden
on the server. For this reason, the server estimates the number of objects
that will be returned and considers the request to take a number of seats
that is proportional to that estimated number.</p><h3 id="execution-time-tweaks-for-watch-requests">Execution time tweaks for watch requests</h3><p>API Priority and Fairness manages <strong>watch</strong> requests, but this involves a
couple more excursions from the baseline behavior. The first concerns
how long a <strong>watch</strong> request is considered to occupy its seat. Depending
on request parameters, the response to a <strong>watch</strong> request may or may not
begin with <strong>create</strong> notifications for all the relevant pre-existing
objects. API Priority and Fairness considers a <strong>watch</strong> request to be
done with its seat once that initial burst of notifications, if any,
is over.</p><p>The normal notifications are sent in a concurrent burst to all
relevant <strong>watch</strong> response streams whenever the server is notified of an
object create/update/delete. To account for this work, API Priority
and Fairness considers every write request to spend some additional
time occupying seats after the actual writing is done. The server
estimates the number of notifications to be sent and adjusts the write
request's number of seats and seat occupancy time to include this
extra work.</p><h3 id="queuing">Queuing</h3><p>Even within a priority level there may be a large number of distinct sources of
traffic. In an overload situation, it is valuable to prevent one stream of
requests from starving others (in particular, in the relatively common case of a
single buggy client flooding the kube-apiserver with requests, that buggy client
would ideally not have much measurable impact on other clients at all). This is
handled by use of a fair-queuing algorithm to process requests that are assigned
the same priority level. Each request is assigned to a <em>flow</em>, identified by the
name of the matching FlowSchema plus a <em>flow distinguisher</em> &#8212; which
is either the requesting user, the target resource's namespace, or nothing &#8212; and the
system attempts to give approximately equal weight to requests in different
flows of the same priority level.
To enable distinct handling of distinct instances, controllers that have
many instances should authenticate with distinct usernames</p><p>After classifying a request into a flow, the API Priority and Fairness
feature then may assign the request to a queue. This assignment uses
a technique known as <a class="glossary-tooltip" title="A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues." href="/docs/reference/glossary/?all=true#term-shuffle-sharding" target="_blank">shuffle sharding</a>, which makes relatively efficient use of
queues to insulate low-intensity flows from high-intensity flows.</p><p>The details of the queuing algorithm are tunable for each priority level, and
allow administrators to trade off memory use, fairness (the property that
independent flows will all make progress when total traffic exceeds capacity),
tolerance for bursty traffic, and the added latency induced by queuing.</p><h3 id="exempt-requests">Exempt requests</h3><p>Some requests are considered sufficiently important that they are not subject to
any of the limitations imposed by this feature. These exemptions prevent an
improperly-configured flow control configuration from totally disabling an API
server.</p><h2 id="resources">Resources</h2><p>The flow control API involves two kinds of resources.
<a href="/docs/reference/generated/kubernetes-api/v1.34/#prioritylevelconfiguration-v1-flowcontrol-apiserver-k8s-io">PriorityLevelConfigurations</a>
define the available priority levels, the share of the available concurrency
budget that each can handle, and allow for fine-tuning queuing behavior.
<a href="/docs/reference/generated/kubernetes-api/v1.34/#flowschema-v1-flowcontrol-apiserver-k8s-io">FlowSchemas</a>
are used to classify individual inbound requests, matching each to a
single PriorityLevelConfiguration.</p><h3 id="prioritylevelconfiguration">PriorityLevelConfiguration</h3><p>A PriorityLevelConfiguration represents a single priority level. Each
PriorityLevelConfiguration has an independent limit on the number of outstanding
requests, and limitations on the number of queued requests.</p><p>The nominal concurrency limit for a PriorityLevelConfiguration is not
specified in an absolute number of seats, but rather in "nominal
concurrency shares." The total concurrency limit for the API Server is
distributed among the existing PriorityLevelConfigurations in
proportion to these shares, to give each level its nominal limit in
terms of seats. This allows a cluster administrator to scale up or
down the total amount of traffic to a server by restarting
<code>kube-apiserver</code> with a different value for <code>--max-requests-inflight</code>
(or <code>--max-mutating-requests-inflight</code>), and all
PriorityLevelConfigurations will see their maximum allowed concurrency
go up (or down) by the same fraction.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>In the versions before <code>v1beta3</code> the relevant
PriorityLevelConfiguration field is named "assured concurrency shares"
rather than "nominal concurrency shares". Also, in Kubernetes release
1.25 and earlier there were no periodic adjustments: the
nominal/assured limits were always applied without adjustment.</div><p>The bounds on how much concurrency a priority level may lend and how
much it may borrow are expressed in the PriorityLevelConfiguration as
percentages of the level's nominal limit. These are resolved to
absolute numbers of seats by multiplying with the nominal limit /
100.0 and rounding. The dynamically adjusted concurrency limit of a
priority level is constrained to lie between (a) a lower bound of its
nominal limit minus its lendable seats and (b) an upper bound of its
nominal limit plus the seats it may borrow. At each adjustment the
dynamic limits are derived by each priority level reclaiming any lent
seats for which demand recently appeared and then jointly fairly
responding to the recent seat demand on the priority levels, within
the bounds just described.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>With the Priority and Fairness feature enabled, the total concurrency limit for
the server is set to the sum of <code>--max-requests-inflight</code> and
<code>--max-mutating-requests-inflight</code>. There is no longer any distinction made
between mutating and non-mutating requests; if you want to treat them
separately for a given resource, make separate FlowSchemas that match the
mutating and non-mutating verbs respectively.</div><p>When the volume of inbound requests assigned to a single
PriorityLevelConfiguration is more than its permitted concurrency level, the
<code>type</code> field of its specification determines what will happen to extra requests.
A type of <code>Reject</code> means that excess traffic will immediately be rejected with
an HTTP 429 (Too Many Requests) error. A type of <code>Queue</code> means that requests
above the threshold will be queued, with the shuffle sharding and fair queuing techniques used
to balance progress between request flows.</p><p>The queuing configuration allows tuning the fair queuing algorithm for a
priority level. Details of the algorithm can be read in the
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness">enhancement proposal</a>, but in short:</p><ul><li><p>Increasing <code>queues</code> reduces the rate of collisions between different flows, at
the cost of increased memory usage. A value of 1 here effectively disables the
fair-queuing logic, but still allows requests to be queued.</p></li><li><p>Increasing <code>queueLengthLimit</code> allows larger bursts of traffic to be
sustained without dropping any requests, at the cost of increased
latency and memory usage.</p></li><li><p>Changing <code>handSize</code> allows you to adjust the probability of collisions between
different flows and the overall concurrency available to a single flow in an
overload situation.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>A larger <code>handSize</code> makes it less likely for two individual flows to collide
(and therefore for one to be able to starve the other), but more likely that
a small number of flows can dominate the apiserver. A larger <code>handSize</code> also
potentially increases the amount of latency that a single high-traffic flow
can cause. The maximum number of queued requests possible from a
single flow is <code>handSize * queueLengthLimit</code>.</div></li></ul><p>Following is a table showing an interesting collection of shuffle
sharding configurations, showing for each the probability that a
given mouse (low-intensity flow) is squished by the elephants (high-intensity flows) for
an illustrative collection of numbers of elephants. See
<a href="https://play.golang.org/p/Gi0PLgVHiUg">https://play.golang.org/p/Gi0PLgVHiUg</a> , which computes this table.</p><table><caption>Example Shuffle Sharding Configurations</caption><thead><tr><th>HandSize</th><th>Queues</th><th>1 elephant</th><th>4 elephants</th><th>16 elephants</th></tr></thead><tbody><tr><td>12</td><td>32</td><td>4.428838398950118e-09</td><td>0.11431348830099144</td><td>0.9935089607656024</td></tr><tr><td>10</td><td>32</td><td>1.550093439632541e-08</td><td>0.0626479840223545</td><td>0.9753101519027554</td></tr><tr><td>10</td><td>64</td><td>6.601827268370426e-12</td><td>0.00045571320990370776</td><td>0.49999929150089345</td></tr><tr><td>9</td><td>64</td><td>3.6310049976037345e-11</td><td>0.00045501212304112273</td><td>0.4282314876454858</td></tr><tr><td>8</td><td>64</td><td>2.25929199850899e-10</td><td>0.0004886697053040446</td><td>0.35935114681123076</td></tr><tr><td>8</td><td>128</td><td>6.994461389026097e-13</td><td>3.4055790161620863e-06</td><td>0.02746173137155063</td></tr><tr><td>7</td><td>128</td><td>1.0579122850901972e-11</td><td>6.960839379258192e-06</td><td>0.02406157386340147</td></tr><tr><td>7</td><td>256</td><td>7.597695465552631e-14</td><td>6.728547142019406e-08</td><td>0.0006709661542533682</td></tr><tr><td>6</td><td>256</td><td>2.7134626662687968e-12</td><td>2.9516464018476436e-07</td><td>0.0008895654642000348</td></tr><tr><td>6</td><td>512</td><td>4.116062922897309e-14</td><td>4.982983350480894e-09</td><td>2.26025764343413e-05</td></tr><tr><td>6</td><td>1024</td><td>6.337324016514285e-16</td><td>8.09060164312957e-11</td><td>4.517408062903668e-07</td></tr></tbody></table><h3 id="flowschema">FlowSchema</h3><p>A FlowSchema matches some inbound requests and assigns them to a
priority level. Every inbound request is tested against FlowSchemas,
starting with those with the numerically lowest <code>matchingPrecedence</code> and
working upward. The first match wins.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Only the first matching FlowSchema for a given request matters. If multiple
FlowSchemas match a single inbound request, it will be assigned based on the one
with the highest <code>matchingPrecedence</code>. If multiple FlowSchemas with equal
<code>matchingPrecedence</code> match the same request, the one with lexicographically
smaller <code>name</code> will win, but it's better not to rely on this, and instead to
ensure that no two FlowSchemas have the same <code>matchingPrecedence</code>.</div><p>A FlowSchema matches a given request if at least one of its <code>rules</code>
matches. A rule matches if at least one of its <code>subjects</code> <em>and</em> at least
one of its <code>resourceRules</code> or <code>nonResourceRules</code> (depending on whether the
incoming request is for a resource or non-resource URL) match the request.</p><p>For the <code>name</code> field in subjects, and the <code>verbs</code>, <code>apiGroups</code>, <code>resources</code>,
<code>namespaces</code>, and <code>nonResourceURLs</code> fields of resource and non-resource rules,
the wildcard <code>*</code> may be specified to match all values for the given field,
effectively removing it from consideration.</p><p>A FlowSchema's <code>distinguisherMethod.type</code> determines how requests matching that
schema will be separated into flows. It may be <code>ByUser</code>, in which one requesting
user will not be able to starve other users of capacity; <code>ByNamespace</code>, in which
requests for resources in one namespace will not be able to starve requests for
resources in other namespaces of capacity; or blank (or <code>distinguisherMethod</code> may be
omitted entirely), in which all requests matched by this FlowSchema will be
considered part of a single flow. The correct choice for a given FlowSchema
depends on the resource and your particular environment.</p><h2 id="defaults">Defaults</h2><p>Each kube-apiserver maintains two sorts of APF configuration objects:
mandatory and suggested.</p><h3 id="mandatory-configuration-objects">Mandatory Configuration Objects</h3><p>The four mandatory configuration objects reflect fixed built-in
guardrail behavior. This is behavior that the servers have before
those objects exist, and when those objects exist their specs reflect
this behavior. The four mandatory objects are as follows.</p><ul><li><p>The mandatory <code>exempt</code> priority level is used for requests that are
not subject to flow control at all: they will always be dispatched
immediately. The mandatory <code>exempt</code> FlowSchema classifies all
requests from the <code>system:masters</code> group into this priority
level. You may define other FlowSchemas that direct other requests
to this priority level, if appropriate.</p></li><li><p>The mandatory <code>catch-all</code> priority level is used in combination with
the mandatory <code>catch-all</code> FlowSchema to make sure that every request
gets some kind of classification. Typically you should not rely on
this catch-all configuration, and should create your own catch-all
FlowSchema and PriorityLevelConfiguration (or use the suggested
<code>global-default</code> priority level that is installed by default) as
appropriate. Because it is not expected to be used normally, the
mandatory <code>catch-all</code> priority level has a very small concurrency
share and does not queue requests.</p></li></ul><h3 id="suggested-configuration-objects">Suggested Configuration Objects</h3><p>The suggested FlowSchemas and PriorityLevelConfigurations constitute a
reasonable default configuration. You can modify these and/or create
additional configuration objects if you want. If your cluster is
likely to experience heavy load then you should consider what
configuration will work best.</p><p>The suggested configuration groups requests into six priority levels:</p><ul><li><p>The <code>node-high</code> priority level is for health updates from nodes.</p></li><li><p>The <code>system</code> priority level is for non-health requests from the
<code>system:nodes</code> group, i.e. Kubelets, which must be able to contact
the API server in order for workloads to be able to schedule on
them.</p></li><li><p>The <code>leader-election</code> priority level is for leader election requests from
built-in controllers (in particular, requests for <code>endpoints</code>, <code>configmaps</code>,
or <code>leases</code> coming from the <code>system:kube-controller-manager</code> or
<code>system:kube-scheduler</code> users and service accounts in the <code>kube-system</code>
namespace). These are important to isolate from other traffic because failures
in leader election cause their controllers to fail and restart, which in turn
causes more expensive traffic as the new controllers sync their informers.</p></li><li><p>The <code>workload-high</code> priority level is for other requests from built-in
controllers.</p></li><li><p>The <code>workload-low</code> priority level is for requests from any other service
account, which will typically include all requests from controllers running in
Pods.</p></li><li><p>The <code>global-default</code> priority level handles all other traffic, e.g.
interactive <code>kubectl</code> commands run by nonprivileged users.</p></li></ul><p>The suggested FlowSchemas serve to steer requests into the above
priority levels, and are not enumerated here.</p><h3 id="maintenance-of-the-mandatory-and-suggested-configuration-objects">Maintenance of the Mandatory and Suggested Configuration Objects</h3><p>Each <code>kube-apiserver</code> independently maintains the mandatory and
suggested configuration objects, using initial and periodic behavior.
Thus, in a situation with a mixture of servers of different versions
there may be thrashing as long as different servers have different
opinions of the proper content of these objects.</p><p>Each <code>kube-apiserver</code> makes an initial maintenance pass over the
mandatory and suggested configuration objects, and after that does
periodic maintenance (once per minute) of those objects.</p><p>For the mandatory configuration objects, maintenance consists of
ensuring that the object exists and, if it does, has the proper spec.
The server refuses to allow a creation or update with a spec that is
inconsistent with the server's guardrail behavior.</p><p>Maintenance of suggested configuration objects is designed to allow
their specs to be overridden. Deletion, on the other hand, is not
respected: maintenance will restore the object. If you do not want a
suggested configuration object then you need to keep it around but set
its spec to have minimal consequences. Maintenance of suggested
objects is also designed to support automatic migration when a new
version of the <code>kube-apiserver</code> is rolled out, albeit potentially with
thrashing while there is a mixed population of servers.</p><p>Maintenance of a suggested configuration object consists of creating
it --- with the server's suggested spec --- if the object does not
exist. OTOH, if the object already exists, maintenance behavior
depends on whether the <code>kube-apiservers</code> or the users control the
object. In the former case, the server ensures that the object's spec
is what the server suggests; in the latter case, the spec is left
alone.</p><p>The question of who controls the object is answered by first looking
for an annotation with key <code>apf.kubernetes.io/autoupdate-spec</code>. If
there is such an annotation and its value is <code>true</code> then the
kube-apiservers control the object. If there is such an annotation
and its value is <code>false</code> then the users control the object. If
neither of those conditions holds then the <code>metadata.generation</code> of the
object is consulted. If that is 1 then the kube-apiservers control
the object. Otherwise the users control the object. These rules were
introduced in release 1.22 and their consideration of
<code>metadata.generation</code> is for the sake of migration from the simpler
earlier behavior. Users who wish to control a suggested configuration
object should set its <code>apf.kubernetes.io/autoupdate-spec</code> annotation
to <code>false</code>.</p><p>Maintenance of a mandatory or suggested configuration object also
includes ensuring that it has an <code>apf.kubernetes.io/autoupdate-spec</code>
annotation that accurately reflects whether the kube-apiservers
control the object.</p><p>Maintenance also includes deleting objects that are neither mandatory
nor suggested but are annotated
<code>apf.kubernetes.io/autoupdate-spec=true</code>.</p><h2 id="health-check-concurrency-exemption">Health check concurrency exemption</h2><p>The suggested configuration gives no special treatment to the health
check requests on kube-apiservers from their local kubelets --- which
tend to use the secured port but supply no credentials. With the
suggested config, these requests get assigned to the <code>global-default</code>
FlowSchema and the corresponding <code>global-default</code> priority level,
where other traffic can crowd them out.</p><p>If you add the following additional FlowSchema, this exempts those
requests from rate limiting.</p><div class="alert alert-caution"><h4 class="alert-heading">Caution:</h4>Making this change also allows any hostile party to then send
health-check requests that match this FlowSchema, at any volume they
like. If you have a web traffic filter or similar external security
mechanism to protect your cluster's API server from general internet
traffic, you can configure rules to block any health check requests
that originate from outside your cluster.</div><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/priority-and-fairness/health-for-strangers.yaml"><code>priority-and-fairness/health-for-strangers.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy priority-and-fairness/health-for-strangers.yaml to clipboard"></div><div class="includecode" id="priority-and-fairness-health-for-strangers-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>flowcontrol.apiserver.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>FlowSchema<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>health-for-strangers<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>matchingPrecedence</span>:<span> </span><span>1000</span><span>
</span></span></span><span><span><span>  </span><span>priorityLevelConfiguration</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>exempt<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>nonResourceRules</span>:<span>
</span></span></span><span><span><span>      </span>- <span>nonResourceURLs</span>:<span>
</span></span></span><span><span><span>          </span>- <span>"/healthz"</span><span>
</span></span></span><span><span><span>          </span>- <span>"/livez"</span><span>
</span></span></span><span><span><span>          </span>- <span>"/readyz"</span><span>
</span></span></span><span><span><span>        </span><span>verbs</span>:<span>
</span></span></span><span><span><span>          </span>- <span>"*"</span><span>
</span></span></span><span><span><span>      </span><span>subjects</span>:<span>
</span></span></span><span><span><span>        </span>- <span>kind</span>:<span> </span>Group<span>
</span></span></span><span><span><span>          </span><span>group</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span><span>"system:unauthenticated"</span><span>
</span></span></span></code></pre></div></div></div><h2 id="observability">Observability</h2><h3 id="metrics">Metrics</h3><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>In versions of Kubernetes before v1.20, the labels <code>flow_schema</code> and
<code>priority_level</code> were inconsistently named <code>flowSchema</code> and <code>priorityLevel</code>,
respectively. If you're running Kubernetes versions v1.19 and earlier, you
should refer to the documentation for your version.</div><p>When you enable the API Priority and Fairness feature, the kube-apiserver
exports additional metrics. Monitoring these can help you determine whether your
configuration is inappropriately throttling important traffic, or find
poorly-behaved workloads that may be harming system health.</p><h4 id="maturity-level-beta">Maturity level BETA</h4><ul><li><p><code>apiserver_flowcontrol_rejected_requests_total</code> is a counter vector
(cumulative since server start) of requests that were rejected,
broken down by the labels <code>flow_schema</code> (indicating the one that
matched the request), <code>priority_level</code> (indicating the one to which
the request was assigned), and <code>reason</code>. The <code>reason</code> label will be
one of the following values:</p><ul><li><code>queue-full</code>, indicating that too many requests were already
queued.</li><li><code>concurrency-limit</code>, indicating that the
PriorityLevelConfiguration is configured to reject rather than
queue excess requests.</li><li><code>time-out</code>, indicating that the request was still in the queue
when its queuing time limit expired.</li><li><code>cancelled</code>, indicating that the request is not purge locked
and has been ejected from the queue.</li></ul></li><li><p><code>apiserver_flowcontrol_dispatched_requests_total</code> is a counter
vector (cumulative since server start) of requests that began
executing, broken down by <code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_current_inqueue_requests</code> is a gauge vector
holding the instantaneous number of queued (not executing) requests,
broken down by <code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_current_executing_requests</code> is a gauge vector
holding the instantaneous number of executing (not waiting in a
queue) requests, broken down by <code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_current_executing_seats</code> is a gauge vector
holding the instantaneous number of occupied seats, broken down by
<code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_request_wait_duration_seconds</code> is a histogram
vector of how long requests spent queued, broken down by the labels
<code>flow_schema</code>, <code>priority_level</code>, and <code>execute</code>. The <code>execute</code> label
indicates whether the request has started executing.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Since each FlowSchema always assigns requests to a single
PriorityLevelConfiguration, you can add the histograms for all the
FlowSchemas for one priority level to get the effective histogram for
requests assigned to that priority level.</div></li><li><p><code>apiserver_flowcontrol_nominal_limit_seats</code> is a gauge vector
holding each priority level's nominal concurrency limit, computed
from the API server's total concurrency limit and the priority
level's configured nominal concurrency shares.</p></li></ul><h4 id="maturity-level-alpha">Maturity level ALPHA</h4><ul><li><p><code>apiserver_current_inqueue_requests</code> is a gauge vector of recent
high water marks of the number of queued requests, grouped by a
label named <code>request_kind</code> whose value is <code>mutating</code> or <code>readOnly</code>.
These high water marks describe the largest number seen in the one
second window most recently completed. These complement the older
<code>apiserver_current_inflight_requests</code> gauge vector that holds the
last window's high water mark of number of requests actively being
served.</p></li><li><p><code>apiserver_current_inqueue_seats</code> is a gauge vector of the sum over
queued requests of the largest number of seats each will occupy,
grouped by labels named <code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_read_vs_write_current_requests</code> is a
histogram vector of observations, made at the end of every
nanosecond, of the number of requests broken down by the labels
<code>phase</code> (which takes on the values <code>waiting</code> and <code>executing</code>) and
<code>request_kind</code> (which takes on the values <code>mutating</code> and
<code>readOnly</code>). Each observed value is a ratio, between 0 and 1, of
the number of requests divided by the corresponding limit on the
number of requests (queue volume limit for waiting and concurrency
limit for executing).</p></li><li><p><code>apiserver_flowcontrol_request_concurrency_in_use</code> is a gauge vector
holding the instantaneous number of occupied seats, broken down by
<code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_priority_level_request_utilization</code> is a
histogram vector of observations, made at the end of each
nanosecond, of the number of requests broken down by the labels
<code>phase</code> (which takes on the values <code>waiting</code> and <code>executing</code>) and
<code>priority_level</code>. Each observed value is a ratio, between 0 and 1,
of a number of requests divided by the corresponding limit on the
number of requests (queue volume limit for waiting and concurrency
limit for executing).</p></li><li><p><code>apiserver_flowcontrol_priority_level_seat_utilization</code> is a
histogram vector of observations, made at the end of each
nanosecond, of the utilization of a priority level's concurrency
limit, broken down by <code>priority_level</code>. This utilization is the
fraction (number of seats occupied) / (concurrency limit). This
metric considers all stages of execution (both normal and the extra
delay at the end of a write to cover for the corresponding
notification work) of all requests except WATCHes; for those it
considers only the initial stage that delivers notifications of
pre-existing objects. Each histogram in the vector is also labeled
with <code>phase: executing</code> (there is no seat limit for the waiting
phase).</p></li><li><p><code>apiserver_flowcontrol_request_queue_length_after_enqueue</code> is a
histogram vector of queue lengths for the queues, broken down by
<code>priority_level</code> and <code>flow_schema</code>, as sampled by the enqueued requests.
Each request that gets queued contributes one sample to its histogram,
reporting the length of the queue immediately after the request was added.
Note that this produces different statistics than an unbiased survey would.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>An outlier value in a histogram here means it is likely that a single flow
(i.e., requests by one user or for one namespace, depending on
configuration) is flooding the API server, and being throttled. By contrast,
if one priority level's histogram shows that all queues for that priority
level are longer than those for other priority levels, it may be appropriate
to increase that PriorityLevelConfiguration's concurrency shares.</div></li><li><p><code>apiserver_flowcontrol_request_concurrency_limit</code> is the same as
<code>apiserver_flowcontrol_nominal_limit_seats</code>. Before the
introduction of concurrency borrowing between priority levels,
this was always equal to <code>apiserver_flowcontrol_current_limit_seats</code>
(which did not exist as a distinct metric).</p></li><li><p><code>apiserver_flowcontrol_lower_limit_seats</code> is a gauge vector holding
the lower bound on each priority level's dynamic concurrency limit.</p></li><li><p><code>apiserver_flowcontrol_upper_limit_seats</code> is a gauge vector holding
the upper bound on each priority level's dynamic concurrency limit.</p></li><li><p><code>apiserver_flowcontrol_demand_seats</code> is a histogram vector counting
observations, at the end of every nanosecond, of each priority
level's ratio of (seat demand) / (nominal concurrency limit).
A priority level's seat demand is the sum, over both queued requests
and those in the initial phase of execution, of the maximum of the
number of seats occupied in the request's initial and final
execution phases.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_high_watermark</code> is a gauge vector
holding, for each priority level, the maximum seat demand seen
during the last concurrency borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_average</code> is a gauge vector
holding, for each priority level, the time-weighted average seat
demand seen during the last concurrency borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_stdev</code> is a gauge vector
holding, for each priority level, the time-weighted population
standard deviation of seat demand seen during the last concurrency
borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_smoothed</code> is a gauge vector
holding, for each priority level, the smoothed enveloped seat demand
determined at the last concurrency adjustment.</p></li><li><p><code>apiserver_flowcontrol_target_seats</code> is a gauge vector holding, for
each priority level, the concurrency target going into the borrowing
allocation problem.</p></li><li><p><code>apiserver_flowcontrol_seat_fair_frac</code> is a gauge holding the fair
allocation fraction determined in the last borrowing adjustment.</p></li><li><p><code>apiserver_flowcontrol_current_limit_seats</code> is a gauge vector
holding, for each priority level, the dynamic concurrency limit
derived in the last adjustment.</p></li><li><p><code>apiserver_flowcontrol_request_execution_seconds</code> is a histogram
vector of how long requests took to actually execute, broken down by
<code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_watch_count_samples</code> is a histogram vector of
the number of active WATCH requests relevant to a given write,
broken down by <code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_work_estimated_seats</code> is a histogram vector
of the number of estimated seats (maximum of initial and final stage
of execution) associated with requests, broken down by <code>flow_schema</code>
and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_request_dispatch_no_accommodation_total</code> is a
counter vector of the number of events that in principle could have led
to a request being dispatched but did not, due to lack of available
concurrency, broken down by <code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_epoch_advance_total</code> is a counter vector of
the number of attempts to jump a priority level's progress meter
backward to avoid numeric overflow, grouped by <code>priority_level</code> and
<code>success</code>.</p></li></ul><h2 id="good-practices-for-using-api-priority-and-fairness">Good practices for using API Priority and Fairness</h2><p>When a given priority level exceeds its permitted concurrency, requests can
experience increased latency or be dropped with an HTTP 429 (Too Many Requests)
error. To prevent these side effects of APF, you can modify your workload or
tweak your APF settings to ensure there are sufficient seats available to serve
your requests.</p><p>To detect whether requests are being rejected due to APF, check the following
metrics:</p><ul><li>apiserver_flowcontrol_rejected_requests_total: the total number of requests
rejected per FlowSchema and PriorityLevelConfiguration.</li><li>apiserver_flowcontrol_current_inqueue_requests: the current number of requests
queued per FlowSchema and PriorityLevelConfiguration.</li><li>apiserver_flowcontrol_request_wait_duration_seconds: the latency added to
requests waiting in queues.</li><li>apiserver_flowcontrol_priority_level_seat_utilization: the seat utilization
per PriorityLevelConfiguration.</li></ul><h3 id="good-practice-workload-modifications">Workload modifications</h3><p>To prevent requests from queuing and adding latency or being dropped due to APF,
you can optimize your requests by:</p><ul><li>Reducing the rate at which requests are executed. A fewer number of requests
over a fixed period will result in a fewer number of seats being needed at a
given time.</li><li>Avoid issuing a large number of expensive requests concurrently. Requests can
be optimized to use fewer seats or have lower latency so that these requests
hold those seats for a shorter duration. List requests can occupy more than 1
seat depending on the number of objects fetched during the request. Restricting
the number of objects retrieved in a list request, for example by using
pagination, will use less total seats over a shorter period. Furthermore,
replacing list requests with watch requests will require lower total concurrency
shares as watch requests only occupy 1 seat during its initial burst of
notifications. If using streaming lists in versions 1.27 and later, watch
requests will occupy the same number of seats as a list request for its initial
burst of notifications because the entire state of the collection has to be
streamed. Note that in both cases, a watch request will not hold any seats after
this initial phase.</li></ul><p>Keep in mind that queuing or rejected requests from APF could be induced by
either an increase in the number of requests or an increase in latency for
existing requests. For example, if requests that normally take 1s to execute
start taking 60s, it is possible that APF will start rejecting requests because
requests are occupying seats for a longer duration than normal due to this
increase in latency. If APF starts rejecting requests across multiple priority
levels without a significant change in workload, it is possible there is an
underlying issue with control plane performance rather than the workload or APF
settings.</p><h3 id="good-practice-apf-settings">Priority and fairness settings</h3><p>You can also modify the default FlowSchema and PriorityLevelConfiguration
objects or create new objects of these types to better accommodate your
workload.</p><p>APF settings can be modified to:</p><ul><li>Give more seats to high priority requests.</li><li>Isolate non-essential or expensive requests that would starve a concurrency
level if it was shared with other flows.</li></ul><h4 id="give-more-seats-to-high-priority-requests">Give more seats to high priority requests</h4><ol><li>If possible, the number of seats available across all priority levels for a
particular <code>kube-apiserver</code> can be increased by increasing the values for the
<code>max-requests-inflight</code> and <code>max-mutating-requests-inflight</code> flags. Alternatively,
horizontally scaling the number of <code>kube-apiserver</code> instances will increase the
total concurrency per priority level across the cluster assuming there is
sufficient load balancing of requests.</li><li>You can create a new FlowSchema which references a PriorityLevelConfiguration
with a larger concurrency level. This new PriorityLevelConfiguration could be an
existing level or a new level with its own set of nominal concurrency shares.
For example, a new FlowSchema could be introduced to change the
PriorityLevelConfiguration for your requests from global-default to workload-low
to increase the number of seats available to your user. Creating a new
PriorityLevelConfiguration will reduce the number of seats designated for
existing levels. Recall that editing a default FlowSchema or
PriorityLevelConfiguration will require setting the
<code>apf.kubernetes.io/autoupdate-spec</code> annotation to false.</li><li>You can also increase the NominalConcurrencyShares for the
PriorityLevelConfiguration which is serving your high priority requests.
Alternatively, for versions 1.26 and later, you can increase the LendablePercent
for competing priority levels so that the given priority level has a higher pool
of seats it can borrow.</li></ol><h4 id="isolate-non-essential-requests-from-starving-other-flows">Isolate non-essential requests from starving other flows</h4><p>For request isolation, you can create a FlowSchema whose subject matches the
user making these requests or create a FlowSchema that matches what the request
is (corresponding to the resourceRules). Next, you can map this FlowSchema to a
PriorityLevelConfiguration with a low share of seats.</p><p>For example, suppose list event requests from Pods running in the default namespace
are using 10 seats each and execute for 1 minute. To prevent these expensive
requests from impacting requests from other Pods using the existing service-accounts
FlowSchema, you can apply the following FlowSchema to isolate these list calls
from other requests.</p><p>Example FlowSchema object to isolate list event requests:</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/priority-and-fairness/list-events-default-service-account.yaml"><code>priority-and-fairness/list-events-default-service-account.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy priority-and-fairness/list-events-default-service-account.yaml to clipboard"></div><div class="includecode" id="priority-and-fairness-list-events-default-service-account-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>flowcontrol.apiserver.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>FlowSchema<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>list-events-default-service-account<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>distinguisherMethod</span>:<span>
</span></span></span><span><span><span>    </span><span>type</span>:<span> </span>ByUser<span>
</span></span></span><span><span><span>  </span><span>matchingPrecedence</span>:<span> </span><span>8000</span><span>
</span></span></span><span><span><span>  </span><span>priorityLevelConfiguration</span>:<span>
</span></span></span><span><span><span>    </span><span>name</span>:<span> </span>catch-all<span>
</span></span></span><span><span><span>  </span><span>rules</span>:<span>
</span></span></span><span><span><span>    </span>- <span>resourceRules</span>:<span>
</span></span></span><span><span><span>      </span>- <span>apiGroups</span>:<span>
</span></span></span><span><span><span>          </span>- <span>'*'</span><span>
</span></span></span><span><span><span>        </span><span>namespaces</span>:<span>
</span></span></span><span><span><span>          </span>- default<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span>- events<span>
</span></span></span><span><span><span>        </span><span>verbs</span>:<span>
</span></span></span><span><span><span>          </span>- list<span>
</span></span></span><span><span><span>      </span><span>subjects</span>:<span>
</span></span></span><span><span><span>        </span>- <span>kind</span>:<span> </span>ServiceAccount<span>
</span></span></span><span><span><span>          </span><span>serviceAccount</span>:<span>
</span></span></span><span><span><span>            </span><span>name</span>:<span> </span>default<span>
</span></span></span><span><span><span>            </span><span>namespace</span>:<span> </span>default</span></span></code></pre></div></div></div><ul><li>This FlowSchema captures all list event calls made by the default service
account in the default namespace. The matching precedence 8000 is lower than the
value of 9000 used by the existing service-accounts FlowSchema so these list
event calls will match list-events-default-service-account rather than
service-accounts.</li><li>The catch-all PriorityLevelConfiguration is used to isolate these requests.
The catch-all priority level has a very small concurrency share and does not
queue requests.</li></ul><h2 id="what-s-next">What's next</h2><ul><li>You can visit flow control <a href="/docs/reference/debug-cluster/flow-control/">reference doc</a> to learn more about troubleshooting.</li><li>For background information on design details for API priority and fairness, see
the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness">enhancement proposal</a>.</li><li>You can make suggestions and feature requests via <a href="https://github.com/kubernetes/community/tree/master/sig-api-machinery">SIG API Machinery</a>
or the feature's <a href="https://kubernetes.slack.com/messages/api-priority-and-fairness">slack channel</a>.</li></ul></div></div><div><div class="td-content"><h1>Installing Addons</h1><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Add-ons extend the functionality of Kubernetes.</p><p>This page lists some of the available add-ons and links to their respective
installation instructions. The list does not try to be exhaustive.</p><h2 id="networking-and-network-policy">Networking and Network Policy</h2><ul><li><a href="https://www.github.com/noironetworks/aci-containers">ACI</a> provides integrated
container networking and network security with Cisco ACI.</li><li><a href="https://antrea.io/">Antrea</a> operates at Layer 3/4 to provide networking and
security services for Kubernetes, leveraging Open vSwitch as the networking
data plane. Antrea is a <a href="https://www.cncf.io/projects/antrea/">CNCF project at the Sandbox level</a>.</li><li><a href="https://www.tigera.io/project-calico/">Calico</a> is a networking and network
policy provider. Calico supports a flexible set of networking options so you
can choose the most efficient option for your situation, including non-overlay
and overlay networks, with or without BGP. Calico uses the same engine to
enforce network policy for hosts, pods, and (if using Istio &amp; Envoy)
applications at the service mesh layer.</li><li><a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/flannel/flannel">Canal</a>
unites Flannel and Calico, providing networking and network policy.</li><li><a href="https://github.com/cilium/cilium">Cilium</a> is a networking, observability,
and security solution with an eBPF-based data plane. Cilium provides a
simple flat Layer 3 network with the ability to span multiple clusters
in either a native routing or overlay/encapsulation mode, and can enforce
network policies on L3-L7 using an identity-based security model that is
decoupled from network addressing. Cilium can act as a replacement for
kube-proxy; it also offers additional, opt-in observability and security features.
Cilium is a <a href="https://www.cncf.io/projects/cilium/">CNCF project at the Graduated level</a>.</li><li><a href="https://github.com/cni-genie/CNI-Genie">CNI-Genie</a> enables Kubernetes to seamlessly
connect to a choice of CNI plugins, such as Calico, Canal, Flannel, or Weave.
CNI-Genie is a <a href="https://www.cncf.io/projects/cni-genie/">CNCF project at the Sandbox level</a>.</li><li><a href="https://contivpp.io/">Contiv</a> provides configurable networking (native L3 using BGP,
overlay using vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich
policy framework. Contiv project is fully <a href="https://github.com/contiv">open sourced</a>.
The <a href="https://github.com/contiv/install">installer</a> provides both kubeadm and
non-kubeadm based installation options.</li><li><a href="https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/">Contrail</a>,
based on <a href="https://tungsten.io">Tungsten Fabric</a>, is an open source, multi-cloud
network virtualization and policy management platform. Contrail and Tungsten
Fabric are integrated with orchestration systems such as Kubernetes, OpenShift,
OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods
and bare metal workloads.</li><li><a href="https://github.com/flannel-io/flannel#deploying-flannel-manually">Flannel</a> is
an overlay network provider that can be used with Kubernetes.</li><li><a href="/docs/concepts/services-networking/gateway/">Gateway API</a> is an open source project managed by
the <a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network</a> community and
provides an expressive, extensible, and role-oriented API for modeling service networking.</li><li><a href="https://github.com/ZTE/Knitter/">Knitter</a> is a plugin to support multiple network
interfaces in a Kubernetes pod.</li><li><a href="https://github.com/k8snetworkplumbingwg/multus-cni">Multus</a> is a Multi plugin for
multiple network support in Kubernetes to support all CNI plugins
(e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK and
VPP based workloads in Kubernetes.</li><li><a href="https://github.com/ovn-org/ovn-kubernetes/">OVN-Kubernetes</a> is a networking
provider for Kubernetes based on <a href="https://github.com/ovn-org/ovn/">OVN (Open Virtual Network)</a>,
a virtual networking implementation that came out of the Open vSwitch (OVS) project.
OVN-Kubernetes provides an overlay based networking implementation for Kubernetes,
including an OVS based implementation of load balancing and network policy.</li><li><a href="https://github.com/akraino-edge-stack/icn-nodus">Nodus</a> is an OVN based CNI
controller plugin to provide cloud native based Service function chaining(SFC).</li><li><a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html">NSX-T</a> Container Plug-in (NCP)
provides integration between VMware NSX-T and container orchestrators such as
Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS
platforms such as Pivotal Container Service (PKS) and OpenShift.</li><li><a href="https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst">Nuage</a>
is an SDN platform that provides policy-based networking between Kubernetes
Pods and non-Kubernetes environments with visibility and security monitoring.</li><li><a href="https://github.com/romana">Romana</a> is a Layer 3 networking solution for pod
networks that also supports the <a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy</a> API.</li><li><a href="https://github.com/spidernet-io/spiderpool">Spiderpool</a> is an underlay and RDMA
networking solution for Kubernetes. Spiderpool is supported on bare metal, virtual machines,
and public cloud environments.</li><li><a href="https://github.com/AliyunContainerService/terway/">Terway</a> is a suite of CNI plugins
based on AlibabaCloud's VPC and ECS network products. It provides native VPC networking
and network policies in AlibabaCloud environments.</li><li><a href="https://github.com/rajch/weave#using-weave-on-kubernetes">Weave Net</a>
provides networking and network policy, will carry on working on both sides
of a network partition, and does not require an external database.</li></ul><h2 id="service-discovery">Service Discovery</h2><ul><li><a href="https://coredns.io">CoreDNS</a> is a flexible, extensible DNS server which can
be <a href="https://github.com/coredns/helm">installed</a>
as the in-cluster DNS for pods.</li></ul><h2 id="visualization-amp-control">Visualization &amp; Control</h2><ul><li><a href="https://github.com/kubernetes/dashboard#kubernetes-dashboard">Dashboard</a>
is a dashboard web interface for Kubernetes.</li></ul><h2 id="infrastructure">Infrastructure</h2><ul><li><a href="https://kubevirt.io/user-guide/#/installation/installation">KubeVirt</a> is an add-on
to run virtual machines on Kubernetes. Usually run on bare-metal clusters.</li><li>The
<a href="https://github.com/kubernetes/node-problem-detector">node problem detector</a>
runs on Linux nodes and reports system issues as either
<a href="/docs/reference/kubernetes-api/cluster-resources/event-v1/">Events</a> or
<a href="/docs/concepts/architecture/nodes/#condition">Node conditions</a>.</li></ul><h2 id="instrumentation">Instrumentation</h2><ul><li><a href="/docs/concepts/cluster-administration/kube-state-metrics/">kube-state-metrics</a></li></ul><h2 id="legacy-add-ons">Legacy Add-ons</h2><p>There are several other add-ons documented in the deprecated
<a href="https://git.k8s.io/kubernetes/cluster/addons">cluster/addons</a> directory.</p><p>Well-maintained ones should be linked to here. PRs welcome!</p></div></div><div><div class="td-content"><h1>Coordinated Leader Election</h1><div class="feature-state-notice feature-beta" title="Feature Gate: CoordinatedLeaderElection"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.33 [beta]</code> (enabled by default: false)</div><p>Kubernetes 1.34 includes a beta feature that allows <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> components to
deterministically select a leader via <em>coordinated leader election</em>.
This is useful to satisfy Kubernetes version skew constraints during cluster upgrades.
Currently, the only builtin selection strategy is <code>OldestEmulationVersion</code>,
preferring the leader with the lowest emulation version, followed by binary
version, followed by creation timestamp.</p><h2 id="enabling-coordinated-leader-election">Enabling coordinated leader election</h2><p>Ensure that <code>CoordinatedLeaderElection</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature
gate</a> is enabled
when you start the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">API Server</a>: and that the <code>coordination.k8s.io/v1beta1</code> API group is
enabled.</p><p>This can be done by setting flags <code>--feature-gates="CoordinatedLeaderElection=true"</code> and
<code>--runtime-config="coordination.k8s.io/v1beta1=true"</code>.</p><h2 id="component-configuration">Component configuration</h2><p>Provided that you have enabled the <code>CoordinatedLeaderElection</code> feature gate <em>and</em><br>have the <code>coordination.k8s.io/v1beta1</code> API group enabled, compatible control plane<br>components automatically use the LeaseCandidate and Lease APIs to elect a leader<br>as needed.</p><p>For Kubernetes 1.34, two control plane components<br>(kube-controller-manager and kube-scheduler) automatically use coordinated<br>leader election when the feature gate and API group are enabled.</p></div></div><div><div class="td-content"><h1>Windows in Kubernetes</h1><div class="lead">Kubernetes supports nodes that run Microsoft Windows.</div><p>Kubernetes supports worker <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">nodes</a>
running either Linux or Microsoft Windows.</p><div class="alert alert-secondary callout third-party-content">&#128711; This item links to a third party project or product that is not part of Kubernetes itself. <a class="alert-more-info" href="#third-party-content-disclaimer">More information</a></div><p>The CNCF and its parent the Linux Foundation take a vendor-neutral approach
towards compatibility. It is possible to join your <a href="https://www.microsoft.com/en-us/windows-server">Windows server</a>
as a worker node to a Kubernetes cluster.</p><p>You can <a href="/docs/tasks/tools/install-kubectl-windows/">install and set up kubectl on Windows</a>
no matter what operating system you use within your cluster.</p><p>If you are using Windows nodes, you can read:</p><ul><li><a href="/docs/concepts/services-networking/windows-networking/">Networking On Windows</a></li><li><a href="/docs/concepts/storage/windows-storage/">Windows Storage In Kubernetes</a></li><li><a href="/docs/concepts/configuration/windows-resource-management/">Resource Management for Windows Nodes</a></li><li><a href="/docs/tasks/configure-pod-container/configure-runasusername/">Configure RunAsUserName for Windows Pods and Containers</a></li><li><a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create A Windows HostProcess Pod</a></li><li><a href="/docs/tasks/configure-pod-container/configure-gmsa/">Configure Group Managed Service Accounts for Windows Pods and Containers</a></li><li><a href="/docs/concepts/security/windows-security/">Security For Windows Nodes</a></li><li><a href="/docs/tasks/debug/debug-cluster/windows/">Windows Debugging Tips</a></li><li><a href="/docs/concepts/windows/user-guide/">Guide for Scheduling Windows Containers in Kubernetes</a></li></ul><p>or, for an overview, read:</p><div class="section-index"><ul><li><a href="/docs/concepts/windows/intro/">Windows containers in Kubernetes</a></li><li><a href="/docs/concepts/windows/user-guide/">Guide for Running Windows Containers in Kubernetes</a></li></ul></div></div></div><div><div class="td-content"><h1>Windows containers in Kubernetes</h1><p>Windows applications constitute a large portion of the services and applications that
run in many organizations. <a href="https://aka.ms/windowscontainers">Windows containers</a>
provide a way to encapsulate processes and package dependencies, making it easier
to use DevOps practices and follow cloud native patterns for Windows applications.</p><p>Organizations with investments in Windows-based applications and Linux-based
applications don't have to look for separate orchestrators to manage their workloads,
leading to increased operational efficiencies across their deployments, regardless
of operating system.</p><h2 id="windows-nodes-in-kubernetes">Windows nodes in Kubernetes</h2><p>To enable the orchestration of Windows containers in Kubernetes, include Windows nodes
in your existing Linux cluster. Scheduling Windows containers in
<a class="glossary-tooltip" title="A Pod represents a set of running containers in your cluster." href="/docs/concepts/workloads/pods/" target="_blank">Pods</a> on Kubernetes is similar to
scheduling Linux-based containers.</p><p>In order to run Windows containers, your Kubernetes cluster must include
multiple operating systems.
While you can only run the <a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a> on Linux,
you can deploy worker nodes running either Windows or Linux.</p><p>Windows <a class="glossary-tooltip" title="A node is a worker machine in Kubernetes." href="/docs/concepts/architecture/nodes/" target="_blank">nodes</a> are
<a href="#windows-os-version-support">supported</a> provided that the operating system is
Windows Server 2019 or Windows Server 2022.</p><p>This document uses the term <em>Windows containers</em> to mean Windows containers with
process isolation. Kubernetes does not support running Windows containers with
<a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container">Hyper-V isolation</a>.</p><h2 id="limitations">Compatibility and limitations</h2><p>Some node features are only available if you use a specific
<a href="#container-runtime">container runtime</a>; others are not available on Windows nodes,
including:</p><ul><li>HugePages: not supported for Windows containers</li><li>Privileged containers: not supported for Windows containers.
<a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">HostProcess Containers</a> offer similar functionality.</li><li>TerminationGracePeriod: requires containerD</li></ul><p>Not all features of shared namespaces are supported. See <a href="#api">API compatibility</a>
for more details.</p><p>See <a href="#windows-os-version-support">Windows OS version compatibility</a> for details on
the Windows versions that Kubernetes is tested against.</p><p>From an API and kubectl perspective, Windows containers behave in much the same
way as Linux-based containers. However, there are some notable differences in key
functionality which are outlined in this section.</p><h3 id="compatibility-linux-similarities">Comparison with Linux</h3><p>Key Kubernetes elements work the same way in Windows as they do in Linux. This
section refers to several key workload abstractions and how they map to Windows.</p><ul><li><p><a href="/docs/concepts/workloads/pods/">Pods</a></p><p>A Pod is the basic building block of Kubernetes&#8211;the smallest and simplest unit in
the Kubernetes object model that you create or deploy. You may not deploy Windows and
Linux containers in the same Pod. All containers in a Pod are scheduled onto a single
Node where each Node represents a specific platform and architecture. The following
Pod capabilities, properties and events are supported with Windows containers:</p><ul><li><p>Single or multiple containers per Pod with process isolation and volume sharing</p></li><li><p>Pod <code>status</code> fields</p></li><li><p>Readiness, liveness, and startup probes</p></li><li><p>postStart &amp; preStop container lifecycle hooks</p></li><li><p>ConfigMap, Secrets: as environment variables or volumes</p></li><li><p><code>emptyDir</code> volumes</p></li><li><p>Named pipe host mounts</p></li><li><p>Resource limits</p></li><li><p>OS field:</p><p>The <code>.spec.os.name</code> field should be set to <code>windows</code> to indicate that the current Pod uses Windows containers.</p><p>If you set the <code>.spec.os.name</code> field to <code>windows</code>,
you must not set the following fields in the <code>.spec</code> of that Pod:</p><ul><li><code>spec.hostPID</code></li><li><code>spec.hostIPC</code></li><li><code>spec.securityContext.seLinuxOptions</code></li><li><code>spec.securityContext.seccompProfile</code></li><li><code>spec.securityContext.fsGroup</code></li><li><code>spec.securityContext.fsGroupChangePolicy</code></li><li><code>spec.securityContext.sysctls</code></li><li><code>spec.shareProcessNamespace</code></li><li><code>spec.securityContext.runAsUser</code></li><li><code>spec.securityContext.runAsGroup</code></li><li><code>spec.securityContext.supplementalGroups</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions</code></li><li><code>spec.containers[*].securityContext.seccompProfile</code></li><li><code>spec.containers[*].securityContext.capabilities</code></li><li><code>spec.containers[*].securityContext.readOnlyRootFilesystem</code></li><li><code>spec.containers[*].securityContext.privileged</code></li><li><code>spec.containers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.containers[*].securityContext.procMount</code></li><li><code>spec.containers[*].securityContext.runAsUser</code></li><li><code>spec.containers[*].securityContext.runAsGroup</code></li></ul><p>In the above list, wildcards (<code>*</code>) indicate all elements in a list.
For example, <code>spec.containers[*].securityContext</code> refers to the SecurityContext object
for all containers. If any of these fields is specified, the Pod will
not be admitted by the API server.</p></li></ul></li><li><p><a href="/docs/concepts/workloads/controllers/">Workload resources</a> including:</p><ul><li>ReplicaSet</li><li>Deployment</li><li>StatefulSet</li><li>DaemonSet</li><li>Job</li><li>CronJob</li><li>ReplicationController</li></ul></li><li><p><a class="glossary-tooltip" title="A way to expose an application running on a set of Pods as a network service." href="/docs/concepts/services-networking/service/" target="_blank">Services</a>
See <a href="/docs/concepts/services-networking/windows-networking/#load-balancing-and-services">Load balancing and Services</a> for more details.</p></li></ul><p>Pods, workload resources, and Services are critical elements to managing Windows
workloads on Kubernetes. However, on their own they are not enough to enable
the proper lifecycle management of Windows workloads in a dynamic cloud native
environment.</p><ul><li><code>kubectl exec</code></li><li>Pod and container metrics</li><li><a class="glossary-tooltip" title="Object that automatically scales the number of pod replicas based on targeted resource utilization or custom metric targets." href="/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank">Horizontal pod autoscaling</a></li><li><a class="glossary-tooltip" title="Provides constraints that limit aggregate resource consumption per namespace." href="/docs/concepts/policy/resource-quotas/" target="_blank">Resource quotas</a></li><li>Scheduler preemption</li></ul><h3 id="kubelet-compatibility">Command line options for the kubelet</h3><p>Some kubelet command line options behave differently on Windows, as described below:</p><ul><li>The <code>--windows-priorityclass</code> lets you set the scheduling priority of the kubelet process
(see <a href="/docs/concepts/configuration/windows-resource-management/#resource-management-cpu">CPU resource management</a>)</li><li>The <code>--kube-reserved</code>, <code>--system-reserved</code> , and <code>--eviction-hard</code> flags update
<a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">NodeAllocatable</a></li><li>Eviction by using <code>--enforce-node-allocable</code> is not implemented</li><li>When running on a Windows node the kubelet does not have memory or CPU
restrictions. <code>--kube-reserved</code> and <code>--system-reserved</code> only subtract from <code>NodeAllocatable</code>
and do not guarantee resource provided for workloads.
See <a href="/docs/concepts/configuration/windows-resource-management/#resource-reservation">Resource Management for Windows nodes</a>
for more information.</li><li>The <code>PIDPressure</code> Condition is not implemented</li><li>The kubelet does not take OOM eviction actions</li></ul><h3 id="api">API compatibility</h3><p>There are subtle differences in the way the Kubernetes APIs work for Windows due to the OS
and container runtime. Some workload properties were designed for Linux, and fail to run on Windows.</p><p>At a high level, these OS concepts are different:</p><ul><li>Identity - Linux uses userID (UID) and groupID (GID) which
are represented as integer types. User and group names
are not canonical - they are just an alias in <code>/etc/groups</code>
or <code>/etc/passwd</code> back to UID+GID. Windows uses a larger binary
<a href="https://docs.microsoft.com/en-us/windows/security/identity-protection/access-control/security-identifiers">security identifier</a> (SID)
which is stored in the Windows Security Access Manager (SAM) database. This
database is not shared between the host and containers, or between containers.</li><li>File permissions - Windows uses an access control list based on (SIDs), whereas
POSIX systems such as Linux use a bitmask based on object permissions and UID+GID,
plus <em>optional</em> access control lists.</li><li>File paths - the convention on Windows is to use <code>\</code> instead of <code>/</code>. The Go IO
libraries typically accept both and just make it work, but when you're setting a
path or command line that's interpreted inside a container, <code>\</code> may be needed.</li><li>Signals - Windows interactive apps handle termination differently, and can
implement one or more of these:<ul><li>A UI thread handles well-defined messages including <code>WM_CLOSE</code>.</li><li>Console apps handle Ctrl-C or Ctrl-break using a Control Handler.</li><li>Services register a Service Control Handler function that can accept
<code>SERVICE_CONTROL_STOP</code> control codes.</li></ul></li></ul><p>Container exit codes follow the same convention where 0 is success, and nonzero is failure.
The specific error codes may differ across Windows and Linux. However, exit codes
passed from the Kubernetes components (kubelet, kube-proxy) are unchanged.</p><h4 id="compatibility-v1-pod-spec-containers">Field compatibility for container specifications</h4><p>The following list documents differences between how Pod container specifications
work between Windows and Linux:</p><ul><li>Huge pages are not implemented in the Windows container
runtime, and are not available. They require <a href="https://docs.microsoft.com/en-us/windows/desktop/Memory/large-page-support">asserting a user
privilege</a>
that's not configurable for containers.</li><li><code>requests.cpu</code> and <code>requests.memory</code> - requests are subtracted
from node available resources, so they can be used to avoid overprovisioning a
node. However, they cannot be used to guarantee resources in an overprovisioned
node. They should be applied to all containers as a best practice if the operator
wants to avoid overprovisioning entirely.</li><li><code>securityContext.allowPrivilegeEscalation</code> -
not possible on Windows; none of the capabilities are hooked up</li><li><code>securityContext.capabilities</code> -
POSIX capabilities are not implemented on Windows</li><li><code>securityContext.privileged</code> -
Windows doesn't support privileged containers, use <a href="/docs/tasks/configure-pod-container/create-hostprocess-pod/">HostProcess Containers</a> instead</li><li><code>securityContext.procMount</code> -
Windows doesn't have a <code>/proc</code> filesystem</li><li><code>securityContext.readOnlyRootFilesystem</code> -
not possible on Windows; write access is required for registry &amp; system
processes to run inside the container</li><li><code>securityContext.runAsGroup</code> -
not possible on Windows as there is no GID support</li><li><code>securityContext.runAsNonRoot</code> -
this setting will prevent containers from running as <code>ContainerAdministrator</code>
which is the closest equivalent to a root user on Windows.</li><li><code>securityContext.runAsUser</code> -
use <a href="/docs/tasks/configure-pod-container/configure-runasusername/"><code>runAsUserName</code></a>
instead</li><li><code>securityContext.seLinuxOptions</code> -
not possible on Windows as SELinux is Linux-specific</li><li><code>terminationMessagePath</code> -
this has some limitations in that Windows doesn't support mapping single files. The
default value is <code>/dev/termination-log</code>, which does work because it does not
exist on Windows by default.</li></ul><h4 id="compatibility-v1-pod">Field compatibility for Pod specifications</h4><p>The following list documents differences between how Pod specifications work between Windows and Linux:</p><ul><li><code>hostIPC</code> and <code>hostpid</code> - host namespace sharing is not possible on Windows</li><li><code>hostNetwork</code> - host networking is not possible on Windows</li><li><code>dnsPolicy</code> - setting the Pod <code>dnsPolicy</code> to <code>ClusterFirstWithHostNet</code> is
not supported on Windows because host networking is not provided. Pods always
run with a container network.</li><li><code>podSecurityContext</code> <a href="#compatibility-v1-pod-spec-containers-securitycontext">see below</a></li><li><code>shareProcessNamespace</code> - this is a beta feature, and depends on Linux namespaces
which are not implemented on Windows. Windows cannot share process namespaces or
the container's root filesystem. Only the network can be shared.</li><li><code>terminationGracePeriodSeconds</code> - this is not fully implemented in Docker on Windows,
see the <a href="https://github.com/moby/moby/issues/25982">GitHub issue</a>.
The behavior today is that the ENTRYPOINT process is sent CTRL_SHUTDOWN_EVENT,
then Windows waits 5 seconds by default, and finally shuts down
all processes using the normal Windows shutdown behavior. The 5
second default is actually in the Windows registry
<a href="https://github.com/moby/moby/issues/25982#issuecomment-426441183">inside the container</a>,
so it can be overridden when the container is built.</li><li><code>volumeDevices</code> - this is a beta feature, and is not implemented on Windows.
Windows cannot attach raw block devices to pods.</li><li><code>volumes</code><ul><li>If you define an <code>emptyDir</code> volume, you cannot set its volume source to <code>memory</code>.</li></ul></li><li>You cannot enable <code>mountPropagation</code> for volume mounts as this is not
supported on Windows.</li></ul><h4 id="compatibility-v1-pod-sec-containers-hostnetwork">Host network access</h4><p>Kubernetes v1.26 to v1.32 included alpha support for running Windows Pods in the host's network namespace.</p><p>Kubernetes v1.34 does <strong>not</strong> include the <code>WindowsHostNetwork</code> feature gate
or support for running Windows Pods in the host's network namespace.</p><h4 id="compatibility-v1-pod-spec-containers-securitycontext">Field compatibility for Pod security context</h4><p>Only the <code>securityContext.runAsNonRoot</code> and <code>securityContext.windowsOptions</code> from the Pod
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context"><code>securityContext</code></a> fields work on Windows.</p><h2 id="node-problem-detector">Node problem detector</h2><p>The node problem detector (see
<a href="/docs/tasks/debug/debug-cluster/monitor-node-health/">Monitor Node Health</a>)
has preliminary support for Windows.
For more information, visit the project's <a href="https://github.com/kubernetes/node-problem-detector#windows">GitHub page</a>.</p><h2 id="pause-container">Pause container</h2><p>In a Kubernetes Pod, an infrastructure or &#8220;pause&#8221; container is first created
to host the container. In Linux, the cgroups and namespaces that make up a pod
need a process to maintain their continued existence; the pause process provides
this. Containers that belong to the same pod, including infrastructure and worker
containers, share a common network endpoint (same IPv4 and / or IPv6 address, same
network port spaces). Kubernetes uses pause containers to allow for worker containers
crashing or restarting without losing any of the networking configuration.</p><p>Kubernetes maintains a multi-architecture image that includes support for Windows.
For Kubernetes v1.34.0 the recommended pause image is <code>registry.k8s.io/pause:3.6</code>.
The <a href="https://github.com/kubernetes/kubernetes/tree/master/build/pause">source code</a>
is available on GitHub.</p><p>Microsoft maintains a different multi-architecture image, with Linux and Windows
amd64 support, that you can find as <code>mcr.microsoft.com/oss/kubernetes/pause:3.6</code>.
This image is built from the same source as the Kubernetes maintained image but
all of the Windows binaries are <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/install/authenticode">authenticode signed</a> by Microsoft.
The Kubernetes project recommends using the Microsoft maintained image if you are
deploying to a production or production-like environment that requires signed
binaries.</p><h2 id="container-runtime">Container runtimes</h2><p>You need to install a
<a class="glossary-tooltip" title="The container runtime is the software that is responsible for running containers." href="/docs/setup/production-environment/container-runtimes" target="_blank">container runtime</a>
into each node in the cluster so that Pods can run there.</p><p>The following container runtimes work with Windows:</p><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><h3 id="containerd">ContainerD</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.20 [stable]</code></div><p>You can use <a class="glossary-tooltip" title="A container runtime with an emphasis on simplicity, robustness and portability" href="https://containerd.io/docs/" target="_blank">ContainerD</a> 1.4.0+
as the container runtime for Kubernetes nodes that run Windows.</p><p>Learn how to <a href="/docs/setup/production-environment/container-runtimes/#containerd">install ContainerD on a Windows node</a>.<div class="alert alert-info"><h4 class="alert-heading">Note:</h4>There is a <a href="/docs/tasks/configure-pod-container/configure-gmsa/#gmsa-limitations">known limitation</a>
when using GMSA with containerd to access Windows network shares, which requires a
kernel patch.</div></p><h3 id="mcr">Mirantis Container Runtime</h3><p><a href="https://docs.mirantis.com/mcr/25.0/overview.html">Mirantis Container Runtime</a> (MCR)
is available as a container runtime for all Windows Server 2019 and later versions.</p><p>See <a href="https://docs.mirantis.com/mcr/25.0/install/mcr-windows.html">Install MCR on Windows Servers</a> for more information.</p><h2 id="windows-os-version-support">Windows OS version compatibility</h2><p>On Windows nodes, strict compatibility rules apply where the host OS version must
match the container base image OS version. Only Windows containers with a container
operating system of Windows Server 2019 are fully supported.</p><p>For Kubernetes v1.34, operating system compatibility for Windows nodes (and Pods)
is as follows:</p><dl><dt>Windows Server LTSC release</dt><dd>Windows Server 2019</dd><dd>Windows Server 2022</dd><dt>Windows Server SAC release</dt><dd>Windows Server version 20H2</dd></dl><p>The Kubernetes <a href="/docs/setup/release/version-skew-policy/">version-skew policy</a> also applies.</p><h2 id="windows-hardware-recommendations">Hardware recommendations and considerations</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The following hardware specifications outlined here should be regarded as sensible default values.
They are not intended to represent minimum requirements or specific recommendations for production environments.
Depending on the requirements for your workload these values may need to be adjusted.</div><ul><li>64-bit processor 4 CPU cores or more, capable of supporting virtualization</li><li>8GB or more of RAM</li><li>50GB or more of free disk space</li></ul><p>Refer to
<a href="https://learn.microsoft.com/en-us/windows-server/get-started/hardware-requirements">Hardware requirements for Windows Server Microsoft documentation</a>
for the most up-to-date information on minimum hardware requirements. For guidance on deciding on resources for
production worker nodes refer to <a href="/docs/setup/production-environment/#production-worker-nodes">Production worker nodes Kubernetes documentation</a>.</p><p>To optimize system resources, if a graphical user interface is not required,
it may be preferable to use a Windows Server OS installation that excludes
the <a href="https://learn.microsoft.com/en-us/windows-server/get-started/install-options-server-core-desktop-experience">Windows Desktop Experience</a>
installation option, as this configuration typically frees up more system
resources.</p><p>In assessing disk space for Windows worker nodes, take note that Windows container images are typically larger than
Linux container images, with container image sizes ranging
from <a href="https://techcommunity.microsoft.com/t5/containers/nano-server-x-server-core-x-server-which-base-image-is-the-right/ba-p/2835785">300MB to over 10GB</a>
for a single image. Additionally, take note that the <code>C:</code> drive in Windows containers represents a virtual free size of
20GB by default, which is not the actual consumed space, but rather the disk size for which a single container can grow
to occupy when using local storage on the host.
See <a href="https://learn.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-storage#storage-limits">Containers on Windows - Container Storage Documentation</a>
for more detail.</p><h2 id="troubleshooting">Getting help and troubleshooting</h2><p>Your main source of help for troubleshooting your Kubernetes cluster should start
with the <a href="/docs/tasks/debug/">Troubleshooting</a>
page.</p><p>Some additional, Windows-specific troubleshooting help is included
in this section. Logs are an important element of troubleshooting
issues in Kubernetes. Make sure to include them any time you seek
troubleshooting assistance from other contributors. Follow the
instructions in the
SIG Windows <a href="https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs">contributing guide on gathering logs</a>.</p><h3 id="reporting-issues-and-feature-requests">Reporting issues and feature requests</h3><p>If you have what looks like a bug, or you would like to
make a feature request, please follow the <a href="https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#reporting-issues-and-feature-requests">SIG Windows contributing guide</a> to create a new issue.
You should first search the list of issues in case it was
reported previously and comment with your experience on the issue and add additional
logs. SIG Windows channel on the Kubernetes Slack is also a great avenue to get some initial support and
troubleshooting ideas prior to creating a ticket.</p><h3 id="validating-the-windows-cluster-operability">Validating the Windows cluster operability</h3><p>The Kubernetes project provides a <em>Windows Operational Readiness</em> specification,
accompanied by a structured test suite. This suite is split into two sets of tests,
core and extended, each containing categories aimed at testing specific areas.
It can be used to validate all the functionalities of a Windows and hybrid system
(mixed with Linux nodes) with full coverage.</p><p>To set up the project on a newly created cluster, refer to the instructions in the
<a href="https://github.com/kubernetes-sigs/windows-operational-readiness/blob/main/README.md">project guide</a>.</p><h2 id="deployment-tools">Deployment tools</h2><p>The kubeadm tool helps you to deploy a Kubernetes cluster, providing the control
plane to manage the cluster it, and nodes to run your workloads.</p><p>The Kubernetes <a href="https://cluster-api.sigs.k8s.io/">cluster API</a> project also provides means to automate deployment of Windows nodes.</p><h2 id="windows-distribution-channels">Windows distribution channels</h2><p>For a detailed explanation of Windows distribution channels see the
<a href="https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19">Microsoft documentation</a>.</p><p>Information on the different Windows Server servicing channels
including their support models can be found at
<a href="https://docs.microsoft.com/en-us/windows-server/get-started/servicing-channels-comparison">Windows Server servicing channels</a>.</p></div></div><div><div class="td-content"><h1>Guide for Running Windows Containers in Kubernetes</h1><p>This page provides a walkthrough for some steps you can follow to run
Windows containers using Kubernetes.
The page also highlights some Windows specific functionality within Kubernetes.</p><p>It is important to note that creating and deploying services and workloads on Kubernetes
behaves in much the same way for Linux and Windows containers.
The <a href="/docs/reference/kubectl/">kubectl commands</a> to interface with the cluster are identical.
The examples in this page are provided to jumpstart your experience with Windows containers.</p><h2 id="objectives">Objectives</h2><p>Configure an example deployment to run Windows containers on a Windows node.</p><h2 id="before-you-begin">Before you begin</h2><p>You should already have access to a Kubernetes cluster that includes a
worker node running Windows Server.</p><h2 id="getting-started-deploying-a-windows-workload">Getting Started: Deploying a Windows workload</h2><p>The example YAML file below deploys a simple webserver application running inside a Windows container.</p><p>Create a manifest named <code>win-webserver.yaml</code> with the contents below:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>win-webserver<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>win-webserver<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>    </span><span># the port that this service should serve on</span><span>
</span></span></span><span><span><span>    </span>- <span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>      </span><span>targetPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>win-webserver<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>NodePort<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>win-webserver<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>win-webserver<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>win-webserver<span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>win-webserver<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>win-webserver<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>     </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>windowswebserver<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span>
</span></span></span><span><span><span>        </span><span>command</span>:<span>
</span></span></span><span><span><span>        </span>- powershell.exe<span>
</span></span></span><span><span><span>        </span>- -command<span>
</span></span></span><span><span><span>        </span>- <span>"&lt;#code used from https://gist.github.com/19WAS85/5424431#&gt; ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add('http://*:80/') ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host('Listening at http://*:80/') ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host '' ;Write-Host('&gt; {0}' -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header='&lt;html&gt;&lt;body&gt;&lt;H1&gt;Windows Container Web Server&lt;/H1&gt;' ;$$callerCountsString='' ;$$callerCounts.Keys | % { $$callerCountsString+='&lt;p&gt;IP {0} callerCount {1} ' -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer='&lt;/body&gt;&lt;/html&gt;' ;$$content='{0}{1}{2}' -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host('&lt; {0}' -f $$responseStatus)  } ; "</span><span>
</span></span></span><span><span><span>     </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>      </span><span>kubernetes.io/os</span>:<span> </span>windows<span>
</span></span></span></code></pre></div><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Port mapping is also supported, but for simplicity this example exposes
port 80 of the container directly to the Service.</div><ol><li><p>Check that all nodes are healthy:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl get nodes
</span></span></code></pre></div></li><li><p>Deploy the service and watch for pod updates:</p><div class="highlight"><pre tabindex="0"><code class="language-bash"><span><span>kubectl apply -f win-webserver.yaml
</span></span><span><span>kubectl get pods -o wide -w
</span></span></code></pre></div><p>When the service is deployed correctly both Pods are marked as Ready. To exit the watch command, press Ctrl+C.</p></li><li><p>Check that the deployment succeeded. To verify:</p><ul><li>Several pods listed from the Linux control plane node, use <code>kubectl get pods</code></li><li>Node-to-pod communication across the network, <code>curl</code> port 80 of your pod IPs from the Linux control plane node
to check for a web server response</li><li>Pod-to-pod communication, ping between pods (and across hosts, if you have more than one Windows node)
using <code>kubectl exec</code></li><li>Service-to-pod communication, <code>curl</code> the virtual service IP (seen under <code>kubectl get services</code>)
from the Linux control plane node and from individual pods</li><li>Service discovery, <code>curl</code> the service name with the Kubernetes <a href="/docs/concepts/services-networking/dns-pod-service/#services">default DNS suffix</a></li><li>Inbound connectivity, <code>curl</code> the NodePort from the Linux control plane node or machines outside of the cluster</li><li>Outbound connectivity, <code>curl</code> external IPs from inside the pod using <code>kubectl exec</code></li></ul></li></ol><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Windows container hosts are not able to access the IP of services scheduled on them due to current platform limitations of the Windows networking stack.
Only Windows pods are able to access service IPs.</div><h2 id="observability">Observability</h2><h3 id="capturing-logs-from-workloads">Capturing logs from workloads</h3><p>Logs are an important element of observability; they enable users to gain insights
into the operational aspect of workloads and are a key ingredient to troubleshooting issues.
Because Windows containers and workloads inside Windows containers behave differently from Linux containers,
users had a hard time collecting logs, limiting operational visibility.
Windows workloads for example are usually configured to log to ETW (Event Tracing for Windows)
or push entries to the application event log.
<a href="https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor">LogMonitor</a>, an open source tool by Microsoft,
is the recommended way to monitor configured log sources inside a Windows container.
LogMonitor supports monitoring event logs, ETW providers, and custom application logs,
piping them to STDOUT for consumption by <code>kubectl logs &lt;pod&gt;</code>.</p><p>Follow the instructions in the LogMonitor GitHub page to copy its binaries and configuration files
to all your containers and add the necessary entrypoints for LogMonitor to push your logs to STDOUT.</p><h2 id="configuring-container-user">Configuring container user</h2><h3 id="using-configurable-container-usernames">Using configurable Container usernames</h3><p>Windows containers can be configured to run their entrypoints and processes
with different usernames than the image defaults.
Learn more about it <a href="/docs/tasks/configure-pod-container/configure-runasusername/">here</a>.</p><h3 id="managing-workload-identity-with-group-managed-service-accounts">Managing Workload Identity with Group Managed Service Accounts</h3><p>Windows container workloads can be configured to use Group Managed Service Accounts (GMSA).
Group Managed Service Accounts are a specific type of Active Directory account that provide automatic password management,
simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.
Containers configured with a GMSA can access external Active Directory Domain resources while carrying the identity configured with the GMSA.
Learn more about configuring and using GMSA for Windows containers <a href="/docs/tasks/configure-pod-container/configure-gmsa/">here</a>.</p><h2 id="taints-and-tolerations">Taints and tolerations</h2><p>Users need to use some combination of <a class="glossary-tooltip" title="A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups." href="/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank">taint</a>
and node selectors in order to schedule Linux and Windows workloads to their respective OS-specific nodes.
The recommended approach is outlined below,
with one of its main goals being that this approach should not break compatibility for existing Linux workloads.</p><p>You can (and should) set <code>.spec.os.name</code> for each Pod, to indicate the operating system
that the containers in that Pod are designed for. For Pods that run Linux containers, set
<code>.spec.os.name</code> to <code>linux</code>. For Pods that run Windows containers, set <code>.spec.os.name</code>
to <code>windows</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>If you are running a version of Kubernetes older than 1.24, you may need to enable
the <code>IdentifyPodOS</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
to be able to set a value for <code>.spec.pod.os</code>.</div><p>The scheduler does not use the value of <code>.spec.os.name</code> when assigning Pods to nodes. You should
use normal Kubernetes mechanisms for
<a href="/docs/concepts/scheduling-eviction/assign-pod-node/">assigning pods to nodes</a>
to ensure that the control plane for your cluster places pods onto nodes that are running the
appropriate operating system.</p><p>The <code>.spec.os.name</code> value has no effect on the scheduling of the Windows pods,
so taints and tolerations (or node selectors) are still required
to ensure that the Windows pods land onto appropriate Windows nodes.</p><h3 id="ensuring-os-specific-workloads-land-on-the-appropriate-container-host">Ensuring OS-specific workloads land on the appropriate container host</h3><p>Users can ensure Windows containers can be scheduled on the appropriate host using taints and tolerations.
All Kubernetes nodes running Kubernetes 1.34 have the following default labels:</p><ul><li>kubernetes.io/os = [windows|linux]</li><li>kubernetes.io/arch = [amd64|arm64|...]</li></ul><p>If a Pod specification does not specify a <code>nodeSelector</code> such as <code>"kubernetes.io/os": windows</code>,
it is possible the Pod can be scheduled on any host, Windows or Linux.
This can be problematic since a Windows container can only run on Windows and a Linux container can only run on Linux.
The best practice for Kubernetes 1.34 is to use a <code>nodeSelector</code>.</p><p>However, in many cases users have a pre-existing large number of deployments for Linux containers,
as well as an ecosystem of off-the-shelf configurations, such as community Helm charts, and programmatic Pod generation cases, such as with operators.
In those situations, you may be hesitant to make the configuration change to add <code>nodeSelector</code> fields to all Pods and Pod templates.
The alternative is to use taints. Because the kubelet can set taints during registration,
it could easily be modified to automatically add a taint when running on Windows only.</p><p>For example: <code>--register-with-taints='os=windows:NoSchedule'</code></p><p>By adding a taint to all Windows nodes, nothing will be scheduled on them (that includes existing Linux Pods).
In order for a Windows Pod to be scheduled on a Windows node,
it would need both the <code>nodeSelector</code> and the appropriate matching toleration to choose Windows.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/os</span>:<span> </span>windows<span>
</span></span></span><span><span><span>    </span><span>node.kubernetes.io/windows-build</span>:<span> </span><span>'10.0.17763'</span><span>
</span></span></span><span><span><span></span><span>tolerations</span>:<span>
</span></span></span><span><span><span>    </span>- <span>key</span>:<span> </span><span>"os"</span><span>
</span></span></span><span><span><span>      </span><span>operator</span>:<span> </span><span>"Equal"</span><span>
</span></span></span><span><span><span>      </span><span>value</span>:<span> </span><span>"windows"</span><span>
</span></span></span><span><span><span>      </span><span>effect</span>:<span> </span><span>"NoSchedule"</span><span>
</span></span></span></code></pre></div><h3 id="handling-multiple-windows-versions-in-the-same-cluster">Handling multiple Windows versions in the same cluster</h3><p>The Windows Server version used by each pod must match that of the node. If you want to use multiple Windows
Server versions in the same cluster, then you should set additional node labels and <code>nodeSelector</code> fields.</p><p>Kubernetes automatically adds a label,
<a href="/docs/reference/labels-annotations-taints/#nodekubernetesiowindows-build"><code>node.kubernetes.io/windows-build</code></a>
to simplify this.</p><p>This label reflects the Windows major, minor, and build number that need to match for compatibility.
Here are values used for each Windows Server version:</p><table><thead><tr><th>Product Name</th><th>Version</th></tr></thead><tbody><tr><td>Windows Server 2019</td><td>10.0.17763</td></tr><tr><td>Windows Server 2022</td><td>10.0.20348</td></tr></tbody></table><h3 id="simplifying-with-runtimeclass">Simplifying with RuntimeClass</h3><p><a href="/docs/concepts/containers/runtime-class/">RuntimeClass</a> can be used to simplify the process of using taints and tolerations.
A cluster administrator can create a <code>RuntimeClass</code> object which is used to encapsulate these taints and tolerations.</p><ol><li><p>Save this file to <code>runtimeClasses.yml</code>. It includes the appropriate <code>nodeSelector</code>
for the Windows OS, architecture, and version.</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>node.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>RuntimeClass<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>windows-2019<span>
</span></span></span><span><span><span></span><span>handler</span>:<span> </span>example-container-runtime-handler<span>
</span></span></span><span><span><span></span><span>scheduling</span>:<span>
</span></span></span><span><span><span>  </span><span>nodeSelector</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/os</span>:<span> </span><span>'windows'</span><span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/arch</span>:<span> </span><span>'amd64'</span><span>
</span></span></span><span><span><span>    </span><span>node.kubernetes.io/windows-build</span>:<span> </span><span>'10.0.17763'</span><span>
</span></span></span><span><span><span>  </span><span>tolerations</span>:<span>
</span></span></span><span><span><span>  </span>- <span>effect</span>:<span> </span>NoSchedule<span>
</span></span></span><span><span><span>    </span><span>key</span>:<span> </span>os<span>
</span></span></span><span><span><span>    </span><span>operator</span>:<span> </span>Equal<span>
</span></span></span><span><span><span>    </span><span>value</span>:<span> </span><span>"windows"</span><span>
</span></span></span></code></pre></div></li><li><p>Run <code>kubectl create -f runtimeClasses.yml</code> using as a cluster administrator</p></li><li><p>Add <code>runtimeClassName: windows-2019</code> as appropriate to Pod specs</p><p>For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>apps/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Deployment<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>iis-2019<span>
</span></span></span><span><span><span>  </span><span>labels</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>iis-2019<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>replicas</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>  </span><span>template</span>:<span>
</span></span></span><span><span><span>    </span><span>metadata</span>:<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>iis-2019<span>
</span></span></span><span><span><span>      </span><span>labels</span>:<span>
</span></span></span><span><span><span>        </span><span>app</span>:<span> </span>iis-2019<span>
</span></span></span><span><span><span>    </span><span>spec</span>:<span>
</span></span></span><span><span><span>      </span><span>runtimeClassName</span>:<span> </span>windows-2019<span>
</span></span></span><span><span><span>      </span><span>containers</span>:<span>
</span></span></span><span><span><span>      </span>- <span>name</span>:<span> </span>iis<span>
</span></span></span><span><span><span>        </span><span>image</span>:<span> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span>
</span></span></span><span><span><span>        </span><span>resources</span>:<span>
</span></span></span><span><span><span>          </span><span>limits</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span><span>1</span><span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span>800Mi<span>
</span></span></span><span><span><span>          </span><span>requests</span>:<span>
</span></span></span><span><span><span>            </span><span>cpu</span>:<span> </span>.1<span>
</span></span></span><span><span><span>            </span><span>memory</span>:<span> </span>300Mi<span>
</span></span></span><span><span><span>        </span><span>ports</span>:<span>
</span></span></span><span><span><span>          </span>- <span>containerPort</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span> </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>matchLabels</span>:<span>
</span></span></span><span><span><span>      </span><span>app</span>:<span> </span>iis-2019<span>
</span></span></span><span><span><span></span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Service<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>iis<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>type</span>:<span> </span>LoadBalancer<span>
</span></span></span><span><span><span>  </span><span>ports</span>:<span>
</span></span></span><span><span><span>  </span>- <span>protocol</span>:<span> </span>TCP<span>
</span></span></span><span><span><span>    </span><span>port</span>:<span> </span><span>80</span><span>
</span></span></span><span><span><span>  </span><span>selector</span>:<span>
</span></span></span><span><span><span>    </span><span>app</span>:<span> </span>iis-2019<span>
</span></span></span></code></pre></div></li></ol></div></div><div><div class="td-content"><h1>Extending Kubernetes</h1><div class="lead">Different ways to change the behavior of your Kubernetes cluster.</div><p>Kubernetes is highly configurable and extensible. As a result, there is rarely a need to fork or
submit patches to the Kubernetes project code.</p><p>This guide describes the options for customizing a Kubernetes cluster. It is aimed at
<a class="glossary-tooltip" title="A person who configures, controls, and monitors clusters." href="/docs/reference/glossary/?all=true#term-cluster-operator" target="_blank">cluster operators</a> who want to understand
how to adapt their Kubernetes cluster to the needs of their work environment. Developers who are
prospective <a class="glossary-tooltip" title="A person who customizes the Kubernetes platform to fit the needs of their project." href="/docs/reference/glossary/?all=true#term-platform-developer" target="_blank">Platform Developers</a> or
Kubernetes Project <a class="glossary-tooltip" title="Someone who donates code, documentation, or their time to help the Kubernetes project or community." href="/docs/reference/glossary/?all=true#term-contributor" target="_blank">Contributors</a> will also
find it useful as an introduction to what extension points and patterns exist, and their
trade-offs and limitations.</p><p>Customization approaches can be broadly divided into <a href="#configuration">configuration</a>, which only
involves changing command line arguments, local configuration files, or API resources; and <a href="#extensions">extensions</a>,
which involve running additional programs, additional network services, or both.
This document is primarily about <em>extensions</em>.</p><h2 id="configuration">Configuration</h2><p><em>Configuration files</em> and <em>command arguments</em> are documented in the <a href="/docs/reference/">Reference</a> section of the online
documentation, with a page for each binary:</p><ul><li><a href="/docs/reference/command-line-tools-reference/kube-apiserver/"><code>kube-apiserver</code></a></li><li><a href="/docs/reference/command-line-tools-reference/kube-controller-manager/"><code>kube-controller-manager</code></a></li><li><a href="/docs/reference/command-line-tools-reference/kube-scheduler/"><code>kube-scheduler</code></a></li><li><a href="/docs/reference/command-line-tools-reference/kubelet/"><code>kubelet</code></a></li><li><a href="/docs/reference/command-line-tools-reference/kube-proxy/"><code>kube-proxy</code></a></li></ul><p>Command arguments and configuration files may not always be changeable in a hosted Kubernetes service or a
distribution with managed installation. When they are changeable, they are usually only changeable
by the cluster operator. Also, they are subject to change in future Kubernetes versions, and
setting them may require restarting processes. For those reasons, they should be used only when
there are no other options.</p><p>Built-in <em>policy APIs</em>, such as <a href="/docs/concepts/policy/resource-quotas/">ResourceQuota</a>,
<a href="/docs/concepts/services-networking/network-policies/">NetworkPolicy</a> and Role-based Access Control
(<a href="/docs/reference/access-authn-authz/rbac/">RBAC</a>), are built-in Kubernetes APIs that provide declaratively configured policy settings.
APIs are typically usable even with hosted Kubernetes services and with managed Kubernetes installations.
The built-in policy APIs follow the same conventions as other Kubernetes resources such as Pods.
When you use a policy APIs that is <a href="/docs/reference/using-api/#api-versioning">stable</a>, you benefit from a
<a href="/docs/reference/using-api/deprecation-policy/">defined support policy</a> like other Kubernetes APIs.
For these reasons, policy APIs are recommended over <em>configuration files</em> and <em>command arguments</em> where suitable.</p><h2 id="extensions">Extensions</h2><p>Extensions are software components that extend and deeply integrate with Kubernetes.
They adapt it to support new types and new kinds of hardware.</p><p>Many cluster administrators use a hosted or distribution instance of Kubernetes.
These clusters come with extensions pre-installed. As a result, most Kubernetes
users will not need to install extensions and even fewer users will need to author new ones.</p><h3 id="extension-patterns">Extension patterns</h3><p>Kubernetes is designed to be automated by writing client programs. Any
program that reads and/or writes to the Kubernetes API can provide useful
automation. <em>Automation</em> can run on the cluster or off it. By following
the guidance in this doc you can write highly available and robust automation.
Automation generally works with any Kubernetes cluster, including hosted
clusters and managed installations.</p><p>There is a specific pattern for writing client programs that work well with
Kubernetes called the <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controller</a>
pattern. Controllers typically read an object's <code>.spec</code>, possibly do things, and then
update the object's <code>.status</code>.</p><p>A controller is a client of the Kubernetes API. When Kubernetes is the client and calls
out to a remote service, Kubernetes calls this a <em>webhook</em>. The remote service is called
a <em>webhook backend</em>. As with custom controllers, webhooks do add a point of failure.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Outside of Kubernetes, the term &#8220;webhook&#8221; typically refers to a mechanism for asynchronous
notifications, where the webhook call serves as a one-way notification to another system or
component. In the Kubernetes ecosystem, even synchronous HTTP callouts are often
described as &#8220;webhooks&#8221;.</div><p>In the webhook model, Kubernetes makes a network request to a remote service.
With the alternative <em>binary Plugin</em> model, Kubernetes executes a binary (program).
Binary plugins are used by the kubelet (for example, <a href="https://kubernetes-csi.github.io/docs/">CSI storage plugins</a>
and <a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">CNI network plugins</a>),
and by kubectl (see <a href="/docs/tasks/extend-kubectl/kubectl-plugins/">Extend kubectl with plugins</a>).</p><h3 id="extension-points">Extension points</h3><p>This diagram shows the extension points in a Kubernetes cluster and the
clients that access it.</p><figure class="diagram-large"><img src="/docs/concepts/extend-kubernetes/extension-points.png" alt="Symbolic representation of seven numbered extension points for Kubernetes"><figcaption><p>Kubernetes extension points</p></figcaption></figure><h4 id="key-to-the-figure">Key to the figure</h4><ol><li><p>Users often interact with the Kubernetes API using <code>kubectl</code>. <a href="#client-extensions">Plugins</a>
customise the behaviour of clients. There are generic extensions that can apply to different clients,
as well as specific ways to extend <code>kubectl</code>.</p></li><li><p>The API server handles all requests. Several types of extension points in the API server allow
authenticating requests, or blocking them based on their content, editing content, and handling
deletion. These are described in the <a href="#api-access-extensions">API Access Extensions</a> section.</p></li><li><p>The API server serves various kinds of <em>resources</em>. <em>Built-in resource kinds</em>, such as
<code>pods</code>, are defined by the Kubernetes project and can't be changed.
Read <a href="#api-extensions">API extensions</a> to learn about extending the Kubernetes API.</p></li><li><p>The Kubernetes scheduler <a href="/docs/concepts/scheduling-eviction/assign-pod-node/">decides</a>
which nodes to place pods on. There are several ways to extend scheduling, which are
described in the <a href="#scheduling-extensions">Scheduling extensions</a> section.</p></li><li><p>Much of the behavior of Kubernetes is implemented by programs called
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a>, that are
clients of the API server. Controllers are often used in conjunction with custom resources.
Read <a href="#combining-new-apis-with-automation">combining new APIs with automation</a> and
<a href="#changing-built-in-resources">Changing built-in resources</a> to learn more.</p></li><li><p>The kubelet runs on servers (nodes), and helps pods appear like virtual servers with their own IPs on
the cluster network. <a href="#network-plugins">Network Plugins</a> allow for different implementations of
pod networking.</p></li><li><p>You can use <a href="#device-plugins">Device Plugins</a> to integrate custom hardware or other special
node-local facilities, and make these available to Pods running in your cluster. The kubelet
includes support for working with device plugins.</p><p>The kubelet also mounts and unmounts
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">volume</a> for pods and their containers.
You can use <a href="#storage-plugins">Storage Plugins</a> to add support for new kinds
of storage and other volume types.</p></li></ol><h4 id="extension-flowchart">Extension point choice flowchart</h4><p>If you are unsure where to start, this flowchart can help. Note that some solutions may involve
several types of extensions.</p><figure class="diagram-large"><img src="/docs/concepts/extend-kubernetes/flowchart.svg" alt="Flowchart with questions about use cases and guidance for implementers. Green circles indicate yes; red circles indicate no."><figcaption><p>Flowchart guide to select an extension approach</p></figcaption></figure><hr><h2 id="client-extensions">Client extensions</h2><p>Plugins for kubectl are separate binaries that add or replace the behavior of specific subcommands.
The <code>kubectl</code> tool can also integrate with <a href="/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins">credential plugins</a>
These extensions only affect a individual user's local environment, and so cannot enforce site-wide policies.</p><p>If you want to extend the <code>kubectl</code> tool, read <a href="/docs/tasks/extend-kubectl/kubectl-plugins/">Extend kubectl with plugins</a>.</p><h2 id="api-extensions">API extensions</h2><h3 id="custom-resource-definitions">Custom resource definitions</h3><p>Consider adding a <em>Custom Resource</em> to Kubernetes if you want to define new controllers, application
configuration objects or other declarative APIs, and to manage them using Kubernetes tools, such
as <code>kubectl</code>.</p><p>For more about Custom Resources, see the
<a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resources</a> concept guide.</p><h3 id="api-aggregation-layer">API aggregation layer</h3><p>You can use Kubernetes' <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">API Aggregation Layer</a>
to integrate the Kubernetes API with additional services such as for <a href="/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/">metrics</a>.</p><h3 id="combining-new-apis-with-automation">Combining new APIs with automation</h3><p>A combination of a custom resource API and a control loop is called the
<a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a> pattern. If your controller takes
the place of a human operator deploying infrastructure based on a desired state, then the controller
may also be following the <a class="glossary-tooltip" title="A specialized controller used to manage a custom resource" href="/docs/concepts/extend-kubernetes/operator/" target="_blank">operator pattern</a>.
The Operator pattern is used to manage specific applications; usually, these are applications that
maintain state and require care in how they are managed.</p><p>You can also make your own custom APIs and control loops that manage other resources, such as storage,
or to define policies (such as an access control restriction).</p><h3 id="changing-built-in-resources">Changing built-in resources</h3><p>When you extend the Kubernetes API by adding custom resources, the added resources always fall
into a new API Groups. You cannot replace or change existing API groups.
Adding an API does not directly let you affect the behavior of existing APIs (such as Pods), whereas
<em>API Access Extensions</em> do.</p><h2 id="api-access-extensions">API access extensions</h2><p>When a request reaches the Kubernetes API Server, it is first <em>authenticated</em>, then <em>authorized</em>,
and is then subject to various types of <em>admission control</em> (some requests are in fact not
authenticated, and get special treatment). See
<a href="/docs/concepts/security/controlling-access/">Controlling Access to the Kubernetes API</a>
for more on this flow.</p><p>Each of the steps in the Kubernetes authentication / authorization flow offers extension points.</p><h3 id="authentication">Authentication</h3><p><a href="/docs/reference/access-authn-authz/authentication/">Authentication</a> maps headers or certificates
in all requests to a username for the client making the request.</p><p>Kubernetes has several built-in authentication methods that it supports. It can also sit behind an
authenticating proxy, and it can send a token from an <code>Authorization:</code> header to a remote service for
verification (an <a href="/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">authentication webhook</a>)
if those don't meet your needs.</p><h3 id="authorization">Authorization</h3><p><a href="/docs/reference/access-authn-authz/authorization/">Authorization</a> determines whether specific
users can read, write, and do other operations on API resources. It works at the level of whole
resources -- it doesn't discriminate based on arbitrary object fields.</p><p>If the built-in authorization options don't meet your needs, an
<a href="/docs/reference/access-authn-authz/webhook/">authorization webhook</a>
allows calling out to custom code that makes an authorization decision.</p><h3 id="dynamic-admission-control">Dynamic admission control</h3><p>After a request is authorized, if it is a write operation, it also goes through
<a href="/docs/reference/access-authn-authz/admission-controllers/">Admission Control</a> steps.
In addition to the built-in steps, there are several extensions:</p><ul><li>The <a href="/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook">Image Policy webhook</a>
restricts what images can be run in containers.</li><li>To make arbitrary admission control decisions, a general
<a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">Admission webhook</a>
can be used. Admission webhooks can reject creations or updates.
Some admission webhooks modify the incoming request data before it is handled further by Kubernetes.</li></ul><h2 id="infrastructure-extensions">Infrastructure extensions</h2><h3 id="device-plugins">Device plugins</h3><p><em>Device plugins</em> allow a node to discover new Node resources (in addition to the
builtin ones like cpu and memory) via a
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">Device Plugin</a>.</p><h3 id="storage-plugins">Storage plugins</h3><p><a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">Container Storage Interface</a> (CSI) plugins provide
a way to extend Kubernetes with supports for new kinds of volumes. The volumes can be backed by
durable external storage, or provide ephemeral storage, or they might offer a read-only interface
to information using a filesystem paradigm.</p><p>Kubernetes also includes support for <a href="/docs/concepts/storage/volumes/#flexvolume">FlexVolume</a> plugins,
which are deprecated since Kubernetes v1.23 (in favour of CSI).</p><p>FlexVolume plugins allow users to mount volume types that aren't natively supported by Kubernetes. When
you run a Pod that relies on FlexVolume storage, the kubelet calls a binary plugin to mount the volume.
The archived <a href="https://git.k8s.io/design-proposals-archive/storage/flexvolume-deployment.md">FlexVolume</a>
design proposal has more detail on this approach.</p><p>The <a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors">Kubernetes Volume Plugin FAQ for Storage Vendors</a>
includes general information on storage plugins.</p><h3 id="network-plugins">Network plugins</h3><p>Your Kubernetes cluster needs a <em>network plugin</em> in order to have a working Pod network
and to support other aspects of the Kubernetes network model.</p><p><a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">Network Plugins</a>
allow Kubernetes to work with different networking topologies and technologies.</p><h3 id="kubelet-image-credential-provider-plugins">Kubelet image credential provider plugins</h3><p><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div>Kubelet image credential providers are plugins for the kubelet to dynamically retrieve image registry
credentials. The credentials are then used when pulling images from container image registries that
match the configuration.</p><p>The plugins can communicate with external services or use local files to obtain credentials. This way,
the kubelet does not need to have static credentials for each registry, and can support various
authentication methods and protocols.</p><p>For plugin configuration details, see
<a href="/docs/tasks/administer-cluster/kubelet-credential-provider/">Configure a kubelet image credential provider</a>.</p><h2 id="scheduling-extensions">Scheduling extensions</h2><p>The scheduler is a special type of controller that watches pods, and assigns
pods to nodes. The default scheduler can be replaced entirely, while
continuing to use other Kubernetes components, or
<a href="/docs/tasks/extend-kubernetes/configure-multiple-schedulers/">multiple schedulers</a>
can run at the same time.</p><p>This is a significant undertaking, and almost all Kubernetes users find they
do not need to modify the scheduler.</p><p>You can control which <a href="/docs/reference/scheduling/config/#scheduling-plugins">scheduling plugins</a>
are active, or associate sets of plugins with different named <a href="/docs/reference/scheduling/config/#multiple-profiles">scheduler profiles</a>.
You can also write your own plugin that integrates with one or more of the kube-scheduler's
<a href="/docs/concepts/scheduling-eviction/scheduling-framework/#extension-points">extension points</a>.</p><p>Finally, the built in <code>kube-scheduler</code> component supports a
<a href="https://git.k8s.io/design-proposals-archive/scheduling/scheduler_extender.md">webhook</a>
that permits a remote HTTP backend (scheduler extension) to filter and / or prioritize
the nodes that the kube-scheduler chooses for a pod.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>You can only affect node filtering
and node prioritization with a scheduler extender webhook; other extension points are
not available through the webhook integration.</div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about infrastructure extensions<ul><li><a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">Device Plugins</a></li><li><a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">Network Plugins</a></li><li>CSI <a href="https://kubernetes-csi.github.io/docs/">storage plugins</a></li></ul></li><li>Learn about <a href="/docs/tasks/extend-kubectl/kubectl-plugins/">kubectl plugins</a></li><li>Learn more about <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resources</a></li><li>Learn more about <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">Extension API Servers</a></li><li>Learn about <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/">Dynamic admission control</a></li><li>Learn about the <a href="/docs/concepts/extend-kubernetes/operator/">Operator pattern</a></li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Compute, Storage, and Networking Extensions</h1><p>This section covers extensions to your cluster that do not come as part as Kubernetes itself.
You can use these extensions to enhance the nodes in your cluster, or to provide the network
fabric that links Pods together.</p><ul><li><p><a href="/docs/concepts/storage/volumes/#csi">CSI</a> and <a href="/docs/concepts/storage/volumes/#flexvolume">FlexVolume</a> storage plugins</p><p><a class="glossary-tooltip" title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." href="/docs/concepts/storage/volumes/#csi" target="_blank">Container Storage Interface</a> (CSI) plugins
provide a way to extend Kubernetes with supports for new kinds of volumes. The volumes can
be backed by durable external storage, or provide ephemeral storage, or they might offer a
read-only interface to information using a filesystem paradigm.</p><p>Kubernetes also includes support for <a href="/docs/concepts/storage/volumes/#flexvolume">FlexVolume</a>
plugins, which are deprecated since Kubernetes v1.23 (in favour of CSI).</p><p>FlexVolume plugins allow users to mount volume types that aren't natively
supported by Kubernetes. When you run a Pod that relies on FlexVolume
storage, the kubelet calls a binary plugin to mount the volume. The archived
<a href="https://git.k8s.io/design-proposals-archive/storage/flexvolume-deployment.md">FlexVolume</a>
design proposal has more detail on this approach.</p><p>The <a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors">Kubernetes Volume Plugin FAQ for Storage Vendors</a>
includes general information on storage plugins.</p></li><li><p><a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">Device plugins</a></p><p>Device plugins allow a node to discover new Node facilities (in addition to the
built-in node resources such as <code>cpu</code> and <code>memory</code>), and provide these custom node-local
facilities to Pods that request them.</p></li><li><p><a href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">Network plugins</a></p><p>Network plugins allow Kubernetes to work with different networking topologies and technologies.
Your Kubernetes cluster needs a <em>network plugin</em> in order to have a working Pod network
and to support other aspects of the Kubernetes network model.</p><p>Kubernetes 1.34 is compatible with <a class="glossary-tooltip" title="Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification." href="/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank">CNI</a>
network plugins.</p></li></ul><div class="section-index"></div></div></div><div><div class="td-content"><h1>Network Plugins</h1><p>Kubernetes (version 1.3 through to the latest 1.34, and likely onwards) lets you use
<a href="https://github.com/containernetworking/cni">Container Network Interface</a>
(CNI) plugins for cluster networking. You must use a CNI plugin that is compatible with your
cluster and that suits your needs. Different plugins are available (both open- and closed- source)
in the wider Kubernetes ecosystem.</p><p>A CNI plugin is required to implement the
<a href="/docs/concepts/services-networking/#the-kubernetes-network-model">Kubernetes network model</a>.</p><p>You must use a CNI plugin that is compatible with the
<a href="https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md">v0.4.0</a> or later
releases of the CNI specification. The Kubernetes project recommends using a plugin that is
compatible with the <a href="https://github.com/containernetworking/cni/blob/spec-v1.0.0/SPEC.md">v1.0.0</a>
CNI specification (plugins can be compatible with multiple spec versions).</p><h2 id="installation">Installation</h2><p>A Container Runtime, in the networking context, is a daemon on a node configured to provide CRI
Services for kubelet. In particular, the Container Runtime must be configured to load the CNI
plugins required to implement the Kubernetes network model.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Prior to Kubernetes 1.24, the CNI plugins could also be managed by the kubelet using the
<code>cni-bin-dir</code> and <code>network-plugin</code> command-line parameters.
These command-line parameters were removed in Kubernetes 1.24, with management of the CNI no
longer in scope for kubelet.</p><p>See <a href="/docs/tasks/administer-cluster/migrating-from-dockershim/troubleshooting-cni-plugin-related-errors/">Troubleshooting CNI plugin-related errors</a>
if you are facing issues following the removal of dockershim.</p></div><p>For specific information about how a Container Runtime manages the CNI plugins, see the
documentation for that Container Runtime, for example:</p><ul><li><a href="https://github.com/containerd/containerd/blob/main/script/setup/install-cni">containerd</a></li><li><a href="https://github.com/cri-o/cri-o/blob/main/contrib/cni/README.md">CRI-O</a></li></ul><p>For specific information about how to install and manage a CNI plugin, see the documentation for
that plugin or <a href="/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model">networking provider</a>.</p><h2 id="network-plugin-requirements">Network Plugin Requirements</h2><h3 id="loopback-cni">Loopback CNI</h3><p>In addition to the CNI plugin installed on the nodes for implementing the Kubernetes network
model, Kubernetes also requires the container runtimes to provide a loopback interface <code>lo</code>, which
is used for each sandbox (pod sandboxes, vm sandboxes, ...).
Implementing the loopback interface can be accomplished by re-using the
<a href="https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go">CNI loopback plugin.</a>
or by developing your own code to achieve this (see
<a href="https://github.com/cri-o/ocicni/blob/release-1.24/pkg/ocicni/util_linux.go#L91">this example from CRI-O</a>).</p><h3 id="support-hostport">Support hostPort</h3><p>The CNI networking plugin supports <code>hostPort</code>. You can use the official
<a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap">portmap</a>
plugin offered by the CNI plugin team or use your own plugin with portMapping functionality.</p><p>If you want to enable <code>hostPort</code> support, you must specify <code>portMappings capability</code> in your
<code>cni-conf-dir</code>. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"name"</span>: <span>"k8s-pod-network"</span>,
</span></span><span><span>  <span>"cniVersion"</span>: <span>"0.4.0"</span>,
</span></span><span><span>  <span>"plugins"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"type"</span>: <span>"calico"</span>,
</span></span><span><span>      <span>"log_level"</span>: <span>"info"</span>,
</span></span><span><span>      <span>"datastore_type"</span>: <span>"kubernetes"</span>,
</span></span><span><span>      <span>"nodename"</span>: <span>"127.0.0.1"</span>,
</span></span><span><span>      <span>"ipam"</span>: {
</span></span><span><span>        <span>"type"</span>: <span>"host-local"</span>,
</span></span><span><span>        <span>"subnet"</span>: <span>"usePodCidr"</span>
</span></span><span><span>      },
</span></span><span><span>      <span>"policy"</span>: {
</span></span><span><span>        <span>"type"</span>: <span>"k8s"</span>
</span></span><span><span>      },
</span></span><span><span>      <span>"kubernetes"</span>: {
</span></span><span><span>        <span>"kubeconfig"</span>: <span>"/etc/cni/net.d/calico-kubeconfig"</span>
</span></span><span><span>      }
</span></span><span><span>    },
</span></span><span><span>    {
</span></span><span><span>      <span>"type"</span>: <span>"portmap"</span>,
</span></span><span><span>      <span>"capabilities"</span>: {<span>"portMappings"</span>: <span>true</span>},
</span></span><span><span>      <span>"externalSetMarkChain"</span>: <span>"KUBE-MARK-MASQ"</span>
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><h3 id="support-traffic-shaping">Support traffic shaping</h3><p><strong>Experimental Feature</strong></p><p>The CNI networking plugin also supports pod ingress and egress traffic shaping. You can use the
official <a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/bandwidth">bandwidth</a>
plugin offered by the CNI plugin team or use your own plugin with bandwidth control functionality.</p><p>If you want to enable traffic shaping support, you must add the <code>bandwidth</code> plugin to your CNI
configuration file (default <code>/etc/cni/net.d</code>) and ensure that the binary is included in your CNI
bin dir (default <code>/opt/cni/bin</code>).</p><div class="highlight"><pre tabindex="0"><code class="language-json"><span><span>{
</span></span><span><span>  <span>"name"</span>: <span>"k8s-pod-network"</span>,
</span></span><span><span>  <span>"cniVersion"</span>: <span>"0.4.0"</span>,
</span></span><span><span>  <span>"plugins"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"type"</span>: <span>"calico"</span>,
</span></span><span><span>      <span>"log_level"</span>: <span>"info"</span>,
</span></span><span><span>      <span>"datastore_type"</span>: <span>"kubernetes"</span>,
</span></span><span><span>      <span>"nodename"</span>: <span>"127.0.0.1"</span>,
</span></span><span><span>      <span>"ipam"</span>: {
</span></span><span><span>        <span>"type"</span>: <span>"host-local"</span>,
</span></span><span><span>        <span>"subnet"</span>: <span>"usePodCidr"</span>
</span></span><span><span>      },
</span></span><span><span>      <span>"policy"</span>: {
</span></span><span><span>        <span>"type"</span>: <span>"k8s"</span>
</span></span><span><span>      },
</span></span><span><span>      <span>"kubernetes"</span>: {
</span></span><span><span>        <span>"kubeconfig"</span>: <span>"/etc/cni/net.d/calico-kubeconfig"</span>
</span></span><span><span>      }
</span></span><span><span>    },
</span></span><span><span>    {
</span></span><span><span>      <span>"type"</span>: <span>"bandwidth"</span>,
</span></span><span><span>      <span>"capabilities"</span>: {<span>"bandwidth"</span>: <span>true</span>}
</span></span><span><span>    }
</span></span><span><span>  ]
</span></span><span><span>}
</span></span></code></pre></div><p>Now you can add the <code>kubernetes.io/ingress-bandwidth</code> and <code>kubernetes.io/egress-bandwidth</code>
annotations to your Pod. For example:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>annotations</span>:<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/ingress-bandwidth</span>:<span> </span>1M<span>
</span></span></span><span><span><span>    </span><span>kubernetes.io/egress-bandwidth</span>:<span> </span>1M<span>
</span></span></span><span><span><span></span><span>...</span><span>
</span></span></span></code></pre></div><h2 id="what-s-next">What's next</h2><ul><li>Learn more about <a href="/docs/concepts/cluster-administration/networking/">Cluster Networking</a></li><li>Learn more about <a href="/docs/concepts/services-networking/network-policies/">Network Policies</a></li><li>Learn about the <a href="/docs/tasks/administer-cluster/migrating-from-dockershim/troubleshooting-cni-plugin-related-errors/">Troubleshooting CNI plugin-related errors</a></li></ul></div></div><div><div class="td-content"><h1>Device Plugins</h1><div class="lead">Device plugins let you configure your cluster with support for devices or resources that require vendor-specific setup, such as GPUs, NICs, FPGAs, or non-volatile main memory.</div><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.26 [stable]</code></div><p>Kubernetes provides a device plugin framework that you can use to advertise system hardware
resources to the <a class="glossary-tooltip" title="An agent that runs on each node in the cluster. It makes sure that containers are running in a pod." href="/docs/reference/command-line-tools-reference/kubelet" target="_blank">Kubelet</a>.</p><p>Instead of customizing the code for Kubernetes itself, vendors can implement a
device plugin that you deploy either manually or as a <a class="glossary-tooltip" title="Ensures a copy of a Pod is running across a set of nodes in a cluster." href="/docs/concepts/workloads/controllers/daemonset" target="_blank">DaemonSet</a>.
The targeted devices include GPUs, high-performance NICs, FPGAs, InfiniBand adapters,
and other similar computing resources that may require vendor specific initialization
and setup.</p><h2 id="device-plugin-registration">Device plugin registration</h2><p>The kubelet exports a <code>Registration</code> gRPC service:</p><pre tabindex="0"><code class="language-gRPC">service Registration {
	rpc Register(RegisterRequest) returns (Empty) {}
}
</code></pre><p>A device plugin can register itself with the kubelet through this gRPC service.
During the registration, the device plugin needs to send:</p><ul><li>The name of its Unix socket.</li><li>The Device Plugin API version against which it was built.</li><li>The <code>ResourceName</code> it wants to advertise. Here <code>ResourceName</code> needs to follow the
<a href="/docs/concepts/configuration/manage-resources-containers/#extended-resources">extended resource naming scheme</a>
as <code>vendor-domain/resourcetype</code>.
(For example, an NVIDIA GPU is advertised as <code>nvidia.com/gpu</code>.)</li></ul><p>Following a successful registration, the device plugin sends the kubelet the
list of devices it manages, and the kubelet is then in charge of advertising those
resources to the API server as part of the kubelet node status update.
For example, after a device plugin registers <code>hardware-vendor.example/foo</code> with the kubelet
and reports two healthy devices on a node, the node status is updated
to advertise that the node has 2 "Foo" devices installed and available.</p><p>Then, users can request devices as part of a Pod specification
(see <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container"><code>container</code></a>).
Requesting extended resources is similar to how you manage requests and limits for
other resources, with the following differences:</p><ul><li>Extended resources are only supported as integer resources and cannot be overcommitted.</li><li>Devices cannot be shared between containers.</li></ul><h3 id="example-pod">Example</h3><p>Suppose a Kubernetes cluster is running a device plugin that advertises resource <code>hardware-vendor.example/foo</code>
on certain nodes. Here is an example of a pod requesting this resource to run a demo workload:</p><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>---</span><span>
</span></span></span><span><span><span></span><span>apiVersion</span>:<span> </span>v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>Pod<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>demo-pod<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>containers</span>:<span>
</span></span></span><span><span><span>    </span>- <span>name</span>:<span> </span>demo-container-1<span>
</span></span></span><span><span><span>      </span><span>image</span>:<span> </span>registry.k8s.io/pause:3.8<span>
</span></span></span><span><span><span>      </span><span>resources</span>:<span>
</span></span></span><span><span><span>        </span><span>limits</span>:<span>
</span></span></span><span><span><span>          </span><span>hardware-vendor.example/foo</span>:<span> </span><span>2</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span># This Pod needs 2 of the hardware-vendor.example/foo devices</span><span>
</span></span></span><span><span><span></span><span># and can only schedule onto a Node that's able to satisfy</span><span>
</span></span></span><span><span><span></span><span># that need.</span><span>
</span></span></span><span><span><span></span><span>#</span><span>
</span></span></span><span><span><span></span><span># If the Node has more than 2 of those devices available, the</span><span>
</span></span></span><span><span><span></span><span># remainder would be available for other Pods to use.</span><span>
</span></span></span></code></pre></div><h2 id="device-plugin-implementation">Device plugin implementation</h2><p>The general workflow of a device plugin includes the following steps:</p><ol><li><p>Initialization. During this phase, the device plugin performs vendor-specific
initialization and setup to make sure the devices are in a ready state.</p></li><li><p>The plugin starts a gRPC service, with a Unix socket under the host path
<code>/var/lib/kubelet/device-plugins/</code>, that implements the following interfaces:</p><pre tabindex="0"><code class="language-gRPC">service DevicePlugin {
      // GetDevicePluginOptions returns options to be communicated with Device Manager.
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}

      // ListAndWatch returns a stream of List of Devices
      // Whenever a Device state change or a Device disappears, ListAndWatch
      // returns the new list
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}

      // Allocate is called during container creation so that the Device
      // Plugin can run device specific operations and instruct Kubelet
      // of the steps to make the Device available in the container
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}

      // GetPreferredAllocation returns a preferred set of devices to allocate
      // from a list of available ones. The resulting preferred allocation is not
      // guaranteed to be the allocation ultimately performed by the
      // devicemanager. It is only designed to help the devicemanager make a more
      // informed allocation decision when possible.
      rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}

      // PreStartContainer is called, if indicated by Device Plugin during registration phase,
      // before each container start. Device plugin can run device specific operations
      // such as resetting the device before making devices available to the container.
      rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}
}
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Plugins are not required to provide useful implementations for
<code>GetPreferredAllocation()</code> or <code>PreStartContainer()</code>. Flags indicating
the availability of these calls, if any, should be set in the <code>DevicePluginOptions</code>
message sent back by a call to <code>GetDevicePluginOptions()</code>. The <code>kubelet</code> will
always call <code>GetDevicePluginOptions()</code> to see which optional functions are
available, before calling any of them directly.</div></li><li><p>The plugin registers itself with the kubelet through the Unix socket at host
path <code>/var/lib/kubelet/device-plugins/kubelet.sock</code>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The ordering of the workflow is important. A plugin MUST start serving gRPC
service before registering itself with kubelet for successful registration.</div></li><li><p>After successfully registering itself, the device plugin runs in serving mode, during which it keeps
monitoring device health and reports back to the kubelet upon any device state changes.
It is also responsible for serving <code>Allocate</code> gRPC requests. During <code>Allocate</code>, the device plugin may
do device-specific preparation; for example, GPU cleanup or QRNG initialization.
If the operations succeed, the device plugin returns an <code>AllocateResponse</code> that contains container
runtime configurations for accessing the allocated devices. The kubelet passes this information
to the container runtime.</p><p>An <code>AllocateResponse</code> contains zero or more <code>ContainerAllocateResponse</code> objects. In these, the
device plugin defines modifications that must be made to a container's definition to provide
access to the device. These modifications include:</p><ul><li><a href="/docs/concepts/overview/working-with-objects/annotations/">Annotations</a></li><li>device nodes</li><li>environment variables</li><li>mounts</li><li>fully-qualified CDI device names</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>The processing of the fully-qualified CDI device names by the Device Manager requires
that the <code>DevicePluginCDIDevices</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled for both the kubelet and the kube-apiserver. This was added as an alpha feature in Kubernetes
v1.28, graduated to beta in v1.29 and to GA in v1.31.</div></li></ol><h3 id="handling-kubelet-restarts">Handling kubelet restarts</h3><p>A device plugin is expected to detect kubelet restarts and re-register itself with the new
kubelet instance. A new kubelet instance deletes all the existing Unix sockets under
<code>/var/lib/kubelet/device-plugins</code> when it starts. A device plugin can monitor the deletion
of its Unix socket and re-register itself upon such an event.</p><h3 id="device-plugin-and-unhealthy-devices">Device plugin and unhealthy devices</h3><p>There are cases when devices fail or are shut down. The responsibility of the Device Plugin
in this case is to notify the kubelet about the situation using the <code>ListAndWatchResponse</code> API.</p><p>Once a device is marked as unhealthy, the kubelet will decrease the allocatable count
for this resource on the Node to reflect how many devices can be used for scheduling new pods.
Capacity count for the resource will not change.</p><p>Pods that were assigned to the failed devices will continue be assigned to this device.
It is typical that code relying on the device will start failing and Pod may get
into Failed phase if <code>restartPolicy</code> for the Pod was not <code>Always</code> or enter the crash loop
otherwise.</p><p>Before Kubernetes v1.31, the way to know whether or not a Pod is associated with the
failed device is to use the <a href="#monitoring-device-plugin-resources">PodResources API</a>.</p><div class="feature-state-notice feature-alpha" title="Feature Gate: ResourceHealthStatus"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.31 [alpha]</code> (enabled by default: false)</div><p>By enabling the feature gate <code>ResourceHealthStatus</code>, the field <code>allocatedResourcesStatus</code>
will be added to each container status, within the <code>.status</code> for each Pod. The <code>allocatedResourcesStatus</code>
field
reports health information for each device assigned to the container.</p><p>For a failed Pod, or where you suspect a fault, you can use this status to understand whether
the Pod behavior may be associated with device failure. For example, if an accelerator is reporting
an over-temperature event, the <code>allocatedResourcesStatus</code> field may be able to report this.</p><h2 id="device-plugin-deployment">Device plugin deployment</h2><p>You can deploy a device plugin as a DaemonSet, as a package for your node's operating system,
or manually.</p><p>The canonical directory <code>/var/lib/kubelet/device-plugins</code> requires privileged access,
so a device plugin must run in a privileged security context.
If you're deploying a device plugin as a DaemonSet, <code>/var/lib/kubelet/device-plugins</code>
must be mounted as a <a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">Volume</a>
in the plugin's <a href="/docs/reference/generated/kubernetes-api/v1.34/#podspec-v1-core">PodSpec</a>.</p><p>If you choose the DaemonSet approach you can rely on Kubernetes to: place the device plugin's
Pod onto Nodes, to restart the daemon Pod after failure, and to help automate upgrades.</p><h2 id="api-compatibility">API compatibility</h2><p>Previously, the versioning scheme required the Device Plugin's API version to match
exactly the Kubelet's version. Since the graduation of this feature to Beta in v1.12
this is no longer a hard requirement. The API is versioned and has been stable since
Beta graduation of this feature. Because of this, kubelet upgrades should be seamless
but there still may be changes in the API before stabilization making upgrades not
guaranteed to be non-breaking.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Although the Device Manager component of Kubernetes is a generally available feature,
the <em>device plugin API</em> is not stable. For information on the device plugin API and
version compatibility, read <a href="/docs/reference/node/device-plugin-api-versions/">Device Plugin API versions</a>.</div><p>As a project, Kubernetes recommends that device plugin developers:</p><ul><li>Watch for Device Plugin API changes in the future releases.</li><li>Support multiple versions of the device plugin API for backward/forward compatibility.</li></ul><p>To run device plugins on nodes that need to be upgraded to a Kubernetes release with
a newer device plugin API version, upgrade your device plugins to support both versions
before upgrading these nodes. Taking that approach will ensure the continuous functioning
of the device allocations during the upgrade.</p><h2 id="monitoring-device-plugin-resources">Monitoring device plugin resources</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [stable]</code></div><p>In order to monitor resources provided by device plugins, monitoring agents need to be able to
discover the set of devices that are in-use on the node and obtain metadata to describe which
container the metric should be associated with. <a href="https://prometheus.io/">Prometheus</a> metrics
exposed by device monitoring agents should follow the
<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/metric-instrumentation.md">Kubernetes Instrumentation Guidelines</a>,
identifying containers using <code>pod</code>, <code>namespace</code>, and <code>container</code> prometheus labels.</p><p>The kubelet provides a gRPC service to enable discovery of in-use devices, and to provide metadata
for these devices:</p><pre tabindex="0"><code class="language-gRPC">// PodResourcesLister is a service provided by the kubelet that provides information about the
// node resources consumed by pods and containers on the node
service PodResourcesLister {
    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
    rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}
    rpc Get(GetPodResourcesRequest) returns (GetPodResourcesResponse) {}
}
</code></pre><h3 id="grpc-endpoint-list"><code>List</code> gRPC endpoint</h3><p>The <code>List</code> endpoint provides information on resources of running pods, with details such as the
id of exclusively allocated CPUs, device id as it was reported by device plugins and id of
the NUMA node where these devices are allocated. Also, for NUMA-based machines, it contains the
information about memory and hugepages reserved for a container.</p><p>Starting from Kubernetes v1.27, the <code>List</code> endpoint can provide information on resources
of running pods allocated in <code>ResourceClaims</code> by the <code>DynamicResourceAllocation</code> API.
Starting from Kubernetes v1.34, this feature is enabled by default.
To disable, <code>kubelet</code> must be started with the following flags:</p><pre tabindex="0"><code>--feature-gates=KubeletPodResourcesDynamicResources=false
</code></pre><pre tabindex="0"><code class="language-gRPC">// ListPodResourcesResponse is the response returned by List function
message ListPodResourcesResponse {
    repeated PodResources pod_resources = 1;
}

// PodResources contains information about the node resources assigned to a pod
message PodResources {
    string name = 1;
    string namespace = 2;
    repeated ContainerResources containers = 3;
}

// ContainerResources contains information about the resources assigned to a container
message ContainerResources {
    string name = 1;
    repeated ContainerDevices devices = 2;
    repeated int64 cpu_ids = 3;
    repeated ContainerMemory memory = 4;
    repeated DynamicResource dynamic_resources = 5;
}

// ContainerMemory contains information about memory and hugepages assigned to a container
message ContainerMemory {
    string memory_type = 1;
    uint64 size = 2;
    TopologyInfo topology = 3;
}

// Topology describes hardware topology of the resource
message TopologyInfo {
        repeated NUMANode nodes = 1;
}

// NUMA representation of NUMA node
message NUMANode {
        int64 ID = 1;
}

// ContainerDevices contains information about the devices assigned to a container
message ContainerDevices {
    string resource_name = 1;
    repeated string device_ids = 2;
    TopologyInfo topology = 3;
}

// DynamicResource contains information about the devices assigned to a container by Dynamic Resource Allocation
message DynamicResource {
    string class_name = 1;
    string claim_name = 2;
    string claim_namespace = 3;
    repeated ClaimResource claim_resources = 4;
}

// ClaimResource contains per-plugin resource information
message ClaimResource {
    repeated CDIDevice cdi_devices = 1 [(gogoproto.customname) = "CDIDevices"];
}

// CDIDevice specifies a CDI device information
message CDIDevice {
    // Fully qualified CDI device name
    // for example: vendor.com/gpu=gpudevice1
    // see more details in the CDI specification:
    // https://github.com/container-orchestrated-devices/container-device-interface/blob/main/SPEC.md
    string name = 1;
}
</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>cpu_ids in the <code>ContainerResources</code> in the <code>List</code> endpoint correspond to exclusive CPUs allocated
to a particular container. If the goal is to evaluate CPUs that belong to the shared pool, the <code>List</code>
endpoint needs to be used in conjunction with the <code>GetAllocatableResources</code> endpoint as explained
below:</p><ol><li>Call <code>GetAllocatableResources</code> to get a list of all the allocatable CPUs</li><li>Call <code>GetCpuIds</code> on all <code>ContainerResources</code> in the system</li><li>Subtract out all of the CPUs from the <code>GetCpuIds</code> calls from the <code>GetAllocatableResources</code> call</li></ol></div><h3 id="grpc-endpoint-getallocatableresources"><code>GetAllocatableResources</code> gRPC endpoint</h3><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.28 [stable]</code></div><p>GetAllocatableResources provides information on resources initially available on the worker node.
It provides more information than kubelet exports to APIServer.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p><code>GetAllocatableResources</code> should only be used to evaluate <a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">allocatable</a>
resources on a node. If the goal is to evaluate free/unallocated resources it should be used in
conjunction with the List() endpoint. The result obtained by <code>GetAllocatableResources</code> would remain
the same unless the underlying resources exposed to kubelet change. This happens rarely but when
it does (for example: hotplug/hotunplug, device health changes), client is expected to call
<code>GetAlloctableResources</code> endpoint.</p><p>However, calling <code>GetAllocatableResources</code> endpoint is not sufficient in case of cpu and/or memory
update and Kubelet needs to be restarted to reflect the correct resource capacity and allocatable.</p></div><pre tabindex="0"><code class="language-gRPC">// AllocatableResourcesResponses contains information about all the devices known by the kubelet
message AllocatableResourcesResponse {
    repeated ContainerDevices devices = 1;
    repeated int64 cpu_ids = 2;
    repeated ContainerMemory memory = 3;
}
</code></pre><p><code>ContainerDevices</code> do expose the topology information declaring to which NUMA cells the device is
affine. The NUMA cells are identified using a opaque integer ID, which value is consistent to
what device plugins report
<a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager">when they register themselves to the kubelet</a>.</p><p>The gRPC service is served over a unix socket at <code>/var/lib/kubelet/pod-resources/kubelet.sock</code>.
Monitoring agents for device plugin resources can be deployed as a daemon, or as a DaemonSet.
The canonical directory <code>/var/lib/kubelet/pod-resources</code> requires privileged access, so monitoring
agents must run in a privileged security context. If a device monitoring agent is running as a
DaemonSet, <code>/var/lib/kubelet/pod-resources</code> must be mounted as a
<a class="glossary-tooltip" title="A directory containing data, accessible to the containers in a pod." href="/docs/concepts/storage/volumes/" target="_blank">Volume</a> in the device monitoring agent's
<a href="/docs/reference/generated/kubernetes-api/v1.34/#podspec-v1-core">PodSpec</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>When accessing the <code>/var/lib/kubelet/pod-resources/kubelet.sock</code> from DaemonSet
or any other app deployed as a container on the host, which is mounting socket as
a volume, it is a good practice to mount directory <code>/var/lib/kubelet/pod-resources/</code>
instead of the <code>/var/lib/kubelet/pod-resources/kubelet.sock</code>. This will ensure
that after kubelet restart, container will be able to re-connect to this socket.</p><p>Container mounts are managed by inode referencing the socket or directory,
depending on what was mounted. When kubelet restarts, socket is deleted
and a new socket is created, while directory stays untouched.
So the original inode for the socket become unusable. Inode to directory
will continue working.</p></div><h3 id="grpc-endpoint-get"><code>Get</code> gRPC endpoint</h3><div class="feature-state-notice feature-beta"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.34 [beta]</code></div><p>The <code>Get</code> endpoint provides information on resources of a running Pod. It exposes information
similar to those described in the <code>List</code> endpoint. The <code>Get</code> endpoint requires <code>PodName</code>
and <code>PodNamespace</code> of the running Pod.</p><pre tabindex="0"><code class="language-gRPC">// GetPodResourcesRequest contains information about the pod
message GetPodResourcesRequest {
    string pod_name = 1;
    string pod_namespace = 2;
}
</code></pre><p>To disable this feature, you must start your kubelet services with the following flag:</p><pre tabindex="0"><code>--feature-gates=KubeletPodResourcesGet=false
</code></pre><p>The <code>Get</code> endpoint can provide Pod information related to dynamic resources
allocated by the dynamic resource allocation API.
Starting from Kubernetes v1.34, this feature is enabled by default.
To disable, <code>kubelet</code> must be started with the following flags:</p><pre tabindex="0"><code>--feature-gates=KubeletPodResourcesDynamicResources=false
</code></pre><h2 id="device-plugin-integration-with-the-topology-manager">Device plugin integration with the Topology Manager</h2><div class="feature-state-notice feature-stable"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.27 [stable]</code></div><p>The Topology Manager is a Kubelet component that allows resources to be co-ordinated in a Topology
aligned manner. In order to do this, the Device Plugin API was extended to include a
<code>TopologyInfo</code> struct.</p><pre tabindex="0"><code class="language-gRPC">message TopologyInfo {
    repeated NUMANode nodes = 1;
}

message NUMANode {
    int64 ID = 1;
}
</code></pre><p>Device Plugins that wish to leverage the Topology Manager can send back a populated TopologyInfo
struct as part of the device registration, along with the device IDs and the health of the device.
The device manager will then use this information to consult with the Topology Manager and make
resource assignment decisions.</p><p><code>TopologyInfo</code> supports setting a <code>nodes</code> field to either <code>nil</code> or a list of NUMA nodes. This
allows the Device Plugin to advertise a device that spans multiple NUMA nodes.</p><p>Setting <code>TopologyInfo</code> to <code>nil</code> or providing an empty list of NUMA nodes for a given device
indicates that the Device Plugin does not have a NUMA affinity preference for that device.</p><p>An example <code>TopologyInfo</code> struct populated for a device by a Device Plugin:</p><pre tabindex="0"><code>pluginapi.Device{ID: "25102017", Health: pluginapi.Healthy, Topology:&amp;pluginapi.TopologyInfo{Nodes: []*pluginapi.NUMANode{&amp;pluginapi.NUMANode{ID: 0,},}}}
</code></pre><h2 id="examples">Device plugin examples</h2><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><p>Here are some examples of device plugin implementations:</p><ul><li><a href="https://github.com/project-akri/akri">Akri</a>, which lets you easily expose heterogeneous leaf devices (such as IP cameras and USB devices).</li><li>The <a href="https://github.com/ROCm/k8s-device-plugin">AMD GPU device plugin</a></li><li>The <a href="https://github.com/squat/generic-device-plugin">generic device plugin</a> for generic Linux devices and USB devices</li><li>The <a href="https://github.com/Project-HAMi/HAMi">HAMi</a> for heterogeneous AI computing virtualization middleware (for example, NVIDIA, Cambricon, Hygon, Iluvatar, MThreads, Ascend, Metax)</li><li>The <a href="https://github.com/intel/intel-device-plugins-for-kubernetes">Intel device plugins</a> for
Intel GPU, FPGA, QAT, VPU, SGX, DSA, DLB and IAA devices</li><li>The <a href="https://github.com/kubevirt/kubernetes-device-plugins">KubeVirt device plugins</a> for
hardware-assisted virtualization</li><li>The <a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA GPU device plugin</a>, NVIDIA's
official device plugin to expose NVIDIA GPUs and monitor GPU health</li><li>The <a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu">NVIDIA GPU device plugin for Container-Optimized OS</a></li><li>The <a href="https://github.com/hustcat/k8s-rdma-device-plugin">RDMA device plugin</a></li><li>The <a href="https://github.com/collabora/k8s-socketcan">SocketCAN device plugin</a></li><li>The <a href="https://github.com/vikaschoudhary16/sfc-device-plugin">Solarflare device plugin</a></li><li>The <a href="https://github.com/intel/sriov-network-device-plugin">SR-IOV Network device plugin</a></li><li>The <a href="https://github.com/Xilinx/FPGA_as_a_Service/tree/master/k8s-device-plugin">Xilinx FPGA device plugins</a> for Xilinx FPGA devices</li></ul><h2 id="what-s-next">What's next</h2><ul><li>Learn about <a href="/docs/tasks/manage-gpus/scheduling-gpus/">scheduling GPU resources</a> using device
plugins</li><li>Learn about <a href="/docs/tasks/administer-cluster/extended-resource-node/">advertising extended resources</a>
on a node</li><li>Learn about the <a href="/docs/tasks/administer-cluster/topology-manager/">Topology Manager</a></li><li>Read about using <a href="/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/">hardware acceleration for TLS ingress</a>
with Kubernetes</li><li>Read more about <a href="/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#extended-resource">Extended Resource allocation by DRA</a></li></ul></div></div><div><div class="td-content"><h1>Extending the Kubernetes API</h1><p>Custom resources are extensions of the Kubernetes API. Kubernetes provides two ways to add custom resources to your cluster:</p><ul><li>The <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">CustomResourceDefinition</a>
(CRD) mechanism allows you to declaratively define a new custom API with an API group, kind, and
schema that you specify.
The Kubernetes control plane serves and handles the storage of your custom resource. CRDs allow you to
create new types of resources for your cluster without writing and running a custom API server.</li><li>The <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">aggregation layer</a>
sits behind the primary API server, which acts as a proxy.
This arrangement is called API Aggregation (AA), which allows you to provide
specialized implementations for your custom resources by writing and
deploying your own API server.
The main API server delegates requests to your API server for the custom APIs that you specify,
making them available to all of its clients.</li></ul><div class="section-index"><hr class="panel-line"><div class="entry"><h5><a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resources</a></h5><p></p></div><div class="entry"><h5><a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">Kubernetes API Aggregation Layer</a></h5><p></p></div></div></div></div><div><div class="td-content"><h1>Custom Resources</h1><p><em>Custom resources</em> are extensions of the Kubernetes API. This page discusses when to add a custom
resource to your Kubernetes cluster and when to use a standalone service. It describes the two
methods for adding custom resources and how to choose between them.</p><h2 id="custom-resources">Custom resources</h2><p>A <em>resource</em> is an endpoint in the <a href="/docs/concepts/overview/kubernetes-api/">Kubernetes API</a> that
stores a collection of <a class="glossary-tooltip" title="An entity in the Kubernetes system, representing part of the state of your cluster." href="/docs/concepts/overview/working-with-objects/#kubernetes-objects" target="_blank">API objects</a>
of a certain kind; for example, the built-in <em>pods</em> resource contains a collection of Pod objects.</p><p>A <em>custom resource</em> is an extension of the Kubernetes API that is not necessarily available in a default
Kubernetes installation. It represents a customization of a particular Kubernetes installation. However,
many core Kubernetes functions are now built using custom resources, making Kubernetes more modular.</p><p>Custom resources can appear and disappear in a running cluster through dynamic registration,
and cluster admins can update custom resources independently of the cluster itself.
Once a custom resource is installed, users can create and access its objects using
<a class="glossary-tooltip" title="A command line tool for communicating with a Kubernetes cluster." href="/docs/reference/kubectl/" target="_blank">kubectl</a>, just as they do for built-in resources
like <em>Pods</em>.</p><h2 id="custom-controllers">Custom controllers</h2><p>On their own, custom resources let you store and retrieve structured data.
When you combine a custom resource with a <em>custom controller</em>, custom resources
provide a true <em>declarative API</em>.</p><p>The Kubernetes <a href="/docs/concepts/overview/kubernetes-api/">declarative API</a>
enforces a separation of responsibilities. You declare the desired state of
your resource. The Kubernetes controller keeps the current state of Kubernetes
objects in sync with your declared desired state. This is in contrast to an
imperative API, where you <em>instruct</em> a server what to do.</p><p>You can deploy and update a custom controller on a running cluster, independently
of the cluster's lifecycle. Custom controllers can work with any kind of resource,
but they are especially effective when combined with custom resources. The
<a href="/docs/concepts/extend-kubernetes/operator/">Operator pattern</a> combines custom
resources and custom controllers. You can use custom controllers to encode domain knowledge
for specific applications into an extension of the Kubernetes API.</p><h2 id="should-i-add-a-custom-resource-to-my-kubernetes-cluster">Should I add a custom resource to my Kubernetes cluster?</h2><p>When creating a new API, consider whether to
<a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">aggregate your API with the Kubernetes cluster APIs</a>
or let your API stand alone.</p><table><thead><tr><th>Consider API aggregation if:</th><th>Prefer a stand-alone API if:</th></tr></thead><tbody><tr><td>Your API is <a href="#declarative-apis">Declarative</a>.</td><td>Your API does not fit the <a href="#declarative-apis">Declarative</a> model.</td></tr><tr><td>You want your new types to be readable and writable using <code>kubectl</code>.</td><td><code>kubectl</code> support is not required</td></tr><tr><td>You want to view your new types in a Kubernetes UI, such as dashboard, alongside built-in types.</td><td>Kubernetes UI support is not required.</td></tr><tr><td>You are developing a new API.</td><td>You already have a program that serves your API and works well.</td></tr><tr><td>You are willing to accept the format restriction that Kubernetes puts on REST resource paths, such as API Groups and Namespaces. (See the <a href="/docs/concepts/overview/kubernetes-api/">API Overview</a>.)</td><td>You need to have specific REST paths to be compatible with an already defined REST API.</td></tr><tr><td>Your resources are naturally scoped to a cluster or namespaces of a cluster.</td><td>Cluster or namespace scoped resources are a poor fit; you need control over the specifics of resource paths.</td></tr><tr><td>You want to reuse <a href="#common-features">Kubernetes API support features</a>.</td><td>You don't need those features.</td></tr></tbody></table><h3 id="declarative-apis">Declarative APIs</h3><p>In a Declarative API, typically:</p><ul><li>Your API consists of a relatively small number of relatively small objects (resources).</li><li>The objects define configuration of applications or infrastructure.</li><li>The objects are updated relatively infrequently.</li><li>Humans often need to read and write the objects.</li><li>The main operations on the objects are CRUD-y (creating, reading, updating and deleting).</li><li>Transactions across objects are not required: the API represents a desired state, not an exact state.</li></ul><p>Imperative APIs are not declarative.
Signs that your API might not be declarative include:</p><ul><li>The client says "do this", and then gets a synchronous response back when it is done.</li><li>The client says "do this", and then gets an operation ID back, and has to check a separate
Operation object to determine completion of the request.</li><li>You talk about Remote Procedure Calls (RPCs).</li><li>Directly storing large amounts of data; for example, &gt; a few kB per object, or &gt; 1000s of objects.</li><li>High bandwidth access (10s of requests per second sustained) needed.</li><li>Store end-user data (such as images, PII, etc.) or other large-scale data processed by applications.</li><li>The natural operations on the objects are not CRUD-y.</li><li>The API is not easily modeled as objects.</li><li>You chose to represent pending operations with an operation ID or an operation object.</li></ul><h2 id="should-i-use-a-configmap-or-a-custom-resource">Should I use a ConfigMap or a custom resource?</h2><p>Use a ConfigMap if any of the following apply:</p><ul><li>There is an existing, well-documented configuration file format, such as a <code>mysql.cnf</code> or
<code>pom.xml</code>.</li><li>You want to put the entire configuration into one key of a ConfigMap.</li><li>The main use of the configuration file is for a program running in a Pod on your cluster to
consume the file to configure itself.</li><li>Consumers of the file prefer to consume via file in a Pod or environment variable in a pod,
rather than the Kubernetes API.</li><li>You want to perform rolling updates via Deployment, etc., when the file is updated.</li></ul><div class="alert alert-info"><h4 class="alert-heading">Note:</h4>Use a <a class="glossary-tooltip" title="Stores sensitive information, such as passwords, OAuth tokens, and ssh keys." href="/docs/concepts/configuration/secret/" target="_blank">Secret</a> for sensitive data, which is similar
to a ConfigMap but more secure.</div><p>Use a custom resource (CRD or Aggregated API) if most of the following apply:</p><ul><li>You want to use Kubernetes client libraries and CLIs to create and update the new resource.</li><li>You want top-level support from <code>kubectl</code>; for example, <code>kubectl get my-object object-name</code>.</li><li>You want to build new automation that watches for updates on the new object, and then CRUD other
objects, or vice versa.</li><li>You want to write automation that handles updates to the object.</li><li>You want to use Kubernetes API conventions like <code>.spec</code>, <code>.status</code>, and <code>.metadata</code>.</li><li>You want the object to be an abstraction over a collection of controlled resources, or a
summarization of other resources.</li></ul><h2 id="adding-custom-resources">Adding custom resources</h2><p>Kubernetes provides two ways to add custom resources to your cluster:</p><ul><li>CRDs are simple and can be created without any programming.</li><li><a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">API Aggregation</a>
requires programming, but allows more control over API behaviors like how data is stored and
conversion between API versions.</li></ul><p>Kubernetes provides these two options to meet the needs of different users, so that neither ease
of use nor flexibility is compromised.</p><p>Aggregated APIs are subordinate API servers that sit behind the primary API server, which acts as
a proxy. This arrangement is called <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">API Aggregation</a>(AA).
To users, the Kubernetes API appears extended.</p><p>CRDs allow users to create new types of resources without adding another API server. You do not
need to understand API Aggregation to use CRDs.</p><p>Regardless of how they are installed, the new resources are referred to as Custom Resources to
distinguish them from built-in Kubernetes resources (like pods).</p><div class="alert alert-info"><h4 class="alert-heading">Note:</h4><p>Avoid using a Custom Resource as data storage for application, end user, or monitoring data:
architecture designs that store application data within the Kubernetes API typically represent
a design that is too closely coupled.</p><p>Architecturally, <a href="https://www.cncf.io/about/faq/#what-is-cloud-native">cloud native</a> application architectures
favor loose coupling between components. If part of your workload requires a backing service for
its routine operation, run that backing service as a component or consume it as an external service.
This way, your workload does not rely on the Kubernetes API for its normal operation.</p></div><h2 id="customresourcedefinitions">CustomResourceDefinitions</h2><p>The <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">CustomResourceDefinition</a>
API resource allows you to define custom resources.
Defining a CRD object creates a new custom resource with a name and schema that you specify.
The Kubernetes API serves and handles the storage of your custom resource.
The name of the CRD object itself must be a valid
<a href="/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names">DNS subdomain name</a> derived from the defined resource name and its API group; see <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition">how to create a CRD</a> for more details.
Further, the name of an object whose kind/resource is defined by a CRD must also be a valid DNS subdomain name.</p><p>This frees you from writing your own API server to handle the custom resource,
but the generic nature of the implementation means you have less flexibility than with
<a href="#api-server-aggregation">API server aggregation</a>.</p><p>Refer to the <a href="https://github.com/kubernetes/sample-controller">custom controller example</a>
for an example of how to register a new custom resource, work with instances of your new resource type,
and use a controller to handle events.</p><h2 id="api-server-aggregation">API server aggregation</h2><p>Usually, each resource in the Kubernetes API requires code that handles REST requests and manages
persistent storage of objects. The main Kubernetes API server handles built-in resources like
<em>pods</em> and <em>services</em>, and can also generically handle custom resources through
<a href="#customresourcedefinitions">CRDs</a>.</p><p>The <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">aggregation layer</a>
allows you to provide specialized implementations for your custom resources by writing and
deploying your own API server.
The main API server delegates requests to your API server for the custom resources that you handle,
making them available to all of its clients.</p><h2 id="choosing-a-method-for-adding-custom-resources">Choosing a method for adding custom resources</h2><p>CRDs are easier to use. Aggregated APIs are more flexible. Choose the method that best meets your needs.</p><p>Typically, CRDs are a good fit if:</p><ul><li>You have a handful of fields</li><li>You are using the resource within your company, or as part of a small open-source project (as
opposed to a commercial product)</li></ul><h3 id="comparing-ease-of-use">Comparing ease of use</h3><p>CRDs are easier to create than Aggregated APIs.</p><table><thead><tr><th>CRDs</th><th>Aggregated API</th></tr></thead><tbody><tr><td>Do not require programming. Users can choose any language for a CRD controller.</td><td>Requires programming and building binary and image.</td></tr><tr><td>No additional service to run; CRDs are handled by API server.</td><td>An additional service to create and that could fail.</td></tr><tr><td>No ongoing support once the CRD is created. Any bug fixes are picked up as part of normal Kubernetes Master upgrades.</td><td>May need to periodically pickup bug fixes from upstream and rebuild and update the Aggregated API server.</td></tr><tr><td>No need to handle multiple versions of your API; for example, when you control the client for this resource, you can upgrade it in sync with the API.</td><td>You need to handle multiple versions of your API; for example, when developing an extension to share with the world.</td></tr></tbody></table><h3 id="advanced-features-and-flexibility">Advanced features and flexibility</h3><p>Aggregated APIs offer more advanced API features and customization of other features; for example, the storage layer.</p><table><thead><tr><th>Feature</th><th>Description</th><th>CRDs</th><th>Aggregated API</th></tr></thead><tbody><tr><td>Validation</td><td>Help users prevent errors and allow you to evolve your API independently of your clients. These features are most useful when there are many clients who can't all update at the same time.</td><td>Yes. Most validation can be specified in the CRD using <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation">OpenAPI v3.0 validation</a>. <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-ratcheting">CRDValidationRatcheting</a> feature gate allows failing validations specified using OpenAPI also can be ignored if the failing part of the resource was unchanged. Any other validations supported by addition of a <a href="/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook-alpha-in-1-8-beta-in-1-9">Validating Webhook</a>.</td><td>Yes, arbitrary validation checks</td></tr><tr><td>Defaulting</td><td>See above</td><td>Yes, either via <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting">OpenAPI v3.0 validation</a> <code>default</code> keyword (GA in 1.17), or via a <a href="/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">Mutating Webhook</a> (though this will not be run when reading from etcd for old objects).</td><td>Yes</td></tr><tr><td>Multi-versioning</td><td>Allows serving the same object through two API versions. Can help ease API changes like renaming fields. Less important if you control your client versions.</td><td><a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">Yes</a></td><td>Yes</td></tr><tr><td>Custom Storage</td><td>If you need storage with a different performance mode (for example, a time-series database instead of key-value store) or isolation for security (for example, encryption of sensitive information, etc.)</td><td>No</td><td>Yes</td></tr><tr><td>Custom Business Logic</td><td>Perform arbitrary checks or actions when creating, reading, updating or deleting an object</td><td>Yes, using <a href="/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">Webhooks</a>.</td><td>Yes</td></tr><tr><td>Scale Subresource</td><td>Allows systems like HorizontalPodAutoscaler and PodDisruptionBudget interact with your new resource</td><td><a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource">Yes</a></td><td>Yes</td></tr><tr><td>Status Subresource</td><td>Allows fine-grained access control where user writes the spec section and the controller writes the status section. Allows incrementing object Generation on custom resource data mutation (requires separate spec and status sections in the resource)</td><td><a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#status-subresource">Yes</a></td><td>Yes</td></tr><tr><td>Other Subresources</td><td>Add operations other than CRUD, such as "logs" or "exec".</td><td>No</td><td>Yes</td></tr><tr><td>strategic-merge-patch</td><td>The new endpoints support PATCH with <code>Content-Type: application/strategic-merge-patch+json</code>. Useful for updating objects that may be modified both locally, and by the server. For more information, see <a href="/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">"Update API Objects in Place Using kubectl patch"</a></td><td>No</td><td>Yes</td></tr><tr><td>Protocol Buffers</td><td>The new resource supports clients that want to use Protocol Buffers</td><td>No</td><td>Yes</td></tr><tr><td>OpenAPI Schema</td><td>Is there an OpenAPI (swagger) schema for the types that can be dynamically fetched from the server? Is the user protected from misspelling field names by ensuring only allowed fields are set? Are types enforced (in other words, don't put an <code>int</code> in a <code>string</code> field?)</td><td>Yes, based on the <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation">OpenAPI v3.0 validation</a> schema (GA in 1.16).</td><td>Yes</td></tr><tr><td>Instance Name</td><td>Does this extension mechanism impose any constraints on the names of objects whose kind/resource is defined this way?</td><td>Yes, such an object's name must be a valid DNS subdomain name.</td><td>No</td></tr></tbody></table><h3 id="common-features">Common Features</h3><p>When you create a custom resource, either via a CRD or an AA, you get many features for your API,
compared to implementing it outside the Kubernetes platform:</p><table><thead><tr><th>Feature</th><th>What it does</th></tr></thead><tbody><tr><td>CRUD</td><td>The new endpoints support CRUD basic operations via HTTP and <code>kubectl</code></td></tr><tr><td>Watch</td><td>The new endpoints support Kubernetes Watch operations via HTTP</td></tr><tr><td>Discovery</td><td>Clients like <code>kubectl</code> and dashboard automatically offer list, display, and field edit operations on your resources</td></tr><tr><td>json-patch</td><td>The new endpoints support PATCH with <code>Content-Type: application/json-patch+json</code></td></tr><tr><td>merge-patch</td><td>The new endpoints support PATCH with <code>Content-Type: application/merge-patch+json</code></td></tr><tr><td>HTTPS</td><td>The new endpoints uses HTTPS</td></tr><tr><td>Built-in Authentication</td><td>Access to the extension uses the core API server (aggregation layer) for authentication</td></tr><tr><td>Built-in Authorization</td><td>Access to the extension can reuse the authorization used by the core API server; for example, RBAC.</td></tr><tr><td>Finalizers</td><td>Block deletion of extension resources until external cleanup happens.</td></tr><tr><td>Admission Webhooks</td><td>Set default values and validate extension resources during any create/update/delete operation.</td></tr><tr><td>UI/CLI Display</td><td>Kubectl, dashboard can display extension resources.</td></tr><tr><td>Unset versus Empty</td><td>Clients can distinguish unset fields from zero-valued fields.</td></tr><tr><td>Client Libraries Generation</td><td>Kubernetes provides generic client libraries, as well as tools to generate type-specific client libraries.</td></tr><tr><td>Labels and annotations</td><td>Common metadata across objects that tools know how to edit for core and custom resources.</td></tr></tbody></table><h2 id="preparing-to-install-a-custom-resource">Preparing to install a custom resource</h2><p>There are several points to be aware of before adding a custom resource to your cluster.</p><h3 id="third-party-code-and-new-points-of-failure">Third party code and new points of failure</h3><p>While creating a CRD does not automatically add any new points of failure (for example, by causing
third party code to run on your API server), packages (for example, Charts) or other installation
bundles often include CRDs as well as a Deployment of third-party code that implements the
business logic for a new custom resource.</p><p>Installing an Aggregated API server always involves running a new Deployment.</p><h3 id="storage">Storage</h3><p>Custom resources consume storage space in the same way that ConfigMaps do. Creating too many
custom resources may overload your API server's storage space.</p><p>Aggregated API servers may use the same storage as the main API server, in which case the same
warning applies.</p><h3 id="authentication-authorization-and-auditing">Authentication, authorization, and auditing</h3><p>CRDs always use the same authentication, authorization, and audit logging as the built-in
resources of your API server.</p><p>If you use RBAC for authorization, most RBAC roles will not grant access to the new resources
(except the cluster-admin role or any role created with wildcard rules). You'll need to explicitly
grant access to the new resources. CRDs and Aggregated APIs often come bundled with new role
definitions for the types they add.</p><p>Aggregated API servers may or may not use the same authentication, authorization, and auditing as
the primary API server.</p><h2 id="accessing-a-custom-resource">Accessing a custom resource</h2><p>Kubernetes <a href="/docs/reference/using-api/client-libraries/">client libraries</a> can be used to access
custom resources. Not all client libraries support custom resources. The <em>Go</em> and <em>Python</em> client
libraries do.</p><p>When you add a custom resource, you can access it using:</p><ul><li><code>kubectl</code></li><li>The Kubernetes dynamic client.</li><li>A REST client that you write.</li><li>A client generated using <a href="https://github.com/kubernetes/code-generator">Kubernetes client generation tools</a>
(generating one is an advanced undertaking, but some projects may provide a client along with
the CRD or AA).</li></ul><h2 id="custom-resource-field-selectors">Custom resource field selectors</h2><p><a href="/docs/concepts/overview/working-with-objects/field-selectors/">Field Selectors</a>
let clients select custom resources based on the value of one or more resource
fields.</p><p>All custom resources support the <code>metadata.name</code> and <code>metadata.namespace</code> field
selectors.</p><p>Fields declared in a <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinition</a>
may also be used with field selectors when included in the <code>spec.versions[*].selectableFields</code> field of the
<a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinition</a>.</p><h3 id="crd-selectable-fields">Selectable fields for custom resources</h3><div class="feature-state-notice feature-stable" title="Feature Gate: CustomResourceFieldSelectors"><span class="feature-state-name">FEATURE STATE:</span>
<code>Kubernetes v1.32 [stable]</code> (enabled by default: true)</div><p>The <code>spec.versions[*].selectableFields</code> field of a <a class="glossary-tooltip" title="Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server." href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank">CustomResourceDefinition</a> may be used to
declare which other fields in a custom resource may be used in field selectors.</p><p>The following example adds the <code>.spec.color</code> and <code>.spec.size</code> fields as
selectable fields.</p><div class="highlight code-sample"><div class="copy-code-icon"><a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/customresourcedefinition/shirt-resource-definition.yaml"><code>customresourcedefinition/shirt-resource-definition.yaml</code>
</a><img src="/images/copycode.svg" class="icon-copycode" title="Copy customresourcedefinition/shirt-resource-definition.yaml to clipboard"></div><div class="includecode" id="customresourcedefinition-shirt-resource-definition-yaml"><div class="highlight"><pre tabindex="0"><code class="language-yaml"><span><span><span>apiVersion</span>:<span> </span>apiextensions.k8s.io/v1<span>
</span></span></span><span><span><span></span><span>kind</span>:<span> </span>CustomResourceDefinition<span>
</span></span></span><span><span><span></span><span>metadata</span>:<span>
</span></span></span><span><span><span>  </span><span>name</span>:<span> </span>shirts.stable.example.com<span>
</span></span></span><span><span><span></span><span>spec</span>:<span>
</span></span></span><span><span><span>  </span><span>group</span>:<span> </span>stable.example.com<span>
</span></span></span><span><span><span>  </span><span>scope</span>:<span> </span>Namespaced<span>
</span></span></span><span><span><span>  </span><span>names</span>:<span>
</span></span></span><span><span><span>    </span><span>plural</span>:<span> </span>shirts<span>
</span></span></span><span><span><span>    </span><span>singular</span>:<span> </span>shirt<span>
</span></span></span><span><span><span>    </span><span>kind</span>:<span> </span>Shirt<span>
</span></span></span><span><span><span>  </span><span>versions</span>:<span>
</span></span></span><span><span><span>  </span>- <span>name</span>:<span> </span>v1<span>
</span></span></span><span><span><span>    </span><span>served</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>storage</span>:<span> </span><span>true</span><span>
</span></span></span><span><span><span>    </span><span>schema</span>:<span>
</span></span></span><span><span><span>      </span><span>openAPIV3Schema</span>:<span>
</span></span></span><span><span><span>        </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>        </span><span>properties</span>:<span>
</span></span></span><span><span><span>          </span><span>spec</span>:<span>
</span></span></span><span><span><span>            </span><span>type</span>:<span> </span>object<span>
</span></span></span><span><span><span>            </span><span>properties</span>:<span>
</span></span></span><span><span><span>              </span><span>color</span>:<span>
</span></span></span><span><span><span>                </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>              </span><span>size</span>:<span>
</span></span></span><span><span><span>                </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>    </span><span>selectableFields</span>:<span>
</span></span></span><span><span><span>    </span>- <span>jsonPath</span>:<span> </span>.spec.color<span>
</span></span></span><span><span><span>    </span>- <span>jsonPath</span>:<span> </span>.spec.size<span>
</span></span></span><span><span><span>    </span><span>additionalPrinterColumns</span>:<span>
</span></span></span><span><span><span>    </span>- <span>jsonPath</span>:<span> </span>.spec.color<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>Color<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>string<span>
</span></span></span><span><span><span>    </span>- <span>jsonPath</span>:<span> </span>.spec.size<span>
</span></span></span><span><span><span>      </span><span>name</span>:<span> </span>Size<span>
</span></span></span><span><span><span>      </span><span>type</span>:<span> </span>string<span>
</span></span></span></code></pre></div></div></div><p>Field selectors can then be used to get only resources with a <code>color</code> of <code>blue</code>:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get shirts.stable.example.com --field-selector spec.color<span>=</span>blue
</span></span></code></pre></div><p>The output should be:</p><pre tabindex="0"><code>NAME       COLOR  SIZE
example1   blue   S
example2   blue   M
</code></pre><h2 id="what-s-next">What's next</h2><ul><li>Learn how to <a href="/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">Extend the Kubernetes API with the aggregation layer</a>.</li><li>Learn how to <a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">Extend the Kubernetes API with CustomResourceDefinition</a>.</li></ul></div></div><div><div class="td-content"><h1>Kubernetes API Aggregation Layer</h1><p>The aggregation layer allows Kubernetes to be extended with additional APIs, beyond what is
offered by the core Kubernetes APIs.
The additional APIs can either be ready-made solutions such as a
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics server</a>, or APIs that you develop yourself.</p><p>The aggregation layer is different from
<a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions</a>,
which are a way to make the <a class="glossary-tooltip" title="Control plane component that serves the Kubernetes API." href="/docs/concepts/architecture/#kube-apiserver" target="_blank">kube-apiserver</a>
recognise new kinds of object.</p><h2 id="aggregation-layer">Aggregation layer</h2><p>The aggregation layer runs in-process with the kube-apiserver. Until an extension resource is
registered, the aggregation layer will do nothing. To register an API, you add an <em>APIService</em>
object, which "claims" the URL path in the Kubernetes API. At that point, the aggregation layer
will proxy anything sent to that API path (e.g. <code>/apis/myextension.mycompany.io/v1/&#8230;</code>) to the
registered APIService.</p><p>The most common way to implement the APIService is to run an <em>extension API server</em> in Pod(s) that
run in your cluster. If you're using the extension API server to manage resources in your cluster,
the extension API server (also written as "extension-apiserver") is typically paired with one or
more <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a>. The apiserver-builder
library provides a skeleton for both extension API servers and the associated controller(s).</p><h3 id="response-latency">Response latency</h3><p>Extension API servers should have low latency networking to and from the kube-apiserver.
Discovery requests are required to round-trip from the kube-apiserver in five seconds or less.</p><p>If your extension API server cannot achieve that latency requirement, consider making changes that
let you meet it.</p><h2 id="what-s-next">What's next</h2><ul><li>To get the aggregator working in your environment, <a href="/docs/tasks/extend-kubernetes/configure-aggregation-layer/">configure the aggregation layer</a>.</li><li>Then, <a href="/docs/tasks/extend-kubernetes/setup-extension-api-server/">setup an extension api-server</a> to work with the aggregation layer.</li><li>Read about <a href="/docs/reference/kubernetes-api/cluster-resources/api-service-v1/">APIService</a> in the API reference</li><li>Learn about <a href="/docs/reference/using-api/declarative-validation/">Declarative Validation Concepts</a>, an internal mechanism for defining validation rules that in the future will help support validation for extension API server development.</li></ul><p>Alternatively: learn how to
<a href="/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">extend the Kubernetes API using Custom Resource Definitions</a>.</p></div></div><div><div class="td-content"><h1>Operator pattern</h1><p>Operators are software extensions to Kubernetes that make use of
<a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</a>
to manage applications and their components. Operators follow
Kubernetes principles, notably the <a href="/docs/concepts/architecture/controller/">control loop</a>.</p><h2 id="motivation">Motivation</h2><p>The <em>operator pattern</em> aims to capture the key aim of a human operator who
is managing a service or set of services. Human operators who look after
specific applications and services have deep knowledge of how the system
ought to behave, how to deploy it, and how to react if there are problems.</p><p>People who run workloads on Kubernetes often like to use automation to take
care of repeatable tasks. The operator pattern captures how you can write
code to automate a task beyond what Kubernetes itself provides.</p><h2 id="operators-in-kubernetes">Operators in Kubernetes</h2><p>Kubernetes is designed for automation. Out of the box, you get lots of
built-in automation from the core of Kubernetes. You can use Kubernetes
to automate deploying and running workloads, <em>and</em> you can automate how
Kubernetes does that.</p><p>Kubernetes' <a class="glossary-tooltip" title="A specialized controller used to manage a custom resource" href="/docs/concepts/extend-kubernetes/operator/" target="_blank">operator pattern</a>
concept lets you extend the cluster's behaviour without modifying the code of Kubernetes
itself by linking <a class="glossary-tooltip" title="A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state." href="/docs/concepts/architecture/controller/" target="_blank">controllers</a> to
one or more custom resources. Operators are clients of the Kubernetes API that act as
controllers for a <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource</a>.</p><h2 id="example">An example operator</h2><p>Some of the things that you can use an operator to automate include:</p><ul><li>deploying an application on demand</li><li>taking and restoring backups of that application's state</li><li>handling upgrades of the application code alongside related changes such
as database schemas or extra configuration settings</li><li>publishing a Service to applications that don't support Kubernetes APIs to
discover them</li><li>simulating failure in all or part of your cluster to test its resilience</li><li>choosing a leader for a distributed application without an internal
member election process</li></ul><p>What might an operator look like in more detail? Here's an example:</p><ol><li>A custom resource named SampleDB, that you can configure into the cluster.</li><li>A Deployment that makes sure a Pod is running that contains the
controller part of the operator.</li><li>A container image of the operator code.</li><li>Controller code that queries the control plane to find out what SampleDB
resources are configured.</li><li>The core of the operator is code to tell the API server how to make
reality match the configured resources.<ul><li>If you add a new SampleDB, the operator sets up PersistentVolumeClaims
to provide durable database storage, a StatefulSet to run SampleDB and
a Job to handle initial configuration.</li><li>If you delete it, the operator takes a snapshot, then makes sure that
the StatefulSet and Volumes are also removed.</li></ul></li><li>The operator also manages regular database backups. For each SampleDB
resource, the operator determines when to create a Pod that can connect
to the database and take backups. These Pods would rely on a ConfigMap
and / or a Secret that has database connection details and credentials.</li><li>Because the operator aims to provide robust automation for the resource
it manages, there would be additional supporting code. For this example,
code checks to see if the database is running an old version and, if so,
creates Job objects that upgrade it for you.</li></ol><h2 id="deploying-operators">Deploying operators</h2><p>The most common way to deploy an operator is to add the
Custom Resource Definition and its associated Controller to your cluster.
The Controller will normally run outside of the
<a class="glossary-tooltip" title="The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers." href="/docs/reference/glossary/?all=true#term-control-plane" target="_blank">control plane</a>,
much as you would run any containerized application.
For example, you can run the controller in your cluster as a Deployment.</p><h2 id="using-operators">Using an operator</h2><p>Once you have an operator deployed, you'd use it by adding, modifying or
deleting the kind of resource that the operator uses. Following the above
example, you would set up a Deployment for the operator itself, and then:</p><div class="highlight"><pre tabindex="0"><code class="language-shell"><span><span>kubectl get SampleDB                   <span># find configured databases</span>
</span></span><span><span>
</span></span><span><span>kubectl edit SampleDB/example-database <span># manually change some settings</span>
</span></span></code></pre></div><p>&#8230;and that's it! The operator will take care of applying the changes
as well as keeping the existing service in good shape.</p><h2 id="writing-operator">Writing your own operator</h2><p>If there isn't an operator in the ecosystem that implements the behavior you
want, you can code your own.</p><p>You also implement an operator (that is, a Controller) using any language / runtime
that can act as a <a href="/docs/reference/using-api/client-libraries/">client for the Kubernetes API</a>.</p><p>Following are a few libraries and tools you can use to write your own cloud native
operator.</p><div class="alert alert-secondary callout third-party-content"><strong>Note:</strong>&#8200;This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div><ul><li><a href="https://juju.is/">Charmed Operator Framework</a></li><li><a href="https://github.com/operator-framework/java-operator-sdk">Java Operator SDK</a></li><li><a href="https://github.com/nolar/kopf">Kopf</a> (Kubernetes Operator Pythonic Framework)</li><li><a href="https://kube.rs/">kube-rs</a> (Rust)</li><li><a href="https://book.kubebuilder.io/">kubebuilder</a></li><li><a href="https://dotnet.github.io/dotnet-operator-sdk/">KubeOps</a> (.NET operator SDK)</li><li><a href="https://docs.ansi.services/mast/user_guide/operator/">Mast</a></li><li><a href="https://metacontroller.github.io/metacontroller/intro.html">Metacontroller</a> along with WebHooks that
you implement yourself</li><li><a href="https://operatorframework.io">Operator Framework</a></li><li><a href="https://github.com/flant/shell-operator">shell-operator</a></li></ul><h2 id="what-s-next">What's next</h2><ul><li>Read the <a class="glossary-tooltip" title="Cloud Native Computing Foundation" href="https://cncf.io/" target="_blank">CNCF</a>
<a href="https://github.com/cncf/tag-app-delivery/blob/163962c4b1cd70d085107fc579e3e04c2e14d59c/operator-wg/whitepaper/Operator-WhitePaper_v1-0.md">Operator White Paper</a>.</li><li>Learn more about <a href="/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resources</a></li><li>Find ready-made operators on <a href="https://operatorhub.io/">OperatorHub.io</a> to suit your use case</li><li><a href="https://operatorhub.io/">Publish</a> your operator for other people to use</li><li>Read <a href="https://web.archive.org/web/20170129131616/https://coreos.com/blog/introducing-operators.html">CoreOS' original article</a>
that introduced the operator pattern (this is an archived version of the original article).</li><li>Read an <a href="https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-building-kubernetes-operators-and-stateful-apps">article</a>
from Google Cloud about best practices for building operators</li></ul></div></div>